<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    
    <title>The Derivative of Scalar Functions of Matrices - MATH-CS COMPASS</title>
    
    
    <script>
    window.MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']]
      }
    };
    </script>
    <script id="MathJax-script" defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="preload" href="https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Main-Regular.woff" as="font" type="font/woff2" crossorigin>
    <link rel="preload" href="https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Math-Italic.woff" as="font" type="font/woff2" crossorigin>
    
    
    

    <link rel="stylesheet" href="../../../css/styles.css?v=1.0.2">
  
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css">

    <link rel="dns-prefetch" href="https://cdn.jsdelivr.net">
    <link rel="dns-prefetch" href="https://cdnjs.cloudflare.com">
    
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Learn about derivatives of Frobenius norm and determinants">
    <meta name="google-site-verification" content="IieEujypYW38caEVe8ymjczouNl56yxAy27iPztcfRA" />
    

    <link rel="manifest" href="/manifest.json">

    <!-- Theme color for Chrome, Android, and other browsers -->
    <meta name="theme-color" content="#007bff">
  
    <!-- iOS specific tags -->
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <meta name="apple-mobile-web-app-title" content="MATH-CS COMPASS">
    <meta name="mobile-web-app-capable" content="yes">

    <!-- Icon references -->
    <link rel="icon" type="image/png" sizes="1280x1280" href="/images/maskable_icon_x1280.png?v=1.0.2">
    <link rel="icon" type="image/png" sizes="512x512" href="/images//maskable_icon_x512.png?v=1.0.2">
    
    <!-- Apple Touch Icons -->
    <link rel="apple-touch-icon" href="/images/maskable_icon_x1280.png?v=1.0.2">
    <link rel="apple-touch-icon" href="/images/maskable_icon_x512.png?v=1.0.2">

    <!-- For Windows -->
    <meta name="msapplication-TileImage" content="/images/test1.png?v=1.0.2">
    <meta name="msapplication-TileColor" content="#007bff">

    <!-- Schema.org Structured Data for AI Optimization -->
    

    <!-- Article Schema for content pages -->
    
    <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "Article",
      "headline": "The Derivative of Scalar Functions of Matrices",
      "description": "Learn about derivatives of Frobenius norm and determinants",
      "datePublished": "2024-01-01",
      "dateModified": "2025-01-01",
      "author": {
        "@type": "Person",
        "name": "Yusuke Yokota",
        "jobTitle": "Mathematics & Computer Science Educator",
        "knowsAbout": [
          "Linear Algebra",
          "Machine Learning",
          "Computer Science",
          "Mathematics"
        ]
      },
      "publisher": {
        "@type": "Organization",
        "name": "MATH-CS COMPASS",
        "url": "https://yusukey01.github.io",
        "logo": {
          "@type": "ImageObject",
          "url": "https://yusukey01.github.io/images/maskable_icon_x512.png"
        }
      },
      "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "https://yusukey01.github.io/Mathematics/Calculus/det.html"
      },
      "about": [
        
        { "@type": "Thing", "name": "Calculus" },
        { "@type": "Thing", "name": "Optimization" }
        
      ],
      "educationalLevel": "university",
      "learningResourceType": "tutorial",
      "educationalUse": "instruction"
    }
    </script>
    
</head>

<body>
    <header>
    <nav class="navbar">
        <!-- Logo as Home Button -->
        <div class="logo-home">
            <a href="../../../index.html" 
               class="logo-link " 
               title="Home - Math-CS Compass">
                <img src="../../images/icon_x512.png" 
                     alt="Math-CS Compass Logo" 
                     class="nav-logo">
            </a>
        </div>

        <div class="search-container">
            <input type="text" id="search-input" placeholder="Search keywords...">
            <button id="search-button" aria-label="Search" title="Search website"><i class="fas fa-search"></i></button>
        </div>
        
        <div class="menu-toggle">
            <i class="fas fa-bars"></i>
        </div>
        
        <ul class="nav-links">
            <li><a href="../Linear_algebra/linear_algebra.html" >I - Linear Algebra to Algebraic Foundations</a></li>
            
            <li><a href="../Calculus/calculus.html" class="active">II - Calculus to Optimization & Analysis</a></li>
            
            <li><a href="../Probability/probability.html" >III - Probability & Statistics</a></li>
            
            <li><a href="../Discrete/discrete_math.html" >IV - Discrete Mathematics & Algorithms</a></li>
            
            <li><a href="../Machine_learning/ml.html" >V - Machine Learning</a></li>
        </ul>
    </nav>
</header>
    
    <!DOCTYPE html>
<html>
    <body>
        <!-- LearningResource Schema for Derivative of Scalar Functions of Matrices -->
        <script type="application/ld+json">
        {
        "@context": "https://schema.org",
        "@type": "LearningResource",
        "name": "The Derivative of Scalar Functions of Matrices",
        "description": "Learn about derivatives of Frobenius norm and determinants",
        "learningResourceType": "lesson",
        "educationalUse": "instruction",
        "educationalLevel": "university",
        "interactivityType": "active",
        "author": {
            "@type": "Person",
            "name": "Yusuke Yokota",
            "jobTitle": "Mathematics & Computer Science Educator",
            "knowsAbout": [
            "Matrix Calculus",
            "Frobenius Norm",
            "Determinant Derivatives",
            "Matrix Derivatives",
            "Linear Algebra",
            "Mathematical Analysis"
            ]
        },
        "publisher": {
            "@type": "Organization",
            "name": "MATH-CS COMPASS",
            "url": "https://yusukey01.github.io"
        },
        "about": [
            { "@type": "Thing", "name": "Matrix Derivatives" },
            { "@type": "Thing", "name": "Frobenius Norm" },
            { "@type": "Thing", "name": "Determinant" },
            { "@type": "Thing", "name": "Matrix Calculus" },
            { "@type": "Thing", "name": "Jacobian Matrix" },
            { "@type": "Thing", "name": "Chain Rule" },
            { "@type": "Thing", "name": "Differential Notation" },
            { "@type": "Thing", "name": "Trace Operator" },
            { "@type": "Thing", "name": "Frobenius Inner Product" },
            { "@type": "Thing", "name": "Cofactor Matrix" },
            { "@type": "Thing", "name": "Adjugate Matrix" },
            { "@type": "Thing", "name": "Characteristic Polynomial" },
            { "@type": "Thing", "name": "Automatic Differentiation" }
        ],
        "teaches": [
            "Computing derivatives of Frobenius norm",
            "Understanding differential notation for matrices",
            "Applying trace operator properties",
            "Computing derivatives of determinants",
            "Using cofactor expansion for derivatives",
            "Relationship between determinants and characteristic polynomials",
            "Comparison of analytical vs automatic differentiation methods"
        ],
        "isPartOf": {
            "@type": "Course",
            "name": "Calculus to Optimization & Analysis",
            "description": "Explore optimization techniques and mathematical analysis with applications",
            "provider": {
            "@type": "Organization",
            "name": "MATH-CS COMPASS",
            "url": "https://yusukey01.github.io"
            },
            "instructor": {
            "@type": "Person",
            "name": "Yusuke Yokota",
            "jobTitle": "Mathematics & Computer Science Educator"
            },
            "courseCode": "II",
            "hasCourseInstance": {
            "@type": "CourseInstance",
            "courseMode": "online",
            "courseWorkload": "PT3H",
            "instructor": {
                "@type": "Person",
                "name": "Yusuke Yokota"
            }
            },
            "offers": {
            "@type": "Offer",
            "price": "0",
            "priceCurrency": "USD",
            "availability": "https://schema.org/InStock",
            "category": "free"
            }
        }
        }
        </script>
        <div class="hero-section">
            <h1 class="webpage-name">The Derivative of Scalar Functions of Matrices
            </h1>
        </div>

        
  <div class="quick-jump-container">
    <button type="button" id="quick-jump-toggle" class="quick-jump-toggle">
      <i class="fas fa-list"></i> <strong>Section II Navigation</strong>
    </button>
    <div id="quick-jump-menu" class="quick-jump-menu">
      <h3>Calculus to Optimization & Analysis</h3>
      <div class="quick-jump-links">
        
        
        <a href="calculus.html">← Back to Section II Overview</a>
        <hr style="margin: 8px 0; border: 1px solid #ddd;">
        
        
        <a href="linear_approximation.html" >Part 1: The Derivative of f:ℝⁿ → ℝ</a>
        <a href="jacobian.html" >Part 2: The Derivative of f:ℝⁿ → ℝⁿ</a>
        <a href="matrix_cal.html" >Part 3: The Derivative of f:ℝⁿˣⁿ → ℝⁿˣⁿ</a>
        <a href="numerical_example1.html" >Part 4: Intro to Numerical Computation</a>
        <a href="det.html" class="current-page">Part 5: Scalar Functions of Matrices</a>
        <a href="mvt.html" >Part 6: The Mean Value Theorem</a>
        <a href="gradient.html" >Part 7: Gradient Descent</a>
        <a href="newton.html" >Part 8: Newton's Method</a>
        <a href="constrained_opt.html" >Part 9: Constrained Optimization</a>
        <a href="riemann.html" >Part 10: Riemann Integration</a>
        <a href="measure.html" >Part 11: Measure Theory</a>
        <a href="lebesgue.html" >Part 12: Lebesgue Integration</a>
        <a href="duality.html" >Part 13: Duality in Optimization</a>
        <a href="fourier_series.html" >Part 14: Fourier Series</a>
        <a href="fourier_transform.html" >Part 15: Fourier Transform</a>
      </div>
    </div>
</div>


        <div class="topic-nav">
            <a href="#frob">Derivative of the Frobenius norm</a>
            <a href="#det">Derivative of the Determinant</a>
        </div>  
        <div class="container">     
            <section id="frob" class="section-content">
            <h2>Derivative of the Frobenius norm</h2>
            <p>
                Derivatives implicitly rely on <strong>norms</strong> to measure the magnitude of changes in both the input \(dx\) 
                and the output \(df\) ensuring a consistent comparison of their scales.
                <br><br>
                The <a href="../Linear_algebra/trace.html"><strong>Frobenius norm</strong></a> of a matrix \(X \in \mathbb{R}^{m \times n}\) is defined as:
                \[
                f(X) = \| X \|_F = \sqrt{\text{tr }(X^TX)} 
                \] 
                <br>
                Now, taking the differential \(df\):
                <br><br>
                First, by the chain rule:
                \[
                df = \frac{1}{2\sqrt{\text{tr }(X^TX)}}d[\text{tr }(X^TX)]
                \]
                Note: for any matrix \(A\),
                \[
                \begin{align*}
                d(\text{tr }(A)) &= \text{tr }(A + dA) - \text{tr }(A) \\\\\
                                &= \text{tr }(A) + \text{tr }(dA) - \text{tr }(A) \\\\
                                &= \text{tr }(dA)
                \end{align*}
                \]
                Thus:
                \[
                df = \frac{1}{2\sqrt{\text{tr }(X^TX)}}\text{tr }[d(X^TX)]
                \]
                By the product rule:
                \[
                \begin{align*}
                df &= \frac{1}{2\sqrt{\text{tr }(X^TX)}}\text{tr }[dX^TX + X^TdX]\\\\
                &= \frac{1}{2\sqrt{\text{tr }(X^TX)}}\text{tr }(dX^TX) + \text{tr }(X^TdX)
                \end{align*}
                \]
                Since 
                \[
                \text{tr }(dX^TX)  = \text{tr }((dX^TX)^T) = \text{tr }(X^TdX),
                \]
                We obtain
                \[ 
                \begin{align*}
                df &= \frac{1}{2\sqrt{\text{tr }(X^TX)}}2\text{tr }(X^TdX)\\\\
                    &= \frac{1}{\sqrt{\text{tr }(X^TX)}}\text{tr }(X^TdX)
                \end{align*}
                \]
                Here, \(\text{tr }(X^TdX)\) represents the <strong>Frobenius inner product</strong> of \(X\) and \(dX\). Then:
                \[
                df = \left\langle \frac{X}{\sqrt{\text{tr }(X^TX)}}, dX \right\rangle_F  \tag{1}
                \]
                Therefore, 
                \[
                \nabla f = \frac{X}{ \| X \|_F}.
                \]
                <br>
                Note:The expression in (1) is equivalent to
                \[
                df = \text{tr }((\nabla f)^TdX) \tag{2}
                \]
                <br>
                The trace operator satisfies linearity and the cyclic property, making it a convenient way to express derivatives 
                in terms of gradients.
                <br>
                For example, consider \(f(A) = x^TAy\) where \(A\) is 
                a \(m \times n\) matrix, \(x \in \mathbb{R}^m\), and  \(y \in \mathbb{R}^n\).
                <br>
                By the product rule,
                \[
                df = x^TdAy
                \]
                Since \(df\) is a scalar,  taking the trace does not change its value:
                \[
                df = \text{tr }(x^TdAy)
                \]
                By the cyclic property of the trace:
                \[
                df = \text{tr }(yx^TdA)
                \]
                Therefore, comparing this with \(df = \text{tr }((\nabla f)^TdA)\), 
                \[
                \nabla f = (yx^T)^T = xy^T
                \]
            </p>
            </section>
            <section id="det" class="section-content">
                <h2>Derivative of the Determinant</h2>
                <p>
                The derivative of the  <a href="../Linear_algebra/determinants.html">determinant</a> of a square matrix \(A \in \mathbb{R}^{n \times n}\) 
                can be expressed using several equivalent forms.  
                <br>
                Recall: 
                \[
                \text{adj }(A) = \text{cofactor }(A)^T = (\det A)A^{-1}.
                \]
                This implies:
                \[
                \text{cofactor }(A) = \text{adj }(A)^T = (\det A)(A^{-1})^T.
                \]
                <br><br>
                By the cofactor expansion of the determinant based on \(i\)-th row of \(A\):
                \[
                \det (A) = A_{i1}C_{i1} + A_{i2}C_{i2} +  \cdots + A_{in}C_{in}
                \]
                Thus, \(\frac{\partial \det A}{\partial A_{ij}} = C_{ij} \) and then:
                \[
                \nabla (\det A) = \text{cofactor } (A)
                \]
                Equivalently, using the expression (2): 
                \[
                \begin{align*}
                d (\det A) &= \text{tr }(\text{cofactor }(A)^T dA) \\\\
                        &= \text{tr }(\text{adj }(A) dA) \\\\
                        &= \text{tr }((\det A)A^{-1} dA)  \tag{3}
                \end{align*}
                \]
                Therefore,
                \[
                \begin{align*}
                \nabla (\det A) &= \text{cofactor } (A) \\\\
                                &= \text{adj }(A)^T  \\\\
                                &= (\det A)(A^{-1})^T
                \end{align*}
                \]
                
                <br>
                Since \(\det A\) is a scalar, the expression (3) can be written as:
                \[
                d(\det A) = (\det A)\text{tr }(A^{-1} dA) \tag{4}
                \]
                Consider the scalar function \(p(x) = \det(xI - A)\), which is the characteristic polynomial of \(A\). 
                In practice, when approximating eigenvalues using numerical methods, we often need to compute the derivative 
                of \(p(x)\) at different values of \(x\).
                <br>
                Applying the expression (4), we have:
                \[
                \begin{align*}
                &d(\det (xI-A))  \\\\
                &= (\det (xI-A)) \text{tr } ((xI-A)^{-1} d(xI -A)).
                \end{align*}
                \]
                Since \(A\) is constant, \(d(xI -A) = dxI\), and also \(dx\) is a scalar. Thus, 
                \[
                \begin{align*}
                &d(\det (xI-A)) \\\\
                &= (\det (xI-A)) \text{tr } ((xI-A)^{-1})dx
                \end{align*}
                \]
                <br>
                Note: While the <strong>analytical approach</strong> provides a useful formula for the differential of many functions, in practice,
                <a href="../Machine_learning/autodiff.html"><strong>reverse mode automatic differentiation</strong></a> offers a more stable and efficient way to compute 
                the gradient of functions. Auto-diff allows us to compute the derivative with respect to matrix parameters directly 
                without explicitly computing like \(A^{-1}\), which can introduce numerical instability in certain cases.
                <br><br>
                Here's an example using auto-diff in PyTorch vs using the analytical formula to compute the derivative of \(p(x)\):
                <pre class="python-code">
                import torch

                # Random square matrix
                def generate_matrix(n):
                    return torch.randn(n, n, dtype=torch.float64, requires_grad=False)

                # Characteristic polynomial p(x) = det(xI - A)
                def p(x, A):
                    return torch.det(x * torch.eye(A.shape[0], dtype=A.dtype, device=A.device) - A)

                # Derivative of p(x) using auto-differentiation
                def dp_torch(x, A):
                    x_tensor = torch.tensor([x], requires_grad=True, dtype=A.dtype, device=A.device)
                    grad = torch.autograd.grad(p(x_tensor, A), x_tensor)[0]
                    return grad.item()

                # Analytical formula d(p(x)) = (det (xI-A))*tr((xI-A)^-1)dx 
                def dp(x, A):
                    return (
                        p(x, A).item() *
                        torch.trace(
                            torch.inverse(x * torch.eye(A.shape[0], dtype=A.dtype, device=A.device) - A)
                        ).item()
                    )
                </pre>
                </p>
            </section>
        </div>
        <script src="/js/main.js"></script>
    </body>
</html>
    
    <footer>
    <div class="footer-content">
        <div class="footer-about">
            <h3>About MATH-CS COMPASS</h3>
            <p>Bridging the gap between pure mathematics and computer science applications.</p>
            <div class="contact-section">
                <strong><a href="https://docs.google.com/forms/d/e/1FAIpQLSf3_RNkegREvfCEIQznn6SCaf-iB5c-Doxn7Ymuu78Uf-PBjg/viewform" target="_blank">
                    Submit Your Questions & Feedback <i class="fas fa-external-link-alt"></i>
                </a></strong>
            </div>
        </div>
        <div class="footer-links">
            <h3>Quick Links</h3>
            <ul>
                
                <li><a href="../../../index.html">Home</a></li>
                <li>
                <a href="../Linear_algebra/linear_algebra.html">
                    <span class="nav-number">I</span>
                    <span>Linear Algebra to Algebraic Foundations</span>
                </a>
                </li>
                <li>
                <a href="../Calculus/calculus.html">
                    <span class="nav-number">II</span>
                    <span>Calculus to Optimization & Analysis</span>
                </a>
                </li>
                <li>
                <a href="../Probability/probability.html">
                    <span class="nav-number">III</span>
                    <span>Probability & Statistics</span>
                </a>
                </li>
                <li>
                <a href="../Discrete/discrete_math.html">
                    <span class="nav-number">IV</span>
                    <span>Discrete Mathematics & Algorithms</span>
                </a>
                </li>
                <li>
                <a href="../Machine_learning/ml.html">
                    <span class="nav-number">V</span>
                    <span>Machine Learning</span>
                </a>
                </li>
            </ul>
        </div>
        <div class="footer-social">
            <h3>Connect With Me</h3>
            <p>
                <a href="https://x.com/MathCSCompass?t=Zi8nyfzszeFw8QizbJwnNg&s=09" target="_blank">
                    <i class="fab fa-x-twitter"></i> X (Twitter)
                </a>
            </p>
            <p>
                <a href="https://www.facebook.com/share/15uPaBWgQN/" target="_blank">
                    <i class="fab fa-facebook"></i> Facebook
                </a>
            </p>
      
            <h3>MATH-CS COMPASS App</h3>
            <div class="app-download-footer">
                <a href="https://play.google.com/store/apps/details?id=io.github.yusukey01.twa" target="_blank" rel="noopener">
                    <img src="../../../images/google-play-badge.png" 
                        alt="Get it on Google Play" 
                        class="google-play-badge">
                </a>
            </div>
            
        </div>

    </div>
    <div class="footer-bottom">
        <p>&copy; 2024-2025 MATH-CS COMPASS. All rights reserved.</p>
    </div>
</footer>
    <script src="../../../js/main.js?v=1.0.2"></script>
    <script src="../../../js/search.js?v=1.0.2"></script>
    

    <button id="go-to-top-btn" class="go-to-top-btn" title="Go to top">
        <i class="fas fa-arrow-up"></i>
    </button>
    <script>
        // Register the service worker
        if ('serviceWorker' in navigator) {
          window.addEventListener('load', () => {
            navigator.serviceWorker.register('/js/service-worker.js')
              .then(registration => {
                console.log('ServiceWorker registration successful with scope: ', registration.scope);
              })
              .catch(error => {
                console.log('ServiceWorker registration failed: ', error);
              });
          });
        }
      </script>
</body>
</html>