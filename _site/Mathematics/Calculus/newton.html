<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    
    <title>Newton's Method - MATH-CS COMPASS</title>
    
    
    <script>
    window.MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']]
      }
    };
    </script>
    <script id="MathJax-script" defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="preload" href="https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Main-Regular.woff" as="font" type="font/woff2" crossorigin>
    <link rel="preload" href="https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Math-Italic.woff" as="font" type="font/woff2" crossorigin>
    
    
    
    <script src="https://cdn.jsdelivr.net/pyodide/v0.23.3/full/pyodide.js" defer></script>
    

    <link rel="stylesheet" href="../../../css/styles.css?v=1.0.2">
  
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css">

    <link rel="dns-prefetch" href="https://cdn.jsdelivr.net">
    <link rel="dns-prefetch" href="https://cdnjs.cloudflare.com">
    
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Learn about line search, Newton's method, and BFGS method.">
    <meta name="google-site-verification" content="IieEujypYW38caEVe8ymjczouNl56yxAy27iPztcfRA" />
    

    <link rel="manifest" href="/manifest.json">

    <!-- Theme color for Chrome, Android, and other browsers -->
    <meta name="theme-color" content="#007bff">
  
    <!-- iOS specific tags -->
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <meta name="apple-mobile-web-app-title" content="MATH-CS COMPASS">
    <meta name="mobile-web-app-capable" content="yes">

    <!-- Icon references -->
    <link rel="icon" type="image/png" sizes="1280x1280" href="/images/maskable_icon_x1280.png?v=1.0.2">
    <link rel="icon" type="image/png" sizes="512x512" href="/images//maskable_icon_x512.png?v=1.0.2">
    
    <!-- Apple Touch Icons -->
    <link rel="apple-touch-icon" href="/images/maskable_icon_x1280.png?v=1.0.2">
    <link rel="apple-touch-icon" href="/images/maskable_icon_x512.png?v=1.0.2">

    <!-- For Windows -->
    <meta name="msapplication-TileImage" content="/images/test1.png?v=1.0.2">
    <meta name="msapplication-TileColor" content="#007bff">

    <!-- Schema.org Structured Data for AI Optimization -->
    

    <!-- Article Schema for content pages -->
    
    <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "Article",
      "headline": "Newton's Method",
      "description": "Learn about line search, Newton's method, and BFGS method.",
      "datePublished": "2024-01-01",
      "dateModified": "2025-01-01",
      "author": {
        "@type": "Person",
        "name": "Yusuke Yokota",
        "jobTitle": "Mathematics & Computer Science Educator",
        "knowsAbout": [
          "Linear Algebra",
          "Machine Learning",
          "Computer Science",
          "Mathematics"
        ]
      },
      "publisher": {
        "@type": "Organization",
        "name": "MATH-CS COMPASS",
        "url": "https://yusukey01.github.io",
        "logo": {
          "@type": "ImageObject",
          "url": "https://yusukey01.github.io/images/maskable_icon_x512.png"
        }
      },
      "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "https://yusukey01.github.io/Mathematics/Calculus/newton.html"
      },
      "about": [
        
        { "@type": "Thing", "name": "Calculus" },
        { "@type": "Thing", "name": "Optimization" }
        
      ],
      "educationalLevel": "university",
      "learningResourceType": "tutorial",
      "educationalUse": "instruction"
    }
    </script>
    
</head>

<body>
    <header>
    <nav class="navbar">
        <!-- Logo as Home Button -->
        <div class="logo-home">
            <a href="../../../index.html" 
               class="logo-link " 
               title="Home - Math-CS Compass">
                <img src="../../images/icon_x512.png" 
                     alt="Math-CS Compass Logo" 
                     class="nav-logo">
            </a>
        </div>

        <div class="search-container">
            <input type="text" id="search-input" placeholder="Search keywords...">
            <button id="search-button" aria-label="Search" title="Search website"><i class="fas fa-search"></i></button>
        </div>
        
        <div class="menu-toggle">
            <i class="fas fa-bars"></i>
        </div>
        
        <ul class="nav-links">
            <li><a href="../Linear_algebra/linear_algebra.html" >I - Linear Algebra to Algebraic Foundations</a></li>
            
            <li><a href="../Calculus/calculus.html" class="active">II - Calculus to Optimization & Analysis</a></li>
            
            <li><a href="../Probability/probability.html" >III - Probability & Statistics</a></li>
            
            <li><a href="../Discrete/discrete_math.html" >IV - Discrete Mathematics & Algorithms</a></li>
            
            <li><a href="../Machine_learning/ml.html" >V - Machine Learning</a></li>
        </ul>
    </nav>
</header>
    
    <!DOCTYPE html>
<html>
    <body> 
        <!-- LearningResource Schema for Newton's Method -->
        <script type="application/ld+json">
        {
        "@context": "https://schema.org",
        "@type": "LearningResource",
        "name": "Newton's Method",
        "description": "Learn about line search, Newton's method, and BFGS method",
        "learningResourceType": "lesson",
        "educationalUse": "instruction",
        "educationalLevel": "university",
        "interactivityType": "active",
        "author": {
            "@type": "Person",
            "name": "Yusuke Yokota",
            "jobTitle": "Mathematics & Computer Science Educator",
            "knowsAbout": [
            "Newton's Method",
            "Optimization Theory",
            "Second-order Methods",
            "BFGS Method",
            "Line Search",
            "Numerical Optimization"
            ]
        },
        "publisher": {
            "@type": "Organization",
            "name": "MATH-CS COMPASS",
            "url": "https://yusukey01.github.io"
        },
        "about": [
            { "@type": "Thing", "name": "Newton's Method" },
            { "@type": "Thing", "name": "Second-order Optimization" },
            { "@type": "Thing", "name": "Line Search" },
            { "@type": "Thing", "name": "BFGS Method" },
            { "@type": "Thing", "name": "L-BFGS Method" },
            { "@type": "Thing", "name": "Quasi-Newton Methods" },
            { "@type": "Thing", "name": "Hessian Matrix" },
            { "@type": "Thing", "name": "Inverse Hessian" },
            { "@type": "Thing", "name": "Armijo Condition" },
            { "@type": "Thing", "name": "Wolfe Conditions" },
            { "@type": "Thing", "name": "Curvature Condition" },
            { "@type": "Thing", "name": "Secant Condition" },
            { "@type": "Thing", "name": "Sherman-Morrison Formula" },
            { "@type": "Thing", "name": "Rank-1 Update" },
            { "@type": "Thing", "name": "Rank-2 Update" },
            { "@type": "Thing", "name": "Rosenbrock Function" },
            { "@type": "Thing", "name": "Limited Memory BFGS" },
            { "@type": "Thing", "name": "Step Size Selection" }
        ],
        "teaches": [
            "Understanding line search methods and step size selection",
            "Implementing Newton's method for optimization",
            "Understanding the role of Hessian matrices in optimization",
            "Learning BFGS quasi-Newton method",
            "Implementing L-BFGS for large-scale problems",
            "Understanding Wolfe conditions for line search",
            "Working with rank-1 and rank-2 matrix updates",
            "Testing optimization algorithms with benchmark functions"
        ],
        "isPartOf": {
            "@type": "Course",
            "name": "Calculus to Optimization & Analysis",
            "description": "Explore optimization techniques and mathematical analysis with applications",
            "provider": {
            "@type": "Organization",
            "name": "MATH-CS COMPASS",
            "url": "https://yusukey01.github.io"
            },
            "instructor": {
            "@type": "Person",
            "name": "Yusuke Yokota",
            "jobTitle": "Mathematics & Computer Science Educator"
            },
            "courseCode": "II",
            "hasCourseInstance": {
            "@type": "CourseInstance",
            "courseMode": "online",
            "courseWorkload": "PT6H",
            "instructor": {
                "@type": "Person",
                "name": "Yusuke Yokota"
            }
            },
            "offers": {
            "@type": "Offer",
            "price": "0",
            "priceCurrency": "USD",
            "availability": "https://schema.org/InStock",
            "category": "free"
            }
        }
        }
        </script>

        <!-- WebApplication Schema for Interactive Code Demo -->
        <script type="application/ld+json">
        {
        "@context": "https://schema.org",
        "@type": "WebApplication",
        "name": "L-BFGS Optimization Implementation",
        "description": "Interactive Python implementation of Limited-memory BFGS optimization algorithm with Rosenbrock function testing",
        "applicationCategory": "EducationalApplication",
        "operatingSystem": "Web Browser",
        "url": "https://yusukey01.github.io/Mathematics/Calculus/newton.html",
        "author": {
            "@type": "Person",
            "name": "Yusuke Yokota"
        },
        "applicationSubCategory": "Optimization Algorithm Implementation",
        "featureList": [
            "Complete L-BFGS implementation in Python",
            "Line search with Wolfe conditions",
            "Rosenbrock function optimization testing",
            "Interactive code execution and testing",
            "Gradient verification with finite differences",
            "Convergence analysis and performance monitoring"
        ],
        "isAccessibleForFree": true,
        "educationalUse": "instruction",
        "educationalLevel": "university",
        "about": [
            { "@type": "Thing", "name": "L-BFGS Implementation Demo" },
            { "@type": "Thing", "name": "Optimization Algorithm Code" },
            { "@type": "Thing", "name": "Newton's Method Programming" }
        ]
        }
        </script> 
        <div class="hero-section">
            <h1 class="webpage-name">Newton's Method
                <span class="subheading">Second-order Optimization Techniques</span>
            </h1>
        </div>

        
  <div class="quick-jump-container">
    <button type="button" id="quick-jump-toggle" class="quick-jump-toggle">
      <i class="fas fa-list"></i> <strong>Section II Navigation</strong>
    </button>
    <div id="quick-jump-menu" class="quick-jump-menu">
      <h3>Calculus to Optimization & Analysis</h3>
      <div class="quick-jump-links">
        
        
        <a href="calculus.html">← Back to Section II Overview</a>
        <hr style="margin: 8px 0; border: 1px solid #ddd;">
        
        
        <a href="linear_approximation.html" >Part 1: The Derivative of f:ℝⁿ → ℝ</a>
        <a href="jacobian.html" >Part 2: The Derivative of f:ℝⁿ → ℝⁿ</a>
        <a href="matrix_cal.html" >Part 3: The Derivative of f:ℝⁿˣⁿ → ℝⁿˣⁿ</a>
        <a href="numerical_example1.html" >Part 4: Intro to Numerical Computation</a>
        <a href="det.html" >Part 5: Scalar Functions of Matrices</a>
        <a href="mvt.html" >Part 6: The Mean Value Theorem</a>
        <a href="gradient.html" >Part 7: Gradient Descent</a>
        <a href="newton.html" class="current-page">Part 8: Newton's Method</a>
        <a href="constrained_opt.html" >Part 9: Constrained Optimization</a>
        <a href="riemann.html" >Part 10: Riemann Integration</a>
        <a href="measure.html" >Part 11: Measure Theory</a>
        <a href="lebesgue.html" >Part 12: Lebesgue Integration</a>
        <a href="duality.html" >Part 13: Duality in Optimization</a>
        <a href="fourier_series.html" >Part 14: Fourier Series</a>
        <a href="fourier_transform.html" >Part 15: Fourier Transform</a>
      </div>
    </div>
</div>

        
        <div class="topic-nav">
            <a href="#LS">Line Search</a>
            <a href="#NM">Newton's Method</a>
            <a href="#BFGS">BFGS Method</a>
        </div>

        <div class="container">
            <section id="LS" class="section-content">
                <h2>Line Search</h2>
                <p>
                    In the previous chapter, we used a "fixed" <strong>learning rate</strong>(or <strong>step size</strong>), which was chosen 
                    based on experimental results. However, the "optimal" step size can be determined by <strong>line search</strong>:
                    \[
                    \eta_t = \arg \min _{\eta > 0} \mathcal{L} (\theta_t + \eta d_t) \in \mathbb{R}.
                    \]
                    Consider a quadratic loss:
                    
                    \[
                    \mathcal{L}(\theta + \eta \, d)
                    = \frac{1}{2}(\theta+ \eta d)^\top A (\theta + \eta \, d) + b^\top (\theta + \eta d) + c.
                    \]
                    
                    where \(\theta, d \in \mathbb{R}^n\), and \(\eta, c \in \mathbb{R}\).
                </p>
                <p>
                    Taking derivative of this function:
                    \[
                    \begin{align*}
                    \frac{d \mathcal{L}(\theta + \eta d)}{d\eta} &= \frac{1}{2} d^\top 2A(\theta + \eta d) + d^\top b  \\\\
                                                                    &= d^\top (A\theta + b) + \eta d^\top A d
                    \end{align*}
                    \]
                    Setting the derivative equal to zero, we have:
                    \[
                    \eta = - \frac{d^\top (A\theta + b)}{d^\top A d}.
                    \]
                    In practice, we don't need the "exact" line search because it can be expensive to solve this sub-optimization 
                    problem "at each step." 
                </p>
                <p>
                    For example, we can start with some initial step size, and then reduce it by a factor \(c \in (0, 1)\)  at each 
                    iteration until we satisfy the following condition: 
                    \[
                    \mathcal{L}(\theta_t + \eta \, d_t) \leq \mathcal{L}(\theta_t) + c \, \eta \, d_t^\top \nabla \mathcal{L}(\theta_t) \tag{1}
                    \]
                    This is called <strong>Armijo condition(Sufficient Decrease condition)</strong> that ensures sufficient 
                    decreasing of our objective function.
                    <br>
                    Note: Usually, we set \(c = 10^{-4}\).
                </p>
            </section>

            <section id="NM" class="section-content">
                <h2>Newton's Method</h2>
                <p>
                    Even though the first-order methods such as the gradient descent is computationally cheap, they do not consider the 
                    <strong>curvature</strong>, which often leads to slower convergence. A classic second-order method, <strong>Newton's method</strong> 
                    is as follows:
                </p>
                <div class="pseudocode">
                    <span class="pseudocode-title">Algorithm 1: NEWTONS_METHOD</span>
                    <strong>Input:</strong> objective function \(\mathcal{L}\), tolerance \(\epsilon\), initial step size \(\eta_o\);
                    <strong>Output:</strong> stationary point \(\theta^*\);
                    <strong>begin</strong>
                    &emsp;\(t \leftarrow 0\);
                    &emsp;Choose an initial point \(\theta_o\);
                    &emsp;<strong>repeat: </strong>
                    &emsp;&emsp;&emsp;Compute gradient: \(g_t = \nabla \mathcal{L}(\theta_t);\)
                    &emsp;&emsp;&emsp;Compute Hessian: \(H_t = \nabla^2 \mathcal{L}(\theta_t);\)
                    &emsp;&emsp;&emsp;Solve \(H_t d_t = -g_t\) for \(d_t\);
                    &emsp;&emsp;&emsp;\(d_t \leftarrow -H_t^{-1}g_t\);
                    &emsp;&emsp;&emsp;Do LINE_SEARCH to get step size \(\eta_t\) along \(d_t\);
                    &emsp;&emsp;&emsp;Update parameters: \(\theta_{t+1} = \theta_t + \eta_t d_t;\)
                    &emsp;&emsp;&emsp;\( t \leftarrow t + 1 ;\)
                    &emsp;<strong>until</strong> \(\| g_t \| < \epsilon\);
                    &emsp;Output \(\theta_t\);
                    <strong>end</strong>
                </div>
                <p>
                A critical issue of Newton's method is computing the inverse Hessian \(H_t^{-1}\) at 
                each \(t\) step, which is obviously expensive.
                </p>
            </section>

            
            <section id="BFGS" class="section-content">
                <h2>BFGS Method</h2>
                <p>
                    In <strong>Quasi-Newton methods</strong>, we approximate the Hessian using the curvature information from the gradient vector at each step. 
                    The most common Quasi-Newton method is called <strong>BFGS(Broyden, Fletcher, Goldfarb and Shanno) method</strong>.
                </p>
                <div class="theorem">
                    <span class="theorem-title">Approximation of Hessian in BFGS</span>
                    \[
                    \begin{align*}
                    H_{t+1} &\approx B_{t+1}  \\\\
                            &= B_{t} + \frac{y_t y_t^\top}{y_t^\top s_t} - \frac{(B_t s_t)(B_t s_t)^\top }{s_t^\top B_t s_t}
                    \end{align*}
                    \]
                    where \(s_t = \theta_t - \theta_{t-1}\) is the step in the parameter space and \(y_t = g_t - g_{t-1}\) is the change in the gradient 
                    of objective \(\mathcal{L}\).
                    <br>
                    Alternatively, we can iteratively update an approximation to the inverse Hessian: 
                    \[
                    \begin{align*}
                    H_{t+1}^{-1} &\approx C_{t+1} \\\\
                                 &= \Big(I - \frac{s_t y_t^\top}{y_t^\top s_t} \Big) C_t \Big( I - \frac{y_t s_t^\top}{y_t^\top s_t}\Big) + \frac{s_t s_t^\top}{y_t^\top s_t}.
                    \end{align*}
                    \]
                </div>
                <p>
                    By the Taylor expansion of the gradient \(g(\theta) = \nabla \mathcal{L}(\theta)\) around \(\theta_t\), neglecting higher-order terms:
                    \[
                    \begin{align*}
                    &g(\theta_{t+1}) \approx  g(\theta_t) + H(\theta_t) (\theta_{t+1}-\theta_t) \\\\
                    &\Longrightarrow y_t = H_t s_t,
                    \end{align*}
                    \]
                    where 
                    \[
                    \begin{align*}
                    &y_t = g(\theta_{t+1}) - g(\theta_t) \\\\
                    &s_t = \theta_{t+1}-\theta_t
                    \end{align*}
                    \]
                    <br> 
                    To efficiently approximate the Hessian \(H_t\), we compute the updated Hessian approximation \(B_{t+1}\) at the end of step \(t\). 
                    It must satisfy the <strong>secant condition</strong>:
                    \[
                    B_{t+1} s_t = y_t.
                    \]
                    This condition ensures that the updated Hessian approximation \(B_{t+1} \) reflects the curvature 
                    relationship between the step \(s_t\) and the gradient change \(y_t\). (Note: \(y_t^\top s_t >0\))
                    <br>
                    Assuming \(B_{t+1}\) is a positive definite rank-2 update of \(B_t\): 
                    \[
                    B_{t+1} = B_t + U,
                    \]
                    the secant condition becomes:
                    \[
                    (B_t + U) s_t = y_t \Longrightarrow U s_t = y_t - B_t s_t.
                    \]
                    Here, the correction matrix \(U\) must account for the discrepancy \(y_t - B_t s_t\). 
                    <br>
                    We assume \(U\) takes the form:
                    \[
                    U = \frac{y_t y_t^\top}{y_t^\top s_t} - \frac{(B_t s_t)(B_t s_t)^\top }{s_t^\top B_t s_t}.
                    \]
                    The first term adds curvature in the direction of \(y_t\), normalized by \(y_t^\top s_t\), ensuring positive definiteness. 
                    The second term removes excess curvature in the direction of \(s_t\) ensuring the secant condition is satisfied without over-correction.
                    <br>
                    Verification:
                    \[
                    \begin{align*}
                    &B_{t+1} = B_t + \frac{y_t y_t^\top}{y_t^\top s_t} - \frac{(B_t s_t)(B_t s_t)^\top }{s_t^\top B_t s_t} \\\\
                    &\Longrightarrow B_{t+1} s_t = B_t s_t + \Big\{\frac{y_t y_t^\top}{y_t^\top s_t} - \frac{(B_t s_t)(B_t s_t)^\top }{s_t^\top B_t s_t}\Big\} s_t\\\\
                    &\Longrightarrow B_{t+1} s_t = B_t s_t + y_t \cdot \frac{y_t^\top s_t}{y_t^\top s_t} - B_t s_t  \cdot \frac{(s_t^\top B_t s_t)}{s_t^\top B_t s_t}\\\\
                    &\Longrightarrow B_{t+1} s_t = B_t s_t  + y_t - B_t s_t \\\\
                    &\Longrightarrow B_{t+1} s_t = y_t
                    \end{align*}
                    \]
                    Therefore, \(B_{t+1}\) satisfies the secant condition, maintaining positive definiteness while approximating 
                    the curvature of the objective function.
                </p>
                <div class="proof">
                    <span class="proof-title">Inverse Hessian Approximation:</span>
                    <br>
                    Let \(B_t^{-1} = C_t\). First, we consider the rank-1 update:
                    \[
                    B_{t+1}^{-1} = \Big(B_t + \frac{y_t y_t^\top}{y_t^\top s_t} \Big)^{-1}.
                    \]
                    Note: A <strong>rank-1 update</strong> modifies the current Hessian approximation by adding or subtracting a rank-1 
                    matrix, which is the outer product of two vectors.
                    <br>
                    Using <a href="../Linear_algebra/woodbury.html"><strong>Sherman-Morrison formula</strong></a>: 
                    \[
                    \begin{align*}
                    B_{t+1}^{-1} &= \Big(B_t + y_t \cdot \frac{y_t^\top}{y_t^\top s_t} \Big)^{-1} \\\\
                                    &= C_t - \frac{C_t y_t \frac{y_t^\top}{y_t^\top s_t} C_t}{1 + \frac{y_t^\top}{y_t^\top s_t} C_t y_t} \\\\
                                    &= C_t - \frac{C_t y_t y_t^\top C_t}{y_t^\top s_t + y_t^\top C_t y_t}.
                    \end{align*}
                    \]
                    Next, we consider the <strong>rank-2 update</strong>: 
                    \[
                    C_{t+1} = \Big[\Big(B_t + \frac{y_t y_t^\top}{y_t^\top s_t}\Big) - (B_t s_t) \cdot \frac{(B_t s_t)^\top}{s_t^\top B_t s_t} \Big]^{-1}.
                    \]
                    Again using Sherman-Morrison formula:
                    \[
                    \begin{align*}
                    C_{t+1} &= \Big(C_t - \frac{C_t y_t y_t^\top C_t}{y_t^\top s_t + y_t^\top C_t y_t}\Big) 
                                + \frac{\Big(C_t - \frac{C_t y_t y_t^\top C_t}{y_t^\top s_t + y_t^\top C_t y_t}\Big)(B_t s_t) \frac{(B_t s_t)^\top}{s_t^\top B_t s_t} \Big(C_t - \frac{C_t y_t y_t^\top C_t}{y_t^\top s_t + y_t^\top C_t y_t}\Big)}
                                        {1 - \frac{(B_t s_t)^\top}{s_t^\top B_t s_t} \Big(C_t - \frac{C_t y_t y_t^\top C_t}{y_t^\top s_t + y_t^\top C_t y_t}\Big)(B_t s_t) } \\\\

                            &= C_t - \frac{C_t y_t y_t^\top C_t}{y_t^\top s_t + y_t^\top C_t y_t}
                                + \frac{\Big(C_t - \frac{C_t y_t y_t^\top C_t}{y_t^\top s_t + y_t^\top C_t y_t}\Big) B_t s_t s_t^\top B_t \Big(C_t - \frac{C_t y_t y_t^\top C_t}{y_t^\top s_t + y_t^\top C_t y_t}\Big)}
                                        {s_t^\top B_t s_t - s_t^T B_t \Big(C_t - \frac{C_t y_t y_t^\top C_t}{y_t^\top s_t + y_t^\top C_t y_t}\Big) B_t s_t } \\\\

                            &= C_t - \frac{C_t y_t y_t^\top C_t}{y_t^\top s_t + y_t^\top C_t y_t}
                                + \frac{\Big(I - \frac{C_t y_t y_t^\top}{y_t^\top s_t + y_t^\top C_t y_t}\Big) s_t s_t^\top  \Big(I - \frac{y_t y_t^\top C_t}{y_t^\top s_t + y_t^\top C_t y_t}\Big)}
                                        {s_t^\top B_t s_t - s_t^\top B_t s_t  + \frac{(y_t^\top s_t)^2}{y_t^\top s_t + y_t^\top C_t y_t}}\\\\

                            &= C_t - \frac{C_t y_t y_t^\top C_t}{y_t^\top s_t + y_t^\top C_t y_t}
                                + \frac{s_t s_t^\top 
                                        - \frac{y_t^\top s_t}{y_t^\top s_t + y_t^\top C_t y_t}(s_t y_t^\top C_t + C_t y_t s_t^\top)
                                        +\Big(\frac{y_t^\top s_t}{y_t^\top s_t + y_t^\top C_t y_t}\Big)^2 C_t y_t y_t^\top C_t}
                                        {\frac{(y_t^\top s_t)^2}{y_t^\top s_t + y_t^\top C_t y_t}}\\\\

                            &= C_t - \frac{C_t y_t y_t^\top C_t}{y_t^\top s_t + y_t^\top C_t y_t}
                                        + \frac{s_t s_t^\top( y_t^\top s_t + y_t^\top C_t y_t) }{(y_t^\top s_t)^2}
                                        - \frac{s_t y_t^\top C_t + C_t y_t s_t^\top}{y_t^\top s_t}
                                        + \frac{C_t y_t y_t^\top C_t}{y_t^\top s_t + y_t^\top C_t y_t} \\\\

                            &= C_t + \frac{s_t s_t^\top}{y_t^\top s_t} 
                                    + \frac{s_t y_t^\top C_t y_t s_t^\top}{(y_t^\top s_t)^2}
                                    - \frac{s_t y_t^\top C_t}{y_t^\top s_t}   
                                    - \frac{C_t y_t s_t^\top}{y_t^\top s_t} \\\\

                            &= \Big(I -\frac{s_t y_t^\top}{y_t^\top s_t}\Big)C_t\Big(I - \frac{y_t s_t^\top}{y_t^\top s_t}\Big) + \frac{s_t s_t^\top}{y_t^\top s_t} 
                    \end{align*}
                    \]
                </div>
                <p>
                    If the initial \(B_o\) is positive definite(Usually, \(B_o = I\)), and the step size \(\eta\) is found via Condition (1) and the following 
                    <strong>curvature condition</strong>: 
                    \[
                    d_t^\top \nabla \mathcal{L}(\theta_t + \eta \, d_t) \geq c_2 \, \eta \, d_t^\top \nabla \mathcal{L}(\theta_t) \tag{2}
                    \]
                    where \(c_2 \in (c, 1)\).
                    <br>
                    Then \(B_{t+1}\) remains positive definite. Condition (1) and (2) are together called <strong>Wolfe conditions</strong>. 
                </p>
                <div class="theorem">
                    <span class="theorem-title">Theorem 1: Wolfe conditions</span>
                    <ol>
                        <li>Sufficient decrease condition</li>
                        \[\mathcal{L}(\theta_t + \eta \, d_t) \leq \mathcal{L}(\theta_t) + c_1 \, \eta \, d_t^\top \nabla \mathcal{L}(\theta_t)\]
                        <li>Curvature condition</li>
                        \[d_t^\top \nabla \mathcal{L}(\theta_t + \eta \, d_t) \geq c_2 \, \eta \, d_t^\top \nabla \mathcal{L}(\theta_t)\]
                        <br>
                        where \(0 < c_1 << c_2 < 1\). For example, \(c_1 = 10^{-4}, c_2 = 0.9\) for Quasi-Newton methods.
                        <br>
                        Note: In <strong>strong Wolfe conditions</strong>, the courvature condition becomes
                        \[ 
                        | d_t^\top \nabla \mathcal{L}(\theta_t + \eta \, d_t) | \geq c_2 \, | \eta \, d_t^\top \nabla \mathcal{L}(\theta_t) |
                        \]
                    </ol>
                </div>
                <p>
                    In practice, we cannot store the whole Hessian approximation for large-scale problems. It costs \(O(n^2)\) memory space. 
                    In <strong>Limited memory BFGS(L-BFGS)</strong>, we only use the \(m\) most recent \((s_t, y_t)\) pairs, and do not store the Hessian approximation 
                    explicitly. So, we can approximate \(H_t^{-1 }g_t\) by computing a sequence of inner products of these vectors. The memory requirement 
                    can be \(O(mn)\), where \(m\) is typically 5 to 20.
                </p>
                <div class="pseudocode">
                    <span class="pseudocode-title">Algorithm 2: L_BFGS</span>
                    <strong>Input:</strong> objective function \(\mathcal{L}\), tolerance \(\epsilon\), initial step size \(\eta_o\); 
                    <strong>Output:</strong> stationary point \(\theta^*\);
                    <strong>begin</strong>
                    &emsp;Choose an initial point \(\theta_o\);
                    &emsp;Initialize \( (s, y) \) storage for \( m \) past updates;
                    &emsp;\(t \leftarrow 0\);
                    &emsp;<strong>repeat: </strong>
                    &emsp;&emsp;Compute gradient \( g_t = \nabla \mathcal{L}(\theta_t) \);
                    &emsp;&emsp;Set \( q_t \leftarrow g_t \);
                    &emsp;&emsp;Loop backward (from latest stored \( (s, y) \) pairs):
                    &emsp;&emsp;&emsp;Compute \( \alpha_i = \frac{s_i^\top q_t}{y_i^\top s_i} \);
                    &emsp;&emsp;&emsp;Update \( q_t \leftarrow q_t - \alpha_i y_i \);
                    &emsp;&emsp;Set \( r_t \leftarrow H_t^0 q_t \);
                    &emsp;&emsp;Loop forward (from oldest stored \( (s, y) \) pairs):
                    &emsp;&emsp;&emsp;Compute \( \beta_i = \frac{y_i^\top r_t}{y_i^\top s_i} \);
                    &emsp;&emsp;&emsp;Update \( r_t \leftarrow r_t + s_i (\alpha_i - \beta_i) \);
                    &emsp;&emsp;Set search direction  \( p_t \leftarrow -r_t \);
                    &emsp;&emsp;Do LINE_SEARCH: find step size \( \eta_t \) along \(p_t\) s.t. \( \mathcal{L}(\theta_t + \eta_t p_t) < \mathcal{L}(\theta_t) \);
                    &emsp;&emsp;Update parameters: \( \theta_{t+1} \leftarrow \theta_t + \eta_t p_t \);
                    &emsp;&emsp;Update gradient \( g_{t+1} \leftarrow \nabla \mathcal{L}(\theta_{t+1}) \);
                    &emsp;&emsp;Store \( (s_t\leftarrow \theta_{t+1} - \theta_t,\, y_t \leftarrow g_{t+1} - g_t) \);
                    &emsp;&emsp;Maintain memory size:If the number of stored \( s, y \) pairs exceeds \( m \), discard the oldest pair;
                    &emsp;&emsp;\( t \leftarrow t + 1\);
                    &emsp;<strong>until</strong> \( \|g_t\| < \epsilon \);
                    &emsp;Output \(\theta_t\);
                    <strong>end</strong>
                </div>
                <p>
                    Note: For computational efficiency and numerical stability, instead of calculating \(\frac{1}{s^\top y}\) multiple times, usually 
                    a new variable \(\rho = \frac{1}{s^\top y}\) will be introduced.
                </p>
                <h3>Python Implementation</h3>
                <p>A sample code for L-BFGS is as follows:</p>

                <div class="code-container">
                    <div class="collapsible-section">
                        <button class="collapsible-btn">Show/Hide Code</button>
                        <div class="collapsible-content">
                            <pre class="python-code">
                            import numpy as np

                            # Line search with Wolfe conditions 
                            def line_search(f, grad_f, theta, p, c1 = 1e-4, c2 = 0.9, max_iter = 100):
                                eta = 1.0
                                eta_low = 0.0
                                eta_high = None
                            
                                phi_0 = f(theta)
                                grad_phi_0 = np.dot(grad_f(theta), p)
                            
                                for _ in range(max_iter):
                                    phi_eta = f(theta + eta * p)
                            
                                    # Check Armijo condition
                                    if phi_eta > phi_0 + c1 * eta * grad_phi_0:
                                        eta_high = eta
                                    else:
                                        # Check Curvature condition
                                        grad_phi_eta = np.dot(grad_f(theta + eta * p), p)
                                        if grad_phi_eta < c2 * grad_phi_0:
                                            eta_low = eta
                                        else:
                                            return eta
                            
                                    # Update step size using bisection method
                                    if eta_high is not None:
                                        eta = (eta_low + eta_high) / 2.0
                                    else:
                                        eta *= 2.0
                            
                                return eta
                            
                            # Limited memory BFGS 
                            def limited_bfgs(f, grad_f, theta0, m = 10, tol = 1e-6, max_iter = 2000):
                                
                                theta = theta0.copy()
                                g = grad_f(theta)
                                s_list = []
                                y_list = []
                                rho_list = [] # We introduce rho =  1/s^T y instead of directly using 1/s^T y for efficiency & stability. 
                            
                                for _ in range(max_iter):
                                    
                                    if np.linalg.norm(g) < tol:
                                        break
                            
                                    q = g.copy()
                                    alpha_list = []  # Need this for the second loop to avoid computing alpha again 
                            
                                    # Loop backward through stored (s, y) pairs
                                    for s, y, rho in reversed(list(zip(s_list, y_list, rho_list))):
                                        alpha = rho * np.dot(s, q)
                                        alpha_list.append(alpha)
                                        q -= alpha * y
                            
                                    # Initial Hessian approximation is identity: H0 = I, so H0 * q = q
                                    r = q
                            
                                    # Loop forward through stored (s, y) pairs
                                    for (s, y, rho), alpha in zip(zip(s_list, y_list, rho_list), reversed(alpha_list)):
                                        beta = rho * np.dot(y, r)
                                        r += s * (alpha - beta)
                            
                                    # Search direction
                                    p = -r
                            
                                    # Compute the step size by Line search satisfying Wolfe conditions
                                    eta = line_search(f, grad_f, theta, p)
                            
                                    # Update parameters
                                    theta += eta * p
                                    grad_next = grad_f(theta)
                            
                                    # Update memory for (s, y) pairs
                                    s = eta * p
                                    y = grad_next - g
                                    if np.dot(s, y) > 1e-6:  
                                        if len(s_list) == m:
                                            s_list.pop(0)
                                            y_list.pop(0)
                                            rho_list.pop(0)
                                        s_list.append(s)
                                        y_list.append(y)
                                        rho_list.append(1.0 / np.dot(y, s))
                                        
                                    # Update gradient
                                    g = grad_next
                            
                                return theta
                            
                            # Objective function and its gradient: 
                            # The Rosenbrock function is commonly used for testing optimization algorithms.
                            def rosenbrock(x):
                                return np.sum(100 * (x[1:] - x[:-1]**2)**2 + (1 - x[:-1])**2)
                            
                            def grad_rosenbrock(x):
                                grad = np.zeros_like(x)
                                grad[:-1] = -400 * x[:-1] * (x[1:] - x[:-1]**2) - 2 * (1 - x[:-1]) # # x_1 to x_n-1
                                grad[1:] += 200 * (x[1:] - x[:-1]**2) # x_2 to x_n
                                return grad
                            
                            # Finite difference gradient to check grad_rosenbrock().
                            def finite_difference_gradient(f, x, epsilon=1e-6):
                                grad = np.zeros_like(x)
                                for i in range(len(x)):
                                    x_forward = x.copy()
                                    x_backward = x.copy()
                                    x_forward[i] += epsilon
                                    x_backward[i] -= epsilon
                                    grad[i] = (f(x_forward) - f(x_backward)) / (2 * epsilon)
                                return grad
                            
                            if __name__ == "__main__":
                                
                                n = 50 # Dimensionality
                                
                                # Randomly generate an initial point x0:
                                # You could try : x0 = np.ones(n) + 0.3 * np.random.randn(n), which represents adding small 
                                # perturbation around the global minimum x* = [1, ... , 1]^T 
                                x0 =  np.random.randn(n)  
                                numeric_opt = limited_bfgs(rosenbrock, grad_rosenbrock, x0)
                                print("Initial point: \n", x0.tolist())
                                print("\n Numerical optimum: \n", numeric_opt.tolist())
                            
                                '''
                                # If you are not sure about the gradient of the objective, always you can compare it with finite difference.
                                grad_analytic = grad_rosenbrock(x0)
                                grad_numeric = finite_difference_gradient(rosenbrock, x0)
                                relative_error_grad = np.linalg.norm(grad_analytic - grad_numeric) / np.linalg.norm(grad_analytic)
                                print("Relative error:", relative_error_grad) 
                                ''' 
                                # Relative error between the numerical optimum and the global optimum
                                global_opt = np.ones(n)  # The actual global optimum of Rosenbrock function is x* = [1, ... , 1]^T
                                relative_error_optimal = np.linalg.norm(numeric_opt - global_opt) / np.linalg.norm(global_opt)
                                print(f"\n Relative error to the global minimum: {relative_error_optimal*100:.8f}%")
                            </pre>
                        </div>
                    </div>
                    <button class="run-button" onclick="runPythonCode(this)">Run Code</button>
                    <div class="python-output"></div>
                </div>
                <p>
                    Note: On the above code, we used the <strong>Rosenbrock function</strong>, which is defined as 
                    \[
                    f(x) = \sum_{i = 1}^{n-1} a (x_{i+1} - x_i^2)^2 + (b - x_i)^2
                    \]
                    where \(x \in \mathbb{R}^n\), and usually we set the coefficients \(a = 100, \, b = 1\).
                </p>
                
                <p>
                    The global minimum can be obtained simply at \(x = [1, 1, \cdots, 1]\), but in numerical optimization, converging to 
                    the global minimum is difficult because of several reasons:
                </p>
                
                <ul style="padding-left: 40px;">
                    <li><strong>Broad "flat" regions & several local minima:</strong> Optimization algorithms can be trapped there.</li>
                    <li><strong>Ill-conditioned:</strong> Small changes in one variable can result in large changes in another.</li>
                    <li><strong>A narrow, curved valley containing the global minimum:</strong> It makes hard for optimization algorithms to navigate. The 
                    steep sides of the valley can cause algorithms to overshoot or converge very slowly.</li>
                </ul>
                <br>
                <p>Rosenbrock function is a common benchmark for testing the robustness and efficiency of optimization algorithms.</p>
            </section>
        </div>
        <script src="/js/main.js"></script>
        <script src="/js/runPythonCode.js"></script>
        <script src="/js/collapsible.js"></script>
    </body>
</html>
    
    <footer>
    <div class="footer-content">
        <div class="footer-about">
            <h3>About MATH-CS COMPASS</h3>
            <p>Bridging the gap between pure mathematics and computer science applications.</p>
            <div class="contact-section">
                <strong><a href="https://docs.google.com/forms/d/e/1FAIpQLSf3_RNkegREvfCEIQznn6SCaf-iB5c-Doxn7Ymuu78Uf-PBjg/viewform" target="_blank">
                    Submit Your Questions & Feedback <i class="fas fa-external-link-alt"></i>
                </a></strong>
            </div>
        </div>
        <div class="footer-links">
            <h3>Quick Links</h3>
            <ul>
                
                <li><a href="../../../index.html">Home</a></li>
                <li>
                <a href="../Linear_algebra/linear_algebra.html">
                    <span class="nav-number">I</span>
                    <span>Linear Algebra to Algebraic Foundations</span>
                </a>
                </li>
                <li>
                <a href="../Calculus/calculus.html">
                    <span class="nav-number">II</span>
                    <span>Calculus to Optimization & Analysis</span>
                </a>
                </li>
                <li>
                <a href="../Probability/probability.html">
                    <span class="nav-number">III</span>
                    <span>Probability & Statistics</span>
                </a>
                </li>
                <li>
                <a href="../Discrete/discrete_math.html">
                    <span class="nav-number">IV</span>
                    <span>Discrete Mathematics & Algorithms</span>
                </a>
                </li>
                <li>
                <a href="../Machine_learning/ml.html">
                    <span class="nav-number">V</span>
                    <span>Machine Learning</span>
                </a>
                </li>
            </ul>
        </div>
        <div class="footer-social">
            <h3>Connect With Me</h3>
            <p>
                <a href="https://x.com/MathCSCompass?t=Zi8nyfzszeFw8QizbJwnNg&s=09" target="_blank">
                    <i class="fab fa-x-twitter"></i> X (Twitter)
                </a>
            </p>
            <p>
                <a href="https://www.facebook.com/share/15uPaBWgQN/" target="_blank">
                    <i class="fab fa-facebook"></i> Facebook
                </a>
            </p>
      
            <h3>MATH-CS COMPASS App</h3>
            <div class="app-download-footer">
                <a href="https://play.google.com/store/apps/details?id=io.github.yusukey01.twa" target="_blank" rel="noopener">
                    <img src="../../../images/google-play-badge.png" 
                        alt="Get it on Google Play" 
                        class="google-play-badge">
                </a>
            </div>
            
        </div>

    </div>
    <div class="footer-bottom">
        <p>&copy; 2024-2025 MATH-CS COMPASS. All rights reserved.</p>
    </div>
</footer>
    <script src="../../../js/main.js?v=1.0.2"></script>
    <script src="../../../js/search.js?v=1.0.2"></script>
    

    <button id="go-to-top-btn" class="go-to-top-btn" title="Go to top">
        <i class="fas fa-arrow-up"></i>
    </button>
    <script>
        // Register the service worker
        if ('serviceWorker' in navigator) {
          window.addEventListener('load', () => {
            navigator.serviceWorker.register('/js/service-worker.js')
              .then(registration => {
                console.log('ServiceWorker registration successful with scope: ', registration.scope);
              })
              .catch(error => {
                console.log('ServiceWorker registration failed: ', error);
              });
          });
        }
      </script>
</body>
</html>