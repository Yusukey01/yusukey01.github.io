<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    
    <title>Linear Approximations - MATH-CS COMPASS</title>
    
    
    <script>
    window.MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']]
      }
    };
    </script>
    <script id="MathJax-script" defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="preload" href="https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Main-Regular.woff" as="font" type="font/woff2" crossorigin>
    <link rel="preload" href="https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Math-Italic.woff" as="font" type="font/woff2" crossorigin>
    
    
    

    <link rel="stylesheet" href="../../../css/styles.css?v=1.0.2">
  
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css">

    <link rel="dns-prefetch" href="https://cdn.jsdelivr.net">
    <link rel="dns-prefetch" href="https://cdnjs.cloudflare.com">
    
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Learn about differentials and linearization, intro to matrix calculus.">
    <meta name="google-site-verification" content="IieEujypYW38caEVe8ymjczouNl56yxAy27iPztcfRA" />
    

    <link rel="manifest" href="/manifest.json">

    <!-- Theme color for Chrome, Android, and other browsers -->
    <meta name="theme-color" content="#007bff">
  
    <!-- iOS specific tags -->
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <meta name="apple-mobile-web-app-title" content="MATH-CS COMPASS">
    <meta name="mobile-web-app-capable" content="yes">

    <!-- Icon references -->
    <link rel="icon" type="image/png" sizes="1280x1280" href="/images/maskable_icon_x1280.png?v=1.0.2">
    <link rel="icon" type="image/png" sizes="512x512" href="/images//maskable_icon_x512.png?v=1.0.2">
    
    <!-- Apple Touch Icons -->
    <link rel="apple-touch-icon" href="/images/maskable_icon_x1280.png?v=1.0.2">
    <link rel="apple-touch-icon" href="/images/maskable_icon_x512.png?v=1.0.2">

    <!-- For Windows -->
    <meta name="msapplication-TileImage" content="/images/test1.png?v=1.0.2">
    <meta name="msapplication-TileColor" content="#007bff">

    <!-- Schema.org Structured Data for AI Optimization -->
    

    <!-- Article Schema for content pages -->
    
    <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "Article",
      "headline": "Linear Approximations",
      "description": "Learn about differentials and linearization, intro to matrix calculus.",
      "datePublished": "2024-01-01",
      "dateModified": "2025-01-01",
      "author": {
        "@type": "Person",
        "name": "Yusuke Yokota",
        "jobTitle": "Mathematics & Computer Science Educator",
        "knowsAbout": [
          "Linear Algebra",
          "Machine Learning",
          "Computer Science",
          "Mathematics"
        ]
      },
      "publisher": {
        "@type": "Organization",
        "name": "MATH-CS COMPASS",
        "url": "https://yusukey01.github.io",
        "logo": {
          "@type": "ImageObject",
          "url": "https://yusukey01.github.io/images/maskable_icon_x512.png"
        }
      },
      "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "https://yusukey01.github.io/Mathematics/Calculus/linear_approximation.html"
      },
      "about": [
        
        { "@type": "Thing", "name": "Calculus" },
        { "@type": "Thing", "name": "Optimization" }
        
      ],
      "educationalLevel": "university",
      "learningResourceType": "tutorial",
      "educationalUse": "instruction"
    }
    </script>
    
</head>

<body>
    <header>
    <nav class="navbar">
        <!-- Logo as Home Button -->
        <div class="logo-home">
            <a href="../../../index.html" 
               class="logo-link " 
               title="Home - Math-CS Compass">
                <img src="../../images/icon_x512.png" 
                     alt="Math-CS Compass Logo" 
                     class="nav-logo">
            </a>
        </div>

        <div class="search-container">
            <input type="text" id="search-input" placeholder="Search keywords...">
            <button id="search-button" aria-label="Search" title="Search website"><i class="fas fa-search"></i></button>
        </div>
        
        <div class="menu-toggle">
            <i class="fas fa-bars"></i>
        </div>
        
        <ul class="nav-links">
            <li><a href="../Linear_algebra/linear_algebra.html" >I - Linear Algebra to Algebraic Foundations</a></li>
            
            <li><a href="../Calculus/calculus.html" class="active">II - Calculus to Optimization & Analysis</a></li>
            
            <li><a href="../Probability/probability.html" >III - Probability & Statistics</a></li>
            
            <li><a href="../Discrete/discrete_math.html" >IV - Discrete Mathematics & Algorithms</a></li>
            
            <li><a href="../Machine_learning/ml.html" >V - Machine Learning</a></li>
        </ul>
    </nav>
</header>
    
    <!DOCTYPE html>
<html>
    <body>
        <!-- LearningResource Schema for Linear Approximations -->
        <script type="application/ld+json">
        {
        "@context": "https://schema.org",
        "@type": "LearningResource",
        "name": "The Derivative of f:ℝⁿ → ℝ",
        "description": "Learn about differentials and linearization, intro to matrix calculus",
        "learningResourceType": "lesson",
        "educationalUse": "instruction",
        "educationalLevel": "university",
        "interactivityType": "active",
        "author": {
            "@type": "Person",
            "name": "Yusuke Yokota",
            "jobTitle": "Mathematics & Computer Science Educator",
            "knowsAbout": [
            "Linear Approximation",
            "Differential Calculus",
            "Matrix Calculus",
            "Vector Calculus",
            "Mathematical Analysis",
            "Multivariable Calculus"
            ]
        },
        "publisher": {
            "@type": "Organization",
            "name": "MATH-CS COMPASS",
            "url": "https://yusukey01.github.io"
        },
        "about": [
            { "@type": "Thing", "name": "Linear Approximation" },
            { "@type": "Thing", "name": "Linearization" },
            { "@type": "Thing", "name": "Differentials" },
            { "@type": "Thing", "name": "Tangent Line" },
            { "@type": "Thing", "name": "Gradient Vector" },
            { "@type": "Thing", "name": "Vector-valued Functions" },
            { "@type": "Thing", "name": "Scalar-valued Functions" },
            { "@type": "Thing", "name": "Product Rule" },
            { "@type": "Thing", "name": "Chain Rule" },
            { "@type": "Thing", "name": "Matrix Calculus" },
            { "@type": "Thing", "name": "Vector Calculus" },
            { "@type": "Thing", "name": "Quadratic Forms" },
            { "@type": "Thing", "name": "L2 Norm" },
            { "@type": "Thing", "name": "Euclidean Norm" },
            { "@type": "Thing", "name": "Linear Operator" }
        ],
        "teaches": [
            "Understanding linear approximation of functions",
            "Computing linearization using tangent lines",
            "Working with differentials in single and multiple variables",
            "Computing gradients of vector-valued functions",
            "Applying the product rule in differential notation",
            "Understanding quadratic forms and their derivatives",
            "Computing derivatives of norms and inner products",
            "Extending calculus concepts to vector spaces"
        ],
        "isPartOf": {
            "@type": "Course",
            "name": "Calculus to Optimization & Analysis",
            "description": "Explore optimization techniques and mathematical analysis with applications",
            "provider": {
            "@type": "Organization",
            "name": "MATH-CS COMPASS",
            "url": "https://yusukey01.github.io"
            },
            "instructor": {
            "@type": "Person",
            "name": "Yusuke Yokota",
            "jobTitle": "Mathematics & Computer Science Educator"
            },
            "courseCode": "II",
            "hasCourseInstance": {
            "@type": "CourseInstance",
            "courseMode": "online",
            "courseWorkload": "PT3H",
            "instructor": {
                "@type": "Person",
                "name": "Yusuke Yokota"
            }
            },
            "offers": {
            "@type": "Offer",
            "price": "0",
            "priceCurrency": "USD",
            "availability": "https://schema.org/InStock",
            "category": "free"
            }
        }
        }
        </script>
        <div class="hero-section">
            <h1 class="webpage-name">The Derivative of \(f:\mathbb{R}^n \rightarrow \mathbb{R}\)
            </h1>
        </div>

        
  <div class="quick-jump-container">
    <button type="button" id="quick-jump-toggle" class="quick-jump-toggle">
      <i class="fas fa-list"></i> <strong>Section II Navigation</strong>
    </button>
    <div id="quick-jump-menu" class="quick-jump-menu">
      <h3>Calculus to Optimization & Analysis</h3>
      <div class="quick-jump-links">
        
        
        <a href="calculus.html">← Back to Section II Overview</a>
        <hr style="margin: 8px 0; border: 1px solid #ddd;">
        
        
        <a href="linear_approximation.html" class="current-page">Part 1: The Derivative of f:ℝⁿ → ℝ</a>
        <a href="jacobian.html" >Part 2: The Derivative of f:ℝⁿ → ℝⁿ</a>
        <a href="matrix_cal.html" >Part 3: The Derivative of f:ℝⁿˣⁿ → ℝⁿˣⁿ</a>
        <a href="numerical_example1.html" >Part 4: Intro to Numerical Computation</a>
        <a href="det.html" >Part 5: Scalar Functions of Matrices</a>
        <a href="mvt.html" >Part 6: The Mean Value Theorem</a>
        <a href="gradient.html" >Part 7: Gradient Descent</a>
        <a href="newton.html" >Part 8: Newton's Method</a>
        <a href="constrained_opt.html" >Part 9: Constrained Optimization</a>
        <a href="riemann.html" >Part 10: Riemann Integration</a>
        <a href="measure.html" >Part 11: Measure Theory</a>
        <a href="lebesgue.html" >Part 12: Lebesgue Integration</a>
        <a href="duality.html" >Part 13: Duality in Optimization</a>
        <a href="fourier_series.html" >Part 14: Fourier Series</a>
        <a href="fourier_transform.html" >Part 15: Fourier Transform</a>
      </div>
    </div>
</div>


        <div class="topic-nav">
            <a href="#lapp">Linear Approximations</a>
            <a href="#diff">Differentials</a>
            <a href="#ex1">Example 1: \(f(x) = x^Tx\) where \(x \in \mathbb{R}^n\)</a>
            <a href="#ex2">Example 2: \(f(x) = x^TAx\) where \(x \in \mathbb{R}^n\) and \(A \in \mathbb{R}^{n \times n}\)</a>
            <a href="#ex3">Example 3: \(f(x) = \| x \|_2\) where \(x \in \mathbb{R}^n\)</a>   
        </div> 

        <div class="container">  
           
            <section id="lapp" class="section-content">
            <h2>Linear Approximations</h2>
            <p>
            <strong>Linear approximation</strong> is a process of approximation a function \(f(x)\) near a point 
            \(x_o\) using a linear function. It simplifies complex functions locally.
            \[
            L(x) = f(x_o) + f'(x_o)(x -x_o) \approx f(x) \tag{1}
            \]
            where \(f'(x_o) = \lim_{x \to x_o} \frac{f(x)-f(x_o)}{x-x_o}\) is derivative at \(x_o\). 
            <br>
            Equivalently, from equation (1), 
            \[
            f(x) - f(x_o) \approx f'(x_o)(x -x_o) 
            \]
            <br>
            \(L(x)\) is called the <strong>linearization</strong> of \(f\) at \(x_o\). The graph of \(L\) is 
            the tangent line at \((x_o, f(x_o))\). Linear approximations form the foundation of differentiation 
            and provide a local linear model for \(f(x)\). This concept extends naturally to  differentials.
            </p>
            </section>
    
            <section id="diff" class="section-content">
            <h2>Differentials</h2>
            <p>
            <strong>Differentials</strong> describe infinitesimally small changes in quantities. 
            If \(y = f(x)\), where \(f\) is a differentiable function, then the differential \(dx\) is 
            an independent variable and the differential \(dy\) is defined as 
            \[
            dy = f'(x)dx. 
            \]
            In practice, \(dx\) and \(dy\) are arbitrary small numbers. Also, if \(dx \neq 0\), then 
            we recover the familiar derivative form:
            \[
            \frac{dy}{dx} = f'(x)
            \]
            where the left side now represents the ratio of differentials.
            <br><br>
            The exact change in \(y\) corresponding to change in \(x\),(\(dx\)) is given by:
            \[
            \delta y  = f(x + dx) -f(x).  
            \]
            As \(dx \to 0\), the approximation improves
            \[
            dy = f'(x)dx \approx \delta y = f(x + dx) -f(x).
            \] 
            More precisely, the relationship can be written as:
            \[
            f(x + dx) - f(x) = f'(x)dx + o(dx),
            \]
            where \(o(dx)\) is the asymptotic notation, which represents higher-order terms that become negligible as \(dx \to 0\). 
            This highlights that the differential \(dy = f'(x)dx\) serves as a linear approximation to \(\delta y\).
            <br><br>
            More generally, the differential of \(f\) can be expressed as:
            \[
            df = f(x + dx) - f(x) = f'(x)dx,
            \] 
            where \(df\) represents the linearized change in \(f(x)\) due to an infinitesimally small change in \(x\).
            <br><br>
            Here, \(df\) is the change in the output, \(dx\) is the change in the input, and most importantly, 
            \(f'(x)\) acts as a <strong>linear operator</strong> that maps \(dx\) to \(df\). 
            <br>
            The flexibility of differential notation extends naturally to <strong>linear algebra</strong>, where derivatives 
            apply not only to scalars \(x \in \mathbb{R}\), but also to vectors \(\vec{x} \in \mathbb{R}^n\) and 
            matrices \(X \in \mathbb{R}^{m \times n}\). 
            </p>
            </section>
        
            <section id="ex1" class="section-content">
            <h2>Example 1: \(f(x) = x^Tx\) where \(x \in \mathbb{R}^n\)</h2>
            <p>
            In this case, the input is the vector \(x\), and the output is the scalar \(x^Tx\).
            To compute the derivative of this function, we start with:
            \[
            f(x) = x^Tx = \sum_{i=1}^n x_i^2 ,
            \]
            where \(x_i\) is the \(i\)-th entry of the vector \(x\).
            Then the <strong>gradient</strong> of \(f\) is:
            \[
            \nabla f = \begin{bmatrix}
                        \frac{\partial f }{\partial x_1} \\ 
                        \frac{\partial f }{\partial x_2} \\ 
                        \vdots \\
                        \frac{\partial f }{\partial x_n}
                        \end{bmatrix}
                     =  \begin{bmatrix} 2x_1 \\ 2x_2\\ \vdots \\  2x_n \end{bmatrix}
                     = 2x
            \]
            <strong>Note: Input: vector & Output: scalar \(\Longrightarrow\) First derivative: column vector (gradient).</strong>
            <br>
            Now, let's derive the same result using differential notation. Note: \(dx \in \mathbb{R}^n\).
            <br><br>
            By the <strong>product rule</strong>, and the commutativity of the vector inner product:
            \[
            \begin{align*}
            d(x^Tx) &= (dx^T)x + x^T(dx) \\\\
                    &= x^Tdx + x^Tdx \\\\
                    &= 2x^Tdx.
            \end{align*}
            \]
            Note: \(dx^T = (dx)^T\) because
            \[\begin{align*}
            d(x^T) &= (x + dx)^T - x^T \\\\
                   &= x^T + (dx)^T - x^T \\\\
                   &= (dx)^T
            \end{align*}.
            \]
            Thus the gradient is 
            \[
            \nabla f = (2x^T)^T = 2x.
            \]
            Note: \(2x^T\) is a "row" vector and to get a column vector \(\nabla f\), we need the transpose of \(2x^T\).
            <div class="theorem">
                <span class="theorem-title">Theorem 1: Product rule</span>  
                If both \(g\) and \(h\) are differentiable and let \(f(x) = g(x)h(x)\), then 
                \[
                df = (dg)h + g(dh). \tag{1}
                \]
                Quick derivation: 
                \[
                \begin{align*}
                df &= g(x+dx)h(x+dx) - g(x)h(x) \\\\
                   &= [g(x) +dg][h(x) + dh]-g(x)h(x) \\\\
                   &= g(x)h(x) + (dg)h + g(dh) + (dg)(dh) -g(x)h(x)
                \end{align*}
                \]
                Then \((dg)(dh)\) is negligible, and we get (1).
            </div>
            </p>
            </section>

            <section id="ex2" class="section-content">
            <h2>Example 2: \(f(x) = x^TAx\) where \(x \in \mathbb{R}^n\) and \(A \in \mathbb{R}^{n \times n}\)</h2>
            <p>
            Let's use the differential representation: 
            \[\begin{align*}
            df &= f(x + dx) -f(x) \\\\
               &= (x + dx)^T A (x + dx) - x^T A x
            \end{align*}
            \]
            Expanding this expression, we get:
            \[
            df = x^TAx + dx^T A x + x^TAdx + dx^T A dx - x^T A x 
            \]
            It is valid to ignore the higher-order term \(dx^T A dx\), which becomes negligible as \(dx \to 0\).
            Then
            \[
            df = dx^TAx + x^TAdx.
            \]
            since \(dx^TAx\) is a scalar, \((dx^TAx)^T = x^TA^Tdx\). Then we get:
            \[
            df = x^TA^Tdx + x^TAdx = x^T(A^T + A)dx
            \]
            Here, 
            \[
            (A^T + A)^T = A + A^T =  (A^T + A)
            \].
            So, \((A^T + A)^T\) is symmetric and thus: 
            \[
            \nabla f = (A+A^T)x.
            \]
            if \(A\) is symmetric (\(x^TAx\) is a quadratic form), \(\nabla f = (A+A^T)x = (A+A)x  = 2Ax\).
            <br>
            Also, Example 1 is a special case: \(A\) is an identity matrix. 
            </p>
            </section>

            <section id="ex3" class="section-content">
            <h2>Example 3: \(f(x) = \| x \|_2\) where \(x \in \mathbb{R}^n\)</h2>
            <p>
            Now, it is simple to find the derivative of <strong>\(L_2\) norm</strong>.
            <br>
            Let \(r = \| x \|\),  and then:
            \[
            \begin{align*}
            & r^2 = x^Tx \\\\
            &\Longrightarrow 2rdr = 2x^Tdr \\\\
            &\Longrightarrow  dr = \frac{x^T}{r} = \frac{x^T}{\| x \|} \\\\\
            &\Longrightarrow  \nabla f = \frac{x}{\| x \|}.
            \end{align*}
            \]
            </p>
            </section>
        </div>
        <script src="/js/main.js"></script>   
    </body>
</html>
    
    <footer>
    <div class="footer-content">
        <div class="footer-about">
            <h3>About MATH-CS COMPASS</h3>
            <p>Bridging the gap between pure mathematics and computer science applications.</p>
            <div class="contact-section">
                <strong><a href="https://docs.google.com/forms/d/e/1FAIpQLSf3_RNkegREvfCEIQznn6SCaf-iB5c-Doxn7Ymuu78Uf-PBjg/viewform" target="_blank">
                    Submit Your Questions & Feedback <i class="fas fa-external-link-alt"></i>
                </a></strong>
            </div>
        </div>
        <div class="footer-links">
            <h3>Quick Links</h3>
            <ul>
                
                <li><a href="../../../index.html">Home</a></li>
                <li>
                <a href="../Linear_algebra/linear_algebra.html">
                    <span class="nav-number">I</span>
                    <span>Linear Algebra to Algebraic Foundations</span>
                </a>
                </li>
                <li>
                <a href="../Calculus/calculus.html">
                    <span class="nav-number">II</span>
                    <span>Calculus to Optimization & Analysis</span>
                </a>
                </li>
                <li>
                <a href="../Probability/probability.html">
                    <span class="nav-number">III</span>
                    <span>Probability & Statistics</span>
                </a>
                </li>
                <li>
                <a href="../Discrete/discrete_math.html">
                    <span class="nav-number">IV</span>
                    <span>Discrete Mathematics & Algorithms</span>
                </a>
                </li>
                <li>
                <a href="../Machine_learning/ml.html">
                    <span class="nav-number">V</span>
                    <span>Machine Learning</span>
                </a>
                </li>
            </ul>
        </div>
        <div class="footer-social">
            <h3>Connect With Me</h3>
            <p>
                <a href="https://x.com/MathCSCompass?t=Zi8nyfzszeFw8QizbJwnNg&s=09" target="_blank">
                    <i class="fab fa-x-twitter"></i> X (Twitter)
                </a>
            </p>
            <p>
                <a href="https://www.facebook.com/share/15uPaBWgQN/" target="_blank">
                    <i class="fab fa-facebook"></i> Facebook
                </a>
            </p>
      
            <h3>MATH-CS COMPASS App</h3>
            <div class="app-download-footer">
                <a href="https://play.google.com/store/apps/details?id=io.github.yusukey01.twa" target="_blank" rel="noopener">
                    <img src="../../../images/google-play-badge.png" 
                        alt="Get it on Google Play" 
                        class="google-play-badge">
                </a>
            </div>
            
        </div>

    </div>
    <div class="footer-bottom">
        <p>&copy; 2024-2025 MATH-CS COMPASS. All rights reserved.</p>
    </div>
</footer>
    <script src="../../../js/main.js?v=1.0.2"></script>
    <script src="../../../js/search.js?v=1.0.2"></script>
    

    <button id="go-to-top-btn" class="go-to-top-btn" title="Go to top">
        <i class="fas fa-arrow-up"></i>
    </button>
    <script>
        // Register the service worker
        if ('serviceWorker' in navigator) {
          window.addEventListener('load', () => {
            navigator.serviceWorker.register('/js/service-worker.js')
              .then(registration => {
                console.log('ServiceWorker registration successful with scope: ', registration.scope);
              })
              .catch(error => {
                console.log('ServiceWorker registration failed: ', error);
              });
          });
        }
      </script>
</body>
</html>