<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    
    <title>Jacobian - MATH-CS COMPASS</title>
    
    
    <script>
    window.MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']]
      }
    };
    </script>
    <script id="MathJax-script" defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="preload" href="https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Main-Regular.woff" as="font" type="font/woff2" crossorigin>
    <link rel="preload" href="https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Math-Italic.woff" as="font" type="font/woff2" crossorigin>
    
    
    

    <link rel="stylesheet" href="../../../css/styles.css?v=1.0.2">
  
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css">

    <link rel="dns-prefetch" href="https://cdn.jsdelivr.net">
    <link rel="dns-prefetch" href="https://cdnjs.cloudflare.com">
    
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Learn about Jacobian, chain rule, backpropagation.">
    <meta name="google-site-verification" content="IieEujypYW38caEVe8ymjczouNl56yxAy27iPztcfRA" />
    

    <link rel="manifest" href="/manifest.json">

    <!-- Theme color for Chrome, Android, and other browsers -->
    <meta name="theme-color" content="#007bff">
  
    <!-- iOS specific tags -->
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <meta name="apple-mobile-web-app-title" content="MATH-CS COMPASS">
    <meta name="mobile-web-app-capable" content="yes">

    <!-- Icon references -->
    <link rel="icon" type="image/png" sizes="1280x1280" href="/images/maskable_icon_x1280.png?v=1.0.2">
    <link rel="icon" type="image/png" sizes="512x512" href="/images//maskable_icon_x512.png?v=1.0.2">
    
    <!-- Apple Touch Icons -->
    <link rel="apple-touch-icon" href="/images/maskable_icon_x1280.png?v=1.0.2">
    <link rel="apple-touch-icon" href="/images/maskable_icon_x512.png?v=1.0.2">

    <!-- For Windows -->
    <meta name="msapplication-TileImage" content="/images/test1.png?v=1.0.2">
    <meta name="msapplication-TileColor" content="#007bff">

    <!-- Schema.org Structured Data for AI Optimization -->
    

    <!-- Article Schema for content pages -->
    
    <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "Article",
      "headline": "Jacobian",
      "description": "Learn about Jacobian, chain rule, backpropagation.",
      "datePublished": "2024-01-01",
      "dateModified": "2025-01-01",
      "author": {
        "@type": "Person",
        "name": "Yusuke Yokota",
        "jobTitle": "Mathematics & Computer Science Educator",
        "knowsAbout": [
          "Linear Algebra",
          "Machine Learning",
          "Computer Science",
          "Mathematics"
        ]
      },
      "publisher": {
        "@type": "Organization",
        "name": "MATH-CS COMPASS",
        "url": "https://yusukey01.github.io",
        "logo": {
          "@type": "ImageObject",
          "url": "https://yusukey01.github.io/images/maskable_icon_x512.png"
        }
      },
      "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "https://yusukey01.github.io/Mathematics/Calculus/jacobian.html"
      },
      "about": [
        
        { "@type": "Thing", "name": "Calculus" },
        { "@type": "Thing", "name": "Optimization" }
        
      ],
      "educationalLevel": "university",
      "learningResourceType": "tutorial",
      "educationalUse": "instruction"
    }
    </script>
    
</head>

<body>
    <header>
    <nav class="navbar">
        <!-- Logo as Home Button -->
        <div class="logo-home">
            <a href="../../../index.html" 
               class="logo-link " 
               title="Home - Math-CS Compass">
                <img src="../../images/icon_x512.png" 
                     alt="Math-CS Compass Logo" 
                     class="nav-logo">
            </a>
        </div>

        <div class="search-container">
            <input type="text" id="search-input" placeholder="Search keywords...">
            <button id="search-button" aria-label="Search" title="Search website"><i class="fas fa-search"></i></button>
        </div>
        
        <div class="menu-toggle">
            <i class="fas fa-bars"></i>
        </div>
        
        <ul class="nav-links">
            <li><a href="../Linear_algebra/linear_algebra.html" >I - Linear Algebra to Algebraic Foundations</a></li>
            
            <li><a href="../Calculus/calculus.html" class="active">II - Calculus to Optimization & Analysis</a></li>
            
            <li><a href="../Probability/probability.html" >III - Probability & Statistics</a></li>
            
            <li><a href="../Discrete/discrete_math.html" >IV - Discrete Mathematics & Algorithms</a></li>
            
            <li><a href="../Machine_learning/ml.html" >V - Machine Learning</a></li>
        </ul>
    </nav>
</header>
    
    <!DOCTYPE html>
<html>
    <body>
        <!-- LearningResource Schema for Jacobian -->
        <script type="application/ld+json">
        {
        "@context": "https://schema.org",
        "@type": "LearningResource",
        "name": "The Derivative of f:ℝⁿ → ℝⁿ",
        "description": "Learn about Jacobian, chain rule, backpropagation",
        "learningResourceType": "lesson",
        "educationalUse": "instruction",
        "educationalLevel": "university",
        "interactivityType": "active",
        "author": {
            "@type": "Person",
            "name": "Yusuke Yokota",
            "jobTitle": "Mathematics & Computer Science Educator",
            "knowsAbout": [
            "Jacobian Matrix",
            "Chain Rule",
            "Backpropagation",
            "Vector Calculus",
            "Neural Networks",
            "Mathematical Analysis"
            ]
        },
        "publisher": {
            "@type": "Organization",
            "name": "MATH-CS COMPASS",
            "url": "https://yusukey01.github.io"
        },
        "about": [
            { "@type": "Thing", "name": "Jacobian Matrix" },
            { "@type": "Thing", "name": "Chain Rule" },
            { "@type": "Thing", "name": "Backpropagation" },
            { "@type": "Thing", "name": "Vector-valued Functions" },
            { "@type": "Thing", "name": "Multivariable Calculus" },
            { "@type": "Thing", "name": "Linear Operator" },
            { "@type": "Thing", "name": "Differential Notation" },
            { "@type": "Thing", "name": "Partial Derivatives" },
            { "@type": "Thing", "name": "Neural Networks" },
            { "@type": "Thing", "name": "Automatic Differentiation" },
            { "@type": "Thing", "name": "Reverse Mode Differentiation" },
            { "@type": "Thing", "name": "Forward Mode Differentiation" },
            { "@type": "Thing", "name": "Loss Function" },
            { "@type": "Thing", "name": "Gradient Computation" },
            { "@type": "Thing", "name": "Vector-Jacobian Product" }
        ],
        "teaches": [
            "Understanding Jacobian matrices for vector-valued functions",
            "Applying the chain rule in multivariable calculus",
            "Computing derivatives using differential notation",
            "Understanding backpropagation algorithm",
            "Implementing reverse mode automatic differentiation",
            "Analyzing computational efficiency of differentiation methods",
            "Working with neural network gradient computation",
            "Understanding matrix multiplication order in chain rule"
        ],
        "isPartOf": {
            "@type": "Course",
            "name": "Calculus to Optimization & Analysis",
            "description": "Explore optimization techniques and mathematical analysis with applications",
            "provider": {
            "@type": "Organization",
            "name": "MATH-CS COMPASS",
            "url": "https://yusukey01.github.io"
            },
            "instructor": {
            "@type": "Person",
            "name": "Yusuke Yokota",
            "jobTitle": "Mathematics & Computer Science Educator"
            },
            "courseCode": "II",
            "hasCourseInstance": {
            "@type": "CourseInstance",
            "courseMode": "online",
            "courseWorkload": "PT3H",
            "instructor": {
                "@type": "Person",
                "name": "Yusuke Yokota"
            }
            },
            "offers": {
            "@type": "Offer",
            "price": "0",
            "priceCurrency": "USD",
            "availability": "https://schema.org/InStock",
            "category": "free"
            }
        }
        }
        </script>
        <div class="hero-section">
            <h1 class="webpage-name"> The Derivative of \(f:\mathbb{R}^n \rightarrow \mathbb{R}^n\) 
            </h1>
        </div>

        
  <div class="quick-jump-container">
    <button type="button" id="quick-jump-toggle" class="quick-jump-toggle">
      <i class="fas fa-list"></i> <strong>Section II Navigation</strong>
    </button>
    <div id="quick-jump-menu" class="quick-jump-menu">
      <h3>Calculus to Optimization & Analysis</h3>
      <div class="quick-jump-links">
        
        
        <a href="calculus.html">← Back to Section II Overview</a>
        <hr style="margin: 8px 0; border: 1px solid #ddd;">
        
        
        <a href="linear_approximation.html" >Part 1: The Derivative of f:ℝⁿ → ℝ</a>
        <a href="jacobian.html" class="current-page">Part 2: The Derivative of f:ℝⁿ → ℝⁿ</a>
        <a href="matrix_cal.html" >Part 3: The Derivative of f:ℝⁿˣⁿ → ℝⁿˣⁿ</a>
        <a href="numerical_example1.html" >Part 4: Intro to Numerical Computation</a>
        <a href="det.html" >Part 5: Scalar Functions of Matrices</a>
        <a href="mvt.html" >Part 6: The Mean Value Theorem</a>
        <a href="gradient.html" >Part 7: Gradient Descent</a>
        <a href="newton.html" >Part 8: Newton's Method</a>
        <a href="constrained_opt.html" >Part 9: Constrained Optimization</a>
        <a href="riemann.html" >Part 10: Riemann Integration</a>
        <a href="measure.html" >Part 11: Measure Theory</a>
        <a href="lebesgue.html" >Part 12: Lebesgue Integration</a>
        <a href="duality.html" >Part 13: Duality in Optimization</a>
        <a href="fourier_series.html" >Part 14: Fourier Series</a>
        <a href="fourier_transform.html" >Part 15: Fourier Transform</a>
      </div>
    </div>
</div>


        <div class="topic-nav">
            <a href="#jacobian">Jacobian</a>
            <a href="#chain">Chain Rule</a>
            <a href="#backp">Backpropagation</a>
        </div>  

        <div class="container">     
            <section id="jacobian" class="section-content">
            <h2>Jacobian</h2>
            <p>
            Consider \(f(x) \in \mathbb{R}^m\), where \(x \in \mathbb{R}^n\). By the differential notation,
            \[
            df = f'(x)dx
            \]
            where \(df \in \mathbb{R}^m \, \), \(dx \in \mathbb{R}^n\) and here, the linear operator \(f'(x)\) is 
            the \(m \times n\) <strong>Jacobian matrix</strong> such that:
            \[
             J_{ij} = \frac{\partial f_i}{\partial x_j}.
            \]
            Here, \(J_{ij}\) represents the rate of change of the \(i\)-th output \(f_i\) with respect to the 
            \(j\)-th input \(x_j\).
            <br><br>
            For example, Let \(f(x) = \begin{bmatrix}f_1(x) \\ f_2(x) \end{bmatrix}\), 
            and \(x = \begin{bmatrix}x_1 \\ x_2 \end{bmatrix} \). Then
            \[
            \begin{align*}
            df &= \begin{bmatrix} \frac{\partial f_1}{\partial x_1} & \frac{\partial f_1}{\partial x_2} \\
                                 \frac{\partial f_2}{\partial x_1} & \frac{\partial f_2}{\partial x_2} 
                 \end{bmatrix} 
                 \begin{bmatrix} dx_1 \\ dx_2 \end{bmatrix} \\\\
              &=  \begin{bmatrix} \frac{\partial f_1}{\partial x_1}dx_1 + \frac{\partial f_1}{\partial x_2}dx_2 \\
                                 \frac{\partial f_2}{\partial x_1}dx_1 + \frac{\partial f_2}{\partial x_2}dx_2 
                 \end{bmatrix}.
            \end{align*}
            \]
            As you can see, the Jacobian matrix \(f'(x)\) acts as a linear operator that maps changes in \(x\) 
            encoded in \(dx\) to corresponding changes in \(f(x)\) encoded in \(df\). 
            <br>
            <strong>Note: Input & Output: vector \(\Longrightarrow\) First derivative: Jacobian matrix.</strong>
            <br><br>
            Instead of expressing the Jacobian component by component, it is often more convenient to use a 
            symbolic representation. 
            <br>
            For example, consider a trivial case: the matrix equation \(f(x) = Ax\) where \(A \in \mathbb{R}^{m \times n}\) 
            is a constant matrix.
            Then 
            \[
            \begin{align*}
            df &= f(x + dx) - f(x) \\\\
               &= A(x + dx) - Ax \\\\
               &= Adx \\\\
               &= f'(x)dx
            \end{align*}
            \]
            Thus \(f'(x) = A \) is the Jacobian matrix. 
            </p>
            </section>

            <section id="chain" class="section-content">
            <h2>Chain Rule</h2>
            <p>
            <div class="theorem">
                <span class="theorem-title">Theorem 1: Chain Rule</span> 
                Suppose \(f(x) = g(h(x))\) where both \(g\) and \(h\) are differentiable. Then
                \[
                \begin{align*}
                df &= f'(x)[dx]  \\\\
                   &= g'(h(x))[h'(x)[dx]] 
                \end{align*}
                \]
            </div>
            Intuitively, if we think about a higher dimensional case, it is clear that the chain rule is not commutative in general. 
            <br>
            For example, consider 
                \[
                x \in \mathbb{R}^n, \, h(x) \in \mathbb{R}^p, \, \text{and } g(h(x)) \in \mathbb{R}^m
                \] 
            Then the output must be the \(m \times n\) <strong>Jacobian matrix</strong> \(f'(x)\).
            <br>
            To construct this \(m \times n\) matrix, we must need the product of the \(m \times p\) Jacobian matrix \(g'(h(x))\) 
            and the \(p \times n\) Jacobian matrix h'(x) in this order. 
            </p>
            </section>

            <section id="backp" class="section-content">
            <h2>Backpropagation</h2>
            <p>
            The backpropagation or more generally, <a href="../Machine_learning/autodiff.html"><strong>reverse mode automatic differentiation</strong></a> is a widely used 
            algorithm in machine learning to train <a href="../Machine_learning/neural_networks.html"><strong>neural networks</strong></a>. 
            It computes gradients of the loss function with respect to the network's parameters(weights and biases) to minimize 
            the loss, improving model performance.
            <br>
            Backpropagation works by explicitly transmitting <strong>gradients</strong> layer by layer, maintaining the order 
            dictated by the <strong>chain rule</strong>. At each layer (or function), the gradient of the loss is computed with 
            respect to the output of the previous layer.
            <br><br>
            Consider a small neural network, which has three layers(functions): \(f_1, f_2, f_3\). This network represents 
            a composition of functions:
            \[
            L(x) = f_3(f_2(f_1(x)))
            \]
            where \(x \in \mathbb{R}^n\) represents the inputs(parameters) and \(L(x) \in \mathbb{R}^m\) is the 
            output often called the <strong>loss function</strong>. 
            <br><br>
            Suppose \(f_1 \in \mathbb{R}^p\), \(f_2 \in \mathbb{R}^q\), and \(f_3 \in \mathbb{R}^m\).
            <br>
            To compute the derivative of this function, we apply the chain rule:
            \[
            dL = f_3'f_2'f_1'
               = (m \times q)(q \times p)(p \times n)
            \]
            This is the multiplication of <strong>Jacobian matrices</strong> for each layer. 
            <br><br>
            In <strong>reverse</strong> mode, the algorithm compute this expression from the left \(f_3'\) backward.
            So, in the context of neural network structure, the backpropagation process appears to follow a "reverse" order, 
            starting from the output layer and moving backward through the network. However, from a mathematical perspective, 
            this corresponds to a standard left-to-right multiplication of Jacobian matrices.
            <br><br>
            This reverse order makes it computationally efficient for large scale neural networks with many inputs (\(n\)) 
            and a single output (\(m = 1\)).  
            \[
            \begin{align*}
            (f_3'f_2')f_1' &= [(1 \times q)(q \times p)](p \times n) \\\\
                           &= (1 \times p)(p \times n) \\\\
                           &= (1 \times n) \\\\
                           &= (\nabla L)^T
            \end{align*}
            \]
            This efficiency is due to backpropagation requiring only the computation of vector-Jacobian products, which are 
            less computationally expensive than full matrix multiplications. At each step, only the product of a row 
            vector(the transpose of a local gradient) with a Jacobian matrix is calculated.
            <br><br>
            In contrast,in <strong>forward mode</strong> differentiation requires full matrix multiplications.
            \[
            \begin{align*}
            f_3'(f_2'f_1') &= (1 \times q)[(q \times p)(p \times n)] \\\\
                           &= (1 \times q)(q \times n)  \\\\
                           &= (1 \times n)  \\\\
                           &= (\nabla L)^T
            \end{align*}
            \]
            </p>
            </section>
        </div>
        <script src="/js/main.js"></script> 
    </body>
</html>
    
    <footer>
    <div class="footer-content">
        <div class="footer-about">
            <h3>About MATH-CS COMPASS</h3>
            <p>Bridging the gap between pure mathematics and computer science applications.</p>
            <div class="contact-section">
                <strong><a href="https://docs.google.com/forms/d/e/1FAIpQLSf3_RNkegREvfCEIQznn6SCaf-iB5c-Doxn7Ymuu78Uf-PBjg/viewform" target="_blank">
                    Submit Your Questions & Feedback <i class="fas fa-external-link-alt"></i>
                </a></strong>
            </div>
        </div>
        <div class="footer-links">
            <h3>Quick Links</h3>
            <ul>
                
                <li><a href="../../../index.html">Home</a></li>
                <li>
                <a href="../Linear_algebra/linear_algebra.html">
                    <span class="nav-number">I</span>
                    <span>Linear Algebra to Algebraic Foundations</span>
                </a>
                </li>
                <li>
                <a href="../Calculus/calculus.html">
                    <span class="nav-number">II</span>
                    <span>Calculus to Optimization & Analysis</span>
                </a>
                </li>
                <li>
                <a href="../Probability/probability.html">
                    <span class="nav-number">III</span>
                    <span>Probability & Statistics</span>
                </a>
                </li>
                <li>
                <a href="../Discrete/discrete_math.html">
                    <span class="nav-number">IV</span>
                    <span>Discrete Mathematics & Algorithms</span>
                </a>
                </li>
                <li>
                <a href="../Machine_learning/ml.html">
                    <span class="nav-number">V</span>
                    <span>Machine Learning</span>
                </a>
                </li>
            </ul>
        </div>
        <div class="footer-social">
            <h3>Connect With Me</h3>
            <p>
                <a href="https://x.com/MathCSCompass?t=Zi8nyfzszeFw8QizbJwnNg&s=09" target="_blank">
                    <i class="fab fa-x-twitter"></i> X (Twitter)
                </a>
            </p>
            <p>
                <a href="https://www.facebook.com/share/15uPaBWgQN/" target="_blank">
                    <i class="fab fa-facebook"></i> Facebook
                </a>
            </p>
      
            <h3>MATH-CS COMPASS App</h3>
            <div class="app-download-footer">
                <a href="https://play.google.com/store/apps/details?id=io.github.yusukey01.twa" target="_blank" rel="noopener">
                    <img src="../../../images/google-play-badge.png" 
                        alt="Get it on Google Play" 
                        class="google-play-badge">
                </a>
            </div>
            
        </div>

    </div>
    <div class="footer-bottom">
        <p>&copy; 2024-2025 MATH-CS COMPASS. All rights reserved.</p>
    </div>
</footer>
    <script src="../../../js/main.js?v=1.0.2"></script>
    <script src="../../../js/search.js?v=1.0.2"></script>
    

    <button id="go-to-top-btn" class="go-to-top-btn" title="Go to top">
        <i class="fas fa-arrow-up"></i>
    </button>
    <script>
        // Register the service worker
        if ('serviceWorker' in navigator) {
          window.addEventListener('load', () => {
            navigator.serviceWorker.register('/js/service-worker.js')
              .then(registration => {
                console.log('ServiceWorker registration successful with scope: ', registration.scope);
              })
              .catch(error => {
                console.log('ServiceWorker registration failed: ', error);
              });
          });
        }
      </script>
</body>
</html>