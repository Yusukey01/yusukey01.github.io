<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    
    <title>Least-Squares - MATH-CS COMPASS</title>
    
    
    <script>
    window.MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']]
      }
    };
    </script>
    <script id="MathJax-script" defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="preload" href="https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Main-Regular.woff" as="font" type="font/woff2" crossorigin>
    <link rel="preload" href="https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Math-Italic.woff" as="font" type="font/woff2" crossorigin>
    
    
    

    <link rel="stylesheet" href="../../../css/styles.css?v=1.0.2">
  
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css">

    <link rel="dns-prefetch" href="https://cdn.jsdelivr.net">
    <link rel="dns-prefetch" href="https://cdnjs.cloudflare.com">
    
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Learn about least-squares problems and linear regression.">
    <meta name="google-site-verification" content="IieEujypYW38caEVe8ymjczouNl56yxAy27iPztcfRA" />
    

    <link rel="manifest" href="/manifest.json">

    <!-- Theme color for Chrome, Android, and other browsers -->
    <meta name="theme-color" content="#007bff">
  
    <!-- iOS specific tags -->
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <meta name="apple-mobile-web-app-title" content="MATH-CS COMPASS">
    <meta name="mobile-web-app-capable" content="yes">

    <!-- Icon references -->
    <link rel="icon" type="image/png" sizes="1280x1280" href="/images/maskable_icon_x1280.png?v=1.0.2">
    <link rel="icon" type="image/png" sizes="512x512" href="/images//maskable_icon_x512.png?v=1.0.2">
    
    <!-- Apple Touch Icons -->
    <link rel="apple-touch-icon" href="/images/maskable_icon_x1280.png?v=1.0.2">
    <link rel="apple-touch-icon" href="/images/maskable_icon_x512.png?v=1.0.2">

    <!-- For Windows -->
    <meta name="msapplication-TileImage" content="/images/test1.png?v=1.0.2">
    <meta name="msapplication-TileColor" content="#007bff">

    <!-- Schema.org Structured Data for AI Optimization -->
    

    <!-- Article Schema for content pages -->
    
    <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "Article",
      "headline": "Least-Squares",
      "description": "Learn about least-squares problems and linear regression.",
      "datePublished": "2024-01-01",
      "dateModified": "2025-01-01",
      "author": {
        "@type": "Person",
        "name": "Yusuke Yokota",
        "jobTitle": "Mathematics & Computer Science Educator",
        "knowsAbout": [
          "Linear Algebra",
          "Machine Learning",
          "Computer Science",
          "Mathematics"
        ]
      },
      "publisher": {
        "@type": "Organization",
        "name": "MATH-CS COMPASS",
        "url": "https://yusukey01.github.io",
        "logo": {
          "@type": "ImageObject",
          "url": "https://yusukey01.github.io/images/maskable_icon_x512.png"
        }
      },
      "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "https://yusukey01.github.io/Mathematics/Linear_algebra/leastsquares.html"
      },
      "about": [
        
        { "@type": "Thing", "name": "Linear Algebra" }
        
      ],
      "educationalLevel": "university",
      "learningResourceType": "tutorial",
      "educationalUse": "instruction"
    }
    </script>
    
</head>

<body>
    <header>
    <nav class="navbar">
        <!-- Logo as Home Button -->
        <div class="logo-home">
            <a href="../../../index.html" 
               class="logo-link " 
               title="Home - Math-CS Compass">
                <img src="../../images/icon_x512.png" 
                     alt="Math-CS Compass Logo" 
                     class="nav-logo">
            </a>
        </div>

        <div class="search-container">
            <input type="text" id="search-input" placeholder="Search keywords...">
            <button id="search-button" aria-label="Search" title="Search website"><i class="fas fa-search"></i></button>
        </div>
        
        <div class="menu-toggle">
            <i class="fas fa-bars"></i>
        </div>
        
        <ul class="nav-links">
            <li><a href="../Linear_algebra/linear_algebra.html" class="active">I - Linear Algebra to Algebraic Foundations</a></li>
            
            <li><a href="../Calculus/calculus.html" >II - Calculus to Optimization & Analysis</a></li>
            
            <li><a href="../Probability/probability.html" >III - Probability & Statistics</a></li>
            
            <li><a href="../Discrete/discrete_math.html" >IV - Discrete Mathematics & Algorithms</a></li>
            
            <li><a href="../Machine_learning/ml.html" >V - Machine Learning</a></li>
        </ul>
    </nav>
</header>
    
    <!DOCTYPE html>
<html>
    <body>
        <!-- LearningResource Schema for Content Pages -->
        <script type="application/ld+json">
        {
        "@context": "https://schema.org",
        "@type": "LearningResource",
        "name": "Least-Squares",
        "description": "Learn about least-squares problems and linear regression.",
        "learningResourceType": "lesson",
        "educationalUse": "instruction",
        "educationalLevel": "university",
        "interactivityType": "active",
        "author": {
            "@type": "Person",
            "name": "Yusuke Yokota",
            "jobTitle": "Mathematics & Computer Science Educator"
        },
        "publisher": {
            "@type": "Organization",
            "name": "MATH-CS COMPASS",
            "url": "https://yusukey01.github.io"
        },
        "about": [
            
            { "@type": "Thing", "name": "Least-Squares" },
            { "@type": "Thing", "name": "Linear Regression" },
            { "@type": "Thing", "name": "Normal Equations" },
            { "@type": "Thing", "name": "Orthogonal Projection" },
            { "@type": "Thing", "name": "QR Factorization" },
            { "@type": "Thing", "name": "Design Matrix" },
            { "@type": "Thing", "name": "Residuals" },
            { "@type": "Thing", "name": "Best-Fit Line" }
            
        ],
        "teaches": [
            "Mathematical concepts",
            "Practical applications",
            "Problem-solving techniques"
        ],
        "isPartOf": {
            "@type": "Course",
            "name": "Linear Algebra",
            "description": "Explore the foundations of Linear Algebra, covering key concepts such as linear equations, vector spaces, eigenvalues, orthogonality, least squares, and stochastic matrices",
            "provider": {
            "@type": "Organization",
            "name": "MATH-CS COMPASS",
            "url": "https://yusukey01.github.io"
            },
            "instructor": {
            "@type": "Person",
            "name": "Yusuke Yokota",
            "jobTitle": "Mathematics & Computer Science Educator"
            },
            "courseCode": "I",
            "hasCourseInstance": {
            "@type": "CourseInstance",
            "courseMode": "online",
            "courseWorkload": "PT2H",
            "instructor": {
                "@type": "Person",
                "name": "Yusuke Yokota"
            }
            },
            "offers": {
            "@type": "Offer",
            "price": "0",
            "priceCurrency": "USD",
            "availability": "https://schema.org/InStock",
            "category": "free"
            }
        }
        }
        </script>
        <!-- WebApplication Schema for Interactive Demos -->
        <script type="application/ld+json">
        {
        "@context": "https://schema.org",
        "@type": "WebApplication",
        "name": "Least-Squares Interactive Demo",
        "description": "Interactive demonstration of least-squares concepts",
        "applicationCategory": "EducationalApplication",
        "operatingSystem": "Web Browser",
        "url": "https://yusukey01.github.io/Mathematics/Linear_algebra/leastsquares.html",
        "author": {
            "@type": "Person",
            "name": "Yusuke Yokota"
        },
        "applicationSubCategory": "Mathematical Visualization",
        "featureList": [
            "Interactive mathematical visualization",
            "Real-time parameter adjustment",
            "Educational demonstrations"
        ],
        "isAccessibleForFree": true,
        "educationalUse": "instruction",
        "educationalLevel": "university"
        }
        </script>
        <div class="hero-section">
            <h1 class="webpage-name">Least-Squares
            </h1>
        </div>

        
  <div class="quick-jump-container">
    <button type="button" id="quick-jump-toggle" class="quick-jump-toggle">
      <i class="fas fa-list"></i> <strong>Section I Navigation</strong>
    </button>
    <div id="quick-jump-menu" class="quick-jump-menu">
      <h3>Linear Algebra to Algebraic Foundations</h3>
      <div class="quick-jump-links">

        
        <a href="linear_algebra.html">‚Üê Back to Section I Overview</a>
        <hr style="margin: 8px 0; border: 1px solid #ddd;">
        
        
        <a href="linear_equations.html" >Part 1: Linear Equations</a>
        <a href="linear_transformation.html" >Part 2: Linear Transformation</a>
        <a href="matrix_algebra.html" >Part 3: Matrix Algebra</a>
        <a href="determinants.html" >Part 4: Determinants</a>
        <a href="vectorspaces.html" >Part 5: Vector Spaces</a>
        <a href="eigenvectors.html" >Part 6: Eigenvalues & Eigenvectors</a>
        <a href="orthogonality.html" >Part 7: Orthogonality</a>
        <a href="leastsquares.html" class="current-page">Part 8: Least-Squares Problems</a>
        <a href="symmetry.html" >Part 9: Symmetry</a>
        <a href="trace.html" >Part 10: Trace and Norms</a>
        <a href="kronecker.html" >Part 11: Kronecker Product & Tensor</a>
        <a href="woodbury.html" >Part 12: Woodbury Matrix Identity</a>
        <a href="stochastic.html" >Part 13: Stochastic Matrix</a>
        <a href="graph_laplacian.html" >Part 14: Graph Laplacians and Spectral Methods</a>
      </div>
    </div>
</div>


        <div class="topic-nav">
            <a href="#lsp">Least-Squares Problems</a>
            <a href="#pinv">Moore-Penrose Pseudo-inverse</a>
            <a href="#lr">Linear Regression</a>
            <a href="#demo">Interactive Linear Regression Demo</a>
        </div> 

        <div class="container">  
           
            <section id="lsp" class="section-content">
                <h2>Least-Squares Problems</h2>
                <p>
                If \(A \in \mathbb{R}^{m \times n}\) and \(b \in \mathbb{R}^m\), a <strong>least-squares solution</strong> of
                \(Ax = b\) is an \(\hat{x} \in \mathbb{R}^n\) s.t.
                \[
                \forall x \in \mathbb{R}^n, \quad \| b - A\hat{x} \| \leq \| b - Ax \|. 
                \]
                We call the norm \(\| b - A\hat{x} \| \) the <strong>least-squares error</strong> of the approximation. 
                <br><br>
                In practice, it is possible that a solution to the equation \(Ax = b\) is required but does not exist
                (i.e., the system is inconsistent). In that case, we try to find an \(x\) that makes \(Ax\) as close to \(b\) 
                as possible. Let
                \[
                \hat{b} = \text{proj}_{\text{Col }A} \, b 
                \]
                Since \(\hat{b} \in \text{Col }A \,\), \(Ax = \hat{b}\) is consistent. 
                This \(\hat{b}\) is the closest point to \(b\) in \(\text{Col }A\).
                Then, there exists a least-squares solution \(\hat{x} \in \mathbb{R}^n\) s.t. \(A\hat{x} = \hat{b}\).
                <br><br>
                Since \(\hat{b}\) is the orthogonal projection of \(b\) onto \(\text{Col }A\), we can say that \(b - \hat{b} = b - A\hat{x}\) 
                is orthogonal to \(\text{Col }A \). (Note:\(b = A\hat{x} + (b - A\hat{x})\))
                <br><br>
                Let \(a_j\) be a column of \(A\), then 
                \[
                \begin{align*}
                a_j \cdot (b-A\hat{x}) = 0 
                        &\Longrightarrow a_j^T(b-A\hat{x}) = 0 \\\\
                        &\Longrightarrow A^T (b-A\hat{x}) = 0 
                \end{align*}
                \]
                and we obtain 
                \[A^TA\hat{x} = A^Tb\]
                So, the set of least-squares solutions of \(Ax =b\) must satisfy the <strong>normal equations</strong>
                \[A^TAx = A^Tb\]
                <br>
                If \(A^TA\) is invertible(= The columns of \(A\) are <strong>linearly independent</strong>.),
                \[\hat{x} = (A^TA)^{-1}A^Tb\]
                Finally, we also get the orthogonal projection of \(b\) onto the \(\text{Col } A\)
                \[
                \begin{align*}
                \hat{b} &= \text{proj}_{\text{Col }A} \, b  \\\\
                        &= A\hat{x} = A(A^TA)^{-1}A^Tb 
                \end{align*}
                \]
                In practice, computing \(A^TA\) makes relatively large errors in \(\hat{x}\). Instead, the <strong>QR factorization</strong>
                gives us more reliable result. Let \(A = QR\). Then 
                \[
                \begin{align*}
                \hat{x} &= (R^TQ^TQR)^{-1}(R^TQ^T)b  \\\\
                        &= (R^TR)^{-1}(R^TQ^T)b.  \tag{1}
                \end{align*}
                \]
                On the other hand, 
                \[
                \begin{align*}
                A\hat{x} = b &\Longrightarrow Q^TQR\hat{x} = Q^Tb  \\\\
                            &\Longrightarrow  R\hat{x} = Q^Tb  \\\\
                            &\Longrightarrow \hat{x} = R^{-1}Q^Tb \\\\
                \end{align*}
                \]
                Comparing with (1), \((R^TR)^{-1}R^T = R^{-1}\). 
                Thus, the unique least-squares solution can be generated by \(\hat{x} = R^{-1}Q^Tb\) but in practice, solving 
                \[R\hat{x} = Q^Tb\]
                by back-substitution is much faster than computing \(R^{-1}\).
                </p>
            </section>

            <section id="pinv" class="section-content">
                <h2>Moore-Penrose Pseudo-inverse (\(A^\dagger\))</h2>
                <p>
                In the previous section, we derived the normal equations \(A^TA\hat{x} = A^Tb\). If \(A^TA\) is invertible, the unique least-squares solution is \(\hat{x} = (A^TA)^{-1}A^Tb\).
                <br><br>
                However, this approach has two major problems in practice:
                <ol style="padding-left: 40px;">
                    <li><strong>Theoretical Failure:</strong> If the columns of \(A\) are linearly dependent (e.g., in an underdetermined system where \(n > m\), or due to collinear features), \(A^TA\) is singular and \((A^TA)^{-1}\) does not exist.</li>
                    
                    <li><strong>Practical Failure:</strong> Even if \(A^TA\) is invertible, it can be <strong>ill-conditioned</strong>. As defined in <a href="symmetry.html#svd">Part 9</a>, the <strong>condition number</strong> \(\kappa(A)\) measures a matrix's numerical sensitivity. The problem is that \(\kappa(A^TA) = \kappa(A)^2\). This squaring of the condition number is catastrophic. A moderately ill-conditioned problem (e.g., \(\kappa(A) = 10^5\)) becomes a hopelessly unstable one (\(\kappa(A^TA) = 10^{10}\)). This guarantees that solving for \(\hat{x}\) by computing \((A^TA)^{-1}\) will have a large numerical error.</li>
                </ol>
                <br>
                Both issues are resolved by using the <strong>Moore-Penrose Pseudo-inverse</strong>, denoted \(A^\dagger\).
                </p>

                <p>
                For any matrix \(A\), the Moore-Penrose Pseudo-inverse \(A^\dagger\) is the <strong>unique</strong> matrix that satisfies the following four algebraic conditions:
                <ol style="padding-left: 40px;">
                    <li>\(A A^\dagger A = A\)</li>
                    <li>\(A^\dagger A A^\dagger = A^\dagger\)</li>
                    <li>\((A A^\dagger)^T = A A^\dagger\) &nbsp;&nbsp;&nbsp; (The projection onto Col(A) is symmetric)</li>
                    <li>\((A^\dagger A)^T = A^\dagger A\) &nbsp;&nbsp;&nbsp; (The projection onto Row(A) is symmetric)</li>
                </ol>
                </p>
                <h3>The Minimum-Norm Least-Squares Solution</h3>
                <p>
                The reason this unique matrix is so powerful is that the vector \(\hat{x} = A^\dagger b\) is <em>always</em> the <strong>unique minimum-norm least-squares solution</strong> to \(Ax=b\).
                <br><br>
                This means \(\hat{x}\) simultaneously satisfies two conditions:
                <ul style="padding-left: 40px;">
                    <li><strong>Least-Squares:</strong> It minimizes the error \(\|Ax - b\|^2\).</li>
                    <li><strong>Minimum-Norm:</strong> If there are multiple solutions that minimize this error (i.e., the system is underdetermined), \(\hat{x}\) is the <em>only</em> solution that also has the smallest Euclidean norm \(\|\hat{x}\|\).</li>
                </ul>
                <br>
                In machine learning, this is incredibly powerful. For an underdetermined system (e.g., more features than data points), the minimum-norm solution is the "simplest" one, which is a form of implicit regularization.
                <br><br>
                The most stable way to compute \(A^\dagger\) is via the <strong>Singular Value Decomposition (SVD)</strong>, which we will define in <a href="symmetry.html#pinv_svd">Part 9</a>.
                </p>
            </section>

            <section id="lr" class="section-content">
                <h2>Linear Regression</h2>
                <p>
                Given a set of observed data points \(\{(x_i, y_i)\}_{i = 1}^{n}\) where \(x_i \in \mathbb{R}^d, \quad y_i \in \mathbb{R}\),
                we assume that the given data can be explained by the <strong>linear model</strong>:
                \[Y = X\beta + \epsilon \]
                where 
                <br>
                <ul style="padding-left: 40px;">
                    <li><strong>design matrix</strong>: \(X \in \mathbb{R}^{n \times d}\)</li>
                    <li><strong>parameter(weight) vector</strong>: \(\beta \in \mathbb{R}^d\)</li>
                    <li><strong>observation vector</strong>: \(Y \in \mathbb{R}^n\)</li>
                    <li><strong>residual vector</strong>:  \(\epsilon = Y - X\beta\) </li>
                </ul>
                <br>
                The dimension \(d\) is the number of features, and \(n\) is the number of data points. 
                The residual \(\epsilon\) is the "difference" between each observed value \(y_i\) and its corresponding predicted \(y\) value.
                The <strong>least-squares hyperplane</strong> represents the set of predicted \(y\) values based on "estimated" parameters(weights) \(\hat{\beta}\), 
                and it must satisfy the normal equations:
                \[X^TX\hat{\beta} = X^TY\]
                <br><br>

                Note: the linear model is linear in terms of <strong>parameters \(\beta \, \)</strong>, not \(X\). We can choose any non-linear transformation
                for each \(x_i\). For example, \(y = \beta_0 + \beta_1 x^2 + \beta_2 \sin(2\pi x)\) is still a linear model. Thus, \(Y\) is modeled as a 
                <strong>linear combination</strong> of features(predictors) \(X\) with respect to the coefficients(weights) \(\beta\).

                <br><br>
                There are lots of details we should cover in this topic. I leave it for 
                <a href="../Probability/linear_regression.html">Section III-Probability & Statistics - Linear Regression</a>.
                <br>
                Finally, \(X^TX\) is called a <strong>symmetric matrix</strong>. We will learn about the special matrices 
                in the next part. 
                </p>
            </section>


            <section id="demo" class="section-content">
                <h2>Interactive Linear Regression Demo</h2>
                <p>
                    Linear regression finds the best-fitting line or curve for a set of data points by minimizing the sum of squared differences between observed and predicted values. This interactive demo lets you explore how polynomial regression works with different datasets.
                </p>
                <p>
                    Try adding your own data points by clicking on the plot, or use the example datasets to see how different polynomial degrees fit various patterns. The demo visually demonstrates key concepts from least-squares theory and the normal equations.
                </p>
                
                <!-- Here, the visualizer will be rendered -->
                <div id="linear-regression-visualizer"></div>
            </section>
           
        </div>    
        
        <script src="/js/main.js"></script>
        <!-- Linear Regression Visualizer -->
        <script src="/js/linear_regression_visualizer2.js"></script>
          
    </body>
</html>
    
    <footer>
    <div class="footer-content">
        <div class="footer-about">
            <h3>About MATH-CS COMPASS</h3>
            <p>Bridging the gap between pure mathematics and computer science applications.</p>
            <div class="contact-section">
                <strong><a href="https://docs.google.com/forms/d/e/1FAIpQLSf3_RNkegREvfCEIQznn6SCaf-iB5c-Doxn7Ymuu78Uf-PBjg/viewform" target="_blank">
                    Submit Your Questions & Feedback <i class="fas fa-external-link-alt"></i>
                </a></strong>
            </div>
        </div>
        <div class="footer-links">
            <h3>Quick Links</h3>
            <ul>
                
                <li><a href="../../../index.html">Home</a></li>
                <li>
                <a href="../Linear_algebra/linear_algebra.html">
                    <span class="nav-number">I</span>
                    <span>Linear Algebra to Algebraic Foundations</span>
                </a>
                </li>
                <li>
                <a href="../Calculus/calculus.html">
                    <span class="nav-number">II</span>
                    <span>Calculus to Optimization & Analysis</span>
                </a>
                </li>
                <li>
                <a href="../Probability/probability.html">
                    <span class="nav-number">III</span>
                    <span>Probability & Statistics</span>
                </a>
                </li>
                <li>
                <a href="../Discrete/discrete_math.html">
                    <span class="nav-number">IV</span>
                    <span>Discrete Mathematics & Algorithms</span>
                </a>
                </li>
                <li>
                <a href="../Machine_learning/ml.html">
                    <span class="nav-number">V</span>
                    <span>Machine Learning</span>
                </a>
                </li>
            </ul>
        </div>
        <div class="footer-social">
            <h3>Connect With Me</h3>
            <p>
                <a href="https://x.com/MathCSCompass?t=Zi8nyfzszeFw8QizbJwnNg&s=09" target="_blank">
                    <i class="fab fa-x-twitter"></i> X (Twitter)
                </a>
            </p>
            <p>
                <a href="https://www.facebook.com/share/15uPaBWgQN/" target="_blank">
                    <i class="fab fa-facebook"></i> Facebook
                </a>
            </p>
      
            <h3>MATH-CS COMPASS App</h3>
            <div class="app-download-footer">
                <a href="https://play.google.com/store/apps/details?id=io.github.yusukey01.twa" target="_blank" rel="noopener">
                    <img src="../../../images/google-play-badge.png" 
                        alt="Get it on Google Play" 
                        class="google-play-badge">
                </a>
            </div>
            
        </div>

    </div>
    <div class="footer-bottom">
        <p>&copy; 2024-2025 MATH-CS COMPASS. All rights reserved.</p>
    </div>
</footer>
    <script src="../../../js/main.js?v=1.0.2"></script>
    <script src="../../../js/search.js?v=1.0.2"></script>
    

    <button id="go-to-top-btn" class="go-to-top-btn" title="Go to top">
        <i class="fas fa-arrow-up"></i>
    </button>
    <script>
        // Register the service worker
        if ('serviceWorker' in navigator) {
          window.addEventListener('load', () => {
            navigator.serviceWorker.register('/js/service-worker.js')
              .then(registration => {
                console.log('ServiceWorker registration successful with scope: ', registration.scope);
              })
              .catch(error => {
                console.log('ServiceWorker registration failed: ', error);
              });
          });
        }
      </script>
</body>
</html>