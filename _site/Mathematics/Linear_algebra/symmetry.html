<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    
    <title>Symmetry - MATH-CS COMPASS</title>
    
    
    <script>
    window.MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']]
      }
    };
    </script>
    <script id="MathJax-script" defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="preload" href="https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Main-Regular.woff" as="font" type="font/woff2" crossorigin>
    <link rel="preload" href="https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Math-Italic.woff" as="font" type="font/woff2" crossorigin>
    
    
    

    <link rel="stylesheet" href="../../../css/styles.css?v=1.0.2">
  
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css">

    <link rel="dns-prefetch" href="https://cdn.jsdelivr.net">
    <link rel="dns-prefetch" href="https://cdnjs.cloudflare.com">
    
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Learn about symmetric matrices, quadratic forms, and Singular Value Decomposition(SVD).">
    <meta name="google-site-verification" content="IieEujypYW38caEVe8ymjczouNl56yxAy27iPztcfRA" />
    

    <link rel="manifest" href="/manifest.json">

    <!-- Theme color for Chrome, Android, and other browsers -->
    <meta name="theme-color" content="#007bff">
  
    <!-- iOS specific tags -->
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <meta name="apple-mobile-web-app-title" content="MATH-CS COMPASS">
    <meta name="mobile-web-app-capable" content="yes">

    <!-- Icon references -->
    <link rel="icon" type="image/png" sizes="1280x1280" href="/images/maskable_icon_x1280.png?v=1.0.2">
    <link rel="icon" type="image/png" sizes="512x512" href="/images//maskable_icon_x512.png?v=1.0.2">
    
    <!-- Apple Touch Icons -->
    <link rel="apple-touch-icon" href="/images/maskable_icon_x1280.png?v=1.0.2">
    <link rel="apple-touch-icon" href="/images/maskable_icon_x512.png?v=1.0.2">

    <!-- For Windows -->
    <meta name="msapplication-TileImage" content="/images/test1.png?v=1.0.2">
    <meta name="msapplication-TileColor" content="#007bff">

    <!-- Schema.org Structured Data for AI Optimization -->
    

    <!-- Article Schema for content pages -->
    
    <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "Article",
      "headline": "Symmetry",
      "description": "Learn about symmetric matrices, quadratic forms, and Singular Value Decomposition(SVD).",
      "datePublished": "2024-01-01",
      "dateModified": "2025-01-01",
      "author": {
        "@type": "Person",
        "name": "Yusuke Yokota",
        "jobTitle": "Mathematics & Computer Science Educator",
        "knowsAbout": [
          "Linear Algebra",
          "Machine Learning",
          "Computer Science",
          "Mathematics"
        ]
      },
      "publisher": {
        "@type": "Organization",
        "name": "MATH-CS COMPASS",
        "url": "https://math-cs-compass.com",
        "logo": {
          "@type": "ImageObject",
          "url": "https://math-cs-compass.com/images/maskable_icon_x512.png"
        }
      },
      "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "https://math-cs-compass.com/Mathematics/Linear_algebra/symmetry.html"
      },
      "about": [
        
        { "@type": "Thing", "name": "Linear Algebra" }
        
      ],
      "educationalLevel": "university",
      "learningResourceType": "tutorial",
      "educationalUse": "instruction"
    }
    </script>
    
</head>

<body>
    <header>
    <nav class="navbar">
        <!-- Logo as Home Button -->
        <div class="logo-home">
            <a href="../../../index.html" 
               class="logo-link " 
               title="Home - Math-CS Compass">
                <img src="../../images/icon_x512.png" 
                     alt="Math-CS Compass Logo" 
                     class="nav-logo">
            </a>
        </div>

        <div class="search-container">
            <input type="text" id="search-input" placeholder="Search keywords...">
            <button id="search-button" aria-label="Search" title="Search website"><i class="fas fa-search"></i></button>
        </div>
        
        <div class="menu-toggle">
            <i class="fas fa-bars"></i>
        </div>
        
        <ul class="nav-links">
            <li><a href="../Linear_algebra/linear_algebra.html" class="active">I - Linear Algebra to Algebraic Foundations</a></li>
            
            <li><a href="../Calculus/calculus.html" >II - Calculus to Optimization & Analysis</a></li>
            
            <li><a href="../Probability/probability.html" >III - Probability & Statistics</a></li>
            
            <li><a href="../Discrete/discrete_math.html" >IV - Discrete Mathematics & Algorithms</a></li>
            
            <li><a href="../Machine_learning/ml.html" >V - Machine Learning</a></li>
        </ul>
    </nav>
</header>
    
    <!DOCTYPE html>
<html>
    <body>
        <!-- LearningResource Schema for Content Pages -->
        <script type="application/ld+json">
        {
        "@context": "https://schema.org",
        "@type": "LearningResource",
        "name": "Symmetry",
        "description": "Learn about symmetric matrices, quadratic forms, and Singular Value Decomposition(SVD).",
        "learningResourceType": "lesson",
        "educationalUse": "instruction",
        "educationalLevel": "university",
        "interactivityType": "active",
        "author": {
            "@type": "Person",
            "name": "Yusuke Yokota",
            "jobTitle": "Mathematics & Computer Science Educator"
        },
        "publisher": {
            "@type": "Organization",
            "name": "MATH-CS COMPASS",
            "url": "https://math-cs-compass.com"
        },
        "about": [
            
            { "@type": "Thing", "name": "Symmetric Matrices" },
            { "@type": "Thing", "name": "Quadratic Forms" },
            { "@type": "Thing", "name": "Singular Value Decomposition" },
            { "@type": "Thing", "name": "SVD" },
            { "@type": "Thing", "name": "Orthogonal Diagonalization" },
            { "@type": "Thing", "name": "Positive Definite" },
            { "@type": "Thing", "name": "Fundamental Subspaces" }
            
        ],
        "teaches": [
            "Mathematical concepts",
            "Practical applications",
            "Problem-solving techniques"
        ],
        "isPartOf": {
            "@type": "Course",
            "name": "Linear Algebra",
            "description": "Explore the foundations of Linear Algebra, covering key concepts such as linear equations, vector spaces, eigenvalues, orthogonality, least squares, and stochastic matrices",
            "provider": {
            "@type": "Organization",
            "name": "MATH-CS COMPASS",
            "url": "https://math-cs-compass.com"
            },
            "instructor": {
            "@type": "Person",
            "name": "Yusuke Yokota",
            "jobTitle": "Mathematics & Computer Science Educator"
            },
            "courseCode": "I",
            "hasCourseInstance": {
            "@type": "CourseInstance",
            "courseMode": "online",
            "courseWorkload": "PT2H",
            "instructor": {
                "@type": "Person",
                "name": "Yusuke Yokota"
            }
            },
            "offers": {
            "@type": "Offer",
            "price": "0",
            "priceCurrency": "USD",
            "availability": "https://schema.org/InStock",
            "category": "free"
            }
        }
        }
        </script>
        <!-- WebApplication Schema for Interactive Demos -->
        <script type="application/ld+json">
        {
        "@context": "https://schema.org",
        "@type": "WebApplication",
        "name": "Symmetry Interactive Demo",
        "description": "Interactive demonstration of symmetry concepts",
        "applicationCategory": "EducationalApplication",
        "operatingSystem": "Web Browser",
        "url": "https://math-cs-compass.com/Mathematics/Linear_algebra/symmetry.html",
        "author": {
            "@type": "Person",
            "name": "Yusuke Yokota"
        },
        "applicationSubCategory": "Mathematical Visualization",
        "featureList": [
            "Interactive mathematical visualization",
            "Real-time parameter adjustment",
            "Educational demonstrations"
        ],
        "isAccessibleForFree": true,
        "educationalUse": "instruction",
        "educationalLevel": "university"
        }
        </script>
        <div class="hero-section">
            <h1 class="webpage-name">Symmetry
            </h1>
        </div>

        
  <div class="quick-jump-container">
    <button type="button" id="quick-jump-toggle" class="quick-jump-toggle">
      <i class="fas fa-list"></i> <strong>Section I Navigation</strong>
    </button>
    <div id="quick-jump-menu" class="quick-jump-menu">
      <h3>Linear Algebra to Algebraic Foundations</h3>
      <div class="quick-jump-links">

        
        <a href="linear_algebra.html">← Back to Section I Overview</a>
        <hr style="margin: 8px 0; border: 1px solid #ddd;">
        
        
        <a href="linear_equations.html" >Part 1: Linear Equations</a>
        <a href="linear_transformation.html" >Part 2: Linear Transformation</a>
        <a href="matrix_algebra.html" >Part 3: Matrix Algebra</a>
        <a href="determinants.html" >Part 4: Determinants</a>
        <a href="vectorspaces.html" >Part 5: Vector Spaces</a>
        <a href="eigenvectors.html" >Part 6: Eigenvalues & Eigenvectors</a>
        <a href="orthogonality.html" >Part 7: Orthogonality</a>
        <a href="leastsquares.html" >Part 8: Least-Squares Problems</a>
        <a href="symmetry.html" class="current-page">Part 9: Symmetry</a>
        <a href="trace.html" >Part 10: Trace and Norms</a>
        <a href="kronecker.html" >Part 11: Kronecker Product & Tensor</a>
        <a href="woodbury.html" >Part 12: Woodbury Matrix Identity</a>
        <a href="stochastic.html" >Part 13: Stochastic Matrix</a>
        <a href="graph_laplacian.html" >Part 14: Graph Laplacians and Spectral Methods</a>
      </div>
    </div>
</div>


        <div class="topic-nav">
            <a href="#sym">Symmetric Matrices</a>
            <a href="#qf">Quadratic Forms</a>
            <a href="#svd">Singular Value Decomposition(SVD)</a>
            <a href="#pinv_svd">Pseudo-inverse via SVD</a>
            <a href="#f_sub">SVD with Fundamental Subspaces</a>
            <a href="#svd-demo">Interactive SVD Visualization</a>
        </div> 

        <div class="container">  
           
            <section id="sym" class="section-content">
            <h2>Symmetric Matrices</h2>
            <p>
            A <strong>symmetric matrix</strong> is a square matrix \(A\) such that
            \[A^T= A\]
            For example, in the context of least-squares solutions, \(\hat{\beta}\) satisfies \(X^TX\beta = X^TY\).
            In this expression, \(X^TX\) is always symmetric. To confirm, let \(X\) be an \(m \times n\) matrix. Then
            \[(X^TX)^T = X^T(X^T)^T = X^TX.\]
            <br>
            An \(n \times n\) matrix \(A\) is said to be <strong>orthogonally diagonalizable</strong> if there exists 
            an orthogonal matrix \(P\) and a diagonal matrix \(D\) such that:
            \[
            A = PDP^T = PDP^{-1}
            \]
            For this diagonalization, \(A\) must have \(n\) linearly independent and orthonormal eigenvectors.
            Importantly, in such cases, \(A\) is symmetric because
            \[
            \begin{align*}
            A^T = (PDP^T)^T &= (P^T)^T D^T P^T \\\\
                            &= PDP^T \\\\
                            &= A.
            \end{align*}
            \]
            The converse is also always true. If \(A\) is symmetric, it is guaranteed to be orthogonally diagonalizable.
            <div class="theorem">
                <span class="theorem-title">Theorem 1:</span>
                An \(n \times n\) matrix \(A\) is orthogonally diagonalizable if and only if \(A\) is
                a symmetric matrix. 
            </div>
            On <a href="eigenvectors.html"><strong>Eigenvalues & Eigenvectors</strong></a>, we considered the example:
            \[
            \begin{align*}
            A &= \begin{bmatrix} 4 & 1 & 1\\ 1 & 4 & 1 \\ 1 & 1 & 4 \\ \end{bmatrix} \\\\
              &= PDP^{-1} \\\\
              &= \begin{bmatrix} -1 & -1 & 1 \\ 0 & 1 & 1 \\ 1 & 0 & 1 \\ \end{bmatrix}
               \begin{bmatrix} 3 & 0 & 0 \\ 0 & 3 & 0 \\ 0 & 0 & 6 \\ \end{bmatrix} 
               \begin{bmatrix} \frac{-1}{3} & \frac{-1}{3} & \frac{2}{3} \\
                               \frac{-1}{3} & \frac{2}{3} & \frac{-1}{3} \\
                               \frac{1}{3} & \frac{1}{3} & \frac{1}{3} \end{bmatrix}.
            \end{align*}
            \] 
            Since, \(A\) is symmetric, it must be orthogonally diagonalizable. To ensure \(P\) is orthonormal, we need 
            orthonormal eigenvectors. This can be achieved using the Gram-Schmidt algorithm on the columns of \(P\).
            Since only \(v_1 \cdot v_2 \neq = 0\) needs adjustment, 
            \[
            \begin{align*}
            v'_2   &= v_2 - \frac{v_2 \cdot v_1}{v_1 \cdot v_1}v_1 \\\\
                   &= \begin{bmatrix} -1\\ 1\\ 0\\ \end{bmatrix} - \frac{1}{2}\begin{bmatrix} -1\\ 0\\ 1\\ \end{bmatrix} \\\\
                   &= \begin{bmatrix} \frac{-1}{2}\\ 1 \\ \frac{-1}{2}  \end{bmatrix}.
            \end{align*}
            \]
            After normalization, the orthonormal eigenvectors are:
            \[ 
            u_1 = \begin{bmatrix} \frac{-1}{\sqrt{2}} \\ 0\\ \frac{1}{\sqrt{2}} \\ \end{bmatrix}
            ,\,
            u_2 = \begin{bmatrix} \frac{-1}{\sqrt{6}}\\ \frac{2}{\sqrt{6}}\\ \frac{-1}{\sqrt{6}}\\ \end{bmatrix},
            \, \text{and }
            u_3 = \begin{bmatrix} \frac{1}{\sqrt{3}} \\  \frac{1}{\sqrt{3}} \\ \frac{1}{\sqrt{3}} \\ \end{bmatrix}
            \]
            Therefore, 
            \[
            \begin{align*}
            A &= PDP^T \\\\
              &= \begin{bmatrix} \frac{-1}{\sqrt{2}} & \frac{-1}{\sqrt{6}} & \frac{1}{\sqrt{3}} \\
                                 0                   & \frac{2}{\sqrt{6}}  & \frac{1}{\sqrt{3}}\\
                                 \frac{1}{\sqrt{2}}  & \frac{-1}{\sqrt{6}} & \frac{1}{\sqrt{3}} \end{bmatrix}
                \begin{bmatrix} 3 & 0 & 0 \\ 0 & 3 & 0 \\ 0 & 0 & 6 \\ \end{bmatrix} 
                \begin{bmatrix} \frac{-1}{\sqrt{2}}   & 0                   & \frac{1}{\sqrt{2}} \\
                                \frac{-1}{\sqrt{6}}   & \frac{2}{\sqrt{6}}  & \frac{-1}{\sqrt{6}}  \\
                                \frac{1}{\sqrt{3}}    & \frac{1}{\sqrt{3}}  & \frac{1}{\sqrt{3}} \end{bmatrix} \\\\
               &= PDP^{-1}
            \end{align*}
            \] 
            </p>
            </section>

            <section id="qf" class="section-content">
            <h2>Quadratic Forms</h2>
            <p>
            A <strong>quadratic form</strong> on \(\mathbb{R}^n\) is a function
            \[
            Q(x) = x^TAx \, \in \mathbb{R}
            \]
            where \(A\) is an \(n\times n\) <strong>symmetric</strong> matrix. 
            <br><br>
            Let's use our symmetric matrix again:
            \[
            A = \begin{bmatrix} 4 & 1 & 1\\ 1 & 4 & 1 \\ 1 & 1 & 4 \\ \end{bmatrix}
            \]
            Then:
            \[
            \begin{align*}
            Q(x) &=  x^TAx \\\\
                 &= 4x_1^2 +4 x_2^2 + 4x_3^2 + 2x_1x_2 + 2x_2x_3 + 2x_3x_1
            \end{align*}
            \]
            <br>
            We can transform this quadratic form into one with no cross-product terms.
            <br>
            Let \(x = Py\) be a change of variable where \(P\) is an invertible matrix and \(y\) is a
            new variable in \(\mathbb{R}^n\). Since \(A\) is symmetric, by Theorem 1, \(A\) is
            orthogonally diagonalizable. Thus
            \[
            \begin{align*}
            x^TAx &= (Py)^TA(Py) \\\\
                  &= y^TP^TAPy  \\\\
                  &= y^T(P^TAP)y \\\\
                  &= y^T(P^TPDP^TP)y \\\\
                  &= y^TDy
            \end{align*}
            \]
            where \(y = P^{-1}x = P^Tx\).
            <br>
            Since \(P^T A P = D\), where \(D\) is a diagonal matrix, we get:
            \[
            \begin{align*}
            x^T A x &= y^T D y \\\\
                    &= \lambda_1 y_1^2 + \lambda_2 y_2^2 + \cdots + \lambda_n y_n^2
            \end{align*}
            \]
            where \(\lambda_1, \lambda_2, \ldots, \lambda_n\) are the eigenvalues of \(A\).
            <br>
            For our example, \(Q(x)\) becomes:
            \[
            3y_1^2 + 3y_2^2 + 6 y_3^2
            \]
            <br>
            Thus, the eigenvalues of \(A\) are directly connected to its quadratic form. First, 
            we define the classification of quadratic forms.
            <br><br>
            A quadratic form \(Q(x)\) is classified as:
            <ol class="centered-list" style="padding-left: 40px;">
                <li><strong>Positive definite</strong>:<br>
                    \(\qquad \qquad A \succ 0\) if \(\, \forall x \neq 0, \, Q(x) > 0\). </li>
                <li><strong>Positive semi-definite</strong>:<br>
                    \(\qquad \qquad A \succeq 0\) if \(\, \forall x, \, Q(x) \geq 0\). </li>
                <li><strong>Negative definite</strong>:<br>
                    \(\qquad \qquad A \prec 0\) if \(\, \forall x \neq 0, \, Q(x) < 0\).</li>
                <li><strong>Negative semi-definite</strong>:<br>
                    \(\qquad \qquad A \preceq 0\) if \(\, \forall x, \, Q(x) \leq 0\).</li>
                <li><strong>Indefinite</strong>: <br>
                    \(\qquad \qquad\) otherwise. </li>
            </ol>
            <div class="theorem">
                <span class="theorem-title">Theorem 2:</span>
                Let \(A\) be an \(n \times n\) symmetric matrix. Then a quadratic form \(x^TAx\) is
                <br>
                <ol style="padding-left: 40px;">
                    <li>positive definite iff the eigenvalues of \(A\) are all positive. </li>
                    <li>negative definite iff  the eigenvalues of \(A\) are all negative.</li>
                </ol>
            </div>
            As mentined above, an orthogonal change of variable \(x= Py\) transforms \(x^TAx\) into 
            \[
            Q(x) = y^T D y = \lambda_1 y_1^2 + \cdots + \lambda_n y_n^2.
            \]
            where \(\lambda_1, \ldots, \lambda_n\) are the eigenvalues of \(A\).
            <br>
            There is a one-to-one correspondence between all \(x \neq 0\) and \(y \neq 0\) because \(P\) is an 
            invertible matrix. So, \(Q(x)\) is determined by the signs of eigenvalues(or <strong>spectrum</strong>) of \(A\). 
            The columns of \(P\) are called the <strong>principal axes</strong> of the quadratic form \(x^TAx\).
            <br><br>
            Note: To check whether a matrix \(A\) is positive definite, it is effective to verify the existence of a 
            <a href="../Probability/mvn.html.html"><strong>Cholesky factorization</strong></a> \(A = R^TR\) where \(R\) 
            is an upper triangular matrix with positive diagonal entries. This factorization is a modified version of 
            the <strong>LU factorization</strong>. 
            </p>
            </section>

            <section id="svd" class="section-content">
            <h2>Singular Value Decomposition(SVD)</h2>
            <p>
            Let \(A\) be an \(m \times n\) matrix. The symmetric matrix \(A^TA\) is orthogonally diagonalizable. 
            Let \(\{v_1, \cdots, v_n\}\) be an orthonormal basis for \(\mathbb{R}^n\) consisting of eigenvectors 
            of \(A^TA\) with corresponding eigenvalues \(\lambda_1, \cdots, \lambda_n\). 
            <br>
            For \(1 \leq i \leq n\), the following holds:
            \[
            \begin{align*}
            \| Av_i \|^2 &= (Av_i)^TAv_i = v_i^T(A^TAv_i)  \\\\
                         &= v_i^T(\lambda_iv_i)  \\\\
                         &= \lambda_i.
            \end{align*}
            \]
            Since \(\| Av_i \|^2 \geq 0\), we conclude that \(\lambda_i \geq 0 \).
            <br><br>
            We assume the eigenvalues are arranged in descending order:
            \[
            \lambda_1 \geq \lambda_2 \geq \cdots \geq \lambda_n
            \]
            <br>
            For \(1 \leq i \leq n\), the <strong>singular values</strong> of \(A\) are defined as 
            \[
            \sigma_i = \sqrt{\lambda_i} = \| Av_i \| \geq 0.
            \]
            If \(A\) has \(r\) nonzero singular values, then \(\{Av_1, \cdots, Av_r\}\) forms an orthogonal basis 
            for \(\text{Col }A\) and 
            \[\text{rank } A = \dim \text{ Col }A = r.
            \]
            <div class="theorem">
                <span class="theorem-title">Theorem 3: Singular Value Decomposition</span>
                Let \(A\) be an \(m \times n\) matrix with rank \(r\). Then there exist an \(m \times n\)
                matrix \(\Sigma = \begin{bmatrix} D & 0 \\ 0 & 0 \\ \end{bmatrix}\) where \(D\) is a \(r \times r\)
                diagonal matrix with the first \(r\) <strong>nonzero</strong> singular values of \(A\), \(\sigma_1 \geq \sigma_2 \geq \cdots \geq \sigma_r > 0\)
                and there exist an \(m \times m\) orthogonal matrix \(U\), and an \(n \times n\) orthogonal matrix \(V\) such that
                \[A = U\Sigma V^T\]    
            </div>
            <div class="proof">
                <span class="proof-title">Proof:</span>
                Normalize orthogonal basis for \(\text{Col }A\) \(\{Av_1, \cdots, Av_r\}\), then we obtain an orthonormal basis
                \(\{u_i, \cdots, u_r\}\), where
                \[
                u_i = \frac{1}{\| Av_i \|}Av_i = \frac{1}{\sigma_i}Av_i \quad 1 \leq i \leq r
                \]
                or, 
                \[
                Av_i = \sigma_i u_i \tag{1}
                \]
                We extend \(\{u_i, \cdots, u_r\}\) to an orthonormal basis for \(\mathbb{R}^m\), \(\{u_i, \cdots, u_r, u_{r+1}, \cdots, u_m\}\).
                <br>Let \(U = \begin{bmatrix}u_1 & \cdots & u_m \end{bmatrix}\) and \(V = \begin{bmatrix}v_1 & \cdots & v_n \end{bmatrix}\).
                <br>Clearly, by construction, \(U\) and \(V\) are orthogonal matrices. Also, by Equation (1),
                \[AV = \begin{bmatrix} \sigma_1u_1 & \cdots & \sigma_ru_r & 0 & \cdots & 0 \end{bmatrix}.\]
                Let \(D\) be a diagonal matrix with diagonal entries \(\sigma_1, \cdots, \sigma_r\) and 
                let \(\Sigma =\begin{bmatrix} D & 0 \\ 0 & 0 \\ \end{bmatrix}\). Then
                \[
                \begin{align*}
                U\Sigma &= \begin{bmatrix}\sigma_1u_1 & \cdots & \sigma_ru_r & 0 & \cdots & 0\end{bmatrix} \\\\
                        &= AV
                \end{align*}
                \]
                Finally, since \(V\) is an orthogonal matrix, 
                \[(U\Sigma ) V^T = (AV)V^T = AI = A.\]
            </div>
            Steps for SVD:
            <ol style="padding-left: 40px;">
                <li>Compute Orthogonal decomposition of \(A^TA\): </li>
                    Compute the eigenvalues and eigenvectors of  \(A^TA\). <br>             
                    Note: In practice, avoid directly computing \(A^TA\) to prevent numerical errors.
                <li>Construct \(V\) and \(\Sigma\):</li>
                    Arrange eigenvalues in descending order.<br>
                    Use the corresponding unit eigenvectors as the columns of \(V\).
                <li>Construct \(U\):</li>
                    For nonzero singular values, compute \(u_k = \frac{1}{\sigma_k}Av_k\).<br>
                    If there are not enough nonzero singular values, extend the orthonormal basis. 
            </ol>
            <br>
            <p>
            The singular values give us a powerful tool to diagnose the stability of a linear system. The <strong>condition number</strong> of a matrix \(A\), denoted \(\kappa(A)\), measures how sensitive the output \(Ax\) is to small changes (like floating-point errors) in the input \(x\).
            <br><br>
            For a general \(m \times n\) matrix \(A\) with rank \(r\), the condition number is defined as the ratio of the largest to the smallest <em>nonzero</em> singular value:
            \[
            \kappa(A) = \frac{\sigma_1}{\sigma_r}
            \]
            If \(A\) is a square, invertible matrix, this simplifies to \(\kappa(A) = \sigma_1 / \sigma_n\).
            <br><br>
            A matrix with a very large condition number (\(\kappa(A) \gg 1\)) is called <strong>ill-conditioned</strong>. This means the matrix is "almost singular." From a CS and engineering perspective, this is a critical numerical red flag. Solving \(Ax=b\) for an ill-conditioned \(A\) can lead to highly inaccurate results, as small rounding errors in \(b\) or \(A\) get magnified into large errors in the solution \(x\).
            </p>
            </p>
            </section>

            <section id="pinv_svd" class="section-content">
            <h2>Pseudo-inverse via SVD</h2>
            <p>
            The <strong>Moore-Penrose Pseudo-inverse (\(A^\dagger\))</strong> is most stably defined and computed using the Singular Value Decomposition (SVD).
            <br><br>
            Given the SVD of \(A = U\Sigma V^T\), where \(U\) is \(m \times m\), \(\Sigma\) is \(m \times n\), and \(V\) is \(n \times n\), the pseudo-inverse is:
            \[
            A^\dagger = V\Sigma^\dagger U^T
            \]
            Here, \(\Sigma^\dagger\) is an \(n \times m\) matrix formed by:
            <ol style="padding-left: 40px;">
                <li>Transposing \(\Sigma\) to get \(\Sigma^T\) (which is \(n \times m\)).</li>
                <li>Inverting every <strong>nonzero</strong> singular value \(\sigma_i\) on the diagonal to get \(1/\sigma_i\).</li>
            </ol>
            <br>
            If \(\Sigma\) has diagonal entries \(\sigma_1, \dots, \sigma_r > 0\), then \(\Sigma^\dagger\) will have diagonal entries \(1/\sigma_1, \dots, 1/\sigma_r\). All other entries are zero.
            </p>

            <h3>SVD, Pseudo-inverse, and Fundamental Subspaces</h3>
            <p>
            This definition has a beautiful geometric interpretation. Recall that \(\hat{x} = A^\dagger b\) is the minimum-norm least-squares solution.
            </p>
            <ul style="padding-left: 40px;">
                <li>A least-squares solution \(\hat{x}\) must map to the projection of \(b\) onto \(\text{Col}(A)\).</li>
                <li>The minimum-norm requirement forces the solution \(\hat{x}\) to lie <em>entirely</em> within the <strong>Row Space</strong> of \(A\), i.e., \(\hat{x} \in \text{Row}(A)\). (Any component in \(\text{Nul}(A)\) would be orthogonal to \(\text{Row}(A)\), adding to the norm of \(\hat{x}\) without changing the result \(A\hat{x}\).)</li>
            </ul>
            <p>
            The SVD beautifully handles this. It decomposes \(A\) into its fundamental subspaces. The pseudo-inverse essentially "inverts" the mapping from \(\text{Row}(A)\) to \(\text{Col}(A)\) and ignores the \(\text{Nul}(A)\).
            <br><br>
            You can also see that \(A^\dagger A = V\Sigma^\dagger U^T U\Sigma V^T = V\Sigma^\dagger\Sigma V^T\). The matrix \(\Sigma^\dagger\Sigma\) is an \(n \times n\) diagonal matrix with 1s for the first \(r\) entries and 0s after. This makes \(A^\dagger A = V \begin{bmatrix} I_r & 0 \\ 0 & 0 \end{bmatrix} V^T\), which is the <strong>orthogonal projection matrix onto the Row Space of \(A\)</strong>. Similarly, \(A A^\dagger\) is the orthogonal projection matrix onto the <strong>Column Space of \(A\)</strong>.
            </p>
            
            <h3>Practical Note: Truncation and Regularization</h3>
            <p>
            In real-world applications (CS/ML), we rarely have singular values that are exactly zero. Instead, we have a spectrum of values, some of which are very small.
            \[ \sigma_1 \geq \sigma_2 \geq \dots \gg \dots \geq \sigma_n \approx 0 \]
            Inverting these tiny singular values (\(1/\sigma_i\)) would introduce massive instability, as we'd be amplifying components related to noise.
            <br><br>
            The practical computation of the pseudo-inverse involves setting a <strong>tolerance</strong> \(\epsilon\). We treat any \(\sigma_i < \epsilon\) as zero.
            \[
            (\sigma_i^\dagger) = \begin{cases} 1/\sigma_i & \text{if } \sigma_i \geq \epsilon \\ 0 & \text{if } \sigma_i < \epsilon \end{cases}
            \]
            This <strong>truncated SVD</strong> is the basis for <strong>Principal Component Regression (PCR)</strong> and is a powerful form of regularization. It explicitly discards the "directions" (singular vectors) that contribute least to the transformation and are most likely to be noise, building a more robust, lower-rank model.
            </p>
            </section>

            <section id="svd-demo" class="section-content">
                <h2>Interactive SVD Visualization</h2>
                <p>
                This interactive demo visualizes the Singular Value Decomposition process for 2×2 matrices. SVD expresses any matrix A as a product A = UΣV<sup>T</sup>, which can be understood geometrically as a composition of three transformations:
                </p>
                <ol style="padding-left: 40px;">
                    <li>V<sup>T</sup>: Rotation/reflection using right singular vectors (orthogonal)</li>
                    <li>Σ: Scaling along the axes (singular values)</li>
                    <li>U: Rotation/reflection using left singular vectors (orthogonal)</li>
                </ol>
                <p>
                Use the visualization to see how each component of SVD contributes to the overall transformation:
                </p>
                
                <!-- SVD Demo Component goes here -->
                <div id="svd-visualization-container">
                    <svd-demo></svd-demo>
                </div>
                
                <p class="mt-4">
                Try different matrices to observe how SVD works with various transformations:
                </p>
                <ul style="padding-left: 40px;">
                    <li><strong>Identity:</strong> No transformation occurs</li>
                    <li><strong>Scaling:</strong> Only the singular values differ from 1</li>
                    <li><strong>Rotation:</strong> U and V are identical rotation matrices</li>
                    <li><strong>Symmetric:</strong> U = V (special case for symmetric matrices)</li>
                    <li><strong>Shear:</strong> Combination of rotation and non-uniform scaling</li>
                </ul>
            </section>

            <section id="f_sub" class="section-content">
            <h2>SVD with Fundamental Subspaces</h2>
            <p>
            Consider an \(m \times n\) matrix \(A = U\Sigma V^T\). 
            <br>
            Let \(u_1, \cdots, u_m\) be the <strong>left singular vectors</strong>, \(v_1, \cdots, v_n\) be
            the <strong>right singular vectors</strong>, \(\sigma_1, \cdots, \sigma_n\) be the singular values, and \(r\) be the rank of \(A\).
            <br><br>
            \(\{Av_1, \cdots, Av_r\} = \{\sigma_1u_1, \cdots, \sigma_ru_r\} \) is an orthogonal basis for \(\text{Col }A\) and then
            \(\{u_1, \cdots, u_r\}\) is an orthonormal basis for \(\text{Col }A\). Also, \((\text{Col }A)^{\perp} =  \text{Nul }A^T \).
            Thus, \(\{u_{r+1}, \cdots, u_m\}\) is an orthonormal basis for \(\text{Nul }A^T\).
            <br><br> 
            Since \(\| Av_i \| = \sigma_i = 0\) iff \(i > r \,\), \(v_{r+1}, \cdots, v_n\) span a subspace of \(\text{Nul }A\)
            of dimension \(n-r\), but we know that \(\dim \text{Nul }A = n - \text{rank } A = n -r\).
            Therefore, \(\{v_{r+1}, \cdots, v_n\}\) is an orthonormal basis for \(\text{Nul }A\). 
            <br><br>
            Finally, since \((\text{Nul }A)^{\perp} = \text{Col }A^T  = \text{Row }A  \), \(\{v_1, \cdots, v_r\}\) is an orthonormal basis for \(\text{Row }A\). 
            <br><br> 
            Now, we assume \(A\) is an \(n \times n\) matrix. From above facts, we derive additional invertible matrix theorems (See: <a href="determinants.html"><strong>invertible matrix</strong></a>): 
            <ol style="padding-left: 40px;">
                <li>\((\text{Col }A)^{\perp} = \{0\}\).</li>
                <li>\((\text{Nul }A)^{\perp} = \mathbb{R}^n\).</li>
                <li>\(\text{Row }A = \mathbb{R}^n\). </li>
                <li> \(A\) has \(n\) nonzero singular values. </li>
            </ol>
            </p>
            </section>
        </div>  
        <script src="/js/main.js"></script>
        <script src="/js/svd_visualizer.js"></script>
    </body>
</html>
    
    <footer>
    <div class="footer-content">
        <div class="footer-about">
            <h3>About MATH-CS COMPASS</h3>
            <p>Bridging the gap between pure mathematics and computer science applications.</p>
            <div class="contact-section">
                <strong><a href="https://docs.google.com/forms/d/e/1FAIpQLSf3_RNkegREvfCEIQznn6SCaf-iB5c-Doxn7Ymuu78Uf-PBjg/viewform" target="_blank">
                    Submit Your Questions & Feedback <i class="fas fa-external-link-alt"></i>
                </a></strong>
            </div>
        </div>
        <div class="footer-links">
            <h3>Quick Links</h3>
            <ul>
                
                <li><a href="../../../index.html">Home</a></li>
                <li>
                <a href="../Linear_algebra/linear_algebra.html">
                    <span class="nav-number">I</span>
                    <span>Linear Algebra to Algebraic Foundations</span>
                </a>
                </li>
                <li>
                <a href="../Calculus/calculus.html">
                    <span class="nav-number">II</span>
                    <span>Calculus to Optimization & Analysis</span>
                </a>
                </li>
                <li>
                <a href="../Probability/probability.html">
                    <span class="nav-number">III</span>
                    <span>Probability & Statistics</span>
                </a>
                </li>
                <li>
                <a href="../Discrete/discrete_math.html">
                    <span class="nav-number">IV</span>
                    <span>Discrete Mathematics & Algorithms</span>
                </a>
                </li>
                <li>
                <a href="../Machine_learning/ml.html">
                    <span class="nav-number">V</span>
                    <span>Machine Learning</span>
                </a>
                </li>
            </ul>
        </div>
        <div class="footer-social">
            <h3>Connect With Me</h3>
            <p>
                <a href="https://x.com/MathCSCompass?t=Zi8nyfzszeFw8QizbJwnNg&s=09" target="_blank">
                    <i class="fab fa-x-twitter"></i> X (Twitter)
                </a>
            </p>
            <p>
                <a href="https://www.facebook.com/share/15uPaBWgQN/" target="_blank">
                    <i class="fab fa-facebook"></i> Facebook
                </a>
            </p>
      
            <h3>MATH-CS COMPASS App</h3>
            <div class="app-download-footer">
                <a href="https://play.google.com/store/apps/details?id=io.github.yusukey01.twa" target="_blank" rel="noopener">
                    <img src="../../../images/google-play-badge.png" 
                        alt="Get it on Google Play" 
                        class="google-play-badge">
                </a>
            </div>
            
        </div>

    </div>
    <div class="footer-bottom">
        <p>&copy; 2024-2025 MATH-CS COMPASS. All rights reserved.</p>
    </div>
</footer>
    <script src="../../../js/main.js?v=1.0.2"></script>
    <script src="../../../js/search.js?v=1.0.2"></script>
    

    <button id="go-to-top-btn" class="go-to-top-btn" title="Go to top">
        <i class="fas fa-arrow-up"></i>
    </button>
    <script>
        // Register the service worker
        if ('serviceWorker' in navigator) {
          window.addEventListener('load', () => {
            navigator.serviceWorker.register('/js/service-worker.js')
              .then(registration => {
                console.log('ServiceWorker registration successful with scope: ', registration.scope);
              })
              .catch(error => {
                console.log('ServiceWorker registration failed: ', error);
              });
          });
        }
      </script>
</body>
</html>