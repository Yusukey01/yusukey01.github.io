<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    
    <title>Eigenvalues & Eigenvectors - MATH-CS COMPASS</title>
    
    
    <script>
    window.MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']]
      }
    };
    </script>
    <script id="MathJax-script" defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="preload" href="https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Main-Regular.woff" as="font" type="font/woff2" crossorigin>
    <link rel="preload" href="https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Math-Italic.woff" as="font" type="font/woff2" crossorigin>
    
    
    

    <link rel="stylesheet" href="../../../css/styles.css?v=1.0.2">
  
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css">

    <link rel="dns-prefetch" href="https://cdn.jsdelivr.net">
    <link rel="dns-prefetch" href="https://cdnjs.cloudflare.com">
    
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Learn about eigenvalues, eigenvectors, eigenspaces, and Diagonalization.">
    <meta name="google-site-verification" content="IieEujypYW38caEVe8ymjczouNl56yxAy27iPztcfRA" />
    

    <link rel="manifest" href="/manifest.json">

    <!-- Theme color for Chrome, Android, and other browsers -->
    <meta name="theme-color" content="#007bff">
  
    <!-- iOS specific tags -->
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <meta name="apple-mobile-web-app-title" content="MATH-CS COMPASS">
    <meta name="mobile-web-app-capable" content="yes">

    <!-- Icon references -->
    <link rel="icon" type="image/png" sizes="1280x1280" href="/images/maskable_icon_x1280.png?v=1.0.2">
    <link rel="icon" type="image/png" sizes="512x512" href="/images//maskable_icon_x512.png?v=1.0.2">
    
    <!-- Apple Touch Icons -->
    <link rel="apple-touch-icon" href="/images/maskable_icon_x1280.png?v=1.0.2">
    <link rel="apple-touch-icon" href="/images/maskable_icon_x512.png?v=1.0.2">

    <!-- For Windows -->
    <meta name="msapplication-TileImage" content="/images/test1.png?v=1.0.2">
    <meta name="msapplication-TileColor" content="#007bff">

    <!-- Schema.org Structured Data for AI Optimization -->
    

    <!-- Article Schema for content pages -->
    
    <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "Article",
      "headline": "Eigenvalues & Eigenvectors",
      "description": "Learn about eigenvalues, eigenvectors, eigenspaces, and Diagonalization.",
      "datePublished": "2024-01-01",
      "dateModified": "2025-01-01",
      "author": {
        "@type": "Person",
        "name": "Yusuke Yokota",
        "jobTitle": "Mathematics & Computer Science Educator",
        "knowsAbout": [
          "Linear Algebra",
          "Machine Learning",
          "Computer Science",
          "Mathematics"
        ]
      },
      "publisher": {
        "@type": "Organization",
        "name": "MATH-CS COMPASS",
        "url": "https://math-cs-compass.com",
        "logo": {
          "@type": "ImageObject",
          "url": "https://math-cs-compass.com/images/maskable_icon_x512.png"
        }
      },
      "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "https://math-cs-compass.com/Mathematics/Linear_algebra/eigenvectors.html"
      },
      "about": [
        
        { "@type": "Thing", "name": "Linear Algebra" }
        
      ],
      "educationalLevel": "university",
      "learningResourceType": "tutorial",
      "educationalUse": "instruction"
    }
    </script>
    
</head>

<body>
    <header>
    <nav class="navbar">
        <!-- Logo as Home Button -->
        <div class="logo-home">
            <a href="../../../index.html" 
               class="logo-link " 
               title="Home - Math-CS Compass">
                <img src="../../images/icon_x512.png" 
                     alt="Math-CS Compass Logo" 
                     class="nav-logo">
            </a>
        </div>

        <div class="search-container">
            <input type="text" id="search-input" placeholder="Search keywords...">
            <button id="search-button" aria-label="Search" title="Search website"><i class="fas fa-search"></i></button>
        </div>
        
        <div class="menu-toggle">
            <i class="fas fa-bars"></i>
        </div>
        
        <ul class="nav-links">
            <li><a href="../Linear_algebra/linear_algebra.html" class="active">I - Linear Algebra to Algebraic Foundations</a></li>
            
            <li><a href="../Calculus/calculus.html" >II - Calculus to Optimization & Analysis</a></li>
            
            <li><a href="../Probability/probability.html" >III - Probability & Statistics</a></li>
            
            <li><a href="../Discrete/discrete_math.html" >IV - Discrete Mathematics & Algorithms</a></li>
            
            <li><a href="../Machine_learning/ml.html" >V - Machine Learning</a></li>
        </ul>
    </nav>
</header>
    
    <!DOCTYPE html>
<html>
    <body> 
        <!-- LearningResource Schema for Content Pages -->
        <script type="application/ld+json">
        {
        "@context": "https://schema.org",
        "@type": "LearningResource",
        "name": "Eigenvalues & Eigenvectors",
        "description": "Learn about eigenvalues, eigenvectors, eigenspaces, and Diagonalization.",
        "learningResourceType": "lesson",
        "educationalUse": "instruction",
        "educationalLevel": "university",
        "interactivityType": "active",
        "author": {
            "@type": "Person",
            "name": "Yusuke Yokota",
            "jobTitle": "Mathematics & Computer Science Educator"
        },
        "publisher": {
            "@type": "Organization",
            "name": "MATH-CS COMPASS",
            "url": "https://math-cs-compass.com"
        },
        "about": [
            
            { "@type": "Thing", "name": "Eigenvalues" },
            { "@type": "Thing", "name": "Eigenvectors" },
            { "@type": "Thing", "name": "Eigenspaces" },
            { "@type": "Thing", "name": "Diagonalization" },
            { "@type": "Thing", "name": "Characteristic Equations" },
            { "@type": "Thing", "name": "Complex Eigenvalues" },
            { "@type": "Thing", "name": "Similarity" },
            { "@type": "Thing", "name": "Matrix Decomposition" }
            
        ],
        "teaches": [
            "Mathematical concepts",
            "Practical applications",
            "Problem-solving techniques"
        ],
        "isPartOf": {
            "@type": "Course",
            "name": "Linear Algebra",
            "description": "Explore the foundations of Linear Algebra, covering key concepts such as linear equations, vector spaces, eigenvalues, orthogonality, least squares, and stochastic matrices",
            "provider": {
            "@type": "Organization",
            "name": "MATH-CS COMPASS",
            "url": "https://math-cs-compass.com"
            },
            "instructor": {
            "@type": "Person",
            "name": "Yusuke Yokota",
            "jobTitle": "Mathematics & Computer Science Educator"
            },
            "courseCode": "I",
            "hasCourseInstance": {
            "@type": "CourseInstance",
            "courseMode": "online",
            "courseWorkload": "PT2H",
            "instructor": {
                "@type": "Person",
                "name": "Yusuke Yokota"
            }
            },
            "offers": {
            "@type": "Offer",
            "price": "0",
            "priceCurrency": "USD",
            "availability": "https://schema.org/InStock",
            "category": "free"
            }
        }
        }
        </script>
        <!-- WebApplication Schema for Interactive Demos -->
        <script type="application/ld+json">
        {
        "@context": "https://schema.org",
        "@type": "WebApplication",
        "name": "Eigenvalues & Eigenvectors Interactive Demo",
        "description": "Interactive demonstration of eigenvalues & eigenvectors concepts",
        "applicationCategory": "EducationalApplication",
        "operatingSystem": "Web Browser",
        "url": "https://math-cs-compass.com/Mathematics/Linear_algebra/eigenvectors.html",
        "author": {
            "@type": "Person",
            "name": "Yusuke Yokota"
        },
        "applicationSubCategory": "Mathematical Visualization",
        "featureList": [
            "Interactive mathematical visualization",
            "Real-time parameter adjustment",
            "Educational demonstrations"
        ],
        "isAccessibleForFree": true,
        "educationalUse": "instruction",
        "educationalLevel": "university"
        }
        </script>
        <div class="hero-section">
            <h1 class="webpage-name">Eigenvalues & Eigenvectors
            </h1>
        </div>

        
  <div class="quick-jump-container">
    <button type="button" id="quick-jump-toggle" class="quick-jump-toggle">
      <i class="fas fa-list"></i> <strong>Section I Navigation</strong>
    </button>
    <div id="quick-jump-menu" class="quick-jump-menu">
      <h3>Linear Algebra to Algebraic Foundations</h3>
      <div class="quick-jump-links">

        
        <a href="linear_algebra.html">‚Üê Back to Section I Overview</a>
        <hr style="margin: 8px 0; border: 1px solid #ddd;">
        
        
        <a href="linear_equations.html" >Part 1: Linear Equations</a>
        <a href="linear_transformation.html" >Part 2: Linear Transformation</a>
        <a href="matrix_algebra.html" >Part 3: Matrix Algebra</a>
        <a href="determinants.html" >Part 4: Determinants</a>
        <a href="vectorspaces.html" >Part 5: Vector Spaces</a>
        <a href="eigenvectors.html" class="current-page">Part 6: Eigenvalues & Eigenvectors</a>
        <a href="orthogonality.html" >Part 7: Orthogonality</a>
        <a href="leastsquares.html" >Part 8: Least-Squares Problems</a>
        <a href="symmetry.html" >Part 9: Symmetry</a>
        <a href="trace.html" >Part 10: Trace and Norms</a>
        <a href="kronecker.html" >Part 11: Kronecker Product & Tensor</a>
        <a href="woodbury.html" >Part 12: Woodbury Matrix Identity</a>
        <a href="stochastic.html" >Part 13: Stochastic Matrix</a>
        <a href="graph_laplacian.html" >Part 14: Graph Laplacians and Spectral Methods</a>
      </div>
    </div>
</div>


        <div class="topic-nav">
            <a href="#eigen">Eigenvectors and Eigenvalues</a>
            <a href="#c_eq">Characteristic equations</a>
            <a href="#diag">Diagonalization</a>
            <a href="#c_eigen">Complex Eigenvalues and Eigenvectors</a>
        </div> 

        <div class="container">  
           
            <section id="eigen" class="section-content">
            <h2>Eigenvectors and Eigenvalues</h2>
            <p>
            An <strong>eigenvector</strong> of an \(n \times n\) matrix \(A\) is a "nonzero" vector \(x\) such that
            \[Ax = \lambda x \tag{1}\]
            for some scalar \(\lambda\), which is called an <strong>eigenvalue</strong> of \(A\) if there is a
            nontrivial solution \(x\) to equation (1). Such an \(x\) is referred to as an eigenvector corresponding to \(\lambda\).
            Equation (1) can be written as: \[(A - \lambda I)x = 0 \tag{2}\] 
            The scalar \(\lambda\) is an eigenvalue of \(A\) iff (2) has a nontrivial solution. The set of all solutions
            of (2) is the null space \(Nul(A - \lambda I) \subseteq \mathbb{R}^n\) and is called the <strong>eigenspace</strong> of \(A\)
            corresponding to the eigenvalue \(\lambda\).

            <div class="theorem">
                <span class="theorem-title">Theorem 1:</span> 
                The eigenvalues of a triangular matrix are its main diagonal entries. 
            </div>
            <div class="proof">
                <span class="proof-title">Proof:</span>
                Suppose \(A \in \mathbb{R}^{3 \times 3} \) is a lower triangular matrix, and \(\lambda\) is an
                eigenvalue of \(A\). Then, 
                \[A - \lambda I = \begin{bmatrix} 
                    a_{11} - \lambda & 0 & 0 \\ 
                    a_{21} & a_{22} - \lambda & 0 \\
                    a_{31} & a_{32} & a_{33} - \lambda \\
                    \end{bmatrix}.
                \]
                Since \(\lambda\) is an eigenvalue of \(A\), \((A-\lambda I)x =0\) has a nontrivial solution. 
                In other words, the equation has a free variable. 
                This occurs if and only if at least one of the main diagonal entries is zero 
                so that \(\lambda\) is equal to one of the main diagonal entries of \(A\). 
                The same reasoning applies to upper triangular matrices and higher-dimensional cases.
            </div>

            For example, 
            \[A = \begin{bmatrix} 0 & 1 & 8 \\  0 & 2 & 7 \\ 0 & 0 & 3 \\ \end{bmatrix}\]
            has eigenvalues; \(0, 2, \text{and } 3\). Since \(A\) has a zero eigenvalue, equation (1) becomes
            the homogeneous equation \(Ax =0\), which must have a nontrivial solution. This happens iff 
            <strong>\(A\) is a singular matrix(NOT invertible).</strong> 
            <br>
            We can verify this by computing:
            \[
            det A = 0(6-0)-(0-0)+8(0-0)=0
            \] 
            Since \(detA=0\), \(A\) is indeed not invertible. 

            <div class="theorem">
                <span class="theorem-title">Theorem 2:</span> 
                If \(v_1, \cdots, v_n\) are eigenvectors corresponding to "distinct" eigenvalues \(\lambda_1, \cdots, \lambda_n\)
                of a square matrix \(A\), then the set \(\{v_1, \cdots, v_n\}\) is linearly independent.
            </div>
            <div class="proof">
                <span class="proof-title">Proof:</span>
                Assume the set of eigenvectors \(\{v_1, \cdots, v_n\}\) is linearly "dependent." 
                By definition, each eigenvector \(v_i\) is nonzero. If the set is dependent, at least 
                one eigenvector \(v_{i+1}\) can be written as a linear combination of the preceding 
                eigenvectors. Let \(i\) be the least index so that \(v_{i+1}\) is a linear combination 
                of the preceding eigenvectors. Then there exist scalars \(c_1, \cdots, c_i\) such that
                \[
                v_{i+1} = c_1v_1 + \cdots + c_iv_i, \quad c_1, \cdots, c_i \in \mathbb{R}. \tag{3}
                \]
                Multiplying both sides of (3) by a square \(A\), we get: 
                \[
                c_1Av_1 + \cdots + c_iAv_i = Av_{i+1}
                \]
                By definition of eigenvalues & eigenvectors, (3) becomes
                \[
                c_1 \lambda_1 v_1 + \cdots + c_i \lambda_i v_i = \lambda_{i+1} v_{i+1}. \tag{4}
                \]
                Subtracting \(\lambda_{i+1}\) times (3) from (4), we get: 
                \[
                c_1(\lambda_1 - \lambda_{i+1}) v_1 + \cdots + c_i(\lambda_i - \lambda_{i+1} )v_i = 0. \tag{5}
                \]
                All coefficients in (5) must be zero since \(\{ v_1, \cdots, v_i\}\) is linearly independent.
                However, \((\lambda_1 - \lambda_{i+1}), \cdots, (\lambda_i - \lambda_{i+1} )\) are nonzero because 
                the eigenvalues are distinct. Hence, \(c_1 = c_2 = \cdots = c_i = 0\). This is a contradiction to equation (3)
                Therefore, the set \(\{v_1, \cdots, v_n\}\) must be a linearly independent set.      
            </div>
            </p>
            </section>

            <section id="c_eq" class="section-content">
            <h2>Characteristic Equations</h2>
            <p>
            A scalar \(\lambda\) is an eigenvalue of an \(n \times n\) matrix \(A\) iff \(\lambda\) satisfies
            the <strong>characteristic equation</strong> 
            \[det (A - \lambda I) = 0.\]
            For example, consider the matrix 
            \[A = \begin{bmatrix} 1 & 2 & 3 \\ 0 & 1 & 4 \\ 5 & 6 & 0 \\ \end{bmatrix}\]
            Then, 
            \[det(A- \lambda I) = (1-\lambda)(-\lambda^2 -\lambda -24) -0 +5(5 +3\lambda) = 0 \]
            Expanding this, we obtain the <strong>characteristic polynomial</strong>:
            \[-\lambda^3 +2\lambda^2 + 38\lambda +1 = 0\]
            Solving this equation for \(\lambda\), we get eigenvalues for \(A\): \(\lambda \approx -5.230, -0.026, 7.256\).
            Like this example, in practice, we typically approximate eigenvalues using numerical methods.
            <br><br>
            The <strong>algebraic multiplicity</strong> of an eigenvalue is its multiplicity as a root of the 
            characteristic equation.
            <br><br>
            For example, if the characteristic polynomial of a matrix is \((\lambda -1)^2 (\lambda -2) = 0\), then 
            the eigenvalue 1 has multiplicity 2. 
            <br><br>
            Next, we introduce the concept of <strong>similarity</strong> of matrices. Suppose \(A\) and \(B\) are 
            \(n \times n\) matrices. Then, \(A\) is said to be similar to \(B\) if there exists an invertible matrix \(P\) such that 
            \[
            P^{-1}AP =B \quad \text{, or equivalently, } A=PBP^{-1}.
            \]
            <div class="theorem">
                <span class="theorem-title">Theorem 3:</span>  
                "If" \(n \times n\) matrices \(A\) and \(B\) are similar, "then" they have the same characteristic
                polynomial and thus the same eigenvalues with the same multiplicities.
                <br>
                Note: the converse of this theorem is not true. Having the same eigenvalues does not imply 
                the matrices are similar.
            </div>
            <div class="proof">
                <span class="proof-title">Proof:</span>
                If \(B = P^{-1}AP\), then 
                \[
                \begin{align*}
                B - \lambda I &= P^{-1}AP - \lambda P^{-1}P \\\\
                              &= P^{-1}(A - \lambda I)P
                \end{align*}
                \]
                By the multiplicative property of determinants, we have
                \[
                \begin{align*}
                \det (B-\lambda I) &= \det (P^{-1}) \det (A-\lambda I) \det (P)\\\\ 
                                   &= \det (A-\lambda I).
                \end{align*}
                \]
                Note: \(\det (P^{-1}P) = \det (I) = 1\).
            </div>
            </p>
            </section>

            <section id="diag" class="section-content">
            <h2>Diagonalization</h2>
            <p>
            A square matrix \(A\) is said to be <strong>diagonalizable</strong> if for some invertible matrix \(P\),
            \(A\) is similar to a diagonal matrix \(D\). \[A = PDP^{-1}\]
            <div class="theorem">
                <span class="theorem-title">Theorem 4: Diagonalization</span>  
                An \(n \times n\) matrix \(A\) is diagonalizable iff \(A\) has \(n\) linearly independent eigenvectors.
                Thus, the columns of \(P\) are linearly independent eigenvectors of \(A\) and the diagonal entries of \(D\)
                are eigenvalues of \(A\) corresponding to the eigenvectors in \(P\). 
                Note: \(P\) and \(D\) are not unique because the order of the diagonal entries in \(D\) can be changed.
            </div>
            <div class="proof">
                <span class="proof-title">Proof:</span>
                Let \(P\) be a square matrix with columns \(v_1, \cdots, v_n\) and \(D\) be a diagonal matrix with
                diagonal entries \(\lambda_1, \cdots, \lambda_n \). Then, 
                \[AP = \begin{bmatrix}Av_1 & \cdots & Av_n \\\end{bmatrix}\]
                \[PD = \begin{bmatrix}\lambda_1 v_1 & \cdots & \lambda_n v_n \\\end{bmatrix}\]
                Assume \(A\) is diagonalizable, so \(A = PDP^{-1}\). 
                By multiplying both sides of this equation on the right by \(P\), we get:
                \[AP = PD\].
                Thus, for each column of \(AP\), we have
                \[
                Av_1 = \lambda_1 v_1, \cdots, Av_n = \lambda_n v_n. \tag{1}
                \]
                Since \(P\) is invertible, its columns are linearly independent and must be nonzero.
                From equation (1), \(\lambda_1, \cdots, \lambda_n \) are eigenvalues, and \(v_1, \cdots, v_n\) are 
                corresponding eigenvectors.
                <br><br>
                Finally, consider any \(n\) linearly independent eigenvectors \(v_1, \cdots, v_n\). We can construct \(P\) and \(D\)
                from these eigenvectors and their corresponding eigenvalues \(\lambda_1, \dots, \lambda_n\).
                If the eigenvectors are linearly independent, then \(P\) is invertible, and we obtain \(A = P D P^{-1}\).
            </div>

            For example, given
            \[A = \begin{bmatrix} 4 & 1 & 1\\ 1 & 4 & 1 \\ 1 & 1 & 4 \\ \end{bmatrix},\] 
            computing the characteristic equation:
            \[det(A- \lambda I) = (4-\lambda)((4-\lambda)^2 - 1) -((4-\lambda )-1)+(1-(4 -\lambda)) = 0 \]
            Simplifying this, we get:
            \[-\lambda^3 +12\lambda^2 -45\lambda -54 = 0,\]
            which factors as
            \[(\lambda - 3)^2(\lambda -6) = 0\]
            Thus, eigenvalues are \(\lambda_1 = 3\), \(\lambda_2 = 3\), and \(\lambda_3 = 6\). Next, we need to find three linearly
            independent eigenvectors corresponding to each eigenvalue. 
            <br>For \(\lambda_1 = 3\) and \(\lambda_2 = 3\):
            \[A-3I = \begin{bmatrix} 1 & 1 & 1\\ 1 & 1 & 1 \\ 1 & 1 & 1 \\ \end{bmatrix} 
            \xrightarrow{\text{rref}}
            \begin{bmatrix} 1 & 1 & 1 \\ 0 & 0 & 0 \\ 0 & 0 & 0 \\\end{bmatrix}
            \]
            We choose the eigenvectors \(v_1 = \begin{bmatrix} -1 \\ 0 \\ 1 \\ \end{bmatrix}\) for \(\lambda_1 = 3\) and
            \(v_2 = \begin{bmatrix} -1 \\ 1 \\ 0 \\ \end{bmatrix}\)  
            <br>
            For \(\lambda_3 = 6\):
            \[
            A-6I = \begin{bmatrix} -2 & 1 & 1\\ 1 & -2 & 1 \\ 1 & 1 & -2 \\ \end{bmatrix}
            \xrightarrow{\text{rref}}
            \begin{bmatrix} 1 & 0 & -1 \\ 0 & 1 & -1 \\ 0 & 0 & 0 \\\end{bmatrix}
            \]
            We choose the eigenvector \(v_3 = \begin{bmatrix} 1 \\ 1 \\ 1 \\ \end{bmatrix}\).
            <br>
            Therefore, 
            \[
            \begin{align*}
            &D = \begin{bmatrix} 3 & 0 & 0 \\ 0 & 3 & 0 \\ 0 & 0 & 6 \\ \end{bmatrix}, \\\\
            &P = \begin{bmatrix} -1 & -1 & 1 \\ 0 & 1 & 1 \\ 1 & 0 & 1 \\ \end{bmatrix}, \\\\
            &P^{-1} = 
            \begin{bmatrix} \frac{-1}{3} & \frac{-1}{3} & \frac{2}{3} \\
            \frac{-1}{3} & \frac{2}{3} & \frac{-1}{3}\\
            \frac{1}{3} & \frac{1}{3} & \frac{1}{3} \\ \end{bmatrix}.
            \end{align*}
            \]
            <br><br>
            As you can see, It is <strong>"not necessary"</strong> that an \(n \times n\) matrix has \(n\) "distinct" eigenvalues 
            in order to be diagonalizable (Note: if the matrix has \(n\) distinct eigenvalues, the matrix must be diagonalizable.)
            </p>
            </section>

            <section id="c_eigen" class="section-content">
            <h2>Complex Eigenvalues and Eigenvectors</h2>
            <p>
            It is possible for the characteristic equation to have complex roots. A complex scalar
            \(\lambda\) satisfies the characteristic equation \(det(A -\lambda I) = 0\) iff there is a nonzero vector
            \(x \in \mathbb{C}^n\) such that \(Ax = \lambda x\). In this case, \(\lambda\) is called a <strong>complex eigenvalue</strong>
            and \(x\) is its corresponding <strong>complex eigenvector</strong>.
            <br><br>
            Rotation matrices are widely used in applications such as computer graphics to represent rotations in 2D or higher-dimensional spaces.
            Analyzing their eigenvalues and eigenvectors can provide insight into their geometric behavior, including scaling and rotation angles.
            <br>
            Rotation matrices are not always diagonalizable in
            \(\mathbb{R}^n\) but in \(\mathbb{C}^n\), real rotation matrices are always diagonalizable.
            (See <a href="orthogonality.html"><strong>Orthogonality </strong></a> 
            and <a href="symmetry.html"><strong>Symmetry</strong></a> .)
            <br><br>
            Consider the 2D rotation matrix \(R = \begin{bmatrix} a & -b \\ b & a \end{bmatrix}\), 
            where \(a\) and \(b\) are real and not both nonzero. Then the "complex" eigenvalues of \(R\) can be found by 
            solving the characteristic equation:
             \[
             \begin{align*}
             \det (R - \lambda I)=0 
             &\Longrightarrow
             \lambda^2 -2a\lambda +(a^2 + b^2) = 0 \\\\
             &\Longrightarrow
             (\lambda -(a+bi))(\lambda -(a-bi)) = 0
             \end{align*}
             \]
            Thus, we get complex eigenvalues \(\lambda = a \pm bi\), and we can find complex eigenvectors: 
            \[
            Rv_1 = \begin{bmatrix} a & -b \\ b & a \\ \end{bmatrix} \begin{bmatrix} 1 \\ -i \\ \end{bmatrix}
               = \begin{bmatrix} a + bi \\ b - ai\\ \end{bmatrix} 
               = (a+bi)\begin{bmatrix} 1 \\ -i \\ \end{bmatrix}
            \]
            Hence, \(v_1 = \begin{bmatrix} 1 \\ -i \\ \end{bmatrix}\) is an eigenvector corresponding to the eigenvalue \(\lambda = a + bi\). 
            Moreover, the <strong>complex conjugate</strong> of \(\lambda\), denoted \(\bar{\lambda} = a - bi\), is an eigenvalue with 
            its corresponding eigenvector \(v_2 = \begin{bmatrix} 1 \\ i \\ \end{bmatrix}\).
            Therefore, we can diagonalize \(R\) in \(\mathbb{C}^2 \,\):
            \[
            \begin{align*}
            R &= PDP^{-1} \\\\
            &= \begin{bmatrix} 1 & 1 \\ -i & i\end{bmatrix}
            \begin{bmatrix} a+bi & 0 \\ 0 & a-bi \end{bmatrix}
            \frac{1}{2}\begin{bmatrix} 1  & i \\ 1 & -i\end{bmatrix}
            \end{align*}
            \]
            <br>
            Now, we can map this representation back to \(\mathbb{R}^2\) as the polar decomposition of \(R\) into 
            its rotation angle \(\varphi\) and scaling factor \(r\).
            <br>
            Let \(r = \sqrt{a^2 + b^2}\) be the magnitude of \(\lambda\), so  \(|\lambda | = r \) and \(\varphi\) be the 
            angle(the <strong>argument</strong> of \(\|\lambda \| = r\)) such that 
            \[\cos \varphi = \frac{a}{r}, \quad \sin \varphi = \frac{b}{r}.\]
            Then \(R\) can be written in polar form as:
            \[
            \begin{align*}
            R &= r \begin{bmatrix} \frac{a}{r} & \frac{-b}{r} \\ \frac{b}{r} & \frac{a}{r} \\ \end{bmatrix} \\\\
              &= \begin{bmatrix} r & 0 \\ 0 & r \\ \end{bmatrix} \begin{bmatrix} \cos \varphi & -\sin \varphi \\ \sin \varphi & \cos \varphi \\ \end{bmatrix}
            \end{align*}
            \]
            <br><br>
            Note: In general, given a nonzero complex number \(z\) corresponding to a point \((a, b)\) in the complex plane, 
            then \( \, a = |z| \cos \varphi , \quad b =  |z| \sin \varphi \, \) and so
            \[z = a + bi = |z| (\cos \varphi + i\sin \varphi ) = |z|e^{i\varphi} \]
            where \(\varphi = \arg z \, \) and \(|z| = \sqrt{a^2 + b^2}\) because \(z\cdot \bar{z} = a^2 + b^2\).
            </p>
            </section>
        </div>
        <script src="/js/main.js"></script>
    </body>
</html>
    
    <footer>
    <div class="footer-content">
        <div class="footer-about">
            <h3>About MATH-CS COMPASS</h3>
            <p>Bridging the gap between pure mathematics and computer science applications.</p>
            <div class="contact-section">
                <strong><a href="https://docs.google.com/forms/d/e/1FAIpQLSf3_RNkegREvfCEIQznn6SCaf-iB5c-Doxn7Ymuu78Uf-PBjg/viewform" target="_blank">
                    Submit Your Questions & Feedback <i class="fas fa-external-link-alt"></i>
                </a></strong>
            </div>
        </div>
        <div class="footer-links">
            <h3>Quick Links</h3>
            <ul>
                
                <li><a href="../../../index.html">Home</a></li>
                <li>
                <a href="../Linear_algebra/linear_algebra.html">
                    <span class="nav-number">I</span>
                    <span>Linear Algebra to Algebraic Foundations</span>
                </a>
                </li>
                <li>
                <a href="../Calculus/calculus.html">
                    <span class="nav-number">II</span>
                    <span>Calculus to Optimization & Analysis</span>
                </a>
                </li>
                <li>
                <a href="../Probability/probability.html">
                    <span class="nav-number">III</span>
                    <span>Probability & Statistics</span>
                </a>
                </li>
                <li>
                <a href="../Discrete/discrete_math.html">
                    <span class="nav-number">IV</span>
                    <span>Discrete Mathematics & Algorithms</span>
                </a>
                </li>
                <li>
                <a href="../Machine_learning/ml.html">
                    <span class="nav-number">V</span>
                    <span>Machine Learning</span>
                </a>
                </li>
            </ul>
        </div>
        <div class="footer-social">
            <h3>Connect With Me</h3>
            <p>
                <a href="https://x.com/MathCSCompass?t=Zi8nyfzszeFw8QizbJwnNg&s=09" target="_blank">
                    <i class="fab fa-x-twitter"></i> X (Twitter)
                </a>
            </p>
            <p>
                <a href="https://www.facebook.com/share/15uPaBWgQN/" target="_blank">
                    <i class="fab fa-facebook"></i> Facebook
                </a>
            </p>
      
            <h3>MATH-CS COMPASS App</h3>
            <div class="app-download-footer">
                <a href="https://play.google.com/store/apps/details?id=io.github.yusukey01.twa" target="_blank" rel="noopener">
                    <img src="../../../images/google-play-badge.png" 
                        alt="Get it on Google Play" 
                        class="google-play-badge">
                </a>
            </div>
            
        </div>

    </div>
    <div class="footer-bottom">
        <p>&copy; 2024-2025 MATH-CS COMPASS. All rights reserved.</p>
    </div>
</footer>
    <script src="../../../js/main.js?v=1.0.2"></script>
    <script src="../../../js/search.js?v=1.0.2"></script>
    

    <button id="go-to-top-btn" class="go-to-top-btn" title="Go to top">
        <i class="fas fa-arrow-up"></i>
    </button>
    <script>
        // Register the service worker
        if ('serviceWorker' in navigator) {
          window.addEventListener('load', () => {
            navigator.serviceWorker.register('/js/service-worker.js')
              .then(registration => {
                console.log('ServiceWorker registration successful with scope: ', registration.scope);
              })
              .catch(error => {
                console.log('ServiceWorker registration failed: ', error);
              });
          });
        }
      </script>
</body>
</html>