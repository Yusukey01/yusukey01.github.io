<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    
    <title>Orthogonality - MATH-CS COMPASS</title>
    
    
    <script>
    window.MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']]
      }
    };
    </script>
    <script id="MathJax-script" defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="preload" href="https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Main-Regular.woff" as="font" type="font/woff2" crossorigin>
    <link rel="preload" href="https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Math-Italic.woff" as="font" type="font/woff2" crossorigin>
    
    
    

    <link rel="stylesheet" href="../../../css/styles.css?v=1.0.2">
  
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css">

    <link rel="dns-prefetch" href="https://cdn.jsdelivr.net">
    <link rel="dns-prefetch" href="https://cdnjs.cloudflare.com">
    
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Learn about inner product, orthogonality, orthogonal projection, Gram-Schmidt algorithm, and QR factorization.">
    <meta name="google-site-verification" content="IieEujypYW38caEVe8ymjczouNl56yxAy27iPztcfRA" />
    

    <link rel="manifest" href="/manifest.json">

    <!-- Theme color for Chrome, Android, and other browsers -->
    <meta name="theme-color" content="#007bff">
  
    <!-- iOS specific tags -->
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <meta name="apple-mobile-web-app-title" content="MATH-CS COMPASS">
    <meta name="mobile-web-app-capable" content="yes">

    <!-- Icon references -->
    <link rel="icon" type="image/png" sizes="1280x1280" href="/images/maskable_icon_x1280.png?v=1.0.2">
    <link rel="icon" type="image/png" sizes="512x512" href="/images//maskable_icon_x512.png?v=1.0.2">
    
    <!-- Apple Touch Icons -->
    <link rel="apple-touch-icon" href="/images/maskable_icon_x1280.png?v=1.0.2">
    <link rel="apple-touch-icon" href="/images/maskable_icon_x512.png?v=1.0.2">

    <!-- For Windows -->
    <meta name="msapplication-TileImage" content="/images/test1.png?v=1.0.2">
    <meta name="msapplication-TileColor" content="#007bff">

    <!-- Schema.org Structured Data for AI Optimization -->
    

    <!-- Article Schema for content pages -->
    
    <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "Article",
      "headline": "Orthogonality",
      "description": "Learn about inner product, orthogonality, orthogonal projection, Gram-Schmidt algorithm, and QR factorization.",
      "datePublished": "2024-01-01",
      "dateModified": "2025-01-01",
      "author": {
        "@type": "Person",
        "name": "Yusuke Yokota",
        "jobTitle": "Mathematics & Computer Science Educator",
        "knowsAbout": [
          "Linear Algebra",
          "Machine Learning",
          "Computer Science",
          "Mathematics"
        ]
      },
      "publisher": {
        "@type": "Organization",
        "name": "MATH-CS COMPASS",
        "url": "https://yusukey01.github.io",
        "logo": {
          "@type": "ImageObject",
          "url": "https://yusukey01.github.io/images/maskable_icon_x512.png"
        }
      },
      "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "https://yusukey01.github.io/Mathematics/Linear_algebra/orthogonality.html"
      },
      "about": [
        
        { "@type": "Thing", "name": "Linear Algebra" }
        
      ],
      "educationalLevel": "university",
      "learningResourceType": "tutorial",
      "educationalUse": "instruction"
    }
    </script>
    
</head>

<body>
    <header>
    <nav class="navbar">
        <!-- Logo as Home Button -->
        <div class="logo-home">
            <a href="../../../index.html" 
               class="logo-link " 
               title="Home - Math-CS Compass">
                <img src="../../images/icon_x512.png" 
                     alt="Math-CS Compass Logo" 
                     class="nav-logo">
            </a>
        </div>

        <div class="search-container">
            <input type="text" id="search-input" placeholder="Search keywords...">
            <button id="search-button" aria-label="Search" title="Search website"><i class="fas fa-search"></i></button>
        </div>
        
        <div class="menu-toggle">
            <i class="fas fa-bars"></i>
        </div>
        
        <ul class="nav-links">
            <li><a href="../Linear_algebra/linear_algebra.html" class="active">I - Linear Algebra to Algebraic Foundations</a></li>
            
            <li><a href="../Calculus/calculus.html" >II - Calculus to Optimization & Analysis</a></li>
            
            <li><a href="../Probability/probability.html" >III - Probability & Statistics</a></li>
            
            <li><a href="../Discrete/discrete_math.html" >IV - Discrete Mathematics & Algorithms</a></li>
            
            <li><a href="../Machine_learning/ml.html" >V - Machine Learning</a></li>
        </ul>
    </nav>
</header>
    
    <!DOCTYPE html>
<html>
    <body>
        <!-- LearningResource Schema for Content Pages -->
        <script type="application/ld+json">
        {
        "@context": "https://schema.org",
        "@type": "LearningResource",
        "name": "Orthogonality",
        "description": "Learn about inner product, orthogonality, orthogonal projection, Gram-Schmidt algorithm, and QR factorization.",
        "learningResourceType": "lesson",
        "educationalUse": "instruction",
        "educationalLevel": "university",
        "interactivityType": "active",
        "author": {
            "@type": "Person",
            "name": "Yusuke Yokota",
            "jobTitle": "Mathematics & Computer Science Educator"
        },
        "publisher": {
            "@type": "Organization",
            "name": "MATH-CS COMPASS",
            "url": "https://yusukey01.github.io"
        },
        "about": [
            
            { "@type": "Thing", "name": "Orthogonality" },
            { "@type": "Thing", "name": "Inner Product" },
            { "@type": "Thing", "name": "Orthogonal Projection" },
            { "@type": "Thing", "name": "Gram-Schmidt Algorithm" },
            { "@type": "Thing", "name": "QR Factorization" },
            { "@type": "Thing", "name": "Orthogonal Sets" },
            { "@type": "Thing", "name": "Orthonormal Basis" },
            { "@type": "Thing", "name": "Orthogonal Matrices" }
            
        ],
        "teaches": [
            "Mathematical concepts",
            "Practical applications",
            "Problem-solving techniques"
        ],
        "isPartOf": {
            "@type": "Course",
            "name": "Linear Algebra",
            "description": "Explore the foundations of Linear Algebra, covering key concepts such as linear equations, vector spaces, eigenvalues, orthogonality, least squares, and stochastic matrices",
            "provider": {
            "@type": "Organization",
            "name": "MATH-CS COMPASS",
            "url": "https://yusukey01.github.io"
            },
            "instructor": {
            "@type": "Person",
            "name": "Yusuke Yokota",
            "jobTitle": "Mathematics & Computer Science Educator"
            },
            "courseCode": "I",
            "hasCourseInstance": {
            "@type": "CourseInstance",
            "courseMode": "online",
            "courseWorkload": "PT2H",
            "instructor": {
                "@type": "Person",
                "name": "Yusuke Yokota"
            }
            },
            "offers": {
            "@type": "Offer",
            "price": "0",
            "priceCurrency": "USD",
            "availability": "https://schema.org/InStock",
            "category": "free"
            }
        }
        }
        </script>
        <!-- WebApplication Schema for Interactive Demos -->
        <script type="application/ld+json">
        {
        "@context": "https://schema.org",
        "@type": "WebApplication",
        "name": "Orthogonality Interactive Demo",
        "description": "Interactive demonstration of orthogonality concepts",
        "applicationCategory": "EducationalApplication",
        "operatingSystem": "Web Browser",
        "url": "https://yusukey01.github.io/Mathematics/Linear_algebra/orthogonality.html",
        "author": {
            "@type": "Person",
            "name": "Yusuke Yokota"
        },
        "applicationSubCategory": "Mathematical Visualization",
        "featureList": [
            "Interactive mathematical visualization",
            "Real-time parameter adjustment",
            "Educational demonstrations"
        ],
        "isAccessibleForFree": true,
        "educationalUse": "instruction",
        "educationalLevel": "university"
        }
        </script>
        <div class="hero-section">
            <h1 class="webpage-name">Orthogonality
            </h1>
        </div>

        
  <div class="quick-jump-container">
    <button type="button" id="quick-jump-toggle" class="quick-jump-toggle">
      <i class="fas fa-list"></i> <strong>Section I Navigation</strong>
    </button>
    <div id="quick-jump-menu" class="quick-jump-menu">
      <h3>Linear Algebra to Algebraic Foundations</h3>
      <div class="quick-jump-links">

        
        <a href="linear_algebra.html">‚Üê Back to Section I Overview</a>
        <hr style="margin: 8px 0; border: 1px solid #ddd;">
        
        
        <a href="linear_equations.html" >Part 1: Linear Equations</a>
        <a href="linear_transformation.html" >Part 2: Linear Transformation</a>
        <a href="matrix_algebra.html" >Part 3: Matrix Algebra</a>
        <a href="determinants.html" >Part 4: Determinants</a>
        <a href="vectorspaces.html" >Part 5: Vector Spaces</a>
        <a href="eigenvectors.html" >Part 6: Eigenvalues & Eigenvectors</a>
        <a href="orthogonality.html" class="current-page">Part 7: Orthogonality</a>
        <a href="leastsquares.html" >Part 8: Least-Squares Problems</a>
        <a href="symmetry.html" >Part 9: Symmetry</a>
        <a href="trace.html" >Part 10: Trace and Norms</a>
        <a href="kronecker.html" >Part 11: Kronecker Product & Tensor</a>
        <a href="woodbury.html" >Part 12: Woodbury Matrix Identity</a>
        <a href="stochastic.html" >Part 13: Stochastic Matrix</a>
        <a href="graph_laplacian.html" >Part 14: Graph Laplacians and Spectral Methods</a>
      </div>
    </div>
</div>


        <div class="topic-nav">
            <a href="#inner">Inner Product & Orthogonality in \(\mathbb{R}^n\)</a>
            <a href="#orthogonal">The Orthogonal Set</a>
            <a href="#proj">The Orthogonal Projection</a>
            <a href="#gs">Gram-Schmidt Algorithm</a>
            <a href="#qr">QR Factorization</a>
            <a href="#interactive">Interactive Visualizer</a>
        </div> 

        <div class="container">  
           
            <section id="inner" class="section-content">
            <h2>Inner Product & Orthogonality in \(\mathbb{R}^n\)</h2>
            <p>
            The <strong>inner product</strong> of two vectors \(u, v \in \mathbb{R}^n \) is a scalar defined as:
            \[
            u \cdot v = u^Tv = u_1v_1 + \cdots + u_nv_n.
            \]
            <br>
            To extend the concept of "length" to higher dimensions, we define the <strong>norm</strong> of 
            a vector \(v \in \mathbb{R}^n\) as the nonnegative scalar:
            \[
            \|v\| = \sqrt{v \cdot v}.
            \]
            This is also called <strong>Euclidean(\(L_2\)) norm</strong>.
            <br>
            A vector with a length of 1 is called a <strong>unit vector</strong>, represented as 
            \[
            u = \frac{v}{\|v\|}
            \]
            The unit vector maintains the "direction" of the original vector. This process is known as <strong>normalization</strong>, which
            is widely used in applications like machine learning.
            <br>
            Now we can define the <strong>distance</strong> between two vectors
            \(u, v \in \mathbb{R}^n \) as
            \[
            \text{dist }(u, v) = \|u - v\|.
            \] 
            This is known as the <strong>Euclidean distance</strong>, which measures the difference or similarity between data points.
            <br>
            To explore orthogonality geometrically, consider the squared distances:
            \[\begin{align*}
            \|u - v\|^2 &= (u-v) \cdot (u-v) \\\\
            &= u \cdot (u-v) + (-v) \cdot (u - v) \\\\
            &= \|u\|^2 - 2u \cdot v + \|v\|^2
            \end{align*}
            \] 
            \[
            \begin{align*}
            \|u - (-v)\|^2 &= (u+v) \cdot (u+v) \\\\
            &= u \cdot (u+v) + (v) \cdot (u + v) \\\\
            &= \|u\|^2 + 2u \cdot v + \|v\|^2
            \end{align*}
            \]
            (Note: \(dist(u, v) \text{ and } dist(u, -v)\) are perpendicular iff they are the same.)
            Thus, if \(u \cdot v = 0\), the terms resemble the Pythagorean theorem:
            \[
            \|u  +v \|^2 = \|u\|^2 + \|v\|^2
            \]
            <br>
            We define the general orthogonality:
            <br>
            Two vectors \(u, v \in \mathbb{R}^n \) are <strong>orthogonal</strong> to each other if their inner product satisfies
            \[u \cdot v =0.\]
            <br>
            If a vector \(v\) is orthogonal to <strong>every</strong> vector in \(W \subseteq \mathbb{R}^n\), then the vector \(v\) is said to be
            orthogonal to \(W\). Also, the set of all vectors that are orthogonal to \(W\) is called the <strong>orthogonal complement</strong>
            of \(W\), denoted by \(W^{\perp}\).
            <br><br>
            This orthogonality provides a useful relationship between the column space and null space of a matrix:
            <div class="theorem">
                <span class="theorem-title">Theorem 1:</span>   
                Let \(A \in \mathbb{R}^{m \times n}\). Then, 
                \[(\text{Row } A)^{\perp} = \text{Nul } A \]
                and
                \[(\text{Col } A)^{\perp} = \text{Nul } A^T\] 
            </div>
            <div class="proof">
                <span class="proof-title">Proof:</span>
                If \(x \in \text{Nul } A\), then \(Ax =0\), which means that \(x\) is orthogonal to each row of \(A\) 
                because the inner product of each row of \(A\) and \(x\) are zero. Then, since the rows of \(A\) span the row space of \(A\),
                \(x\) is orthogonal to \(\text{Row } A \). Also, if \(x\) is orthogonal to \(\text{Row } A \), then \(x\) is orthogonal to every row of \(A\)
                and thus \(Ax =0\), or \(x \in \text{Nul } A\). Therefore, \((\text{Row } A)^{\perp} = \text{Nul } A \).
                <br>Next, since the first equation is true for any matrix, we consider for \(A^T\). Since \(\text{Row } A^T = \text{Col } A\), 
                we conclude that \((\text{Col } A)^{\perp} = \text{Nul } A^T \). 
            </div>
            </p>
            </section>

            <section id="orthogonal" class="section-content">
            <h2>The Orthogonal Set</h2>
            <p>
            A set of vectors \(\{v_1, v_2, \cdots, v_p\}\) in \(\mathbb{R}^n\)  is called an <strong>orthogonal set</strong>
            if \(v_i \cdot v_j =0\), \(i \neq j\). Additionally, an <strong>orthogonal basis</strong> for \(W \subseteq \mathbb{R}^n\) is
            a basis for \(W\) that forms an orthogonal set. 
            <div class="theorem">
                <span class="theorem-title">Theorem 2:</span>    
                Let \(\{v_1, v_2, \cdots, v_p\}\) be an orthogonal basis for \(W \subseteq \mathbb{R}^n\). For each
                \(y \in W\), the weights in the linear combination \(y = c_1v_1 + \cdots + c_pv_p\) are given by
                \[c_j = \frac{y \cdot v_j}{v_j \cdot v_j} \qquad (j = 1, \cdots, p)\]
            </div>
            <div class="proof">
                <span class="proof-title">Proof:</span>
                Since \(v_1\) is orthogonal to all other vectors in the set, 
                \[
                \begin{align*}
                y \cdot v_1     &= (c_1v_1 + \cdots + c_pv_p) \cdot v_1  \\\\
                                &= c_1(v_1 \cdot v_1) + \cdots + c_p(v_p \cdot v_1) \\\\
                                &= c_1(v_1 \cdot v_1).
                \end{align*}
                \]
                By definition of the basis, \(v_1\) is nonzero, \(v_1 \cdot v_1\) must be nonzero, and 
                then we can solve this equation for \(c_1\). Similarly, we can compute \(y \cdot v_j\) 
                and solve for \(c_j \quad (j = 2, \cdots, p)\).
            </div>
            A set \(\{u_1, \cdots, u_p\}\) is said to be an <strong>orthonormal set</strong> if it is an orthogonal set of unit vectors. If \(W\)
            is the subspace spanned by such a set, then \(\{u_1, \cdots, u_p\}\)  is an <strong>orthonormal basis</strong> for \(W\). 
            Matrices with orthonormal columns are central to many practical applications due to their unique properties. 
            <div class="theorem">
                <span class="theorem-title">Theorem 3:</span>
                A matrix \(U \in \mathbb{R}^{m \times n} \) has orthonormal columns iff \(U^TU = I\).   
            </div>
            <div class="proof">
                <span class="proof-title">Proof:</span>
                \[
                \begin{align*}
                 U^TU &= \begin{bmatrix} u_1^T \\ u_2^T \\  \vdots \\ u_n^T \end{bmatrix}
                        \begin{bmatrix} u_1 & u_2 & \cdots & u_n \end{bmatrix} \\\\
                        &= \begin{bmatrix} u_1^Tu_1 & u_1^Tu_2 & \cdots & u_1^Tu_n \\
                                        u_2^Tu_1 & u_2^Tu_2 & \cdots & u_2^Tu_n \\
                                        \vdots   &  \vdots  & \ddots & \vdots \\
                                        u_n^Tu_1 & u_n^Tu_2 & \cdots & u_n^Tu_n 
                        \end{bmatrix}
                \end{align*}
                \]
                The columns of \(U\) are orthogonal iff 
                \[
                u_i^Tu_j = 0, i \neq j \quad (i, j = 1, \cdots, n)
                \]
                and, the columns of \(U\) have unit length iff
                \[
                u_i^Tu_i = 1, (i = 1, \cdots, n).
                \] 
                Thus, \(U^TU = I\) and the converse is also true.
            </div>
            <div class="theorem">
                <span class="theorem-title">Theorem 4:</span>
                Let \(U \in \mathbb{R}^{m \times n} \) with orthonormal columns, and let \(x, y \in \mathbb{R}^n\).
                Then
                    <br>
                    <ol style="padding-left: 40px;">
                        <li>\(\| Ux \| = \| x \| \)</li>
                        <li>\((Ux) \cdot (Uy) = x \cdot y \)</li>
                        <li>\((Ux) \cdot (Uy) = 0\) iff \(x \cdot y = 0\)</li>
                    </ol>
            </div>
            <div class="proof">
                <span class="proof-title">Proof:</span>
                For Property 1, 
                \[
                \begin{align*}
                 \| Ux \|^2 &= (Ux)^T(Ux) \\\\
                            &= x^TU^TUx  \\\\
                            &= x^TIx  \\\\
                            &= x^Tx.
                 \end{align*}
                 \]
                 Therefore, 
                 \[
                  \| Ux \| = \| x \|.
                \]
                For Property 2, 
                \[
                \begin{align*}
                (Ux) \cdot (Uy) &= (Ux)^T(Uy) \\\\
                                &= x^TU^TUy   \\\\
                                &= x^TIy = x^Ty \\\\
                                &= x \cdot y.
                \end{align*}
                \]
                Note: We can also prove properties 1 and 3 from the Property 2.   
            </div>
            These results are particularly significant for <strong>square</strong> matrices.
            we define a square invertible matrix \(U\) as an <strong>orthogonal matrix</strong> if 
            \[U^{-1} = U^T.\]
            By Theorem 3, this matrix \(U\) has <strong>orthonormal</strong> columns (the name "orthogonal matrix" is 
            somewhat misleading since it refers to "orthonormal" columns). 
            Notably, the rows of \(U\) form an orthonormal basis of \(\mathbb{R}^n\).
            <br><br>
            For example, the 2D rotation matrix  is an orthogonal matrix. 
            \[
            R_{\theta} = \begin{bmatrix} \cos\theta & - \sin\theta \\ \sin\theta & \cos\theta \end{bmatrix}
            \]
            Verify:
            \[
            \begin{align*}
            R_{\theta}^TR_{\theta} &= R_{\theta}^{-1}R_{\theta}\\\\
                                    &= \begin{bmatrix} \cos\theta & \sin\theta \\ -\sin\theta & \cos\theta \end{bmatrix}
                                        \begin{bmatrix} \cos\theta & - \sin\theta \\ \sin\theta & \cos\theta \end{bmatrix}\\\\
                                    &=  \begin{bmatrix} \cos^2 \theta + \sin^2 \theta & 0 \\ 0 & \sin^2 \theta + \cos^2 \theta \end{bmatrix}\\\\
                                    &= I
            \end{align*}
            \]
            Also, \[\det R_{\theta} = \cos^2 \theta + \sin^2 \theta = 1\] 
            This property generalizes to any orthogonal matrix \(U\):
            \[
            \begin{align*}
            1 = \det I &= \det ( U^TU )  \\\\
                       &= \det U^T \det U  \\\\
                       &= \det U \det U  \\\\
                       &= (\det U)^2 
            \end{align*}
            \]
            (Note: Since \(U\) is a square matrix, \(\det U^T = \det U\).)
            <br>Thus, for any orthogonal matrix \(U\),
            \[
            \det U = \pm 1
            \]
            Lastly, consider the eigenvalues of \(R_{\theta}\).
            \[
            \begin{align*}
            \det (R_{\theta} - \lambda I ) &= (\cos\theta - \lambda)^2 - \sin^2 \theta  \\\\
                                &= \lambda^2 -2 \lambda \cos\theta +1
            \end{align*}
            \]
            Solve the characteristic equation:
            \[
            \lambda^2 -2 \lambda\cos\theta +1 = 0,
            \]
            We obtain:
            \[
            \begin{align*}
            \lambda &=\frac{2\cos\theta \pm 2\sqrt{(\cos^2\theta -1)}i}{2} \\\\
                    &= \cos\theta \pm i \sin\theta.
            \end{align*}
            \]
            Then, 
            \[
            |\lambda | = 1
            \]
            <br>
            Let \(x\) be the nonzero eigenvector correspoding to the eigenvalue of the orthogonal matrix \(U\), then 
            \[
            \begin{align*}
            Ux = \lambda x  &\Longrightarrow \| Ux \| = |\lambda | \| x \|  \\\\
                            &\Longrightarrow \| x \| = |\lambda | \| x \| 
            \end{align*}
            \]
            Since \(\| x \| \neq 0 \), we conclude that \(|\lambda | = 1\ \).
            <br>
            (Note: \(\| Ux \| =  \| x \|\) is also true in the case \(x\) is a <strong>complex vector</strong>. For complex vectors, an orthogonal matrix
            generalizes to a <strong>unitary matrix</strong> s.t. \(U^*U = I\) where \(U^*\) is the <strong>conjugate transpose</strong>
            of \(U\). Then you can prove this fact like we did in Theorem 4. So, the orthogonal matrix is just a special case of 
            a <strong>"U"</strong>nitary matrix with real entries.)
            </p>
            </section>

            <section id="proj" class="section-content">
            <h2>The Orthogonal Projection</h2>
            <p>
            Given a nonzero vector \(u \in \mathbb{R}^n\). We often need to decompose a vector \(y\in \mathbb{R}^n\)
            into two vectors: 
            \[
            y= \hat{y} + z \tag{2}
            \] 
            where \(\hat{y} = \alpha u\) is a multiple of \(u\) for some scalar \(\alpha\) and the vector \(z\) is orthogonal to \(u\).
            <br>
            For any scalar \(\alpha\), let \(z = y - \alpha u\) so that (2) holds. Then \(y - \hat{y}\) is 
            <strong>orthogonal</strong> to \(u\) iff
            \[
            (y - \alpha u) \cdot u = y \cdot u - \alpha (u \cdot u) = 0
            \]
            \[
            \Longrightarrow \alpha  = \frac{y \cdot u}{u \cdot u}
            \]
            Thus, the <strong>orthogonal projection of \(y\) onto \(u\) </strong> is
            \[
            \hat{y} = \frac{y \cdot u}{u \cdot u} u
            \]
            <div class="theorem">
                <span class="theorem-title">Theorem 5: Orthogonal Decomposition</span>
                Let \(W \subseteq \mathbb{R}^n\). Then \(\forall y \in \mathbb{R}^n\) can be written uniquely in the form
                \[y= \hat{y} + z\]
                where \(\hat{y} \in W\) and \(z \in W^{\perp}\). If \(\{u_1, \cdots, u_p\}\) is any orthogonal basis of \(W\),
                then 
                \[
                \hat{y} = \frac{y \cdot u_1}{u_1 \cdot u_1} u_1 + \cdots + \frac{y \cdot u_p}{u_p \cdot u_p} u_p \tag{3}
                \] 
                and \[z = y - \hat{y}.\]
                Note: (3) is called the <strong>orthogonal projection of \(y\) onto \(W\) </strong> denoted by \(\text{proj}_W y\).
            </div>
            <div class="proof">
                <span class="proof-title">Proof:</span>
                Let \(\{u_1, \cdots, u_p\}\) is any orthogonal basis of \(W\) and 
                \(\hat{y} = \frac{y \cdot u_1}{u_1 \cdot u_1} u_1 + \cdots + \frac{y \cdot u_p}{u_p \cdot u_p} u_p\).
                Since \(\hat{y}\) is a linear combination of \(u_1, \cdots u_p\), \(\quad \hat{y} \in W\). 
                <br>
                Let \(z = y - \hat{y}\). Because \(u_i\) is orthogonal to each \(u_j\) in the basis for \(W\) with \(i \neq j\),
                \[
                \begin{align*}
                z \cdot u_i &= (y -\hat{y}) \cdot u_i  \\\\
                            &= y \cdot u_i - (\frac{y \cdot u_i}{u_i \cdot u_i})u_i \cdot u_i - 0 - \cdots - 0 \\\\
                            &= y \cdot u_i - y \cdot u_i = 0
                \end{align*}
                \]
                Hence \(z\) is orthogonal to each \(u_i\) in the basis for \(W\). Therefore, \(z\) is orthogonal to every vector in \(W\).
                In other words, \(z \in W^{\perp}\).
                <br>
                Next, suppose \(y\) can also be written as \(y = \hat{y_1} + z_1\) where \(y_1 \in W \text{ and } z_1 \in W^{\perp}\).
                Then \[\hat{y} +z = \hat{y_1} + z_1 \Longrightarrow \hat{y} - \hat{y_1} = z_1 - z \]
                This equation implies that the vector \((\hat{y} - \hat{y_1})\) is in both \(W\) and \(W^{\perp}\). Thus,
                since the only vector that can be  in both \(W\) and \(W^{\perp}\) is the zero vector, \((\hat{y} - \hat{y_1}) = 0\). 
                Therefore, \(\hat{y} = \hat{y_1} \text{ and } z = z_1\).
            </div>
            The orthogonal projection of \(y\) onto \(W\) is <strong>the best approximation to \(y\) by elements of W</strong>. 
            This is significant in practical applications. 
            <div class="theorem">
                <span class="theorem-title">Theorem 6: Best Approximation</span>
                Let \(W \subseteq \mathbb{R}^n\) and \(y \in \mathbb{R}^n\). Also let \(\hat{y}\) be 
                the orthogonal projection of \(y\) onto \(W\). Then \(\hat{y}\) is the closest point in \(W\) to \(y\) 
                such that  \[\| y - \hat{y} \| < \| y - v \| \tag{4}\] for all \(v \in W\) distinct from \(\hat{y}\).
            </div>

            <div class="proof">
                <span class="proof-title">Proof:</span>
                Choose \(v \in W\) distinct from \(\hat{y} \in W\). Then \((\hat{y} - v) \in W\). By the Orthogonal Decomposition
                Theorem, \((y - \hat{y})\) is orthogonal to \(W\). Moreover,  \((y - \hat{y})\) is orthogonal to \((\hat{y} - v) \in W\)
                Consider \[y - v = (y - \hat{y}) + (\hat{y} -v). \] 
                By the Pythagorean Theorem, 
                \[\|y - v\|^2 = \|y - \hat{y}\|^2 + \|\hat{y} -v \|^2  > 0\]
                Thus, we can conclude \(\| y - \hat{y} \| < \| y - v \|\).
            </div>
            The formula (3) becomes much simpler when the basis for \(W\) is an <strong>orthonormal</strong> set as follows:
            <br>
            If \(\{u_1, \cdots, u_p\}\) is an orthonormal basis for \(W \subseteq \mathbb{R}^n\), then
            \[
            \text{proj}_W y = (y \cdot u_1)u_1 + \cdots + (y \cdot u_p)u_p
            \]
            If \(U = [u_1, \cdots, u_p]\), then
            \[
            \forall y \in \mathbb{R}^n, \quad \text{proj}_W y = UU^Ty
            \]
            because each weight can be written as \(u_1^Ty, \cdots, u_p^Ty\), which are the entries of \(U^Ty\).
            <br>
            Besides having orthonormal columns, if \(U\) is an \(n \times n\) <strong>square</strong> matrix
            (\(U\) is the <strong>orthogonal</strong> matrix),
            \[
            \forall y \in \mathbb{R}^n, \quad \text{proj}_W y = UU^Ty = Iy = y.
            \]
            This implies the column space \(W\) is equal to \(\mathbb{R}^n\).
            </p>
            </section>

            <section id="gs" class="section-content">
            <h2>Gram-Schmidt Algorithm</h2>
            <p>
            The <Strong>Gram-Schmidt algorithm</Strong> generates an orthogonal (or orthonormal) basis for <strong>any</strong>
            nonzero subspace of \(\mathbb{R}^n\). Let us explore this process through a simple example. 
            <br><br>
            Consider the basis \({x_1, x_2, x_3}\) for a nonzero subspace \(W\) of \(\mathbb{R}^3\), where:
            \(x_1 = \begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix}, \quad 
            x_2 =\begin{bmatrix} 0 \\ 1 \\ 0 \end{bmatrix}, \text{ and } 
            x_3 = \begin{bmatrix} 0 \\ 0 \\ 1\end{bmatrix} \)
            <br>
            Let \(v_1 = x_1\) and \(W_1 = Span \{v_1\}\). Then 
            \[
            \begin{align*}
            v_2 &= x_2 - \text{proj}_{W_1} x_2  \\\\
                &= x_2 - \frac{x_2 \cdot v_1} {v_1 \cdot v_1}v_1 \\\\
                &= \begin{bmatrix} 0 \\ 1 \\ 0 \end{bmatrix} - \frac{2}{14}\begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix} \\\\
                &= \begin{bmatrix} \frac{-1}{7} \\ \frac{5}{7} \\ \frac{-3}{7} \end{bmatrix}
            \end{align*}
            \]
            <br>Next, \(W_2 = Span \{v_1, v_2\}\) and then
            \[
            \begin{align*}
            v_3 &= x_3 - \text{proj}_{W_2} x_3  \\\\
                &= x_3 - \frac{x_3 \cdot v_1} {v_1 \cdot v_1}v_1 - \frac{x_3 \cdot v_2} {v_2 \cdot v_2}v_2 \\\\
                &= \begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix} - \frac{3}{14}\begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix} 
                    - \frac{\frac{-3}{7}}{\frac{5}{7}}\begin{bmatrix} \frac{-1}{7} \\ \frac{5}{7} \\ \frac{-3}{7} \end{bmatrix} \\\\
                &= \begin{bmatrix} \frac{-3}{10} \\ 0 \\ \frac{1}{10} \end{bmatrix}
            \end{align*}
            \]
            Thus, the orthogonal basis for \(W\) is \(W_3 = Span\{v_1, v_2, v_3\}\). 
            <br><br>
            Finally, we construct an orthonormal basis for \(W\) by normalizing each \(v_k\):
            \[
            \{u_1 = \begin{bmatrix} \frac{1}{\sqrt{14}} \\ \frac{2}{\sqrt{14}} \\ \frac{3}{\sqrt{14}} \end{bmatrix}, \,
              u_2 = \begin{bmatrix} \frac{-1}{\sqrt{35}} \\ \frac{5}{\sqrt{35}} \\ \frac{-3}{\sqrt{35}} \end{bmatrix}, \,
              u_3 = \begin{bmatrix} \frac{-3}{\sqrt{10}} \\ \ 0 \\ \frac{1}{\sqrt{10}} \end{bmatrix} \}
            \]
            The Gram-Schmidt algorithm at the \(k\)th step:
            \[
            v_k = x_k - \sum_{j=1}^{k-1} (x_k \cdot u_j)u_j  ,\quad u_k = \frac{v_k}{\| v_k \|}
            \]
            <br>
            The resulting vectors look different from the original ones. This is because the algorithm systematically redefines each 
            vector to be orthogonal to the previous ones while <strong>maintaining the span of the original vectors</strong>. Essentially, 
            it transforms the basis into an orthonormal one without changing the space that the vectors describe.
            </p>
            </section>

            <section id="qr" class="section-content">
            <h2>QR Factorization</h2>
            <p>
            The <strong>QR factorization</strong> is a practical decomposition of matrices with linearly independent columns.
            It is widely used in numerical applications, such as eigenvalue estimation.
            (Note:Solving the characteristic polynomial is often impossible.)
            <br><br>
            Let \(A\) be an \(m \times n\) matrix with linearly independent columns. Then A can be factorized as
            \(A = QR\), where \(Q\) is an \(m \times n\) matrix with columns forming an orthonormal basis for \(Col A\)
            and \(R\) is an \(n \times n\) upper triangular invertible matrix with positive diagonal entries. 
            <br><br>
            Consider \(A = \begin{bmatrix} 1 & 0 & 0 \\ 2 & 1 & 0 \\ 3 & 0 & 1 \end{bmatrix}\) again. 
            By the GS algorithm, we have already obtained \(Q\) from the previous example:
            \[Q = \begin{bmatrix} \frac{1}{\sqrt{14}} &  \frac{-1}{\sqrt{35}} & \frac{-3}{\sqrt{10}} \\ 
                                \frac{2}{\sqrt{14}} &  \frac{5}{\sqrt{35}} &  0  \\
                                \frac{3}{\sqrt{14}} &  \frac{-3}{\sqrt{35}} & \frac{1}{\sqrt{10}}
                \end{bmatrix}\]
            Since the columns of \(Q\) are orthonormal, \(Q^TQ=I\). Then, \(Q^TA = Q^TQR = IR = R\).
            \[
            \begin{align*}
            R = Q^TA &= \begin{bmatrix}
                        \frac{1}{\sqrt{14}}  & \frac{2}{\sqrt{14}}  & \frac{3}{\sqrt{14}} \\
                        \frac{-1}{\sqrt{35}} & \frac{5}{\sqrt{35}} & \frac{-3}{\sqrt{35}} \\
                        \frac{-3}{\sqrt{10}} & \ 0                 & \frac{1}{\sqrt{10}} 
                        \end{bmatrix}
                        \begin{bmatrix} 1 & 0 & 0 \\ 2 & 1 & 0 \\ 3 & 0 & 1 \end{bmatrix} \\\\
                    &= \begin{bmatrix}
                        \frac{14}{\sqrt{14}} & \frac{2}{\sqrt{14}} & \frac{3}{\sqrt{14}} \\
                        0                   & \frac{5}{\sqrt{35}} & \frac{-3}{\sqrt{35}} \\
                        0                   &  0                  & \frac{1}{\sqrt{10}} 
                        \end{bmatrix}
            \end{align*}
            \]  
            </p>
            </section>

            <section id="interactive" class="section-content">
                <h2>Interactive Visualizer</h2>
                <p>
                    This interactive demonstrates fundamental concepts like inner products, orthogonal decomposition, and orthogonalization 
                    that are essential for understanding many applications in machine learning.
                </p>
                
                <!-- Here, the visualizer will be rendered -->
                <div id="orthogonality-visualizer"></div>
                <div id="orthogonality-visualizer-3d"></div>
                <p class="mt-3">
                    In machine learning, orthogonality plays a critical role in feature engineering, dimensionality reduction, 
                    gradient-based optimization, and neural network initialization. Try changing the demo type in the dropdown menu 
                    above to explore different aspects of orthogonality!
                </p>
            </section>
        </div>     
        <script src="/js/main.js"></script>
        <script src="/js/orthogonality_visualizer.js"></script>
        <script src="/js/orthogonality_visualizer_3d.js"></script>
    </body>
</html>
    
    <footer>
    <div class="footer-content">
        <div class="footer-about">
            <h3>About MATH-CS COMPASS</h3>
            <p>Bridging the gap between pure mathematics and computer science applications.</p>
            <div class="contact-section">
                <strong><a href="https://docs.google.com/forms/d/e/1FAIpQLSf3_RNkegREvfCEIQznn6SCaf-iB5c-Doxn7Ymuu78Uf-PBjg/viewform" target="_blank">
                    Submit Your Questions & Feedback <i class="fas fa-external-link-alt"></i>
                </a></strong>
            </div>
        </div>
        <div class="footer-links">
            <h3>Quick Links</h3>
            <ul>
                
                <li><a href="../../../index.html">Home</a></li>
                <li>
                <a href="../Linear_algebra/linear_algebra.html">
                    <span class="nav-number">I</span>
                    <span>Linear Algebra to Algebraic Foundations</span>
                </a>
                </li>
                <li>
                <a href="../Calculus/calculus.html">
                    <span class="nav-number">II</span>
                    <span>Calculus to Optimization & Analysis</span>
                </a>
                </li>
                <li>
                <a href="../Probability/probability.html">
                    <span class="nav-number">III</span>
                    <span>Probability & Statistics</span>
                </a>
                </li>
                <li>
                <a href="../Discrete/discrete_math.html">
                    <span class="nav-number">IV</span>
                    <span>Discrete Mathematics & Algorithms</span>
                </a>
                </li>
                <li>
                <a href="../Machine_learning/ml.html">
                    <span class="nav-number">V</span>
                    <span>Machine Learning</span>
                </a>
                </li>
            </ul>
        </div>
        <div class="footer-social">
            <h3>Connect With Me</h3>
            <p>
                <a href="https://x.com/MathCSCompass?t=Zi8nyfzszeFw8QizbJwnNg&s=09" target="_blank">
                    <i class="fab fa-x-twitter"></i> X (Twitter)
                </a>
            </p>
            <p>
                <a href="https://www.facebook.com/share/15uPaBWgQN/" target="_blank">
                    <i class="fab fa-facebook"></i> Facebook
                </a>
            </p>
      
            <h3>MATH-CS COMPASS App</h3>
            <div class="app-download-footer">
                <a href="https://play.google.com/store/apps/details?id=io.github.yusukey01.twa" target="_blank" rel="noopener">
                    <img src="../../../images/google-play-badge.png" 
                        alt="Get it on Google Play" 
                        class="google-play-badge">
                </a>
            </div>
            
        </div>

    </div>
    <div class="footer-bottom">
        <p>&copy; 2024-2025 MATH-CS COMPASS. All rights reserved.</p>
    </div>
</footer>
    <script src="../../../js/main.js?v=1.0.2"></script>
    <script src="../../../js/search.js?v=1.0.2"></script>
    

    <button id="go-to-top-btn" class="go-to-top-btn" title="Go to top">
        <i class="fas fa-arrow-up"></i>
    </button>
    <script>
        // Register the service worker
        if ('serviceWorker' in navigator) {
          window.addEventListener('load', () => {
            navigator.serviceWorker.register('/js/service-worker.js')
              .then(registration => {
                console.log('ServiceWorker registration successful with scope: ', registration.scope);
              })
              .catch(error => {
                console.log('ServiceWorker registration failed: ', error);
              });
          });
        }
      </script>
</body>
</html>