<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    
    <title>Stochastic Matrix - MATH-CS COMPASS</title>
    
    
    <script>
    window.MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']]
      }
    };
    </script>
    <script id="MathJax-script" defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="preload" href="https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Main-Regular.woff" as="font" type="font/woff2" crossorigin>
    <link rel="preload" href="https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Math-Italic.woff" as="font" type="font/woff2" crossorigin>
    
    
    

    <link rel="stylesheet" href="../../../css/styles.css?v=1.0.2">
  
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css">

    <link rel="dns-prefetch" href="https://cdn.jsdelivr.net">
    <link rel="dns-prefetch" href="https://cdnjs.cloudflare.com">
    
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Learn about stochastic matrices and its applications.">
    <meta name="google-site-verification" content="IieEujypYW38caEVe8ymjczouNl56yxAy27iPztcfRA" />
    

    <link rel="manifest" href="/manifest.json">

    <!-- Theme color for Chrome, Android, and other browsers -->
    <meta name="theme-color" content="#007bff">
  
    <!-- iOS specific tags -->
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <meta name="apple-mobile-web-app-title" content="MATH-CS COMPASS">
    <meta name="mobile-web-app-capable" content="yes">

    <!-- Icon references -->
    <link rel="icon" type="image/png" sizes="1280x1280" href="/images/maskable_icon_x1280.png?v=1.0.2">
    <link rel="icon" type="image/png" sizes="512x512" href="/images//maskable_icon_x512.png?v=1.0.2">
    
    <!-- Apple Touch Icons -->
    <link rel="apple-touch-icon" href="/images/maskable_icon_x1280.png?v=1.0.2">
    <link rel="apple-touch-icon" href="/images/maskable_icon_x512.png?v=1.0.2">

    <!-- For Windows -->
    <meta name="msapplication-TileImage" content="/images/test1.png?v=1.0.2">
    <meta name="msapplication-TileColor" content="#007bff">

    <!-- Schema.org Structured Data for AI Optimization -->
    

    <!-- Article Schema for content pages -->
    
    <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "Article",
      "headline": "Stochastic Matrix",
      "description": "Learn about stochastic matrices and its applications.",
      "datePublished": "2024-01-01",
      "dateModified": "2025-01-01",
      "author": {
        "@type": "Person",
        "name": "Yusuke Yokota",
        "jobTitle": "Mathematics & Computer Science Educator",
        "knowsAbout": [
          "Linear Algebra",
          "Machine Learning",
          "Computer Science",
          "Mathematics"
        ]
      },
      "publisher": {
        "@type": "Organization",
        "name": "MATH-CS COMPASS",
        "url": "https://yusukey01.github.io",
        "logo": {
          "@type": "ImageObject",
          "url": "https://yusukey01.github.io/images/maskable_icon_x512.png"
        }
      },
      "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "https://yusukey01.github.io/Mathematics/Linear_algebra/stochastic.html"
      },
      "about": [
        
        { "@type": "Thing", "name": "Linear Algebra" }
        
      ],
      "educationalLevel": "university",
      "learningResourceType": "tutorial",
      "educationalUse": "instruction"
    }
    </script>
    
</head>

<body>
    <header>
    <nav class="navbar">
        <!-- Logo as Home Button -->
        <div class="logo-home">
            <a href="../../../index.html" 
               class="logo-link " 
               title="Home - Math-CS Compass">
                <img src="../../images/icon_x512.png" 
                     alt="Math-CS Compass Logo" 
                     class="nav-logo">
            </a>
        </div>

        <div class="search-container">
            <input type="text" id="search-input" placeholder="Search keywords...">
            <button id="search-button" aria-label="Search" title="Search website"><i class="fas fa-search"></i></button>
        </div>
        
        <div class="menu-toggle">
            <i class="fas fa-bars"></i>
        </div>
        
        <ul class="nav-links">
            <li><a href="../Linear_algebra/linear_algebra.html" class="active">I - Linear Algebra to Algebraic Foundations</a></li>
            
            <li><a href="../Calculus/calculus.html" >II - Calculus to Optimization & Analysis</a></li>
            
            <li><a href="../Probability/probability.html" >III - Probability & Statistics</a></li>
            
            <li><a href="../Discrete/discrete_math.html" >IV - Discrete Mathematics & Algorithms</a></li>
            
            <li><a href="../Machine_learning/ml.html" >V - Machine Learning</a></li>
        </ul>
    </nav>
</header>
    
    <!DOCTYPE html>
<html>
    <body>
        <!-- LearningResource Schema for Content Pages -->
        <script type="application/ld+json">
        {
        "@context": "https://schema.org",
        "@type": "LearningResource",
        "name": "Stochastic Matrix",
        "description": "Learn about stochastic matrices and its applications.",
        "learningResourceType": "lesson",
        "educationalUse": "instruction",
        "educationalLevel": "university",
        "interactivityType": "active",
        "author": {
            "@type": "Person",
            "name": "Yusuke Yokota",
            "jobTitle": "Mathematics & Computer Science Educator"
        },
        "publisher": {
            "@type": "Organization",
            "name": "MATH-CS COMPASS",
            "url": "https://yusukey01.github.io"
        },
        "about": [
            
            { "@type": "Thing", "name": "Stochastic Matrix" },
            { "@type": "Thing", "name": "Transition Matrix" },
            { "@type": "Thing", "name": "Markov Chains" },
            { "@type": "Thing", "name": "Steady-State Vector" },
            { "@type": "Thing", "name": "Stationary Distribution" },
            { "@type": "Thing", "name": "Probability Vector" },
            { "@type": "Thing", "name": "Spectral Radius" },
            { "@type": "Thing", "name": "Doubly Stochastic" }
            
        ],
        "teaches": [
            "Mathematical concepts",
            "Practical applications",
            "Problem-solving techniques"
        ],
        "isPartOf": {
            "@type": "Course",
            "name": "Linear Algebra",
            "description": "Explore the foundations of Linear Algebra, covering key concepts such as linear equations, vector spaces, eigenvalues, orthogonality, least squares, and stochastic matrices",
            "provider": {
            "@type": "Organization",
            "name": "MATH-CS COMPASS",
            "url": "https://yusukey01.github.io"
            },
            "instructor": {
            "@type": "Person",
            "name": "Yusuke Yokota",
            "jobTitle": "Mathematics & Computer Science Educator"
            },
            "courseCode": "I",
            "hasCourseInstance": {
            "@type": "CourseInstance",
            "courseMode": "online",
            "courseWorkload": "PT2H",
            "instructor": {
                "@type": "Person",
                "name": "Yusuke Yokota"
            }
            },
            "offers": {
            "@type": "Offer",
            "price": "0",
            "priceCurrency": "USD",
            "availability": "https://schema.org/InStock",
            "category": "free"
            }
        }
        }
        </script>
        <!-- WebApplication Schema for Interactive Demos -->
        <script type="application/ld+json">
        {
        "@context": "https://schema.org",
        "@type": "WebApplication",
        "name": "Stochastic Matrix Interactive Demo",
        "description": "Interactive demonstration of stochastic matrix concepts",
        "applicationCategory": "EducationalApplication",
        "operatingSystem": "Web Browser",
        "url": "https://yusukey01.github.io/Mathematics/Linear_algebra/stochastic.html",
        "author": {
            "@type": "Person",
            "name": "Yusuke Yokota"
        },
        "applicationSubCategory": "Mathematical Visualization",
        "featureList": [
            "Interactive mathematical visualization",
            "Real-time parameter adjustment",
            "Educational demonstrations"
        ],
        "isAccessibleForFree": true,
        "educationalUse": "instruction",
        "educationalLevel": "university"
        }
        </script>
        <div class="hero-section">
            <h1 class="webpage-name">Stochastic Matrix
            </h1>
        </div>

        
  <div class="quick-jump-container">
    <button type="button" id="quick-jump-toggle" class="quick-jump-toggle">
      <i class="fas fa-list"></i> <strong>Section I Navigation</strong>
    </button>
    <div id="quick-jump-menu" class="quick-jump-menu">
      <h3>Linear Algebra to Algebraic Foundations</h3>
      <div class="quick-jump-links">

        
        <a href="linear_algebra.html">‚Üê Back to Section I Overview</a>
        <hr style="margin: 8px 0; border: 1px solid #ddd;">
        
        
        <a href="linear_equations.html" >Part 1: Linear Equations</a>
        <a href="linear_transformation.html" >Part 2: Linear Transformation</a>
        <a href="matrix_algebra.html" >Part 3: Matrix Algebra</a>
        <a href="determinants.html" >Part 4: Determinants</a>
        <a href="vectorspaces.html" >Part 5: Vector Spaces</a>
        <a href="eigenvectors.html" >Part 6: Eigenvalues & Eigenvectors</a>
        <a href="orthogonality.html" >Part 7: Orthogonality</a>
        <a href="leastsquares.html" >Part 8: Least-Squares Problems</a>
        <a href="symmetry.html" >Part 9: Symmetry</a>
        <a href="trace.html" >Part 10: Trace and Norms</a>
        <a href="kronecker.html" >Part 11: Kronecker Product & Tensor</a>
        <a href="woodbury.html" >Part 12: Woodbury Matrix Identity</a>
        <a href="stochastic.html" class="current-page">Part 13: Stochastic Matrix</a>
        <a href="graph_laplacian.html" class="current-page">Part 14: Graph Laplacians and Spectral Methods</a>
      </div>
    </div>
</div>


        <div class="topic-nav">
            <a href="#sto">Stochastic Matrix</a>
            <a href="#ssv">Steady-State Vector</a>
        </div> 

        <div class="container">  
           
            <section id="sto" class="section-content">
            <h2>Stochastic Matrix</h2>
            <p>
            A <strong>probability vector</strong> \(x \in \mathbb{R}^n\) is a vector with nonnegative entries that add up 
            to 1, and A <strong>stochastic matrix</strong>(or <strong>transition matrix</strong>) \(P \in \mathbb{R}^{n \times n}\) is 
            a square matrix whose "columns" are probability vectors. 
            <br><br>
            Note: In this page, we will use the <strong>column-stochastic matrix</strong>, but we can also use the 
            <strong>row-stochastic matrix</strong>. Essentially, the choice between a row-stochastic and a column-stochastic 
            matrix is a matter of convention and convenience, and both formulations are equivalent up to taking a transpose. 
            Using the row-stochastic matrix is often natural in Markov chains because each row directly tells 
            you the probabilities of transitioning from a given state to all other states. In linear algebra context, due to 
            the form of \(Ax = b\), the column-stochastic matrix can be chosen more often. (Also, many programming environments 
            default to column vectors.)
            <br><br>
            A <a href="../Probability/markov.html"><strong>Markov chain</strong></a> is a sequence of probability 
            vectors \(x_0, x_1, x_2, \cdots \) together with a stochastic matrix \(P\) 
            such that 
            \[
            x_1 = P x_0, \quad x_2 = P x_1, \quad x_3 = P x_2, \quad \cdots.
            \]
            So, the Markov chain is explained by the first-order difference equation:
            \[
            x_{k+1} = P x_k \quad \text{for } k = 0, 1, 2, \cdots.
            \]
            Here, \(x_k\) is called a <strong>state vector</strong> and we have:
            \[
            x_k = P^k x_0  \quad \text{for } k = 0, 1, 2, \cdots.
            \]

            <div class="proof">
                <span class="proof-title">Example: </span>
                Consider the following two states:
                <ul>
                    <li> State 1: a student is sick</li>
                    <li> State 2: a student is not sick</li>
                </ul>
                We observed an initial state distribution:
                \[
                x_0 = \begin{bmatrix} 0.1 \\ 0.9 \end{bmatrix}
                \]
                which means now, 10% of students are sick 90% are not.
                <br><br>
                Moreover, we assume the following conditions:
                <ul>
                    <li> 70% of sick students recover the next day and 30% remain sick.</li>
                    <li> 5% of not sick students become sick the next day and 95% remain not sick</li>
                </ul>
                So, our stochastic matrix can be written as 
                \[
                P = \begin{bmatrix} 0.3 & 0.05 \\ 0.7 & 0.95 \end{bmatrix}.
                \]
                Then, 
                \[
                x_1 = P x_0 = \begin{bmatrix} 0.3 & 0.05 \\ 0.7 & 0.95 \end{bmatrix} \begin{bmatrix} 0.1 \\ 0.9 \end{bmatrix} = \begin{bmatrix} 0.075 \\ 0.925 \end{bmatrix}
                \]
                This means that on the next day, approximately 7.5% students are expected to be sick and 92.5% are not.
                <br><br>
                We can keep going this process:
                \[
                \begin{align*}
                &x_2 = P x_1 = \begin{bmatrix} 0.3 & 0.05 \\ 0.7 & 0.95 \end{bmatrix} \begin{bmatrix} 0.075 \\ 0.925 \end{bmatrix} = \begin{bmatrix} 0.06875 \\ 0.93125 \end{bmatrix} \\\\
                &x_3 = P x_2 = \begin{bmatrix} 0.3 & 0.05 \\ 0.7 & 0.95 \end{bmatrix} \begin{bmatrix} 0.06875 \\ 0.93125 \end{bmatrix} = \begin{bmatrix} 0.0671875 \\ 0.9328125 \end{bmatrix}
                \end{align*}
                \]
                and so on. 
            </div>
            </p>
            </section>

            <section id="ssv" class="section-content">
            <h2>Steady-State Vector</h2>
            <p>
            A <strong>steady-state vector</strong> \(q\) for a stochastic matrix \(P\) is defined as 
            \[
            P q = q.
            \]
            In statistics, we also call it <strong>stationary distribution</strong>. 
            <br><br>
            The Markov chain, a sequence of vectors \(\{x_k : k = 1, 2, \cdots\}\) converges to the unique steady-state 
            vector \(q\) as \(k \to \infty\). Importantly, the initial state does not effect on the long-term behavior of the 
            Markov chain. 
            <br><br>
            In short, the reason why it happens is related to the fact that the steady-state vector \(q\) is 
            an <strong>eigenvector</strong> of the stochastic matrix corresponding to the largest eigenvalue \(\lambda_1 = 1\). Let's 
            dig a little deeper. 
            <br><br>
            For any matrix \(A\), the <strong>spectral radius</strong> \(\rho(A)\) is defined by the maximum absolute value of its 
            eigenvalues. It satisfies
            \[
            \rho(A) \leq \| A \|
            \]
            for any submultiplicative matrix norm \(\| \cdot \| \).
            <br><br>
            Remember, by definition, every column of a column-stochastic matrix sums to 1, which means the maximum absolute 
            column sum (<a href="trace.html"><strong>1-norm</strong></a>, \(\| P \|_1\)) is always 1. Thus:
            \[
            \rho(P) \leq \| P \|_1 = 1
            \]
            (For the row-stochastic matrix, we use the <strong>infinity norm</strong> \(\| P \|_{\infty} =1\).)
            <br>
            Since we already know that 1 is an eigenvalue of \(P\), the spectral radius must be exactly 1, and no eigenvalue 
            can have greater than 1. This means that as \(k \to \infty\), the contribution from all components other than the 
            one corresponding to \(\lambda_1 =1\) decays exponentially fast. Therefore, any initial distribution converges to the 
            stationary distribution.
            <br>
            <div class="proof">
                <span class="proof-title">Example: </span>
                In our example, 
                \[
                \begin{align*}
                & P q = q \\\\
                &\begin{bmatrix} 0.3 & 0.05 \\ 0.7 & 0.95 \end{bmatrix} \begin{bmatrix} q_1 \\ q_2 \end{bmatrix} = \begin{bmatrix} q_1 \\ q_2 \end{bmatrix}\\\\
                & q_1 + q_2 = 1 \\\\
                &\Longrightarrow q = \begin{bmatrix} \frac{1}{15} \\ \frac{14}{15} \end{bmatrix}
                \end{align*}
                \]
                which means that in the long run, about 6.67 % of the students will be sick and about 93.33 % will be not sick.
                <br><br>
                Find eigenvalues and corresponding eigenvectors:
                \[
                \begin{align*}
                &\det(P - \lambda I) = 0 \\\\
                &\Longrightarrow \lambda^2 - 1.25\lambda + 0.25 = 0 \\\\
                &\Longrightarrow (\lambda -1)(\lambda -0.25) = 0 \\\\
                &\Longrightarrow \lambda_1 = 1, \quad \lambda_2 = 0.25.
                \end{align*}
                \]
                For \(\lambda_1 = 1\), solving \((P -I)v_1 = 0\), we obtain the corresponding eigenvector:
                \[
                v_1 = \begin{bmatrix} 1 \\ 14 \end{bmatrix}.
                \]
                (Scaling by \(\frac{1}{15}\), we can get the stationary distribution \(q = \frac{1}{15}v_1\)).
                <br><br>
                For \(\lambda_2 = 0.25\), solving \((P -0.25I)v_2 = 0\), we obtain the corresponding eigenvector:
                \[
                v_2 =  \begin{bmatrix} - 1 \\  1 \end{bmatrix}.
                \]
                So, 
                \[
                V = \begin{bmatrix} 1 & - 1 \\ 14 & 1 \end{bmatrix}, \quad V^{-1} = \frac{1}{15} \begin{bmatrix}  1 & 1 \\ -14 & 1 \end{bmatrix}, 
                \]
                and 
                \[
                D = \begin{bmatrix} \lambda_1 & 0\\ 0 & \lambda_2 \end{bmatrix} = \begin{bmatrix} 1 & 0 \\ 0 & 0.25 \end{bmatrix}
                \]
                Thus, the transition matrix \(P\) after \(k\) steps can be written as: 
                <br>
                \[
                \begin{align*}
                P^k &= V D^k V^{-1} \\\\
                    &= \begin{bmatrix} 1 & - 1 \\ 14 & 1 \end{bmatrix} 
                      \begin{bmatrix} 1^k & 0 \\ 0 & (0.25)^k \end{bmatrix} 
                      \frac{1}{15}\begin{bmatrix} 1 & 1 \\ -14 & 1 \end{bmatrix}.
                \end{align*}
                \]
                <br>
                The error in the state distribution after \(k\) steps is dominated by the term \(\lambda_2^k = (0.25)^k\). Thus, the 
                convergence rate of the Markov chain toward the stationary distribution is exponential, with each additional step reducing 
                the error roughly by a factor of \(0.25\):
                <br>
                \[
                e_k \approx e_0 (0.25)^k
                \]
                where \(e_0\) is the initial error.
            </div>
             In this example, \(P\) is <a href="eigenvectors.html"><strong>diagonalizable</strong></a>, but in general, the stochastic matrices are not always 
             diagonalizable. 
             <br><br>
             A matrix \(A\) is said to be <strong>doubly stochastic</strong> if the sum of each row and the sum of each column are 1. 
             In this case, the matrix is not always diagonalizable, but in the case \(n =2\), it becomes a symmetric matrix and thus, it is diagonalizable.
             Consider the following doubly stochastic matrix for \(t \neq 0\):
             \[
             A = \begin{bmatrix} 1 - t & t \\  t & 1 -t \end{bmatrix}
             \]
             Since the trace of \(A\) is \(2-2t\), and every stochastic matrix has an eigenvalue \(\lambda_1 =  1\) 
             corresponding to an eigenvector \(v_1 = \begin{bmatrix} 1 \\ 1 \end{bmatrix}\).
             \[
             \lambda_1 + \lambda_2 = 1 + \lambda_2  = 2 - 2t
             \]
             Thus, \(\lambda_2 = 1 -2t\). Moreover, since the matrix is symmetric, and the two eigenvalues are distinct, their corresponding 
             eigenvectors must be orthogonal each other. Therefore, we have \(v_2 = \begin{bmatrix} 1 \\ -1 \end{bmatrix}\) and 
             \[
             A = \begin{bmatrix} 1 & 1 \\ 1 & -1 \end{bmatrix} 
             \begin{bmatrix} 1  & 0 \\ 0 & 1-2t \end{bmatrix} 
             \frac{1}{-2}\begin{bmatrix} -1 & -1 \\ -1 & 1 \end{bmatrix}.
             \]
            </p>
            </section>
        </div>
       
        <script src="/js/main.js"></script>
    </body>
</html>
    
    <footer>
    <div class="footer-content">
        <div class="footer-about">
            <h3>About MATH-CS COMPASS</h3>
            <p>Bridging the gap between pure mathematics and computer science applications.</p>
            <div class="contact-section">
                <strong><a href="https://docs.google.com/forms/d/e/1FAIpQLSf3_RNkegREvfCEIQznn6SCaf-iB5c-Doxn7Ymuu78Uf-PBjg/viewform" target="_blank">
                    Submit Your Questions & Feedback <i class="fas fa-external-link-alt"></i>
                </a></strong>
            </div>
        </div>
        <div class="footer-links">
            <h3>Quick Links</h3>
            <ul>
                
                <li><a href="../../../index.html">Home</a></li>
                <li>
                <a href="../Linear_algebra/linear_algebra.html">
                    <span class="nav-number">I</span>
                    <span>Linear Algebra to Algebraic Foundations</span>
                </a>
                </li>
                <li>
                <a href="../Calculus/calculus.html">
                    <span class="nav-number">II</span>
                    <span>Calculus to Optimization & Analysis</span>
                </a>
                </li>
                <li>
                <a href="../Probability/probability.html">
                    <span class="nav-number">III</span>
                    <span>Probability & Statistics</span>
                </a>
                </li>
                <li>
                <a href="../Discrete/discrete_math.html">
                    <span class="nav-number">IV</span>
                    <span>Discrete Mathematics & Algorithms</span>
                </a>
                </li>
                <li>
                <a href="../Machine_learning/ml.html">
                    <span class="nav-number">V</span>
                    <span>Machine Learning</span>
                </a>
                </li>
            </ul>
        </div>
        <div class="footer-social">
            <h3>Connect With Me</h3>
            <p>
                <a href="https://x.com/MathCSCompass?t=Zi8nyfzszeFw8QizbJwnNg&s=09" target="_blank">
                    <i class="fab fa-x-twitter"></i> X (Twitter)
                </a>
            </p>
            <p>
                <a href="https://www.facebook.com/share/15uPaBWgQN/" target="_blank">
                    <i class="fab fa-facebook"></i> Facebook
                </a>
            </p>
      
            <h3>MATH-CS COMPASS App</h3>
            <div class="app-download-footer">
                <a href="https://play.google.com/store/apps/details?id=io.github.yusukey01.twa" target="_blank" rel="noopener">
                    <img src="../../../images/google-play-badge.png" 
                        alt="Get it on Google Play" 
                        class="google-play-badge">
                </a>
            </div>
            
        </div>

    </div>
    <div class="footer-bottom">
        <p>&copy; 2024-2025 MATH-CS COMPASS. All rights reserved.</p>
    </div>
</footer>
    <script src="../../../js/main.js?v=1.0.2"></script>
    <script src="../../../js/search.js?v=1.0.2"></script>
    

    <button id="go-to-top-btn" class="go-to-top-btn" title="Go to top">
        <i class="fas fa-arrow-up"></i>
    </button>
    <script>
        // Register the service worker
        if ('serviceWorker' in navigator) {
          window.addEventListener('load', () => {
            navigator.serviceWorker.register('/js/service-worker.js')
              .then(registration => {
                console.log('ServiceWorker registration successful with scope: ', registration.scope);
              })
              .catch(error => {
                console.log('ServiceWorker registration failed: ', error);
              });
          });
        }
      </script>
</body>
</html>