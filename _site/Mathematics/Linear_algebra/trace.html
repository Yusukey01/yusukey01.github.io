<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    
    <title>Trace & Norms - MATH-CS COMPASS</title>
    
    
    <script>
    window.MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']]
      }
    };
    </script>
    <script id="MathJax-script" defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="preload" href="https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Main-Regular.woff" as="font" type="font/woff2" crossorigin>
    <link rel="preload" href="https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Math-Italic.woff" as="font" type="font/woff2" crossorigin>
    
    
    

    <link rel="stylesheet" href="../../../css/styles.css?v=1.0.2">
  
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css">

    <link rel="dns-prefetch" href="https://cdn.jsdelivr.net">
    <link rel="dns-prefetch" href="https://cdnjs.cloudflare.com">
    
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Learn about trace, norms, and introduction to Metric Spaces.">
    <meta name="google-site-verification" content="IieEujypYW38caEVe8ymjczouNl56yxAy27iPztcfRA" />
    

    <link rel="manifest" href="/manifest.json">

    <!-- Theme color for Chrome, Android, and other browsers -->
    <meta name="theme-color" content="#007bff">
  
    <!-- iOS specific tags -->
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <meta name="apple-mobile-web-app-title" content="MATH-CS COMPASS">
    <meta name="mobile-web-app-capable" content="yes">

    <!-- Icon references -->
    <link rel="icon" type="image/png" sizes="1280x1280" href="/images/maskable_icon_x1280.png?v=1.0.2">
    <link rel="icon" type="image/png" sizes="512x512" href="/images//maskable_icon_x512.png?v=1.0.2">
    
    <!-- Apple Touch Icons -->
    <link rel="apple-touch-icon" href="/images/maskable_icon_x1280.png?v=1.0.2">
    <link rel="apple-touch-icon" href="/images/maskable_icon_x512.png?v=1.0.2">

    <!-- For Windows -->
    <meta name="msapplication-TileImage" content="/images/test1.png?v=1.0.2">
    <meta name="msapplication-TileColor" content="#007bff">

    <!-- Schema.org Structured Data for AI Optimization -->
    

    <!-- Article Schema for content pages -->
    
    <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "Article",
      "headline": "Trace & Norms",
      "description": "Learn about trace, norms, and introduction to Metric Spaces.",
      "datePublished": "2024-01-01",
      "dateModified": "2025-01-01",
      "author": {
        "@type": "Person",
        "name": "Yusuke Yokota",
        "jobTitle": "Mathematics & Computer Science Educator",
        "knowsAbout": [
          "Linear Algebra",
          "Machine Learning",
          "Computer Science",
          "Mathematics"
        ]
      },
      "publisher": {
        "@type": "Organization",
        "name": "MATH-CS COMPASS",
        "url": "https://yusukey01.github.io",
        "logo": {
          "@type": "ImageObject",
          "url": "https://yusukey01.github.io/images/maskable_icon_x512.png"
        }
      },
      "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "https://yusukey01.github.io/Mathematics/Linear_algebra/trace.html"
      },
      "about": [
        
        { "@type": "Thing", "name": "Linear Algebra" }
        
      ],
      "educationalLevel": "university",
      "learningResourceType": "tutorial",
      "educationalUse": "instruction"
    }
    </script>
    
</head>

<body>
    <header>
    <nav class="navbar">
        <!-- Logo as Home Button -->
        <div class="logo-home">
            <a href="../../../index.html" 
               class="logo-link " 
               title="Home - Math-CS Compass">
                <img src="../../images/icon_x512.png" 
                     alt="Math-CS Compass Logo" 
                     class="nav-logo">
            </a>
        </div>

        <div class="search-container">
            <input type="text" id="search-input" placeholder="Search keywords...">
            <button id="search-button" aria-label="Search" title="Search website"><i class="fas fa-search"></i></button>
        </div>
        
        <div class="menu-toggle">
            <i class="fas fa-bars"></i>
        </div>
        
        <ul class="nav-links">
            <li><a href="../Linear_algebra/linear_algebra.html" class="active">I - Linear Algebra to Algebraic Foundations</a></li>
            
            <li><a href="../Calculus/calculus.html" >II - Calculus to Optimization & Analysis</a></li>
            
            <li><a href="../Probability/probability.html" >III - Probability & Statistics</a></li>
            
            <li><a href="../Discrete/discrete_math.html" >IV - Discrete Mathematics & Algorithms</a></li>
            
            <li><a href="../Machine_learning/ml.html" >V - Machine Learning</a></li>
        </ul>
    </nav>
</header>
    
    <!DOCTYPE html>
<html>
    <body>
        <!-- LearningResource Schema for Content Pages -->
        <script type="application/ld+json">
        {
        "@context": "https://schema.org",
        "@type": "LearningResource",
        "name": "Trace & Norms",
        "description": "Learn about trace, norms, and introduction to Metric Spaces.",
        "learningResourceType": "lesson",
        "educationalUse": "instruction",
        "educationalLevel": "university",
        "interactivityType": "active",
        "author": {
            "@type": "Person",
            "name": "Yusuke Yokota",
            "jobTitle": "Mathematics & Computer Science Educator"
        },
        "publisher": {
            "@type": "Organization",
            "name": "MATH-CS COMPASS",
            "url": "https://yusukey01.github.io"
        },
        "about": [
            
            { "@type": "Thing", "name": "Trace" },
            { "@type": "Thing", "name": "Matrix Norms" },
            { "@type": "Thing", "name": "Frobenius Norm" },
            { "@type": "Thing", "name": "Spectral Norm" },
            { "@type": "Thing", "name": "Nuclear Norm" },
            { "@type": "Thing", "name": "Metric Spaces" },
            { "@type": "Thing", "name": "p-Norms" },
            { "@type": "Thing", "name": "Mathematical Analysis" }
            
        ],
        "teaches": [
            "Mathematical concepts",
            "Practical applications",
            "Problem-solving techniques"
        ],
        "isPartOf": {
            "@type": "Course",
            "name": "Linear Algebra",
            "description": "Explore the foundations of Linear Algebra, covering key concepts such as linear equations, vector spaces, eigenvalues, orthogonality, least squares, and stochastic matrices",
            "provider": {
            "@type": "Organization",
            "name": "MATH-CS COMPASS",
            "url": "https://yusukey01.github.io"
            },
            "instructor": {
            "@type": "Person",
            "name": "Yusuke Yokota",
            "jobTitle": "Mathematics & Computer Science Educator"
            },
            "courseCode": "I",
            "hasCourseInstance": {
            "@type": "CourseInstance",
            "courseMode": "online",
            "courseWorkload": "PT2H",
            "instructor": {
                "@type": "Person",
                "name": "Yusuke Yokota"
            }
            },
            "offers": {
            "@type": "Offer",
            "price": "0",
            "priceCurrency": "USD",
            "availability": "https://schema.org/InStock",
            "category": "free"
            }
        }
        }
        </script>
        <!-- WebApplication Schema for Interactive Demos -->
        <script type="application/ld+json">
        {
        "@context": "https://schema.org",
        "@type": "WebApplication",
        "name": "Trace & Norms Interactive Demo",
        "description": "Interactive demonstration of trace & norms concepts",
        "applicationCategory": "EducationalApplication",
        "operatingSystem": "Web Browser",
        "url": "https://yusukey01.github.io/Mathematics/Linear_algebra/trace.html",
        "author": {
            "@type": "Person",
            "name": "Yusuke Yokota"
        },
        "applicationSubCategory": "Mathematical Visualization",
        "featureList": [
            "Interactive mathematical visualization",
            "Real-time parameter adjustment",
            "Educational demonstrations"
        ],
        "isAccessibleForFree": true,
        "educationalUse": "instruction",
        "educationalLevel": "university"
        }
        </script>
        <div class="hero-section">
            <h1 class="webpage-name">Trace & Norms
            </h1>
        </div>

        
  <div class="quick-jump-container">
    <button type="button" id="quick-jump-toggle" class="quick-jump-toggle">
      <i class="fas fa-list"></i> <strong>Section I Navigation</strong>
    </button>
    <div id="quick-jump-menu" class="quick-jump-menu">
      <h3>Linear Algebra to Algebraic Foundations</h3>
      <div class="quick-jump-links">

        
        <a href="linear_algebra.html">‚Üê Back to Section I Overview</a>
        <hr style="margin: 8px 0; border: 1px solid #ddd;">
        
        
        <a href="linear_equations.html" >Part 1: Linear Equations</a>
        <a href="linear_transformation.html" >Part 2: Linear Transformation</a>
        <a href="matrix_algebra.html" >Part 3: Matrix Algebra</a>
        <a href="determinants.html" >Part 4: Determinants</a>
        <a href="vectorspaces.html" >Part 5: Vector Spaces</a>
        <a href="eigenvectors.html" >Part 6: Eigenvalues & Eigenvectors</a>
        <a href="orthogonality.html" >Part 7: Orthogonality</a>
        <a href="leastsquares.html" >Part 8: Least-Squares Problems</a>
        <a href="symmetry.html" >Part 9: Symmetry</a>
        <a href="trace.html" class="current-page">Part 10: Trace and Norms</a>
        <a href="kronecker.html" >Part 11: Kronecker Product & Tensor</a>
        <a href="woodbury.html" >Part 12: Woodbury Matrix Identity</a>
        <a href="stochastic.html" >Part 13: Stochastic Matrix</a>
        <a href="graph_laplacian.html" >Part 14: Graph Laplacians and Spectral Methods</a>
      </div>
    </div>
</div>


        <div class="topic-nav">
            <a href="#tr">Trace</a>
            <a href="#norms">Norms</a>
            <a href="#ms">Introduction to Metric Spaces</a>
        </div> 

        <div class="container">  
           
            <section id="tr" class="section-content">
            <h2>Trace</h2>
            <p>
            The <strong>trace</strong> of an \(n \times n\) matrix \(A\) is the sum of the diagonal entries of \(A\) and
            denoted by \(\text{tr }(A)\).
            \[
            \text{tr }(A) =\sum_{k=1}^n a_{kk} 
            \]
            We list some properties of the trace. 
            <ol style="padding-left: 40px;">
                <li>\(\text{tr }(cA) = c (\text{tr }A), \, c \in \mathbb{R}\)</li><br>
                <li>\(\text{tr }(A+B) = \text{tr }A + \text{tr }B, \, B \in \mathbb{R}^{n \times n}\)</li><br>
                <li>\(\text{tr }A^T = \text{tr }A \)</li> <br>
                <li>\(\text{tr }(A^TB) = \sum_{i=1}^m\sum_{j=1}^n a_{ij}b_{ij}\)</li> <br>
                <li>\(\text{tr }(AB) = \text{tr }(BA) \)</li>
                Note: \(A\) can be \(m \times n\) and  \(B\) can be \(n \times m\). More generally, <br>
                \(\text{tr }(ABC) = \text{tr }(CAB) = \text{tr }(BCA) \): <strong>cyclic permutation property</strong> <br><br>
                <li>\(\text{tr }A =\sum_{i=1}^n \lambda_i \) where \(\lambda_1, \cdots, \lambda_n\) are the eigenvalues of \(A\)</li>
            </ol>
            <br>
            For example, consider a <a href="symmetry.html"><strong>quadratic form</strong></a> \(x^TAx \in \mathbb{R}\). 
            <br>
            Let \(x\) be a vector in \(\mathbb{R}^n\). By Property 1 and 5, 
            \[
            \begin{align*}
            x^TAx = \text{tr }(x^TAx) &= \text{tr }(xx^TA) \\\\
                                      &= (x^Tx)\text{tr }(A)
            \end{align*}
            \]
            <br>
            Property 4 is actually the definition of the <a href="orthogonality.html"><strong>inner product</strong></a> for matrices. 
            <br>
            We call it the <strong>Frobenius inner product</strong>.  For matrices \(A, B \in \mathbb{R}^{m \times n}\),
            \[
            \left\langle A, B \right\rangle_F = \text{tr }(A^TB) = \sum_{i=1}^m\sum_{j=1}^n a_{ij}b_{ij}
            \]
            Also, like the Euclidean(\(l_2\)) norm \(\|v\|_2 = \sqrt{v \cdot v} = \sqrt{v^Tv}\) of vector, we define 
            the <strong>Frobenius norm</strong> of a "matrix" \(A\):
            \[
            \| A \|_F = \sqrt{\left\langle A, A \right\rangle_F} = \sqrt{\text{tr }(A^TA)}
            \] 
            </p>
            </section>

            <section id="norms" class="section-content">
            <h2>Norms</h2>
            <p>
            Norms play a key role in <strong>normalization</strong> and <strong>regularization</strong>. Depending on the
            application, we choose an appropriate norm. In machine learning, norms are fundamental for training stability of models. 
            Norms are used to scale data to a "common" range. It ensures that every single feature contributes
            "equally" to the training of the model and reduces sensitivity to the scale of each feature. This
            <strong>normalization</strong> process is important in the "distance(= norm)" based models such as 
            <a href="../Machine_learning/svm.html"><strong>support vector machines(SVMs)</strong></a> and <strong>\(k\)-nearest neighbor(k-NN)</strong>. 
            Also, <a href="../Machine_learning/regression.html"><strong>regularization process</strong></a> uses norms to "penalize" the model complexity, which avoids 
            overfitting and then helps the model <strong>generalize</strong> to new data.
            <br><br>
            Another popular norm of a matrix is the <strong>nuclear norm(trace norm)</strong>. 
            <br>Given a matrix \(A \in \mathbb{R}^{m \times n}\), then the nuclear norm of \(A\) is defined as the sum of its
            <strong>singular values</strong>.
            \[\| A\|_{\ast} = \sum_{i} \sigma_i \geq 0\]
            <div class="theorem">
                <span class="theorem-title">Theorem 1:</span>  
                Let a matrix \(A = U\Sigma V^T\) by SVD. Then
                \[
                \| A\|_{\ast} = \sum_{i} \sigma_i = \text{tr }(\Sigma) = \text{tr }(\sqrt{A^TA})
                \].
            </div>
            <div class="proof">
                <span class="proof-title">Proof:</span>
                Suppose \(A = U\Sigma V^T\) by the singular value decomposition. 
                Since \(A^TA = (V\Sigma U^T)(U\Sigma V^T) = V\Sigma^2 V^T\), 
                \[\sqrt{A^TA} = V\Sigma V^T\]
                Then by the cyclic permutation property of trace, 
                \[
                \begin{align*}
                \text{tr }(\sqrt{A^TA}) = \text{tr }(V\Sigma V^T) &= \text{tr }(\Sigma V^T V) \\\\
                                                                  &= \text{tr }(\Sigma )
                \end{align*}
                \]
            </div>
            <br>
            Also, the <strong>induced norm</strong> of \(A\) is defined as
            \[\| A \|_p = \max_{x \neq 0} \frac{\| Ax \|_p}{\| x \|_p} = \max_{\| x \| =1} \| Ax \|_p \tag{1}\]
            Typically, when \(p =2\), we call it the <strong>spectral norm</strong> of \(A\), which is just
            the largest singular value of \(A\).
            \[\| A \|_2 = \max_{i} \sigma_i  = \sqrt{\lambda_{\max} (A^TA)}\] 
            where \(\lambda_{\max}\) is the largest eigenvalue of \(A^TA\).
            <br>The spectral norm is commonly used to control the <strong>Lipschiz continuity</strong> of
            functions in the <a href="../Machine_learning/neural_networks.html"><strong>neural network</strong></a> in order to prevent <strong>vanishing or exploding
            gradients</strong>.
            <br><br>
            \(\| x \|_p \) in (1) is called the <strong>\(p\)-norm</strong> of the vector \(x\). The Euclidean norm is 
            a specific case of \(p\)-norm where \(p =2\). In general, 
            \[\| x \|_p = (\sum_{i =1} ^n |x_i|^p)^{\frac{1}{p}} \quad , p \geq 1\]
            When \(p =1\), we call it <strong>Manhattan norm</strong>, which is commonly used in the grid-based
            environments. Also, if we need <strong>sparsity</strong> in models such as Lasso regression, the Manhattan
            norm is used as a regularizer. 
            <br>Moreover, when \(p\) approaches \(\infty\), we define the <strong>maximum norm</strong>: 
            \[\| x \|_\infty = \lim_{p \to \infty} \| x \|_p = \max_{i} |x_i|\]
            In numerical analysis, the maximum norm is used for "worst-case" error analysis. Similarly,
            in machine learning, it is used when we want to minimize the worst error rather than the total
            error across sample data.
            </p>
            </section>

            <section id="ms" class="section-content">
            <h2>Introduction to Metric Spaces</h2>
            <p>
            We have learned about <a href="vectorspaces.html"><strong>vector spaces</strong></a> and 
            norms, which measure "distance" between points(vectors) in \(\mathbb{R}^n\). Although this concept may seem
            "general" as it extends beyond the familiar 3D space, the vector space with norms is actually a specific
            case of a more general mathematical structure called a <strong>metric space</strong>. 
            <br><br>
            Understanding this broader framework helps us see how the norms we use in linear algebra fit into the larger 
            picture of <strong>mathematical analysis</strong>. As an introduction to this perspective, we define the metric 
            space \((M, d)\) as follows.
        
            <div class="theorem">
                <span class="theorem-title">Metric Space \((M, d)\):</span> 
                Suppose \(M\) is a set and \(d\) is a real function defined on the Cartesian product
                \(M \times M\). Then \(d\) is called a distance function, or <strong>metric</strong> on \(M\) if and only if 
                \(\forall a, b, c \in M\),
                <ol style="padding-left: 40px;">
                    <li>Positivity: \(d(a,b) \geq 0\) with equality iff \(a = b\)</li>
                    <li>Symmetry: \(d(a,b) = d(b,a)\)</li>
                    <li>The triangle inequality \(d(a,b) \leq d(a,c) + d(c, b)\) </li>
                </ol>
            </div>

            Often we just say "the metric space \(M\)" if the metric \(d\) is obvious.
            <br>Note: Suppose \(X\) and \(Y\) are sets. Then <strong>Cartesian product </strong>of \(X\) and \(Y\)
            is a set of "ordered" pair 
            \[
            \{(x, y) | x \in X, \, y\in Y\}
            \] 
            Now, you can see that any vector space equipped with a norm is a metric space. We call it 
            a <strong>normed vector space</strong>. More specifically, in linear algebra, we have worked within
            the framework of <strong>inner product spaces</strong> whose norm is defined by the inner product
            \[
            d(u, v) = \|u-v\|= \sqrt{\langle u-v, u-v \rangle} 
            \]
            The most familiar example of inner product space is the <strong>Euclidean space</strong> \(\mathbb{R}^n\).

            In <strong>machine learning</strong>, understanding how inner product spaces naturally extend to normed spaces and then to metric 
            spaces provides the mathematical foundation for defining <strong>similarity</strong> measures, regularization techniques, 
            and convergence criteria in optimization algorithms.
            </p>
            </section>
        </div>
        <script src="/js/main.js"></script>
    </body>
</html>
    
    <footer>
    <div class="footer-content">
        <div class="footer-about">
            <h3>About MATH-CS COMPASS</h3>
            <p>Bridging the gap between pure mathematics and computer science applications.</p>
            <div class="contact-section">
                <strong><a href="https://docs.google.com/forms/d/e/1FAIpQLSf3_RNkegREvfCEIQznn6SCaf-iB5c-Doxn7Ymuu78Uf-PBjg/viewform" target="_blank">
                    Submit Your Questions & Feedback <i class="fas fa-external-link-alt"></i>
                </a></strong>
            </div>
        </div>
        <div class="footer-links">
            <h3>Quick Links</h3>
            <ul>
                
                <li><a href="../../../index.html">Home</a></li>
                <li>
                <a href="../Linear_algebra/linear_algebra.html">
                    <span class="nav-number">I</span>
                    <span>Linear Algebra to Algebraic Foundations</span>
                </a>
                </li>
                <li>
                <a href="../Calculus/calculus.html">
                    <span class="nav-number">II</span>
                    <span>Calculus to Optimization & Analysis</span>
                </a>
                </li>
                <li>
                <a href="../Probability/probability.html">
                    <span class="nav-number">III</span>
                    <span>Probability & Statistics</span>
                </a>
                </li>
                <li>
                <a href="../Discrete/discrete_math.html">
                    <span class="nav-number">IV</span>
                    <span>Discrete Mathematics & Algorithms</span>
                </a>
                </li>
                <li>
                <a href="../Machine_learning/ml.html">
                    <span class="nav-number">V</span>
                    <span>Machine Learning</span>
                </a>
                </li>
            </ul>
        </div>
        <div class="footer-social">
            <h3>Connect With Me</h3>
            <p>
                <a href="https://x.com/MathCSCompass?t=Zi8nyfzszeFw8QizbJwnNg&s=09" target="_blank">
                    <i class="fab fa-x-twitter"></i> X (Twitter)
                </a>
            </p>
            <p>
                <a href="https://www.facebook.com/share/15uPaBWgQN/" target="_blank">
                    <i class="fab fa-facebook"></i> Facebook
                </a>
            </p>
      
            <h3>MATH-CS COMPASS App</h3>
            <div class="app-download-footer">
                <a href="https://play.google.com/store/apps/details?id=io.github.yusukey01.twa" target="_blank" rel="noopener">
                    <img src="../../../images/google-play-badge.png" 
                        alt="Get it on Google Play" 
                        class="google-play-badge">
                </a>
            </div>
            
        </div>

    </div>
    <div class="footer-bottom">
        <p>&copy; 2024-2025 MATH-CS COMPASS. All rights reserved.</p>
    </div>
</footer>
    <script src="../../../js/main.js?v=1.0.2"></script>
    <script src="../../../js/search.js?v=1.0.2"></script>
    

    <button id="go-to-top-btn" class="go-to-top-btn" title="Go to top">
        <i class="fas fa-arrow-up"></i>
    </button>
    <script>
        // Register the service worker
        if ('serviceWorker' in navigator) {
          window.addEventListener('load', () => {
            navigator.serviceWorker.register('/js/service-worker.js')
              .then(registration => {
                console.log('ServiceWorker registration successful with scope: ', registration.scope);
              })
              .catch(error => {
                console.log('ServiceWorker registration failed: ', error);
              });
          });
        }
      </script>
</body>
</html>