<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    
    <title>Multivariate Distributions - MATH-CS COMPASS</title>
    
    
    <script>
    window.MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']]
      }
    };
    </script>
    <script id="MathJax-script" defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="preload" href="https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Main-Regular.woff" as="font" type="font/woff2" crossorigin>
    <link rel="preload" href="https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Math-Italic.woff" as="font" type="font/woff2" crossorigin>
    
    
    

    <link rel="stylesheet" href="../../../css/styles.css?v=1.0.2">
  
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css">

    <link rel="dns-prefetch" href="https://cdn.jsdelivr.net">
    <link rel="dns-prefetch" href="https://cdnjs.cloudflare.com">
    
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Learn about multivariate normal distribution, Dirichlet distribution, and Wishart distribution.">
    <meta name="google-site-verification" content="IieEujypYW38caEVe8ymjczouNl56yxAy27iPztcfRA" />
    

    <link rel="manifest" href="/manifest.json">

    <!-- Theme color for Chrome, Android, and other browsers -->
    <meta name="theme-color" content="#007bff">
  
    <!-- iOS specific tags -->
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <meta name="apple-mobile-web-app-title" content="MATH-CS COMPASS">
    <meta name="mobile-web-app-capable" content="yes">

    <!-- Icon references -->
    <link rel="icon" type="image/png" sizes="1280x1280" href="/images/maskable_icon_x1280.png?v=1.0.2">
    <link rel="icon" type="image/png" sizes="512x512" href="/images//maskable_icon_x512.png?v=1.0.2">
    
    <!-- Apple Touch Icons -->
    <link rel="apple-touch-icon" href="/images/maskable_icon_x1280.png?v=1.0.2">
    <link rel="apple-touch-icon" href="/images/maskable_icon_x512.png?v=1.0.2">

    <!-- For Windows -->
    <meta name="msapplication-TileImage" content="/images/test1.png?v=1.0.2">
    <meta name="msapplication-TileColor" content="#007bff">

    <!-- Schema.org Structured Data for AI Optimization -->
    

    <!-- Article Schema for content pages -->
    
    <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "Article",
      "headline": "Multivariate Distributions",
      "description": "Learn about multivariate normal distribution, Dirichlet distribution, and Wishart distribution.",
      "datePublished": "2024-01-01",
      "dateModified": "2025-01-01",
      "author": {
        "@type": "Person",
        "name": "Yusuke Yokota",
        "jobTitle": "Mathematics & Computer Science Educator",
        "knowsAbout": [
          "Linear Algebra",
          "Machine Learning",
          "Computer Science",
          "Mathematics"
        ]
      },
      "publisher": {
        "@type": "Organization",
        "name": "MATH-CS COMPASS",
        "url": "https://math-cs-compass.com",
        "logo": {
          "@type": "ImageObject",
          "url": "https://math-cs-compass.com/images/maskable_icon_x512.png"
        }
      },
      "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "https://math-cs-compass.com/Mathematics/Probability/mvn.html"
      },
      "about": [
        
        { "@type": "Thing", "name": "Probability" },
        { "@type": "Thing", "name": "Statistics" }
        
      ],
      "educationalLevel": "university",
      "learningResourceType": "tutorial",
      "educationalUse": "instruction"
    }
    </script>
    
</head>

<body>
    <header>
    <nav class="navbar">
        <!-- Logo as Home Button -->
        <div class="logo-home">
            <a href="../../../index.html" 
               class="logo-link " 
               title="Home - Math-CS Compass">
                <img src="../../images/icon_x512.png" 
                     alt="Math-CS Compass Logo" 
                     class="nav-logo">
            </a>
        </div>

        <div class="search-container">
            <input type="text" id="search-input" placeholder="Search keywords...">
            <button id="search-button" aria-label="Search" title="Search website"><i class="fas fa-search"></i></button>
        </div>
        
        <div class="menu-toggle">
            <i class="fas fa-bars"></i>
        </div>
        
        <ul class="nav-links">
            <li><a href="../Linear_algebra/linear_algebra.html" >I - Linear Algebra to Algebraic Foundations</a></li>
            
            <li><a href="../Calculus/calculus.html" >II - Calculus to Optimization & Analysis</a></li>
            
            <li><a href="../Probability/probability.html" class="active">III - Probability & Statistics</a></li>
            
            <li><a href="../Discrete/discrete_math.html" >IV - Discrete Mathematics & Algorithms</a></li>
            
            <li><a href="../Machine_learning/ml.html" >V - Machine Learning</a></li>
        </ul>
    </nav>
</header>
    
    <!DOCTYPE html>
<html>
    <body>
        <!-- Meta script for mvn.html -->
        <script type="application/ld+json">
        {
        "@context": "https://schema.org",
        "@type": "LearningResource",
        "name": "Multivariate Distributions",
        "description": "Learn about multivariate normal distribution, Dirichlet distribution, and Wishart distribution.",
        "learningResourceType": "lesson",
        "educationalUse": "instruction",
        "educationalLevel": "university",
        "interactivityType": "active",
        "author": {
            "@type": "Person",
            "name": "Yusuke Yokota",
            "jobTitle": "Mathematics & Computer Science Educator"
        },
        "publisher": {
            "@type": "Organization",
            "name": "MATH-CS COMPASS",
            "url": "https://math-cs-compass.com"
        },
        "about": [
            { "@type": "Thing", "name": "Multivariate Distributions" },
            { "@type": "Thing", "name": "Multivariate Normal Distribution" },
            { "@type": "Thing", "name": "MVN" },
            { "@type": "Thing", "name": "Bivariate Normal Distribution" },
            { "@type": "Thing", "name": "Dirichlet Distribution" },
            { "@type": "Thing", "name": "Wishart Distribution" },
            { "@type": "Thing", "name": "Inverse Wishart Distribution" },
            { "@type": "Thing", "name": "Covariance Matrix" },
            { "@type": "Thing", "name": "Mean Vector" },
            { "@type": "Thing", "name": "Mahalanobis Distance" },
            { "@type": "Thing", "name": "Quadratic Form" },
            { "@type": "Thing", "name": "Correlation Coefficient" },
            { "@type": "Thing", "name": "Probability Simplex" },
            { "@type": "Thing", "name": "Multivariate Beta Function" },
            { "@type": "Thing", "name": "Conjugate Prior" },
            { "@type": "Thing", "name": "Multinomial Distribution" },
            { "@type": "Thing", "name": "Positive Definite Matrix" },
            { "@type": "Thing", "name": "Degrees of Freedom" },
            { "@type": "Thing", "name": "Scale Matrix" },
            { "@type": "Thing", "name": "Multivariate Gamma Function" },
            { "@type": "Thing", "name": "Chi-Squared Distribution" },
            { "@type": "Thing", "name": "Joint Probability Distribution" },
            { "@type": "Thing", "name": " Cholesky Decomposition" }
           
        ],
        "teaches": [
            "Multivariate probability distributions",
            "Covariance matrix estimation",
            "Bayesian conjugate priors",
            "Matrix-valued distributions"
        ],
        "isPartOf": {
            "@type": "Course",
            "name": "Probability & Statistics",
            "description": "Explore fundamental concepts of probability and statistics essential for machine learning",
            "provider": {
            "@type": "Organization",
            "name": "MATH-CS COMPASS",
            "url": "https://math-cs-compass.com"
            },
            "instructor": {
            "@type": "Person",
            "name": "Yusuke Yokota",
            "jobTitle": "Mathematics & Computer Science Educator"
            },
            "courseCode": "III",
            "hasCourseInstance": {
            "@type": "CourseInstance",
            "courseMode": "online",
            "courseWorkload": "PT2H30M",
            "instructor": {
                "@type": "Person",
                "name": "Yusuke Yokota"
            }
            },
            "offers": {
            "@type": "Offer",
            "price": "0",
            "priceCurrency": "USD",
            "availability": "https://schema.org/InStock",
            "category": "free"
            }
        }
        }
        </script>
        <div class="hero-section">
            <h1 class="webpage-name">Multivariate Distributions
            </h1>
        </div>

        
  <div class="quick-jump-container">
    <button type="button" id="quick-jump-toggle" class="quick-jump-toggle">
      <i class="fas fa-list"></i> <strong>Section III Navigation</strong>
    </button>
    <div id="quick-jump-menu" class="quick-jump-menu">
      <h3>Probability & Statistics</h3>
      <div class="quick-jump-links">

         
        <a href="probability.html">← Back to Section III Overview</a>
        <hr style="margin: 8px 0; border: 1px solid #ddd;">
        
        
        <a href="basic.html" >Part 1: Basic Probability Ideas</a>
        <a href="random_variables.html" >Part 2: Random Variables</a>
        <a href="gamma.html" >Part 3: Gamma & Beta Distribution</a>
        <a href="gaussian.html" >Part 4: Normal (Gaussian) Distribution</a>
        <a href="student.html" >Part 5: Student's t-Distribution</a>
        <a href="covariance.html" >Part 6: Covariance</a>
        <a href="correlation.html" >Part 7: Correlation</a>
        <a href="mvn.html" class="current-page">Part 8: Multivariate Distributions</a>
        <a href="mle.html" >Part 9: Maximum Likelihood Estimation</a>
        <a href="hypothesis_testing.html" >Part 10: Statistical Inference & Hypothesis Testing</a>
        <a href="linear_regression.html" >Part 11: Linear Regression</a>
        <a href="entropy.html" >Part 12: Entropy</a>
        <a href="convergence.html" >Part 13: Convergence</a>
        <a href="bayesian.html" >Part 14: Intro to Bayesian Statistics</a>
        <a href="expfamily.html" >Part 15: The Exponential Family</a>
        <a href="fisher_info.html" >Part 16: Fisher Information Matrix</a>
        <a href="decision_theory.html" >Part 17: Bayesian Decision Theory</a>
        <a href="markov.html" >Part 18: Markov Chains</a>
        <a href="monte_carlo.html" >Part 19: Monte Carlo Methods</a>
        <a href="importance_sampling.html" >Part 20: Importance Sampling</a>
        <a href="gaussian_process.html" >Part 21: Gaussian Processes</a>
      </div>
    </div>
</div>


        <div class="topic-nav">
            <a href="#mn">Multivariate Normal Distribution</a>
            <a href="#cholesky">Cholesky Decomposition</a>
            <a href="#dirichlet">Dirichlet Distribution</a>
            <a href="#wishart"> Wishart Distribution</a>
        </div> 

        <div class="container">  
            <section id="mn" class="section-content">
            <h2>Multivariate Normal Distribution</h2>
            <p>
            In machine learning, the most important joint probability distribution for continuous random variables is the 
            <strong>multivariate normal distribution</strong> (MVN). 
            The multivariate normal distribution of an \(n\) dimensional random vector \(\boldsymbol{x} \in \mathbb{R}^n \) is denoted as 
            \[
            \boldsymbol{X} \sim \mathcal{N}(\boldsymbol{\mu}, \Sigma)
            \]
            where \(\boldsymbol{\mu} = \mathbb{E} [\boldsymbol{x}] \in \mathbb{R}^n\) is the mean vector, and 
            \(\Sigma = \text{Cov }[\boldsymbol{x}] \in \mathbb{R}^{n \times n}\) is the covariance matrix. 
            <br>
            The p.d.f. is given by
            \[
            f(\boldsymbol{x}) = \frac{1}{\sqrt{(2\pi)^n \det(\Sigma)}} \exp \Big[-\frac{1}{2}(\boldsymbol{x} - \boldsymbol{\mu})^T \Sigma^{-1} (\boldsymbol{x} -\boldsymbol{\mu}) \Big]. \tag{1}
            \]
            The expression inside the exponential (ignoring the factor of -\frac{1}{2}) is the squared <strong>Mahalanobis distance</strong> between 
            the data vector \(\boldsymbol{x}\) and the mean vector \(\boldsymbol{\mu}\), given by 
            \[
            d_{\Sigma} (\boldsymbol{x}, \boldsymbol{\mu})^2 = (\boldsymbol{x} - \boldsymbol{\mu})^T \Sigma^{-1} (\boldsymbol{x} -\boldsymbol{\mu}).
            \]
            <br>
            If \(\boldsymbol{x} \in \mathbb{R}^2\), the MVN is known as the <strong>bivariate normal distribution</strong>.
            In this case, 
            \[
            \begin{align*}
            \Sigma  &=  \begin{bmatrix}
                            \text{Var } (X_1) & \text{Cov }[X_1, X_2] \\
                            \text{Cov }[X_2, X_1] & \text{Var } (X_2)
                        \end{bmatrix} \\\\
                    &=  \begin{bmatrix}
                            \sigma_1^2 & \rho \sigma_1 \sigma_2 \\
                            \rho \sigma_1 \sigma_2 & \sigma_2^2
                        \end{bmatrix} \\\\
            \end{align*}
            \]
            where \(\rho\) is the correlation coefficient defined by 
            \[
            \text{Corr }[X_1, X_2] = \frac{\text{Cov }[X_1, X_2]}{\sqrt{\text{Var }(X_1)\text{Var }(X_2)}}.
            \]
            Then 
            \[
            \begin{align*}
            \det (\Sigma) &= \sigma_1^2 \sigma_2^2 - \rho^2 \sigma_1^2 \sigma_2^2 \\\\
                          &= \sigma_1^2 \sigma_2^2 (1 - \rho^2) 
            \end{align*}
            \]
            and 
            \[
            \begin{align*}
            \Sigma^{-1} &= \frac{1}{\det (\Sigma )}
                          \begin{bmatrix}
                           \sigma_2^2 & -\rho \sigma_1 \sigma_2 \\
                           -\rho \sigma_1 \sigma_2 & \sigma_1^2
                          \end{bmatrix} \\\\
                        &= \frac{1}{1 - \rho^2}
                          \begin{bmatrix}
                           \frac{1}{\sigma_1^2 } & \frac{-\rho} {\sigma_1 \sigma_2} \\
                           \frac{-\rho} {\sigma_1 \sigma_2} & \frac{1}{\sigma_2^2 }
                          \end{bmatrix} 
            \end{align*}
            \]
            Note that in Expression (1), \((\boldsymbol{x} - \boldsymbol{\mu})^T \Sigma^{-1} (\boldsymbol{x} -\boldsymbol{\mu})\) is a <strong>quadratic form</strong>. So, 
            \[
            \begin{align*}
            (\boldsymbol{x} - \boldsymbol{\mu})^T \Sigma^{-1} (\boldsymbol{x} -\boldsymbol{\mu}) 
                &= \frac{1}{1 - \rho^2} \begin{bmatrix} X_1 - \mu_1 & X_2 - \mu_2 \end{bmatrix}
                    \begin{bmatrix}
                    \frac{1}{\sigma_1^2 } & \frac{-\rho} {\sigma_1 \sigma_2} \\
                    \frac{-\rho} {\sigma_1 \sigma_2} & \frac{1}{\sigma_2^2 }
                    \end{bmatrix} 
                    \begin{bmatrix} X_1 - \mu_1 \\ X_2 - \mu_2 \end{bmatrix} \\\\
                    &= \frac{1}{1 - \rho^2}\Big[\frac{1}{\sigma_1^2 }(X_1 - \mu_1)^2 
                                            -\frac{2\rho} {\sigma_1 \sigma_2}(X_1 - \mu_1)(X_2 - \mu_2) 
                                            +\frac{1}{\sigma_2^2 }(X_2 - \mu_2)^2 \Big].
            \end{align*}
            \] 
            Therefore, we obtain the p.d.f for the bivariate normal distribution:
            \[
            f(\boldsymbol{x}) = \frac{1}{2\pi \sigma_1 \sigma_2 \sqrt{(1 - \rho^2)}} 
                   \exp\Big\{-\frac{1}{2(1 - \rho^2)}
                                                \Big[\Big(\frac{X_1 - \mu_1}{\sigma_1}\Big)^2
                                                 -2\rho \Big(\frac{X_1 - \mu_1} {\sigma_1}\Big)  \Big(\frac{X_2 - \mu_2} {\sigma_2}\Big)
                                                 +\Big(\frac{X_2 - \mu_2}{\sigma_2}\Big)^2
                                                \Big]
                    \Big\}
            \]
            When \(\rho = -1 \text{ or } 1\), this p.d.f is undefined and \(f\) is said to be <strong>degenerate</strong>.
            </p>
            </section>

            <section id="cholesky" class="section-content">
                <h2>Cholesky Decomposition</h2>
                <p>
                    In the previous section, we introduced the multivariate Gaussian distribution 
                    \( \mathcal{N}(\boldsymbol{\mu}, \boldsymbol{\Sigma}) \), where the covariance matrix 
                    \( \boldsymbol{\Sigma} \) determines the shape and orientation of the distribution.  
                    To efficiently generate samples from such a distribution, we often need to 
                    transform uncorrelated standard normal samples into correlated ones that follow 
                    \( \boldsymbol{\Sigma} \).  
                    The <strong>Cholesky decomposition</strong> provides a convenient and numerically stable way 
                    to perform this transformation.
                </p>
                <div class="theorem">
                    <span class="theorem-title">Cholesky Decomposition:</span>
                    <p>
                    Let \( \boldsymbol{A} \in \mathbb{R}^{d \times d} \) be a 
                    <a href="../Linear_algebra/symmetry.html"><strong>symmetric positive definite</strong></a> matrix. 
                    Then there exists a unique lower triangular matrix \( \boldsymbol{L} \) with positive diagonal 
                    entries such that
                    \[
                    \boldsymbol{A} = \boldsymbol{L}\boldsymbol{L}^\top.
                    \]
                    (Equivalently, \( \boldsymbol{A} = \boldsymbol{R}^\top \boldsymbol{R} \) where
                    \( \boldsymbol{R} = \boldsymbol{L}^\top \) is upper triangular.)
                    </p>
                </div>

                <p>
                    Let \( \boldsymbol{y} \sim \mathcal{N}(\boldsymbol{\mu}, \boldsymbol{\Sigma}) \) and apply the
                    Cholesky decomposition to its covariance matrix:
                    \[
                    \boldsymbol{\Sigma} = \boldsymbol{L}\boldsymbol{L}^\top.
                    \]
                    If \( \boldsymbol{x} \sim \mathcal{N}(\boldsymbol{0}, \boldsymbol{I}) \), which can be obtained by sampling
                    \( d \) independent standard normal variables, then
                    \[
                    \boldsymbol{y} = \boldsymbol{L}\boldsymbol{x} + \boldsymbol{\mu}
                    \]
                    has the desired covariance:
                    \[
                    \begin{align*}
                    \text{Cov}[\boldsymbol{y}]
                    &= \boldsymbol{L}\,\text{Cov}[\boldsymbol{x}]\,\boldsymbol{L}^\top \\
                    &= \boldsymbol{L}\boldsymbol{I}\boldsymbol{L}^\top \\
                    &= \boldsymbol{\Sigma}.
                    \end{align*}
                    \]
                </p>

                <div class="proof">
                    <span class="proof-title">Proof:</span>
                    <p>
                    By linearity of expectation,
                    \[
                    \mathbb{E}[\boldsymbol{y}] 
                    = \mathbb{E}[\boldsymbol{L}\boldsymbol{x} + \boldsymbol{\mu}]
                    = \boldsymbol{L}\mathbb{E}[\boldsymbol{x}] + \boldsymbol{\mu}
                    = \boldsymbol{\mu}.
                    \]
                    The covariance of a random vector is given by
                    \[
                    \text{Cov}[\boldsymbol{y}]
                    = \mathbb{E}\!\left[(\boldsymbol{y} - \mathbb{E}[\boldsymbol{y}])
                        (\boldsymbol{y} - \mathbb{E}[\boldsymbol{y}])^\top \right].
                    \]
                    Substituting \( \boldsymbol{y} = \boldsymbol{L}\boldsymbol{x} + \boldsymbol{\mu} \) and using 
                    \( \mathbb{E}[\boldsymbol{x}] = \boldsymbol{0} \),
                    \[
                    \begin{align*}
                    \text{Cov}[\boldsymbol{y}]
                    &= \mathbb{E}\!\left[ (\boldsymbol{L}\boldsymbol{x})
                                        (\boldsymbol{L}\boldsymbol{x})^\top \right] \\\\
                    &= \boldsymbol{L}\,\mathbb{E}\!\left[\boldsymbol{x}\boldsymbol{x}^\top\right]\boldsymbol{L}^\top \\\\
                    &= \boldsymbol{L}\,\text{Cov}[\boldsymbol{x}]\,\boldsymbol{L}^\top \\\\
                    &= \boldsymbol{L}\boldsymbol{I}\boldsymbol{L}^\top \\\\
                    &= \boldsymbol{L}\boldsymbol{L}^\top \\\\
                    &= \boldsymbol{\Sigma}.
                    \end{align*}
                    \]
                    Therefore, \( \boldsymbol{y} \sim \mathcal{N}(\boldsymbol{\mu}, \boldsymbol{L}\boldsymbol{L}^\top) \).
                    </p>
                </div>
            </section>

        
            <section id="dirichlet" class="section-content">
            <h2>Dirichlet Distribution</h2>

            <p>
            The <strong>Dirichlet distribution</strong> is a multivariate generalization of <a href="gamma.html"><strong>beta distribution</strong></a>. 
            It has support over the the \((K - 1)\)-dimensional <strong>probability simplex</strong>, defined by
            \[ 
            S_K = \Bigl\{(x_1, x_2, \dots, x_K) \in \mathbb{R}^K: x_k \ge 0,\ \sum_{k=1}^K x_k = 1 \Bigr\}. 
            \] 
            A random vector \(\boldsymbol{x} \in \mathbb{R}^K\) is said to have a Dirichlet distribution with parameters 
            \(\boldsymbol{\alpha} = (\alpha_1, \alpha_2, \dots, \alpha_K)\) (with each \(\alpha_k > 0\)) if its probability 
            density function is given by 
            \[ 
            f(x_1, \dots, x_K; \boldsymbol{\alpha}) = \frac{1}{B(\boldsymbol{\alpha})} \prod_{k=1}^K x_k^{\alpha_k - 1}, \quad (x_1, \dots, x_K) \in S_K, 
            \]
            or
            \[
            \text{Dir }(\boldsymbol{\alpha}) = \frac{1}{B(\boldsymbol{\alpha})} \prod_{k=1}^K x_k^{\alpha_k - 1} \mathbb{I}(\boldsymbol{x} \in S_k),
            \] 
            where the <strong>multivariate beta function</strong> \(B(\boldsymbol{\alpha})\) is defined as 
            \[ 
            B(\boldsymbol{\alpha}) = \frac{\prod_{k=1}^K \Gamma(\alpha_k)}{\Gamma\Bigl(\sum_{k=1}^K \alpha_k\Bigr)}. 
            \]
            <div class="theorem">
                <span class="theorem-title">Moments:</span>
                Let \(\alpha_0 = \sum_{k=1}^K \alpha_k\). Then for each \(k\),
                <ul style="padding-left: 40px;">
                    <li><strong>Mean:</strong> 
                        \[
                        \mathbb{E}[x_k] = \frac{\alpha_k}{\alpha_0}.
                        \]
                    </li>
                    <li><strong>Variance:</strong>
                        \[
                        \operatorname{Var}[x_k] = \frac{\alpha_k (\alpha_0 - \alpha_k)}{\alpha_0^2 (\alpha_0+1)}.
                        \]
                    </li>
                    Note: Often we use a symmetric Dirichlet prior of the \(\alpha_k = \frac{\alpha}{K}\). Then 
                    \[
                    \mathbb{E}[x_k] = \frac{1}{K}, \quad \operatorname{Var}[x_k] = \frac{K-1}{K^2 (\alpha +1)}.
                    \]
                    We can see that increasing \(\alpha\) increases the precision(decreases the variance) of the distribution. 
                    <li><strong>Covariance:</strong> For \(i \neq j\),
                        \[
                        \operatorname{Cov}[x_i, x_j] = \frac{-\alpha_i \alpha_j}{\alpha_0^2 (\alpha_0+1)}.
                        \]
                    </li>
                </ul>
            </div>
            The parameters \(\alpha_k\) can be thought of as "pseudocounts" or prior observations of each category. When all 
            \(\alpha_k\) are equal (i.e., \(\boldsymbol{\alpha} = \alpha\, \mathbf{1}\)), the distribution is said to be <strong>uniform</strong>
            over the simplex when \(\alpha = 1\) or symmetric if \(\alpha \ne 1\). This symmetry makes the Dirichlet distribution a 
            natural <a href="markov.html"><strong>prior in Bayesian models</strong></a> where no category is favored a priori.
            <br><br>
            The Dirichlet distribution is widely used as a conjugate prior for the parameters 
            of a multinomial distribution in <a href="bayesian.html"><strong>Bayesian statistics</strong></a>, as well as in machine learning models 
            such as latent Dirichlet allocation (LDA) for topic modeling.
            </p>
            </section>

            <section id="wishart" class="section-content">
            <h2>Wishart Distribution</h2>
            <p>
            The <strong>Wishart distribution</strong> is a fundamental multivariate distribution that generalizes the <a href="gamma.html"><strong>gamma distribution</strong></a> to 
            <a href="../Linear_algebra/symmetry.html"><strong>positive definite matrices</strong></a>. 
         
            The p.d.f. of Wishart distribution is defined as follows:
            \[
            \operatorname{Wi}(\boldsymbol{\Sigma} | \boldsymbol{S}, \nu) 
            = \frac{1}{Z} |\boldsymbol{\Sigma}| ^{(\nu-D-1)/2} \exp \Bigl( - \frac{1}{2} \text{ tr } (\boldsymbol{S}^{-1} \boldsymbol{\Sigma})\Bigr)
            \]
            where 
            \[
            Z = 2^{\nu D/2} | \boldsymbol{S} |^{-\nu / 2}\,\Gamma_D (\frac{\nu}{2}).
            \] 
            Here, \(\Gamma_D(\cdot)\) denotes the multivariate gamma function. The parameters are:
            <ul style="padding-left: 40px;">
                <li>\(\nu\): the <strong>degrees of freedom</strong> (which must satisfy \(\nu \ge D\) for the distribution to be well-defined),</li>
                <li>\(\boldsymbol{S}\): the <strong>scale matrix</strong> (a \(D \times D\) positive definite matrix).</li>
            </ul>
            Note: \(\text{ mean } = \nu \boldsymbol{S}\).
            <br><br>
            If \(D=1\), ir reduces to the gamma distribution:
            \[
            \operatorname{Wi}(\lambda, s^{-1}, \nu) = \operatorname{Gamma}(\lambda | \text{ shape } = \frac{\nu}{2}, \text{ rate } = \frac{1}{2s}).
            \]
            If we set \(s=2\), this further reduces to the <strong>chi-squared distribution</strong>.
            <br><br>
            <strong>Applications:</strong>
            <ul style="padding-left: 40px;">
                <li>Covariance Matrix Estimation:</li>
                In multivariate statistics, the Wishart distribution is used to model the uncertainty in <a href="covariance.html"><strong>covariance matrix</strong></a> estimates.
                <br>
                Given \(D\) i.i.d. samples from \(\mathcal{N}(0, \boldsymbol{\Sigma})\), the sample covariance matrix
                \[
                \boldsymbol{S} = \sum_{i=1}^D x_i {x_i}^T
                \]
                follows a Wishart distribution:
                \[
                \boldsymbol{S} \sim \operatorname{Wi}( \boldsymbol{\Sigma}, D).
                \]
                <li>Bayesian Inference:</li>
                    It serves as a conjugate prior for the covariance matrix in multivariate normal models, enabling closed-form 
                    updates of the posterior distribution. We often use the <strong>inverse Wishart distribution</strong>:
                    <br>
                    For \(\nu > D -1\) and \(\boldsymbol{S} \succ 0\),
                    \[
                    \operatorname{IW}(\boldsymbol{\Sigma} | \boldsymbol{S}^{-1}, \nu) 
                    = \frac{1}{Z_{IW}} |\boldsymbol{\Sigma}| ^{-(\nu+D+1)/2} \exp \Bigl( - \frac{1}{2} \text{ tr } (\boldsymbol{S} \boldsymbol{\Sigma}^{-1})\Bigr)
                    \]
                    where 
                    \[
                    Z_{IW} = 2^{\nu D/2} | \boldsymbol{S} |^{\nu / 2}\,\Gamma_D (\frac{\nu}{2}).
                    \] 
                    Then 
                    \[
                    \boldsymbol{\Sigma} \sim \operatorname{IW}( \boldsymbol{S}, \nu).
                    \]
                    (Remember, \(\lambda \sim \text{Gamma }(a, b)\) then \(\frac{1}{\lambda} \sim  \operatorname{IG}(a,b)\). 
                    So, similarly, the IW is multivariate generalization of the IG.)
            </ul>
            Note: One is about sampling properties of the estimator (frequentist), and the other is about prior beliefs and posterior 
            inference (Bayesian). In practice, both deal with uncertainty in covariance matrices, and the same distribution arises in 
            both roles — just with different interpretations. 
            </p>
            </section>
        </div> 
        <script src="/js/main.js"></script>   
    </body>
</html>
    
    <footer>
    <div class="footer-content">
        <div class="footer-about">
            <h3>About MATH-CS COMPASS</h3>
            <p>Bridging the gap between pure mathematics and computer science applications.</p>
            <div class="contact-section">
                <strong><a href="https://docs.google.com/forms/d/e/1FAIpQLSf3_RNkegREvfCEIQznn6SCaf-iB5c-Doxn7Ymuu78Uf-PBjg/viewform" target="_blank">
                    Submit Your Questions & Feedback <i class="fas fa-external-link-alt"></i>
                </a></strong>
            </div>
        </div>
        <div class="footer-links">
            <h3>Quick Links</h3>
            <ul>
                
                <li><a href="../../../index.html">Home</a></li>
                <li>
                <a href="../Linear_algebra/linear_algebra.html">
                    <span class="nav-number">I</span>
                    <span>Linear Algebra to Algebraic Foundations</span>
                </a>
                </li>
                <li>
                <a href="../Calculus/calculus.html">
                    <span class="nav-number">II</span>
                    <span>Calculus to Optimization & Analysis</span>
                </a>
                </li>
                <li>
                <a href="../Probability/probability.html">
                    <span class="nav-number">III</span>
                    <span>Probability & Statistics</span>
                </a>
                </li>
                <li>
                <a href="../Discrete/discrete_math.html">
                    <span class="nav-number">IV</span>
                    <span>Discrete Mathematics & Algorithms</span>
                </a>
                </li>
                <li>
                <a href="../Machine_learning/ml.html">
                    <span class="nav-number">V</span>
                    <span>Machine Learning</span>
                </a>
                </li>
            </ul>
        </div>
        <div class="footer-social">
            <h3>Connect With Me</h3>
            <p>
                <a href="https://x.com/MathCSCompass?t=Zi8nyfzszeFw8QizbJwnNg&s=09" target="_blank">
                    <i class="fab fa-x-twitter"></i> X (Twitter)
                </a>
            </p>
            <p>
                <a href="https://www.facebook.com/share/15uPaBWgQN/" target="_blank">
                    <i class="fab fa-facebook"></i> Facebook
                </a>
            </p>
      
            <h3>MATH-CS COMPASS App</h3>
            <div class="app-download-footer">
                <a href="https://play.google.com/store/apps/details?id=io.github.yusukey01.twa" target="_blank" rel="noopener">
                    <img src="../../../images/google-play-badge.png" 
                        alt="Get it on Google Play" 
                        class="google-play-badge">
                </a>
            </div>
            
        </div>

    </div>
    <div class="footer-bottom">
        <p>&copy; 2024-2025 MATH-CS COMPASS. All rights reserved.</p>
    </div>
</footer>
    <script src="../../../js/main.js?v=1.0.2"></script>
    <script src="../../../js/search.js?v=1.0.2"></script>
    

    <button id="go-to-top-btn" class="go-to-top-btn" title="Go to top">
        <i class="fas fa-arrow-up"></i>
    </button>
    <script>
        // Register the service worker
        if ('serviceWorker' in navigator) {
          window.addEventListener('load', () => {
            navigator.serviceWorker.register('/js/service-worker.js')
              .then(registration => {
                console.log('ServiceWorker registration successful with scope: ', registration.scope);
              })
              .catch(error => {
                console.log('ServiceWorker registration failed: ', error);
              });
          });
        }
      </script>
</body>
</html>