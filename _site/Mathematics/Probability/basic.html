<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    
    <title>Basic Probability Ideas - MATH-CS COMPASS</title>
    
    
    <script>
    window.MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']]
      }
    };
    </script>
    <script id="MathJax-script" defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="preload" href="https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Main-Regular.woff" as="font" type="font/woff2" crossorigin>
    <link rel="preload" href="https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Math-Italic.woff" as="font" type="font/woff2" crossorigin>
    
    
    

    <link rel="stylesheet" href="../../../css/styles.css?v=1.0.2">
  
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css">

    <link rel="dns-prefetch" href="https://cdn.jsdelivr.net">
    <link rel="dns-prefetch" href="https://cdnjs.cloudflare.com">
    
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Learn about basic probability ideas such as conditional probability and Bayes' theorem.">
    <meta name="google-site-verification" content="IieEujypYW38caEVe8ymjczouNl56yxAy27iPztcfRA" />
    

    <link rel="manifest" href="/manifest.json">

    <!-- Theme color for Chrome, Android, and other browsers -->
    <meta name="theme-color" content="#007bff">
  
    <!-- iOS specific tags -->
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <meta name="apple-mobile-web-app-title" content="MATH-CS COMPASS">
    <meta name="mobile-web-app-capable" content="yes">

    <!-- Icon references -->
    <link rel="icon" type="image/png" sizes="1280x1280" href="/images/maskable_icon_x1280.png?v=1.0.2">
    <link rel="icon" type="image/png" sizes="512x512" href="/images//maskable_icon_x512.png?v=1.0.2">
    
    <!-- Apple Touch Icons -->
    <link rel="apple-touch-icon" href="/images/maskable_icon_x1280.png?v=1.0.2">
    <link rel="apple-touch-icon" href="/images/maskable_icon_x512.png?v=1.0.2">

    <!-- For Windows -->
    <meta name="msapplication-TileImage" content="/images/test1.png?v=1.0.2">
    <meta name="msapplication-TileColor" content="#007bff">

    <!-- Schema.org Structured Data for AI Optimization -->
    

    <!-- Article Schema for content pages -->
    
    <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "Article",
      "headline": "Basic Probability Ideas",
      "description": "Learn about basic probability ideas such as conditional probability and Bayes' theorem.",
      "datePublished": "2024-01-01",
      "dateModified": "2025-01-01",
      "author": {
        "@type": "Person",
        "name": "Yusuke Yokota",
        "jobTitle": "Mathematics & Computer Science Educator",
        "knowsAbout": [
          "Linear Algebra",
          "Machine Learning",
          "Computer Science",
          "Mathematics"
        ]
      },
      "publisher": {
        "@type": "Organization",
        "name": "MATH-CS COMPASS",
        "url": "https://math-cs-compass.com",
        "logo": {
          "@type": "ImageObject",
          "url": "https://math-cs-compass.com/images/maskable_icon_x512.png"
        }
      },
      "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "https://math-cs-compass.com/Mathematics/Probability/basic.html"
      },
      "about": [
        
        { "@type": "Thing", "name": "Probability" },
        { "@type": "Thing", "name": "Statistics" }
        
      ],
      "educationalLevel": "university",
      "learningResourceType": "tutorial",
      "educationalUse": "instruction"
    }
    </script>
    
</head>

<body>
    <header>
    <nav class="navbar">
        <!-- Logo as Home Button -->
        <div class="logo-home">
            <a href="../../../index.html" 
               class="logo-link " 
               title="Home - Math-CS Compass">
                <img src="../../images/icon_x512.png" 
                     alt="Math-CS Compass Logo" 
                     class="nav-logo">
            </a>
        </div>

        <div class="search-container">
            <input type="text" id="search-input" placeholder="Search keywords...">
            <button id="search-button" aria-label="Search" title="Search website"><i class="fas fa-search"></i></button>
        </div>
        
        <div class="menu-toggle">
            <i class="fas fa-bars"></i>
        </div>
        
        <ul class="nav-links">
            <li><a href="../Linear_algebra/linear_algebra.html" >I - Linear Algebra to Algebraic Foundations</a></li>
            
            <li><a href="../Calculus/calculus.html" >II - Calculus to Optimization & Analysis</a></li>
            
            <li><a href="../Probability/probability.html" class="active">III - Probability & Statistics</a></li>
            
            <li><a href="../Discrete/discrete_math.html" >IV - Discrete Mathematics & Algorithms</a></li>
            
            <li><a href="../Machine_learning/ml.html" >V - Machine Learning</a></li>
        </ul>
    </nav>
</header>
    
    <!DOCTYPE html>
<html>
    <body>
        <!-- Meta script for basic.html -->
        <script type="application/ld+json">
        {
        "@context": "https://schema.org",
        "@type": "LearningResource",
        "name": "Basic Probability Ideas",
        "description": "Learn about basic probability ideas such as conditional probability and Bayes' theorem.",
        "learningResourceType": "lesson",
        "educationalUse": "instruction",
        "educationalLevel": "university",
        "interactivityType": "active",
        "author": {
            "@type": "Person",
            "name": "Yusuke Yokota",
            "jobTitle": "Mathematics & Computer Science Educator"
        },
        "publisher": {
            "@type": "Organization",
            "name": "MATH-CS COMPASS",
            "url": "https://math-cs-compass.com"
        },
        "about": [
            { "@type": "Thing", "name": "Basic Probability" },
            { "@type": "Thing", "name": "Conditional Probability" },
            { "@type": "Thing", "name": "Law of Total Probability" },
            { "@type": "Thing", "name": "Bayes' Theorem" },
            { "@type": "Thing", "name": "Sample Space" },
            { "@type": "Thing", "name": "Events" },
            { "@type": "Thing", "name": "Probability Axioms" },
            { "@type": "Thing", "name": "Permutations" },
            { "@type": "Thing", "name": "Combinations" },
            { "@type": "Thing", "name": "Independence" },
            { "@type": "Thing", "name": "Mutually Exclusive Events" }
        ],
        "teaches": [
            "Fundamental probability concepts",
            "Conditional probability calculations",
            "Bayes' theorem applications",
            "Counting principles and combinatorics"
        ],
        "isPartOf": {
            "@type": "Course",
            "name": "Probability & Statistics",
            "description": "Explore fundamental concepts of probability and statistics essential for machine learning",
            "provider": {
            "@type": "Organization",
            "name": "MATH-CS COMPASS",
            "url": "https://math-cs-compass.com"
            },
            "instructor": {
            "@type": "Person",
            "name": "Yusuke Yokota",
            "jobTitle": "Mathematics & Computer Science Educator"
            },
            "courseCode": "III",
            "hasCourseInstance": {
            "@type": "CourseInstance",
            "courseMode": "online",
            "courseWorkload": "PT2H30M",
            "instructor": {
                "@type": "Person",
                "name": "Yusuke Yokota"
            }
            },
            "offers": {
            "@type": "Offer",
            "price": "0",
            "priceCurrency": "USD",
            "availability": "https://schema.org/InStock",
            "category": "free"
            }
        }
        }
        </script>

        <div class="hero-section">
            <h1 class="webpage-name">Basic Probability Ideas
            </h1>
        </div>

        
  <div class="quick-jump-container">
    <button type="button" id="quick-jump-toggle" class="quick-jump-toggle">
      <i class="fas fa-list"></i> <strong>Section III Navigation</strong>
    </button>
    <div id="quick-jump-menu" class="quick-jump-menu">
      <h3>Probability & Statistics</h3>
      <div class="quick-jump-links">

         
        <a href="probability.html">‚Üê Back to Section III Overview</a>
        <hr style="margin: 8px 0; border: 1px solid #ddd;">
        
        
        <a href="basic.html" class="current-page">Part 1: Basic Probability Ideas</a>
        <a href="random_variables.html" >Part 2: Random Variables</a>
        <a href="gamma.html" >Part 3: Gamma & Beta Distribution</a>
        <a href="gaussian.html" >Part 4: Normal (Gaussian) Distribution</a>
        <a href="student.html" >Part 5: Student's t-Distribution</a>
        <a href="covariance.html" >Part 6: Covariance</a>
        <a href="correlation.html" >Part 7: Correlation</a>
        <a href="mvn.html" >Part 8: Multivariate Distributions</a>
        <a href="mle.html" >Part 9: Maximum Likelihood Estimation</a>
        <a href="hypothesis_testing.html" >Part 10: Statistical Inference & Hypothesis Testing</a>
        <a href="linear_regression.html" >Part 11: Linear Regression</a>
        <a href="entropy.html" >Part 12: Entropy</a>
        <a href="convergence.html" >Part 13: Convergence</a>
        <a href="bayesian.html" >Part 14: Intro to Bayesian Statistics</a>
        <a href="expfamily.html" >Part 15: The Exponential Family</a>
        <a href="fisher_info.html" >Part 16: Fisher Information Matrix</a>
        <a href="decision_theory.html" >Part 17: Bayesian Decision Theory</a>
        <a href="markov.html" >Part 18: Markov Chains</a>
        <a href="monte_carlo.html" >Part 19: Monte Carlo Methods</a>
        <a href="importance_sampling.html" >Part 20: Importance Sampling</a>
        <a href="gaussian_process.html" >Part 21: Gaussian Processes</a>
      </div>
    </div>
</div>


        <div class="topic-nav">
            <a href="#pro">Probability</a>
            <a href="#conditional">Conditional Probability</a>
            <a href="#total">Law of Total Probability</a>
            <a href="#bayes">Bayes' Theorem</a>
        </div> 

        <div class="container">  
           
            <section id="pro" class="section-content">
            <h2>Probability</h2>
            <p>
            The collection of every possible outcome of an experiment is called a <strong>sample space</strong> denoted as \(S\).
            It can be discrete or continuous. An <strong>event</strong> \(A\) is a set of outcomes of an experiment, or a 
            subset of the sample space \(A \subseteq S\). 
            <br><br>
            The <strong>probability</strong> of \(A\) denoted as \(P(A)\) satisfies the following axioms:
            <ol style="padding-left: 40px;">
                <li>\( 0 \leq P(A) \leq 1\)</li>
                <li>\(P(S) = 1\) and \(P(\emptyset) = 0\) </li>
                <li>If the events \(A\) and \(B\) are <strong>mutually exclusive</strong>, \(P(A \cup B) = P(A) + P(B)\)</li>
                Note: Mutually exclusive means that the two events \(A\) and \(B\) cannot occur at the same time.
            </ol>
            and can be calculated as: 
            \[
            P(A) = \frac{\text{number of outcomes in } A}{\text{number of outcomes in } S}.
            \]
            <br>
            Using event algebra, the following basic facts can be derived:
            <br> 
            <ol style="padding-left: 40px;">
                <li><strong>Complement</strong>: \(P(\bar{A}) = 1 - P(A)\)</li>
                Note: \(1 = P(S) = P(A \cup \bar{A}) = P(A) + P(\bar{A})\)
                <br><br>
                <li><strong>Addition</strong>:\(P(A \cup B) = P(A) + P(B) - P(A \cap B)\)</li>
                Note: If \(A\) and \(B\) are mutually exclusive , \(P(A \cap B) = P(\emptyset) = 0\).
                <br><br>
                <li><strong>Inclusion</strong>: If \(B \subset A\), then \(A \cap B = B\), and so \(P(A) - P(B) = P(A \cap \bar{B})\)</li>
                <br>
                <li><strong>de Morgan's laws</strong>: <br>
                    \(P(\overline{A \cup B}) = P( \bar{A} \cap \bar{B})\) <br>
                    \(P(\overline{A \cap B}) = P( \bar{A} \cup \bar{B})\)</li>
            </ol>
            <br>
            There are some counting rules to find all possible outcomes in \(S\) quickly.
            <br>
            <ol style="padding-left: 40px;">
                <li><strong>Multiplication principle</strong>:</li>
                If an experiment consists of a sequence of operations \(O_1, O_2, \ldots, O_r\) 
                with \(n_1, n_2, \ldots, n_r\) outcomes respectively, then the total number of possible outcomes 
                is the product \(n_1n_2\cdots n_r\).
                <br><br>
                <li><strong>Permutation</strong>:</li>
                Sampling without replacement where the order of sampling matters. 
                \[
                \begin{align*}
                {}_n P_r &= n(n-1)(n-2)\cdots (n-1+1) \\\\
                         &= \frac{n!}{(n-r)!}
                \end{align*}
                \] 
                (Multiplying the number of possible outcomes at each step until \(r\)th one is made.)
                <br><br>
                <li><strong>Combinations</strong>:</li>
                Sampling without replacement where the oder of sampling does not matter. 
                \[
                {}_n C_r = \binom{n}{r} = \frac{n!}{r!(n-r)!} = \frac{ {}_n P_r }{r!}
                \] 
                Note: This is the <strong>Binomial coefficient</strong> for the binomial expansion. 
                <br><br>
                We can generalize the binomial coefficient in \(k \geq 2\) groups so that there are \(r_i\) in 
                group \(i \quad (1 \leq i \leq k)\) and \(r_1 + r_2 + \cdots + r_k = n\):
                \[
                \frac{n!}{r_1 ! r_2 ! \cdots r_k !}
                \]
                This is called the <strong>multinomial coefficient</strong>. 
                <br>For example, if we deal 52 cards evenly among 4 players, giving each player 13 cards, the total 
                number of different hands within the 4 players is calculated by  
                \[
                \frac{52!}{13!13!13!13!}
                \]  
            </ol>
            </p>
            </section>

            <section id="conditional" class="section-content">
            <h2>Conditional Probability</h2>
            <p>
            The <strong>conditional probability</strong> of an event \(A\) given an event \(B\) is defined as 
            \[
            P(A \mid B) = \frac{P(A \cap B)}{P(B)}. \tag{1}
            \]
            which means 
            \[
            P(A \cap B) = P(A \mid B) P(B).  \tag{2}
            \]
            Also, from Equation (1),
            \[
            P(B \mid A) = \frac{P(B \cap A)}{P(A)} = \frac{P(A \cap B)}{P(A)}.
            \]
            Using Equation (2), 
            \[
            P(B \mid A) = \frac{P(A \mid B) P(B)}{P(A)}.  \tag{3}
            \]
            This is known as <strong>Bayes' theorem</strong> or inverse probability law. We will discuss it in a more 
            useful form later. 
            <br><br>
            If \(A\) and \(B\)  are mutually <strong>independent</strong>, 
            \[
            P(A \mid B) = P(A), \text{ and } P(B \mid A) = P(B).
            \]
            Thus 
            \[
            P(A \cap B) = P(A) P(B).
            \]
            In addition, in this case, 
            \[
            \begin{align*}
            P(A)P(\bar{B}) &= P(A)[1 - P(B)] \\\\
                           &= P(A) - P(A)P(B) \\\\
                           &= P(A) - P(A \cap B) \\\\
                           &= P(A \cap \bar{B})
            \end{align*}
            \]
            Similarly, \( P(\bar{A})P(B) = P(\bar{A} \cap B)\) and \( P(\bar{A})P(\bar{B}) = P(\bar{A} \cap \bar{B})\).
            <br><br>
            Note: Two events are mutually independent if the occurrence of one event does not affect the probability 
            of the occurrence of the other event. Be careful not to confuse it with mutually exclusive.
            </p>
            </section>

            <section id="total" class="section-content">
            <h2>Law of Total Probability</h2>
            <p>
            <div class="theorem">
                <span class="theorem-title">Theorem 1: Law of Total Probability</span> 
                Let the sample space \(S\) be decomposed into \(k\) mutually exclusive events \(B_1, B_2, \cdots B_k\).
                Then for any event \(A\), 
                \[
                \begin{align*}
                P(A) &= P(A \mid B_1)P(B_1) + P(A \mid B_2)P(B_2) + \cdots +  P(A \mid B_k)P(B_k) \\\\
                     &= \sum_{i =1} ^k P(A \mid B_i)P(B_i)
                \end{align*}
                \]
            </div>
            <div class="proof">
                <span class="proof-title">Proof:</span>
                \begin{align*}
                P(A) &= P[(A \cap B_1) \cup (A \cap B_2) \cup \cdots \cup (A \cap B_k)] \\\\
                     &= P(A \cap B_1) +  P(A \cap B_2) + \cdots +  P(A \cap B_k) \\\\
                     &= P(A \mid B_1)P(B_1) + P(A \mid B_2)P(B_2) + \cdots +  P(A \mid B_k)P(B_k) \\\\
                     &= \sum_{i =1} ^k P(A \mid B_i)P(B_i)
                \end{align*}
            </div>
            </p>
            </section>

            <section id="bayes" class="section-content">
            <h2>Bayes' Theorem</h2>
            <p>
            We revisit Equation (3): 
            \[
            P(B \mid A) = \frac{P(A \mid B) P(B)}{P(A)}.
            \]
            \(P(B)\) is called the <strong>prior probability</strong> of \(B\) and \(P(B \mid A)\) is called the 
            <strong>posterior probability</strong> of \(B\). This is the foundation of <strong>Bayesian statistics</strong>.
            <br><br>
            Here, using the Law of Total Probability, we can get a more general form of Bayes' Theorem.
            <div class="theorem">
                <span class="theorem-title">Theorem 2: Bayes' Theorem</span>
                Let mutually exclusive events \(B_1, B_2, \cdots B_k\) be partitions of the sample space \(S\) with the 
                condition that \(P(B_i) > 0\) for \(i = 1, 2, \cdots, k\). Then for \(j = 1, 2, \cdots, k\),
                \[
                \begin{align*}
                P(B_j \mid A) &= \frac{P(A \mid B_j)P(B_j)}{\sum_{i=1}^k P(A \mid B_i)P(B_i)} \\\\
                              &= \frac{P(B_j \cap A)}{P(A)}
                \end{align*}
                \]
            </div>
            This is a powerful tool in machine learning such as <strong>classification</strong>. For example, in medical 
            diagnosis, \(B_1, B_2, \cdots B_k\) can be possible diseases and let \(A\) be observed symptoms, then \(P(B_j \cap A)\) 
            represents the probability of having diseas \(B_j\) given the symptoms \(A\).
            </p>
            </section>
        </div> 
        <script src="/js/main.js"></script> 
    </body>
</html>
    
    <footer>
    <div class="footer-content">
        <div class="footer-about">
            <h3>About MATH-CS COMPASS</h3>
            <p>Bridging the gap between pure mathematics and computer science applications.</p>
            <div class="contact-section">
                <strong><a href="https://docs.google.com/forms/d/e/1FAIpQLSf3_RNkegREvfCEIQznn6SCaf-iB5c-Doxn7Ymuu78Uf-PBjg/viewform" target="_blank">
                    Submit Your Questions & Feedback <i class="fas fa-external-link-alt"></i>
                </a></strong>
            </div>
        </div>
        <div class="footer-links">
            <h3>Quick Links</h3>
            <ul>
                
                <li><a href="../../../index.html">Home</a></li>
                <li>
                <a href="../Linear_algebra/linear_algebra.html">
                    <span class="nav-number">I</span>
                    <span>Linear Algebra to Algebraic Foundations</span>
                </a>
                </li>
                <li>
                <a href="../Calculus/calculus.html">
                    <span class="nav-number">II</span>
                    <span>Calculus to Optimization & Analysis</span>
                </a>
                </li>
                <li>
                <a href="../Probability/probability.html">
                    <span class="nav-number">III</span>
                    <span>Probability & Statistics</span>
                </a>
                </li>
                <li>
                <a href="../Discrete/discrete_math.html">
                    <span class="nav-number">IV</span>
                    <span>Discrete Mathematics & Algorithms</span>
                </a>
                </li>
                <li>
                <a href="../Machine_learning/ml.html">
                    <span class="nav-number">V</span>
                    <span>Machine Learning</span>
                </a>
                </li>
            </ul>
        </div>
        <div class="footer-social">
            <h3>Connect With Me</h3>
            <p>
                <a href="https://x.com/MathCSCompass?t=Zi8nyfzszeFw8QizbJwnNg&s=09" target="_blank">
                    <i class="fab fa-x-twitter"></i> X (Twitter)
                </a>
            </p>
            <p>
                <a href="https://www.facebook.com/share/15uPaBWgQN/" target="_blank">
                    <i class="fab fa-facebook"></i> Facebook
                </a>
            </p>
      
            <h3>MATH-CS COMPASS App</h3>
            <div class="app-download-footer">
                <a href="https://play.google.com/store/apps/details?id=io.github.yusukey01.twa" target="_blank" rel="noopener">
                    <img src="../../../images/google-play-badge.png" 
                        alt="Get it on Google Play" 
                        class="google-play-badge">
                </a>
            </div>
            
        </div>

    </div>
    <div class="footer-bottom">
        <p>&copy; 2024-2025 MATH-CS COMPASS. All rights reserved.</p>
    </div>
</footer>
    <script src="../../../js/main.js?v=1.0.2"></script>
    <script src="../../../js/search.js?v=1.0.2"></script>
    

    <button id="go-to-top-btn" class="go-to-top-btn" title="Go to top">
        <i class="fas fa-arrow-up"></i>
    </button>
    <script>
        // Register the service worker
        if ('serviceWorker' in navigator) {
          window.addEventListener('load', () => {
            navigator.serviceWorker.register('/js/service-worker.js')
              .then(registration => {
                console.log('ServiceWorker registration successful with scope: ', registration.scope);
              })
              .catch(error => {
                console.log('ServiceWorker registration failed: ', error);
              });
          });
        }
      </script>
</body>
</html>