<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    
    <title>Covariance - MATH-CS COMPASS</title>
    
    
    <script>
    window.MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']]
      }
    };
    </script>
    <script id="MathJax-script" defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="preload" href="https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Main-Regular.woff" as="font" type="font/woff2" crossorigin>
    <link rel="preload" href="https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Math-Italic.woff" as="font" type="font/woff2" crossorigin>
    
    
    
    <script src="https://cdn.jsdelivr.net/pyodide/v0.23.3/full/pyodide.js" defer></script>
    

    <link rel="stylesheet" href="../../../css/styles.css?v=1.0.2">
  
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css">

    <link rel="dns-prefetch" href="https://cdn.jsdelivr.net">
    <link rel="dns-prefetch" href="https://cdnjs.cloudflare.com">
    
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Learn about covariance matrix and principal component analysis(PCA).">
    <meta name="google-site-verification" content="IieEujypYW38caEVe8ymjczouNl56yxAy27iPztcfRA" />
    

    <link rel="manifest" href="/manifest.json">

    <!-- Theme color for Chrome, Android, and other browsers -->
    <meta name="theme-color" content="#007bff">
  
    <!-- iOS specific tags -->
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <meta name="apple-mobile-web-app-title" content="MATH-CS COMPASS">
    <meta name="mobile-web-app-capable" content="yes">

    <!-- Icon references -->
    <link rel="icon" type="image/png" sizes="1280x1280" href="/images/maskable_icon_x1280.png?v=1.0.2">
    <link rel="icon" type="image/png" sizes="512x512" href="/images//maskable_icon_x512.png?v=1.0.2">
    
    <!-- Apple Touch Icons -->
    <link rel="apple-touch-icon" href="/images/maskable_icon_x1280.png?v=1.0.2">
    <link rel="apple-touch-icon" href="/images/maskable_icon_x512.png?v=1.0.2">

    <!-- For Windows -->
    <meta name="msapplication-TileImage" content="/images/test1.png?v=1.0.2">
    <meta name="msapplication-TileColor" content="#007bff">

    <!-- Schema.org Structured Data for AI Optimization -->
    

    <!-- Article Schema for content pages -->
    
    <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "Article",
      "headline": "Covariance",
      "description": "Learn about covariance matrix and principal component analysis(PCA).",
      "datePublished": "2024-01-01",
      "dateModified": "2025-01-01",
      "author": {
        "@type": "Person",
        "name": "Yusuke Yokota",
        "jobTitle": "Mathematics & Computer Science Educator",
        "knowsAbout": [
          "Linear Algebra",
          "Machine Learning",
          "Computer Science",
          "Mathematics"
        ]
      },
      "publisher": {
        "@type": "Organization",
        "name": "MATH-CS COMPASS",
        "url": "https://math-cs-compass.com",
        "logo": {
          "@type": "ImageObject",
          "url": "https://math-cs-compass.com/images/maskable_icon_x512.png"
        }
      },
      "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "https://math-cs-compass.com/Mathematics/Probability/covariance.html"
      },
      "about": [
        
        { "@type": "Thing", "name": "Probability" },
        { "@type": "Thing", "name": "Statistics" }
        
      ],
      "educationalLevel": "university",
      "learningResourceType": "tutorial",
      "educationalUse": "instruction"
    }
    </script>
    
</head>

<body>
    <header>
    <nav class="navbar">
        <!-- Logo as Home Button -->
        <div class="logo-home">
            <a href="../../../index.html" 
               class="logo-link " 
               title="Home - Math-CS Compass">
                <img src="../../images/icon_x512.png" 
                     alt="Math-CS Compass Logo" 
                     class="nav-logo">
            </a>
        </div>

        <div class="search-container">
            <input type="text" id="search-input" placeholder="Search keywords...">
            <button id="search-button" aria-label="Search" title="Search website"><i class="fas fa-search"></i></button>
        </div>
        
        <div class="menu-toggle">
            <i class="fas fa-bars"></i>
        </div>
        
        <ul class="nav-links">
            <li><a href="../Linear_algebra/linear_algebra.html" >I - Linear Algebra to Algebraic Foundations</a></li>
            
            <li><a href="../Calculus/calculus.html" >II - Calculus to Optimization & Analysis</a></li>
            
            <li><a href="../Probability/probability.html" class="active">III - Probability & Statistics</a></li>
            
            <li><a href="../Discrete/discrete_math.html" >IV - Discrete Mathematics & Algorithms</a></li>
            
            <li><a href="../Machine_learning/ml.html" >V - Machine Learning</a></li>
        </ul>
    </nav>
</header>
    
    <!DOCTYPE html>
<html>
    <body>
        <!-- Meta script for covariance.html -->
        <script type="application/ld+json">
        {
        "@context": "https://schema.org",
        "@type": "LearningResource",
        "name": "Covariance",
        "description": "Learn about covariance matrix and principal component analysis(PCA).",
        "learningResourceType": "lesson",
        "educationalUse": "instruction",
        "educationalLevel": "university",
        "interactivityType": "active",
        "author": {
            "@type": "Person",
            "name": "Yusuke Yokota",
            "jobTitle": "Mathematics & Computer Science Educator"
        },
        "publisher": {
            "@type": "Organization",
            "name": "MATH-CS COMPASS",
            "url": "https://math-cs-compass.com"
        },
        "about": [
            { "@type": "Thing", "name": "Covariance Matrix" },
            { "@type": "Thing", "name": "Principal Component Analysis" },
            { "@type": "Thing", "name": "PCA" },
            { "@type": "Thing", "name": "Singular Value Decomposition" },
            { "@type": "Thing", "name": "SVD" },
            { "@type": "Thing", "name": "Dimensionality Reduction" },
            { "@type": "Thing", "name": "Eigenvalues" },
            { "@type": "Thing", "name": "Eigenvectors" },
            { "@type": "Thing", "name": "Principal Components" },
            { "@type": "Thing", "name": "Variance Explained" },
            { "@type": "Thing", "name": "Multivariate Statistics" }
        ],
        "teaches": [
            "Covariance matrix properties",
            "Principal component analysis",
            "SVD applications in PCA",
            "Dimensionality reduction techniques"
        ],
        "isPartOf": {
            "@type": "Course",
            "name": "Probability & Statistics",
            "description": "Explore fundamental concepts of probability and statistics essential for machine learning",
            "provider": {
            "@type": "Organization",
            "name": "MATH-CS COMPASS",
            "url": "https://math-cs-compass.com"
            },
            "instructor": {
            "@type": "Person",
            "name": "Yusuke Yokota",
            "jobTitle": "Mathematics & Computer Science Educator"
            },
            "courseCode": "III",
            "hasCourseInstance": {
            "@type": "CourseInstance",
            "courseMode": "online",
            "courseWorkload": "PT2H30M",
            "instructor": {
                "@type": "Person",
                "name": "Yusuke Yokota"
            }
            },
            "offers": {
            "@type": "Offer",
            "price": "0",
            "priceCurrency": "USD",
            "availability": "https://schema.org/InStock",
            "category": "free"
            }
        }
        }
        </script>

        <!-- WebApplication Schema for Interactive Demos (for pages with Python code) -->
        <script type="application/ld+json">
        {
        "@context": "https://schema.org",
        "@type": "WebApplication",
        "name": "Covariance Interactive Demo",
        "description": "Interactive demonstration of covariance concepts",
        "applicationCategory": "EducationalApplication",
        "operatingSystem": "Web Browser",
        "url": "https://math-cs-compass.com/Mathematics/Probability/covariance.html",
        "author": {
            "@type": "Person",
            "name": "Yusuke Yokota"
        },
        "applicationSubCategory": "Mathematical Visualization",
        "featureList": [
            "Interactive mathematical visualization",
            "Real-time parameter adjustment",
            "Educational demonstrations",
            "Python code execution"
        ],
        "isAccessibleForFree": true,
        "educationalUse": "instruction",
        "educationalLevel": "university"
        }
        </script>
        <!-- WebApplication Schema for Interactive Demos (for pages with visualizations/demos) -->
        <script type="application/ld+json">
        {
        "@context": "https://schema.org",
        "@type": "WebApplication",
        "name": "Covariance Interactive Demo",
        "description": "Interactive demonstration of covariance concepts with visualizations",
        "applicationCategory": "EducationalApplication",
        "operatingSystem": "Web Browser",
        "url": "https://math-cs-compass.com/Mathematics/Probability/covariance.html",
        "author": {
            "@type": "Person",
            "name": "Yusuke Yokota"
        },
        "applicationSubCategory": "Mathematical Visualization",
        "featureList": [
            "Interactive mathematical visualization",
            "Real-time parameter adjustment",
            "Educational demonstrations",
            "Statistical plotting and analysis"
        ],
        "isAccessibleForFree": true,
        "educationalUse": "instruction",
        "educationalLevel": "university"
        }
        </script>
        <div class="hero-section">
            <h1 class="webpage-name">Covariance
            </h1>
        </div>

        
  <div class="quick-jump-container">
    <button type="button" id="quick-jump-toggle" class="quick-jump-toggle">
      <i class="fas fa-list"></i> <strong>Section III Navigation</strong>
    </button>
    <div id="quick-jump-menu" class="quick-jump-menu">
      <h3>Probability & Statistics</h3>
      <div class="quick-jump-links">

         
        <a href="probability.html">‚Üê Back to Section III Overview</a>
        <hr style="margin: 8px 0; border: 1px solid #ddd;">
        
        
        <a href="basic.html" >Part 1: Basic Probability Ideas</a>
        <a href="random_variables.html" >Part 2: Random Variables</a>
        <a href="gamma.html" >Part 3: Gamma & Beta Distribution</a>
        <a href="gaussian.html" >Part 4: Normal (Gaussian) Distribution</a>
        <a href="student.html" >Part 5: Student's t-Distribution</a>
        <a href="covariance.html" class="current-page">Part 6: Covariance</a>
        <a href="correlation.html" >Part 7: Correlation</a>
        <a href="mvn.html" >Part 8: Multivariate Distributions</a>
        <a href="mle.html" >Part 9: Maximum Likelihood Estimation</a>
        <a href="hypothesis_testing.html" >Part 10: Statistical Inference & Hypothesis Testing</a>
        <a href="linear_regression.html" >Part 11: Linear Regression</a>
        <a href="entropy.html" >Part 12: Entropy</a>
        <a href="convergence.html" >Part 13: Convergence</a>
        <a href="bayesian.html" >Part 14: Intro to Bayesian Statistics</a>
        <a href="expfamily.html" >Part 15: The Exponential Family</a>
        <a href="fisher_info.html" >Part 16: Fisher Information Matrix</a>
        <a href="decision_theory.html" >Part 17: Bayesian Decision Theory</a>
        <a href="markov.html" >Part 18: Markov Chains</a>
        <a href="monte_carlo.html" >Part 19: Monte Carlo Methods</a>
        <a href="importance_sampling.html" >Part 20: Importance Sampling</a>
        <a href="gaussian_process.html" >Part 21: Gaussian Processes</a>
      </div>
    </div>
</div>


        <div class="topic-nav">
            <a href="#covariance">Covariance Matrix</a>
            <a href="#pca">Principal Component Analysis (PCA)</a>
            <a href="#svd">PCA with Singular Value Decomposition(SVD)</a>
        </div> 

        <div class="container">  
           
            <section id="covariance" class="section-content">
            <h2>Covariance Matrix</h2>
            <p> 
            From this section, we consider <strong>multivariate</strong> models. First of all, we need a way to measure 
            the dependence of variables each other.
            <br><br>
            The <strong>covariance</strong> between two random variables \(X\) and \(Y\) is defined as 
            \[
             \text{Cov }[X, Y] = \mathbb{E}[(X - \mathbb{E}[X])(Y - \mathbb{E}[Y])].
            \]
            Here, \((X - \mathbb{E}[X])\) and \((Y - \mathbb{E}[Y])\) are <strong>mean deviations</strong> which represent 
            how far the random variables \(X\) and \(Y\) deviate from their respective expected values(means). So, the covariance 
            measures how these deviation of random variables vary together(<strong>joint variation</strong>).
            <br><br> 
            Note: \(\text{Cov }[X, Y] = \mathbb{E}[XY]- \mathbb{E}[X]\mathbb{E}[Y]\). So, if \(X\) and \(Y\) are independent, 
            \(\mathbb{E}[XY] = \mathbb{E}[X]\mathbb{E}[Y]\) and thus \(\text{Cov }[X, Y] = 0\). However, in general, the converse is 
            NOT true. Zero covariance only tells us there is <strong>no linear dependence</strong>. The positive(negative) covariance 
            indicates positive(negative) <strong>linear</strong> association.
            <br><br>
            In practice, often we extend this idea to \(n\) random variables. Consider a vector \(x \in \mathbb{R}^n\) whose each 
            entries represent random variables \(X_1, \cdots X_n\).
            <br>
            The <strong>population covariance matrix </strong> of the vector \(x\) is defined as
            \[
            \begin{align*}
            \Sigma  &= \text{Cov }[x] \\\\ 
                    &= \mathbb{E}[(x - \mathbb{E}[x])(x - \mathbb{E}[x])^T]  \\\\
                    &= \begin{bmatrix}
                             \text{Var }[X_1] & \text{Cov }[X_1, X_2] & \cdots & \text{Cov }[X_1, X_n] \\
                             \text{Cov }[X_2, X_1] &  \text{Var }[X_2] & \cdots & \text{Cov }[X_2, X_n] \\
                             \vdots  & \vdots & \ddots & \vdots \\
                             \text{Cov }[X_n, X_1] &   \text{Cov }[X_n, X_2] & \cdots & \text{Var }[X_n]
                             \end{bmatrix} \\\\
                    &= \mathbb{E }[xx^T] - \mu \mu^T   
            \end{align*}
            \]
            Each diagonal entry \(\Sigma_{ii}\) represents the <strong>variance</strong> of \(X_i\) because
            \[
            \text{Cov }[X_i, X_i] = \mathbb{E}[(X_i - \mathbb{E}[X_i])^2]  = \sigma^2
            \]
            Also, the <strong>total variance</strong> of \(\Sigma \) is defined as the trace of \(\Sigma \):
            \[
            \text{tr } (\Sigma ) = \sum_{i=1}^n \text{Var }[X_i].
            \]
            <br>
            The covariance matrix is <strong>symmetric</strong> because the covariance itself is symmetric:
            \[
            \Sigma_{ij} = \text{Cov }[X_i, X_j] = \text{Cov }[X_j, X_i] = \Sigma_{ji}.
            \]
            (Or, for any matrix \(A\), \(\, AA^T\) is symmetric because \((AA^T)^T = (A^T)^TA^T = AA^T \).)
            <br><br>
            Since \(\Sigma\) is symmetric, it is always <a href="../Linear_algebra/symmetry.html"><strong>orthogonally diagonalizable</strong></a>: 
            \[
            \Sigma = P D P^T  
            \]
            where \(P\) is an orthogonal matrix whose columns are unit eigenvectors of \(\Sigma\), and \(D\) is a diagonal matrix 
            whose diagonal entries are eigenvalues of \(\Sigma\) corresponding to its eigenvectors. 
            <br><br>
            Note: The total variance of \(\Sigma\) is equal to the sum of its eigenvalues.(ONLY the total!)
            \[
            \text{tr } (\Sigma ) = \sum_{i=1}^n \text{Var }[X_i] = \text{tr } (D) = \sum_{i=1}^n \lambda_i
            \]
            <br>
            Moreover, the covariance matrix is always <strong>positive semi-definite</strong>:
            <br>
            For any vector \(v \in \mathbb{R}^n\), 
            \[
            \begin{align*}
            v^T \Sigma v &=  v^T\mathbb{E}[(x - \mathbb{E}[x])(x - \mathbb{E}[x])^T] v \\\\
                         &=  \mathbb{E}[v^T(x - \mathbb{E}[x])(x - \mathbb{E}[x])^T v] \\\\
                         &=  \mathbb{E}[(v^T(x - \mathbb{E}[x]))^2] \geq 0
            \end{align*}
            \]
            Thus, the diagonal entries of \(D\) are <strong>non-negative</strong>. In other words, the eigenvalues of \(\, \Sigma\) 
            always satisfies: 
            \[
            \lambda_1 \geq \lambda_2 \geq \cdots \geq \lambda_n \geq 0.
            \]
            <br><br>
            SideNote: \(\mathbb{E }[xx^T]\) is called the <strong>autocorrelation matrix</strong> denoted as \(R_{xx}\):
            \[
            \begin{align*}
                    R_{xx} 
                    &= \mathbb{E }[xx^T] \\\\
                    &= \begin{bmatrix}
                            \mathbb{E }[X_1^2] & \mathbb{E }[X_1 X_2] & \cdots & \mathbb{E }[X_1 X_n] \\
                            \mathbb{E }[X_2 X_1] &  \mathbb{E }[X_2^2] & \cdots & \mathbb{E }[X_2 X_n] \\
                            \vdots  & \vdots & \ddots & \vdots \\
                            \mathbb{E }[X_n X_1] &   \mathbb{E }[X_n X_2] & \cdots & \mathbb{E }[X_n^2]
                        \end{bmatrix} \\\\
            \end{align*}
            \]
            </p>
            </section>

            <section id="pca" class="section-content">
            <h2>Principal Component Analysis (PCA)</h2>
            <p> 
            The orthogonal diagonalization of the covariance matrix \(\Sigma\) is important in statistics and machine learning for 
            analyzing data and reducing dimensionality:  
            \[
            \Sigma = P D P^T  
            \]
            Here, the column vectors of \(P\) (unit eigenvectors of \(\Sigma \)) are called the <strong>principal components(PCs)</strong> 
            of the data, and the diagonal entries of \(D\) (eigenvalues of \(\Sigma \)) represent the <strong>variances</strong> along these 
            principal components. Each eigenvalue indicates "how much" of the <strong>total variance</strong> of \(\Sigma \) is captured by 
            its corresponding principal component. 
            <br><br>
            The <strong>first principal component(PC1)</strong> corresponds to the largest eigenvalue and represents the direction in the 
            data space where the distribution varies the most(= the direction of maximizing the variance in data). The second principal 
            component(PC2) captures the next largest variance and is orthogonal to PC1. Similarly, each subsequent PC capturing less variance 
            and maintaining orthogonality to all previous PCs.
            <br><br>
            For example, suppose you have an observed data matrix \(X \in \mathbb{R}^{m \times 10}\), which contains \(m\) data points, each 
            represented as a vector \(x_i \in \mathbb{R}^{10} \). 
            Computing the <strong>sample covariance matrix</strong> and diagonalizing it:
            \[
            S = \frac{1}{m-1}(X-\bar{X})^T (X-\bar{X}) = PDP^T
            \] 
            you examine the eigenvalues to determine how much variance each principal component captures comparing with 
            the <strong>total variance</strong> of \(S\). 
            <br>
            Let's say PC1: 40%, PC2: 30%, and PC3: 25%. Then, you project the data onto the subspace spanned by the first 3 most 
            significant principal components. Even though you discarded the remaining 7 dimensions (noise dimensions), you still retain 
            the most significant patterns(trends) in the data. 
            \[
             x_i \in \mathbb{R}^{10} \to z_i \in \mathbb{R}^3
            \]
            The vector \(z_i\) is known as the <strong>latent vector</strong>.
            <br><br>
            This <strong>dimensionality reduction</strong> process is called <strong>principal component analysis(PCA)</strong>. By reducing 
            dimensions, PCA enables efficient analysis of large datasets while retaining their most meaningful structure. PCA is 
            widely used in machine learning, image processing, and exploratory data analysis for dimensionality reduction and noise filtering. 
            </p>
            </section>

            <section id="svd" class="section-content">
            <h2>PCA with Singular Value Decomposition(SVD)</h2>
            <p>
            In practice, the <a href="../Linear_algebra/symmetry.html"><strong>singular value decomposition</strong></a> plays an 
            important role in the PCA due to its mathematical properties and computational advantages.
            <br><br>
            Given a data matrix in mean-deviation form \(X \in \mathbb{R}^{m \times n}\) where \(m\) is the number of data points 
            and \(n\) is the number of features, the matrix \(X^TX\) is the covariance matrix.
            <br><br>
            Note: in PCA, it is common to use \(X^TX \in  \mathbb{R}^{n \times n}\) rather than \(XX^T \in \mathbb{R}^{m \times m}\) 
            because we are typically more interested in understanding the relationships among the \(n\) "features" than among 
            \(m\) data points.
            <br><br>
            Using the SVD of \(X\), we compute the covariance matrix \(X^TX\) : 
            \[
            \begin{align*}
            X^TX &= (U\Sigma V^T)^T(U\Sigma V^T)  \\\\
                 &= V\Sigma^TU^TU\Sigma V^T \\\\
                 &= V\Sigma^2V^T
            \end{align*}
            \] 
            This result is equivalent to the eigendecomposition \(X^TX = PDP^T\). \(\Sigma^2\) contains the eigenvalues \(\lambda_i\) 
            of \(X^TX\), with \(\sigma_i ^2 = \lambda_i\). Also, \(V\) (right singular vectors) corresponds 
            to the eigenvectors (principal components) of \(X^TX\).
            <br>
            Thus, the singular values \(\sigma_i\) represent the <strong>standard deviations</strong> along the principal components, 
            and the squared singular values \(\sigma_i^2\) represent the variances.
            <br><br>
            SVD offers several numerical advantages for PCA. For high-dimensional data, computing \(X^TX\) would be 
            expensive or memory-intensive, but SVD can avoid explicitly forming \(X^TX\) working directly with \(X\), which 
            allows direct access to the principal components and variances. 
            <div class="code-container">
                <div class="collapsible-section">
                <button class="collapsible-btn">Show/Hide Code</button>
                <div class="collapsible-content">
                <pre class="python-code">
                    import numpy as np

                    # Random data matrix ( m data points with n features)
                    def generate_data(m, n):
                        data = np.random.randn(m, n) 

                        # Make some correlations 
                        data[:, 2] = 0.7 * data[:, 0] + 0.5 * data[:, 2]
                        data[:, 3] = 0.2 * data[:, 0] + 0.5 * data[:, 1] 
                        data[:, 4] = -0.3 * data[:, 1] + 0.2 * data[:, 2]
                        data[:, 5] = 0.4 * data[:, 0] + 0.1 * data[:, 1]
                        data[:, 6] = 0.8 * data[:, 3] + -0.3 * data[:, 2]

                        # Mean-deviation form 
                        return (data - np.mean(data, axis=0) )

                    # PCA with covariance matrix (As refference)
                    def pca_via_covariance(data):
                        
                        # "Sample" covariance matrix 
                        # Note: Dividing by (m-1) provides "unbiased" estimate of the population covariance. 
                        cov_matrix = np.dot(data.T, data) / (data.shape[0] - 1)

                        # Eigendecomposition 
                        # np.linalg.eigh() is for symmetric matrix (real, diagonaizable), better than np.linalg.eig() 
                        eigvals, eigvecs = np.linalg.eigh(cov_matrix) 

                        #  Make eigenvalues & eigenvectors in descending order
                        idx = np.argsort(eigvals)[::-1] 
                        eigvals = eigvals[idx]
                        eigvecs = eigvecs[:, idx] 

                        # Each variance vs total variance
                        ratio = eigvals / np.sum(eigvals)

                        return eigvals, eigvecs, ratio

                    # PCA with SVD (we use this function)
                    def pca_with_svd(data):
                        
                        # Singular Value Decomposition (we don't need the matrix U: use "_" )
                        _, S, vt = np.linalg.svd(data, full_matrices=False)
                        
                        # Get eigenvalues via singular values: lambda_i = (S_i)^2  / m - 1
                        eigvals = (S ** 2) / (data.shape[0] - 1)
                        
                        # Each variance vs total variance
                        ratio = eigvals / np.sum(eigvals)
                        
                        return eigvals, vt.T, ratio 

                    # Set 90% threshold for the number of PCs
                    def threshold(var_ratio, t):
                        
                        # Compute cumulative variance
                        cum_variance = np.cumsum(var_ratio)
                        
                        # Find the number of components for 90% variance retention
                        num_pcs = np.argmax(cum_variance >= t) + 1
                        return num_pcs

                    # Dimension of data 
                    m = 1000000 
                    n = 10 

                    # Run PCA
                    eigvals, eigvecs, ratio = pca_with_svd(generate_data(m, n))

                    # Threshold for variance retention
                    t = 0.95
                    num_pcs = threshold(ratio, t)

                    # Print results
                    print("Eigenvalues:")
                    for i, val in enumerate(eigvals):
                        print(f"  Lambda {i + 1}: {val:.6f}")

                    print("\nExplained Variance Ratio (%):")
                    for i, var in enumerate(ratio):
                        print(f"  PC{i + 1}: {var * 100:.2f}%")

                    print(f"\nTo retain {t * 100:.0f}% of variance, use the first {num_pcs} PCs.")
                </pre>
            </div>
                </div>
                <button class="run-button" onclick="runPythonCode(this)">Run Code</button>
                <div class="python-output"></div>
            </div>
            </p>
            </section>
        </div>  
        <script src="/js/main.js"></script>
        <script src="/js/runPythonCode.js"></script>
        <script src="/js/collapsible.js"></script>
    </body>
</html>
    
    <footer>
    <div class="footer-content">
        <div class="footer-about">
            <h3>About MATH-CS COMPASS</h3>
            <p>Bridging the gap between pure mathematics and computer science applications.</p>
            <div class="contact-section">
                <strong><a href="https://docs.google.com/forms/d/e/1FAIpQLSf3_RNkegREvfCEIQznn6SCaf-iB5c-Doxn7Ymuu78Uf-PBjg/viewform" target="_blank">
                    Submit Your Questions & Feedback <i class="fas fa-external-link-alt"></i>
                </a></strong>
            </div>
        </div>
        <div class="footer-links">
            <h3>Quick Links</h3>
            <ul>
                
                <li><a href="../../../index.html">Home</a></li>
                <li>
                <a href="../Linear_algebra/linear_algebra.html">
                    <span class="nav-number">I</span>
                    <span>Linear Algebra to Algebraic Foundations</span>
                </a>
                </li>
                <li>
                <a href="../Calculus/calculus.html">
                    <span class="nav-number">II</span>
                    <span>Calculus to Optimization & Analysis</span>
                </a>
                </li>
                <li>
                <a href="../Probability/probability.html">
                    <span class="nav-number">III</span>
                    <span>Probability & Statistics</span>
                </a>
                </li>
                <li>
                <a href="../Discrete/discrete_math.html">
                    <span class="nav-number">IV</span>
                    <span>Discrete Mathematics & Algorithms</span>
                </a>
                </li>
                <li>
                <a href="../Machine_learning/ml.html">
                    <span class="nav-number">V</span>
                    <span>Machine Learning</span>
                </a>
                </li>
            </ul>
        </div>
        <div class="footer-social">
            <h3>Connect With Me</h3>
            <p>
                <a href="https://x.com/MathCSCompass?t=Zi8nyfzszeFw8QizbJwnNg&s=09" target="_blank">
                    <i class="fab fa-x-twitter"></i> X (Twitter)
                </a>
            </p>
            <p>
                <a href="https://www.facebook.com/share/15uPaBWgQN/" target="_blank">
                    <i class="fab fa-facebook"></i> Facebook
                </a>
            </p>
      
            <h3>MATH-CS COMPASS App</h3>
            <div class="app-download-footer">
                <a href="https://play.google.com/store/apps/details?id=io.github.yusukey01.twa" target="_blank" rel="noopener">
                    <img src="../../../images/google-play-badge.png" 
                        alt="Get it on Google Play" 
                        class="google-play-badge">
                </a>
            </div>
            
        </div>

    </div>
    <div class="footer-bottom">
        <p>&copy; 2024-2025 MATH-CS COMPASS. All rights reserved.</p>
    </div>
</footer>
    <script src="../../../js/main.js?v=1.0.2"></script>
    <script src="../../../js/search.js?v=1.0.2"></script>
    

    <button id="go-to-top-btn" class="go-to-top-btn" title="Go to top">
        <i class="fas fa-arrow-up"></i>
    </button>
    <script>
        // Register the service worker
        if ('serviceWorker' in navigator) {
          window.addEventListener('load', () => {
            navigator.serviceWorker.register('/js/service-worker.js')
              .then(registration => {
                console.log('ServiceWorker registration successful with scope: ', registration.scope);
              })
              .catch(error => {
                console.log('ServiceWorker registration failed: ', error);
              });
          });
        }
      </script>
</body>
</html>