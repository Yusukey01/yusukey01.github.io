<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    
    <title>Monte Carlo Methods - MATH-CS COMPASS</title>
    
    
    <script>
    window.MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']]
      }
    };
    </script>
    <script id="MathJax-script" defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="preload" href="https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Main-Regular.woff" as="font" type="font/woff2" crossorigin>
    <link rel="preload" href="https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Math-Italic.woff" as="font" type="font/woff2" crossorigin>
    
    
    

    <link rel="stylesheet" href="../../../css/styles.css?v=1.0.2">
  
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css">

    <link rel="dns-prefetch" href="https://cdn.jsdelivr.net">
    <link rel="dns-prefetch" href="https://cdnjs.cloudflare.com">
    
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Intro to Monte Carlo approximation in the context of Bayesian statistics.">
    <meta name="google-site-verification" content="IieEujypYW38caEVe8ymjczouNl56yxAy27iPztcfRA" />
    

    <link rel="manifest" href="/manifest.json">

    <!-- Theme color for Chrome, Android, and other browsers -->
    <meta name="theme-color" content="#007bff">
  
    <!-- iOS specific tags -->
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <meta name="apple-mobile-web-app-title" content="MATH-CS COMPASS">
    <meta name="mobile-web-app-capable" content="yes">

    <!-- Icon references -->
    <link rel="icon" type="image/png" sizes="1280x1280" href="/images/maskable_icon_x1280.png?v=1.0.2">
    <link rel="icon" type="image/png" sizes="512x512" href="/images//maskable_icon_x512.png?v=1.0.2">
    
    <!-- Apple Touch Icons -->
    <link rel="apple-touch-icon" href="/images/maskable_icon_x1280.png?v=1.0.2">
    <link rel="apple-touch-icon" href="/images/maskable_icon_x512.png?v=1.0.2">

    <!-- For Windows -->
    <meta name="msapplication-TileImage" content="/images/test1.png?v=1.0.2">
    <meta name="msapplication-TileColor" content="#007bff">

    <!-- Schema.org Structured Data for AI Optimization -->
    

    <!-- Article Schema for content pages -->
    
    <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "Article",
      "headline": "Monte Carlo Methods",
      "description": "Intro to Monte Carlo approximation in the context of Bayesian statistics.",
      "datePublished": "2024-01-01",
      "dateModified": "2025-01-01",
      "author": {
        "@type": "Person",
        "name": "Yusuke Yokota",
        "jobTitle": "Mathematics & Computer Science Educator",
        "knowsAbout": [
          "Linear Algebra",
          "Machine Learning",
          "Computer Science",
          "Mathematics"
        ]
      },
      "publisher": {
        "@type": "Organization",
        "name": "MATH-CS COMPASS",
        "url": "https://yusukey01.github.io",
        "logo": {
          "@type": "ImageObject",
          "url": "https://yusukey01.github.io/images/maskable_icon_x512.png"
        }
      },
      "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "https://yusukey01.github.io/Mathematics/Probability/monte_carlo.html"
      },
      "about": [
        
        { "@type": "Thing", "name": "Probability" },
        { "@type": "Thing", "name": "Statistics" }
        
      ],
      "educationalLevel": "university",
      "learningResourceType": "tutorial",
      "educationalUse": "instruction"
    }
    </script>
    
</head>

<body>
    <header>
    <nav class="navbar">
        <!-- Logo as Home Button -->
        <div class="logo-home">
            <a href="../../../index.html" 
               class="logo-link " 
               title="Home - Math-CS Compass">
                <img src="../../images/icon_x512.png" 
                     alt="Math-CS Compass Logo" 
                     class="nav-logo">
            </a>
        </div>

        <div class="search-container">
            <input type="text" id="search-input" placeholder="Search keywords...">
            <button id="search-button" aria-label="Search" title="Search website"><i class="fas fa-search"></i></button>
        </div>
        
        <div class="menu-toggle">
            <i class="fas fa-bars"></i>
        </div>
        
        <ul class="nav-links">
            <li><a href="../Linear_algebra/linear_algebra.html" >I - Linear Algebra to Algebraic Foundations</a></li>
            
            <li><a href="../Calculus/calculus.html" >II - Calculus to Optimization & Analysis</a></li>
            
            <li><a href="../Probability/probability.html" class="active">III - Probability & Statistics</a></li>
            
            <li><a href="../Discrete/discrete_math.html" >IV - Discrete Mathematics & Algorithms</a></li>
            
            <li><a href="../Machine_learning/ml.html" >V - Machine Learning</a></li>
        </ul>
    </nav>
</header>
    
    <!DOCTYPE html>
<html>
    <body>
        <!-- Meta script for monte_carlo.html -->
        <script type="application/ld+json">
        {
        "@context": "https://schema.org",
        "@type": "LearningResource",
        "name": "Monte Carlo Methods",
        "description": "Intro to Monte Carlo approximation in the context of Bayesian statistics.",
        "learningResourceType": "lesson",
        "educationalUse": "instruction",
        "educationalLevel": "university",
        "interactivityType": "active",
        "author": {
            "@type": "Person",
            "name": "Yusuke Yokota",
            "jobTitle": "Mathematics & Computer Science Educator"
        },
        "publisher": {
            "@type": "Organization",
            "name": "MATH-CS COMPASS",
            "url": "https://yusukey01.github.io"
        },
        "about": [
            { "@type": "Thing", "name": "Monte Carlo Methods" },
            { "@type": "Thing", "name": "Credible Intervals" },
            { "@type": "Thing", "name": "Monte Carlo Approximation" },
            { "@type": "Thing", "name": "Highest Posterior Density" },
            { "@type": "Thing", "name": "HPD" },
            { "@type": "Thing", "name": "Highest Density Intervals" },
            { "@type": "Thing", "name": "HDI" },
            { "@type": "Thing", "name": "Markov Chain Monte Carlo" },
            { "@type": "Thing", "name": "MCMC" },
            { "@type": "Thing", "name": "Posterior Distribution" },
            { "@type": "Thing", "name": "Bayesian Inference" },
            { "@type": "Thing", "name": "Empirical CDF" },
            { "@type": "Thing", "name": "Quantiles" },
            { "@type": "Thing", "name": "Central Credible Interval" },
            { "@type": "Thing", "name": "Metropolis-Hastings" },
            { "@type": "Thing", "name": "Gibbs Sampling" },
            { "@type": "Thing", "name": "Hamiltonian Monte Carlo" },
            { "@type": "Thing", "name": "HMC" },
            { "@type": "Thing", "name": "Unnormalized Posterior" },
            { "@type": "Thing", "name": "Likelihood" },
            { "@type": "Thing", "name": "Prior Distribution" },
            { "@type": "Thing", "name": "Normalizing Constant" },
            { "@type": "Thing", "name": "Stationary Distribution" }
        ],
        "teaches": [
            "Monte Carlo sampling techniques",
            "Credible interval construction",
            "MCMC fundamentals",
            "Bayesian computational methods"
        ],
        "isPartOf": {
            "@type": "Course",
            "name": "Probability & Statistics",
            "description": "Explore fundamental concepts of probability and statistics essential for machine learning",
            "provider": {
            "@type": "Organization",
            "name": "MATH-CS COMPASS",
            "url": "https://yusukey01.github.io"
            },
            "instructor": {
            "@type": "Person",
            "name": "Yusuke Yokota",
            "jobTitle": "Mathematics & Computer Science Educator"
            },
            "courseCode": "III",
            "hasCourseInstance": {
            "@type": "CourseInstance",
            "courseMode": "online",
            "courseWorkload": "PT2H30M",
            "instructor": {
                "@type": "Person",
                "name": "Yusuke Yokota"
            }
            },
            "offers": {
            "@type": "Offer",
            "price": "0",
            "priceCurrency": "USD",
            "availability": "https://schema.org/InStock",
            "category": "free"
            }
        }
        }
        </script>
          <!-- WebApplication Schema for Interactive Tools -->
        <script type="application/ld+json">
        {
        "@context": "https://schema.org",
        "@type": "WebApplication",
        "name": "Monte Carlo Methods Interactive Tool",
        "description": "Interactive tool for exploring monte carlo methods concepts with real-time visualizations and computational examples",
        "applicationCategory": "EducationalApplication",
        "operatingSystem": "Web Browser",
        "url": "https://yusukey01.github.io/Mathematics/Probability/monte_carlo.html",
        "author": {
            "@type": "Person",
            "name": "Yusuke Yokota"
        },
        "applicationSubCategory": "Mathematical Computation Tool",
        "featureList": [
            "Interactive mathematical visualization",
            "Real-time parameter adjustment",
            "Educational demonstrations",
            "Statistical computation",
            "Code execution and examples",
            "Monte Carlo simulation",
            "Probability distribution visualization"
        ],
        "isAccessibleForFree": true,
        "educationalUse": "instruction",
        "educationalLevel": "university"
        }
        </script>
        <div class="hero-section">
            <h1 class="webpage-name">Monte Carlo Methods
            </h1>
        </div>

        
  <div class="quick-jump-container">
    <button type="button" id="quick-jump-toggle" class="quick-jump-toggle">
      <i class="fas fa-list"></i> <strong>Section III Navigation</strong>
    </button>
    <div id="quick-jump-menu" class="quick-jump-menu">
      <h3>Probability & Statistics</h3>
      <div class="quick-jump-links">

         
        <a href="probability.html">← Back to Section III Overview</a>
        <hr style="margin: 8px 0; border: 1px solid #ddd;">
        
        
        <a href="basic.html" >Part 1: Basic Probability Ideas</a>
        <a href="random_variables.html" >Part 2: Random Variables</a>
        <a href="gamma.html" >Part 3: Gamma & Beta Distribution</a>
        <a href="gaussian.html" >Part 4: Normal (Gaussian) Distribution</a>
        <a href="student.html" >Part 5: Student's t-Distribution</a>
        <a href="covariance.html" >Part 6: Covariance</a>
        <a href="correlation.html" >Part 7: Correlation</a>
        <a href="mvn.html" >Part 8: Multivariate Distributions</a>
        <a href="mle.html" >Part 9: Maximum Likelihood Estimation</a>
        <a href="hypothesis_testing.html" >Part 10: Statistical Inference & Hypothesis Testing</a>
        <a href="linear_regression.html" >Part 11: Linear Regression</a>
        <a href="entropy.html" >Part 12: Entropy</a>
        <a href="convergence.html" >Part 13: Convergence</a>
        <a href="bayesian.html" >Part 14: Intro to Bayesian Statistics</a>
        <a href="expfamily.html" >Part 15: The Exponential Family</a>
        <a href="fisher_info.html" >Part 16: Fisher Information Matrix</a>
        <a href="decision_theory.html" >Part 17: Bayesian Decision Theory</a>
        <a href="markov.html" >Part 18: Markov Chains</a>
        <a href="monte_carlo.html" class="current-page">Part 19: Monte Carlo Methods</a>
        <a href="importance_sampling.html" class="current-page">Part 20: Importance Sampling</a>
        <a href="gaussian_process.html" class="current-page">Part 21: Gaussian Processes</a>
      </div>
    </div>
</div>


        <div class="topic-nav">
            <a href="#CI">Credible Intervals</a>
            <a href="#MC">Monte Carlo Approximation</a>
            <a href="#demo">Demo: Monte Carlo Approximation for CIs</a>
            <a href="#HPD">Highest Posterior Density (HPD)</a>
            <a href="#MCMC">Intro to Markov Chain Monte Carlo (MCMC)</a>
        </div> 

        <div class="container">  
            <section id="CI" class="section-content">
                <h2>Credible Intervals</h2>
                <p>
                    In practice, a probability distribution is often a high-dimensional object, making it difficult to fully capture its characteristics. 
                    In Bayesian statistics, we frequently summarize the uncertainty of a posterior distribution using a <strong>credible interval</strong>. 
                    A credible interval is not the same as a <a href="hypothesis_testing.html"><strong>confidence interval</strong></a> in frequentist statistics, even though they may sometimes numerically coincide.
                </p>
                <p> 
                    Formally, for a given significance level \(\alpha \in (0,1)\), a \(100(1-\alpha)\%\) <strong>credible interval</strong> is defined as:
                    \[
                    C_{\alpha}(\mathcal{D}) = (l, u) \quad \text{such that} \quad P(l \leq \theta \leq u \mid \mathcal{D}) = 1 - \alpha,
                    \]
                    where \(l\) and \(u\) denote the lower and upper bounds, respectively. Here, \(\mathcal{D}\) represents the observed data, and the probability is computed under the posterior distribution \(p(\theta \mid \mathcal{D})\).
                </p>
                <p>
                    This interval contains \(1-\alpha\) of the posterior probability mass. A common choice is the <strong>central credible interval</strong>, where \(\frac{1-\alpha}{2}\) of the posterior probability lies in each tail. 
                </p>
                <p>
                    If the cumulative distribution function (cdf) \(F\) of the posterior distribution is known, the central credible interval can be computed as:
                    \[
                    \begin{align*}
                    l &= F^{-1}\left( \frac{\alpha}{2} \right), \\\\
                    u &= F^{-1}\left( 1 - \frac{\alpha}{2} \right),
                    \end{align*}
                    \]
                    where \(F^{-1}\) denotes the quantile function (the inverse of the cdf).
                </p>
                <p>
                    As an example, if the posterior distribution is approximately normal with posterior mean \(\mu\) and posterior standard deviation \(\sigma\), 
                    a rough \(95\%\) credible interval can be set as:
                    \[
                    (l, u) = (\mu - 2\sigma, \mu + 2\sigma),
                    \]
                    because for a standard normal distribution \(\mathcal{N}(0,1)\), approximately \(95\%\) of the probability mass lies within two standard deviations from the mean.
                </p>
                <p>
                    More precisely, when \(p(\theta \mid \mathcal{D}) = \mathcal{N}(\mu, \sigma^2)\) and \(\alpha = 0.05\), the central credible interval endpoints are:
                    \[
                    \begin{align*}
                    l &= \mu + \sigma \Phi^{-1}\left( \frac{\alpha}{2} \right), \\\\
                    u &= \mu + \sigma \Phi^{-1}\left( 1 - \frac{\alpha}{2} \right),
                    \end{align*}
                    \]
                    where \(\Phi\) is the cumulative distribution function (cdf) of the standard normal distribution.
                </p>
                <p>
                    In particular, if the posterior distribution is the standard normal \(p(\theta \mid \mathcal{D}) = \mathcal{N}(0,1)\), then:
                    \[
                    \begin{align*}
                    l &= \Phi^{-1}(0.025) \approx -1.96, \\\\
                    u &= \Phi^{-1}(0.975) \approx 1.96,
                    \end{align*}
                    \]
                    so the \(95\%\) credible interval is approximately \((-1.96, 1.96)\).
                </p>
            </section>

            <section id="MC" class="section-content">
                <h2>Monte Carlo Approximation</h2>
                <p> 
                    In general, it is often difficult to compute the inverse cdf of the posterior distribution exactly. 
                    In such cases, a simple and practical alternative is to use a <strong>Monte Carlo approximation</strong> 
                    based on samples drawn from the posterior. 
                </p> 
                <p> 
                    Suppose we can generate samples \(\theta^{(1)}, \theta^{(2)}, \dots, \theta^{(S)}\) independently from the posterior \(p(\theta \mid \mathcal{D})\).
                    Then, to approximate a \((1-\alpha)\)% credible interval:
                </p> 
                <ul style="padding-left: 40px;"> 
                    <li>Sort the samples in increasing order: \(\theta^{(1)} \leq \theta^{(2)} \leq \cdots \leq \theta^{(S)}\).</li> 
                    <li>Set the lower endpoint \(l\) to be the \(\left(\frac{\alpha}{2}\right)\)-quantile, and the upper endpoint \(u\) to be the \(\left(1 - \frac{\alpha}{2}\right)\)-quantile.</li> 
                </ul> 
                <p> 
                    More precisely, approximate:
                    \[
                    \begin{align*}
                    l &\approx \theta^{\left(\left\lceil S \times \frac{\alpha}{2} \right\rceil\right)}, \\\\
                    u &\approx \theta^{\left(\left\lceil S \times \left(1 - \frac{\alpha}{2}\right) \right\rceil\right)},
                    \end{align*}
                    \]
                    where \(\lceil \cdot \rceil\) denotes the ceiling function, rounding up to the nearest integer if necessary.
                </p> 
                <p> 
                    Intuitively, instead of computing the inverse cdf analytically, we use the <strong>empirical cumulative distribution function (ecdf)</strong> 
                    formed by the sorted samples to estimate the desired quantiles. 
                    As the number of samples \(S \to \infty\), the empirical quantiles converge to the true quantiles of the posterior by the law of large numbers.
                </p>
                <p>
                    This Monte Carlo method is especially powerful when the posterior distribution is complex or high-dimensional, and no closed-form expression 
                    for the cdf or its inverse is available.
                </p>

            </section>

            <section id="demo" class="section-content">
              
                <h2>Demo: Monte Carlo Approximation for CIs</h2>
                <div id="monte_carlo_visualizer"></div>

                <p>
                Note: This demo does not perform full Bayesian inference from data. Instead, it uses predefined probability distributions 
                to simulate "posterior-like" behavior. This allows us to illustrate how Monte Carlo methods are used to approximate credible 
                intervals — particularly central and HPD intervals—without needing a prior or likelihood.
                </p>

                <p>
                <strong>Monte Carlo methods</strong> are widely used in Bayesian statistics to approximate posterior distributions, especially 
                when closed-form solutions are unavailable. One common application is estimating credible intervals using random 
                samples drawn from the posterior.
                </p>

                <p>
                To construct a <strong>central credible interval</strong> via Monte Carlo sampling:
                </p>

                <ol style="padding-left: 40px;">
                <li>Draw many random samples from the posterior distribution</li>
                <li>Sort the samples in ascending order</li>
                <li>Select the appropriate quantiles (e.g., the 2.5% and 97.5% quantiles for a 95% interval)</li>
                </ol>

                <p>
                As the number of samples increases, the approximation of the posterior improves. 
                However, central credible intervals based on quantiles may not accurately reflect the distribution's 
                shape — especially in skewed or multimodal cases.
                </p>

                <p>
                For example, in a <strong>bimodal distribution</strong> (with two distinct peaks), the interval may:
                </p>

                <ul style="padding-left: 40px;">
                <li>Fall in a low-probability region between the modes</li>
                <li>Omit one of the peaks entirely</li>
                <li>Misrepresent the true uncertainty or structure of the posterior</li>
                </ul>

                <p>
                In such cases, <strong>highest posterior density (HPD)</strong> regions are preferred. 
                They represent the region(s) of highest probability mass and always include the most probable values, 
                even if the result is disjoint. 
                </p>
            </section>

            <section id="HPD" class="section-content">
                <h2>Highest Posterior Density (HPD)</h2>

                <p>
                The <strong>highest posterior density (HPD)</strong> region is defined as the set of values of the parameter &theta; where the posterior 
                density exceeds a certain threshold \(p^*\), such that the total probability mass within this region equals the desired confidence level \(1 - \alpha\):
                </p>

                <p style="text-align: center;">
                \[
                1 - \alpha = \int_{\theta: p(\theta \mid \mathcal{D}) > p^*} p(\theta \mid \mathcal{D}) \, d\theta
                \]
                \[
                C_{\alpha} (\mathcal{D}) = \{\theta : p(\theta \mid \mathcal{D}) \geq p^*\}
                \]
                </p>

                <p>
                In one dimension, this HPD region corresponds to the interval(s) containing the highest probability density values, 
                with total mass equal to the target confidence level (e.g., 95%). In this case, HPD regions are sometimes also called 
                <strong>highest density intervals (HDIs)</strong>.
                </p>

                <p>
                Unlike central credible intervals, HPD regions are not required to be symmetric or centered around the mean. 
                This makes them especially useful when the posterior distribution is skewed or multimodal. 
                In such cases, HPD intervals better capture the actual regions of highest likelihood.
                </p>

                <p>
                In the demo, you can observe that the HPD interval is often narrower than the central credible interval, and may exclude low-density regions 
                that the central interval mistakenly includes.
                </p>

            </section>

            <section id="MCMC" class="section-content">
                <h2>Markov Chain Monte Carlo (MCMC)</h2>

                <p>
                    So far, our demo uses predefined distributions to illustrate how Monte Carlo methods can approximate credible intervals.
                    However, in real Bayesian inference, the full posterior distribution \( p(\theta \mid \mathcal{D}) \) is rarely 
                    available in closed form.
                </p>
    
                <p>
                    Instead, we typically work with the <strong>unnormalized posterior</strong>:
                    \[
                    p(\theta \mid \mathcal{D}) \propto p(\mathcal{D} \mid \theta) \cdot p(\theta) =  p^*(\theta)
                    \]
                    where

                <ul style="padding-left: 40px;">
                    <li>\( p(\mathcal{D} \mid \theta) \): the <strong>likelihood</strong>, expressing how probable the data is given a parameter</li>
                    <li>\( p(\theta) \): the <strong>prior</strong> belief about the parameter before seeing data</li>
                </ul>

                This proportionality holds because the normalizing constant \( p(\mathcal{D}) \) does not depend on \(\theta\) and can be 
                ignored during sampling. 
                </p>

                <p>
                    Remember, to compute the actual <a href="bayesian.html"><strong>posterior</strong></a> \( p(\theta \mid \mathcal{D}) \), we would need to normalize \( p^*(\theta) \) by computing:
                    \[
                    p(\mathcal{D}) = \int p(\mathcal{D} \mid \theta) \cdot p(\theta) \, d\theta
                    \]
                    which is often intractable, especially in high-dimensional problems.
                </p>
                  

                <p>
                    This is where <strong>Markov Chain Monte Carlo (MCMC)</strong> methods become essential. MCMC algorithms constructs 
                    a <a href="markov.html"><strong>Markov chain</strong></a> whose long-run behavior (stationary distribution) approximates 
                    the posterior distribution. This allows us to generate samples from the posterior without needing to compute the 
                    normalization constant \( p(\mathcal{D}) \).
                </p>
                    
                <p>
                    The following are three classic MCMC methods that are foundational to Bayesian computation:
                </p>
                    
                    <ul style="padding-left: 40px;">
                      <li><strong>Metropolis-Hastings:</strong> A general-purpose algorithm that proposes new samples and accepts or rejects them based on a probability ratio</li>
                      <li><strong>Gibbs Sampling:</strong> Efficient when conditional distributions are known, commonly used in simpler models and probabilistic graphical models</li>
                      <li><strong>Hamiltonian Monte Carlo (HMC):</strong> A gradient-based method well-suited for high-dimensional posteriors; still widely used in probabilistic programming languages like Stan and PyMC</li>
                    </ul>
                    
                <p>
                    While these methods are not used to train large-scale models like LLMs, they remain essential for Bayesian modeling, especially in scientific and uncertainty-aware applications. We will revisit 
                    each of them in the future. 
                </p>

                <p>
                    In modern machine learning and AI — particularly in large-scale models like transformers or <strong>Large Language Models (LLMs)</strong> — MCMC is not used for training, due to 
                    its high computational cost and poor scalability in very high-dimensional parameter spaces. These models are instead trained 
                    using <a href="../Calculus/gradient.html">gradient-based optimization</a> (e.g., stochastic gradient descent or Adam) with maximum likelihood estimation. 
                    However, Bayesian ideas remain important. Methods inspired by MCMC are still used for uncertainty quantification, 
                    probabilistic modeling, simulation-based inference, and decision-making under uncertainty. 
                </p>
            </section>
        </div>

        <script src="/js/main.js"></script>
        <script src="https://cdn.jsdelivr.net/npm/jstat@latest/dist/jstat.min.js"></script>
        <script src="/js/monte_carlo_visualizer.js"></script> 

    </body>
</html>
    
    <footer>
    <div class="footer-content">
        <div class="footer-about">
            <h3>About MATH-CS COMPASS</h3>
            <p>Bridging the gap between pure mathematics and computer science applications.</p>
            <div class="contact-section">
                <strong><a href="https://docs.google.com/forms/d/e/1FAIpQLSf3_RNkegREvfCEIQznn6SCaf-iB5c-Doxn7Ymuu78Uf-PBjg/viewform" target="_blank">
                    Submit Your Questions & Feedback <i class="fas fa-external-link-alt"></i>
                </a></strong>
            </div>
        </div>
        <div class="footer-links">
            <h3>Quick Links</h3>
            <ul>
                
                <li><a href="../../../index.html">Home</a></li>
                <li>
                <a href="../Linear_algebra/linear_algebra.html">
                    <span class="nav-number">I</span>
                    <span>Linear Algebra to Algebraic Foundations</span>
                </a>
                </li>
                <li>
                <a href="../Calculus/calculus.html">
                    <span class="nav-number">II</span>
                    <span>Calculus to Optimization & Analysis</span>
                </a>
                </li>
                <li>
                <a href="../Probability/probability.html">
                    <span class="nav-number">III</span>
                    <span>Probability & Statistics</span>
                </a>
                </li>
                <li>
                <a href="../Discrete/discrete_math.html">
                    <span class="nav-number">IV</span>
                    <span>Discrete Mathematics & Algorithms</span>
                </a>
                </li>
                <li>
                <a href="../Machine_learning/ml.html">
                    <span class="nav-number">V</span>
                    <span>Machine Learning</span>
                </a>
                </li>
            </ul>
        </div>
        <div class="footer-social">
            <h3>Connect With Me</h3>
            <p>
                <a href="https://x.com/MathCSCompass?t=Zi8nyfzszeFw8QizbJwnNg&s=09" target="_blank">
                    <i class="fab fa-x-twitter"></i> X (Twitter)
                </a>
            </p>
            <p>
                <a href="https://www.facebook.com/share/15uPaBWgQN/" target="_blank">
                    <i class="fab fa-facebook"></i> Facebook
                </a>
            </p>
      
            <h3>MATH-CS COMPASS App</h3>
            <div class="app-download-footer">
                <a href="https://play.google.com/store/apps/details?id=io.github.yusukey01.twa" target="_blank" rel="noopener">
                    <img src="../../../images/google-play-badge.png" 
                        alt="Get it on Google Play" 
                        class="google-play-badge">
                </a>
            </div>
            
        </div>

    </div>
    <div class="footer-bottom">
        <p>&copy; 2024-2025 MATH-CS COMPASS. All rights reserved.</p>
    </div>
</footer>
    <script src="../../../js/main.js?v=1.0.2"></script>
    <script src="../../../js/search.js?v=1.0.2"></script>
    

    <button id="go-to-top-btn" class="go-to-top-btn" title="Go to top">
        <i class="fas fa-arrow-up"></i>
    </button>
    <script>
        // Register the service worker
        if ('serviceWorker' in navigator) {
          window.addEventListener('load', () => {
            navigator.serviceWorker.register('/js/service-worker.js')
              .then(registration => {
                console.log('ServiceWorker registration successful with scope: ', registration.scope);
              })
              .catch(error => {
                console.log('ServiceWorker registration failed: ', error);
              });
          });
        }
      </script>
</body>
</html>