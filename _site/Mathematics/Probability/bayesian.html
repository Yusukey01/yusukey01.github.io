<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    
    <title>Intro to Bayesian Statistics - MATH-CS COMPASS</title>
    
    
    <script>
    window.MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']]
      }
    };
    </script>
    <script id="MathJax-script" defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="preload" href="https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Main-Regular.woff" as="font" type="font/woff2" crossorigin>
    <link rel="preload" href="https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Math-Italic.woff" as="font" type="font/woff2" crossorigin>
    
    
    

    <link rel="stylesheet" href="../../../css/styles.css?v=1.0.2">
  
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css">

    <link rel="dns-prefetch" href="https://cdn.jsdelivr.net">
    <link rel="dns-prefetch" href="https://cdnjs.cloudflare.com">
    
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Learn about Bayesian inference and  conjugate priors.">
    <meta name="google-site-verification" content="IieEujypYW38caEVe8ymjczouNl56yxAy27iPztcfRA" />
    

    <link rel="manifest" href="/manifest.json">

    <!-- Theme color for Chrome, Android, and other browsers -->
    <meta name="theme-color" content="#007bff">
  
    <!-- iOS specific tags -->
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <meta name="apple-mobile-web-app-title" content="MATH-CS COMPASS">
    <meta name="mobile-web-app-capable" content="yes">

    <!-- Icon references -->
    <link rel="icon" type="image/png" sizes="1280x1280" href="/images/maskable_icon_x1280.png?v=1.0.2">
    <link rel="icon" type="image/png" sizes="512x512" href="/images//maskable_icon_x512.png?v=1.0.2">
    
    <!-- Apple Touch Icons -->
    <link rel="apple-touch-icon" href="/images/maskable_icon_x1280.png?v=1.0.2">
    <link rel="apple-touch-icon" href="/images/maskable_icon_x512.png?v=1.0.2">

    <!-- For Windows -->
    <meta name="msapplication-TileImage" content="/images/test1.png?v=1.0.2">
    <meta name="msapplication-TileColor" content="#007bff">

    <!-- Schema.org Structured Data for AI Optimization -->
    

    <!-- Article Schema for content pages -->
    
    <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "Article",
      "headline": "Intro to Bayesian Statistics",
      "description": "Learn about Bayesian inference and  conjugate priors.",
      "datePublished": "2024-01-01",
      "dateModified": "2025-01-01",
      "author": {
        "@type": "Person",
        "name": "Yusuke Yokota",
        "jobTitle": "Mathematics & Computer Science Educator",
        "knowsAbout": [
          "Linear Algebra",
          "Machine Learning",
          "Computer Science",
          "Mathematics"
        ]
      },
      "publisher": {
        "@type": "Organization",
        "name": "MATH-CS COMPASS",
        "url": "https://math-cs-compass.com",
        "logo": {
          "@type": "ImageObject",
          "url": "https://math-cs-compass.com/images/maskable_icon_x512.png"
        }
      },
      "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "https://math-cs-compass.com/Mathematics/Probability/bayesian.html"
      },
      "about": [
        
        { "@type": "Thing", "name": "Probability" },
        { "@type": "Thing", "name": "Statistics" }
        
      ],
      "educationalLevel": "university",
      "learningResourceType": "tutorial",
      "educationalUse": "instruction"
    }
    </script>
    
</head>

<body>
    <header>
    <nav class="navbar">
        <!-- Logo as Home Button -->
        <div class="logo-home">
            <a href="../../../index.html" 
               class="logo-link " 
               title="Home - Math-CS Compass">
                <img src="../../images/icon_x512.png" 
                     alt="Math-CS Compass Logo" 
                     class="nav-logo">
            </a>
        </div>

        <div class="search-container">
            <input type="text" id="search-input" placeholder="Search keywords...">
            <button id="search-button" aria-label="Search" title="Search website"><i class="fas fa-search"></i></button>
        </div>
        
        <div class="menu-toggle">
            <i class="fas fa-bars"></i>
        </div>
        
        <ul class="nav-links">
            <li><a href="../Linear_algebra/linear_algebra.html" >I - Linear Algebra to Algebraic Foundations</a></li>
            
            <li><a href="../Calculus/calculus.html" >II - Calculus to Optimization & Analysis</a></li>
            
            <li><a href="../Probability/probability.html" class="active">III - Probability & Statistics</a></li>
            
            <li><a href="../Discrete/discrete_math.html" >IV - Discrete Mathematics & Algorithms</a></li>
            
            <li><a href="../Machine_learning/ml.html" >V - Machine Learning</a></li>
        </ul>
    </nav>
</header>
    
    <!DOCTYPE html>
<html>
    <body>
        <!-- LearningResource Schema for Content Pages -->
        <script type="application/ld+json">
        {
        "@context": "https://schema.org",
        "@type": "LearningResource",
        "name": "Intro to Bayesian Statistics",
        "description": "Learn about Bayesian inference and  conjugate priors.",
        "learningResourceType": "lesson",
        "educationalUse": "instruction",
        "educationalLevel": "university",
        "interactivityType": "active",
        "author": {
            "@type": "Person",
            "name": "Yusuke Yokota",
            "jobTitle": "Mathematics & Computer Science Educator"
        },
        "publisher": {
            "@type": "Organization",
            "name": "MATH-CS COMPASS",
            "url": "https://math-cs-compass.com"
        },
        "about": [
            
            { "@type": "Thing", "name": "Bayesian Statistics" },
            { "@type": "Thing", "name": "Bayesian Inference" },
            { "@type": "Thing", "name": "Conjugate Prior" },
            { "@type": "Thing", "name": "Prior Distribution" },
            { "@type": "Thing", "name": "Posterior Distribution" },
            { "@type": "Thing", "name": "Likelihood" },
            { "@type": "Thing", "name": "Marginal Likelihood" },
            { "@type": "Thing", "name": "Beta-Binomial Model" },
            { "@type": "Thing", "name": "Normal Distribution Model" }
            
        ],
        "teaches": [
            "Mathematical concepts",
            "Practical applications",
            "Problem-solving techniques"
        ],
        "isPartOf": {
            "@type": "Course",
            "name": "Probability & Statistics",
            "description": "Explore fundamental concepts of probability and statistics essential for machine learning",
            "provider": {
            "@type": "Organization",
            "name": "MATH-CS COMPASS",
            "url": "https://math-cs-compass.com"
            },
            "instructor": {
            "@type": "Person",
            "name": "Yusuke Yokota",
            "jobTitle": "Mathematics & Computer Science Educator"
            },
            "courseCode": "III",
            "hasCourseInstance": {
            "@type": "CourseInstance",
            "courseMode": "online",
            "courseWorkload": "PT2H30M",
            "instructor": {
                "@type": "Person",
                "name": "Yusuke Yokota"
            }
            },
            "offers": {
            "@type": "Offer",
            "price": "0",
            "priceCurrency": "USD",
            "availability": "https://schema.org/InStock",
            "category": "free"
            }
        }
        }
        </script>
        <!-- WebApplication Schema for Interactive Demos -->
        <script type="application/ld+json">
        {
        "@context": "https://schema.org",
        "@type": "WebApplication",
        "name": "Intro to Bayesian Statistics Interactive Demo",
        "description": "Interactive demonstration of intro to bayesian statistics concepts",
        "applicationCategory": "EducationalApplication",
        "operatingSystem": "Web Browser",
        "url": "https://math-cs-compass.com/Mathematics/Probability/bayesian.html",
        "author": {
            "@type": "Person",
            "name": "Yusuke Yokota"
        },
        "applicationSubCategory": "Mathematical Visualization",
        "featureList": [
            "Interactive mathematical visualization",
            "Real-time parameter adjustment",
            "Educational demonstrations"
        ],
        "isAccessibleForFree": true,
        "educationalUse": "instruction",
        "educationalLevel": "university"
        }
        </script>
        <div class="hero-section">
            <h1 class="webpage-name">Intro to Bayesian Statistics</h1>
        </div>

        
  <div class="quick-jump-container">
    <button type="button" id="quick-jump-toggle" class="quick-jump-toggle">
      <i class="fas fa-list"></i> <strong>Section III Navigation</strong>
    </button>
    <div id="quick-jump-menu" class="quick-jump-menu">
      <h3>Probability & Statistics</h3>
      <div class="quick-jump-links">

         
        <a href="probability.html">‚Üê Back to Section III Overview</a>
        <hr style="margin: 8px 0; border: 1px solid #ddd;">
        
        
        <a href="basic.html" >Part 1: Basic Probability Ideas</a>
        <a href="random_variables.html" >Part 2: Random Variables</a>
        <a href="gamma.html" >Part 3: Gamma & Beta Distribution</a>
        <a href="gaussian.html" >Part 4: Normal (Gaussian) Distribution</a>
        <a href="student.html" >Part 5: Student's t-Distribution</a>
        <a href="covariance.html" >Part 6: Covariance</a>
        <a href="correlation.html" >Part 7: Correlation</a>
        <a href="mvn.html" >Part 8: Multivariate Distributions</a>
        <a href="mle.html" >Part 9: Maximum Likelihood Estimation</a>
        <a href="hypothesis_testing.html" >Part 10: Statistical Inference & Hypothesis Testing</a>
        <a href="linear_regression.html" >Part 11: Linear Regression</a>
        <a href="entropy.html" >Part 12: Entropy</a>
        <a href="convergence.html" >Part 13: Convergence</a>
        <a href="bayesian.html" class="current-page">Part 14: Intro to Bayesian Statistics</a>
        <a href="expfamily.html" >Part 15: The Exponential Family</a>
        <a href="fisher_info.html" >Part 16: Fisher Information Matrix</a>
        <a href="decision_theory.html" >Part 17: Bayesian Decision Theory</a>
        <a href="markov.html" >Part 18: Markov Chains</a>
        <a href="monte_carlo.html" >Part 19: Monte Carlo Methods</a>
        <a href="importance_sampling.html" >Part 20: Importance Sampling</a>
        <a href="gaussian_process.html" >Part 21: Gaussian Processes</a>
      </div>
    </div>
</div>


        <div class="topic-nav">
            <a href="#bayesian">Bayesian Inference</a>
            <a href="#conjugate">Conjugate Prior</a>
            <a href="#normal">Univariate Normal Distribution Model</a>
        </div> 

        <div class="container">  
           
            <section id="bayesian" class="section-content">
            <h2>Bayesian Inference</h2>
            <p> 
            In <strong>Bayesian statistics</strong>, we assume that prior knowledge about the unknown parameters can be explained by a probability 
            distribution, which is called <strong>prior distribution</strong>, \(p(\theta)\). This means that the unknown parameters \(\theta\) are 
            treated as <strong>random variables</strong>, while the data \(\mathcal{D}\) are considered fixed. This approach is opposite 
            to that of frequentist statistics, where parameters are fixed and data are considered random. 
            <br><br>
            Bayesian inference updates our belief(= prior distribution) by using the observed data and
            expresses our updated belief about the parameters as the <strong>posterior distribution</strong>, \(p(\theta | \mathcal{D})\) 
            using Bayes' rule:
            \[
            \begin{align*}
            p(\theta | \mathcal{D}) &= \frac{p(\theta)p(\mathcal{D}| \theta)}{p(\mathcal{D})} \\\\
            \end{align*}
            \]
            The components are defined as follows:
            <ol style="padding-left: 40px;">
                <li><strong>Prior distribution</strong> \(p(\theta)\):</li> 
                Represents our belief about the parameters \(\theta\) before observing any data. 
                It encodes prior knowledge, assumptions, or uncertainty about \(\theta\).
                <li><strong>Likelihood</strong> \(p(\mathcal{D}| \theta)\):</li>
                Describes how likely the observed data \(\mathcal{D}\) is, given a specific value of \(\theta\). 
                This reflects the model of the data-generating process.
                <li><strong>Marginal likelihood</strong>(or <strong>evidence</strong>) \(p(\mathcal{D})\): </li>
                A normalization constant, which is important when we evaluate different models, otherwise we can ignore this term because 
                it is just a constant with respect to parameters \(\theta\).
                For continuous parameters, marginal likelihood is computed by integrating over all possible values of \(\theta\):  
                \[
                p(\mathcal{D}) = \int p(\theta')p(\mathcal{D} | \theta') \, d\theta'.
                \]
                Note: In the integral for the marginal likelihood, we use \(\theta'\) instead of \(\theta\) to clarify that 
                the integration is over the entire parameter space, not a specific parameter. 
                <br><br>
                Note: For discrete  parameters, marginal likelihood is given by:
                \[
                p(\mathcal{D}) = \sum_{\theta'} p(\theta')p(\mathcal{D} | \theta').
                \]
            </ol>
            Note: Please check <a href="bayesian.html"><strong>Credible intervals</strong></a> too.
            </p>
            </section>

            <section id="conjugate" class="section-content">
            <h2>Conjugate Prior</h2>
            <p> 
            In general, specifying a prior is a bottleneck of the Bayesian inference. Here, we introduce some special case of priors.
            <br><br>
            A prior \(p(\theta) \in \mathcal{F}\) is said to be <strong>conjugate prior</strong> for a likelihood function \(p(\mathcal{D} | \theta)\) if 
            the posterior is in the same parameterized family as the prior: \(p(\mathcal{D} | \theta) \in \mathcal{F}\). 
            <br><br>
            Through the following example, we introduce basic Bayesian inference ideas and an example of a conjugate prior.
            <div class="proof">
                <span class="proof-title">Example 1: Beta-Binomial Model</span>
                Consider tossing a coin \(N\) times. Let \(\theta \in [0, 1]\) be a chance of getting head. We record the outcomes as 
                \(\mathcal{D} = \{y_n \in \{0, 1\} : n = 1 : N\}\). We assume the data are iid.
                <br><br>
                If we consider a sequence of coin tosses, the <strong>likelihood</strong> can be written as a Bernoulli likelihood model: 
                \[
                \begin{align*}
                p(\mathcal{D} | \theta) &= \prod_{n = 1}^N \theta^{y_n}(1 - \theta)^{1-y_n} \\\\
                                        &= \theta^{N_1}(1 - \theta)^{N_0}
                \end{align*}
                \]
                where \(N_1\) and \(N_0\) are the number of heads and tails respectively. (Sample size: \(N_1 + N_0 = N\))
                <br><br>
                Alternatively, we can consider the Binomial likelihood model: 
                The likelihood has the following form:
                \[
                \begin{align*}
                p(\mathcal{D} | \theta) &= \text{Bin } (y | N, \theta)  \\\\
                                        &= \begin{pmatrix} N \\ y \end{pmatrix} \theta^y (1 - \theta)^{N - y} \\\\
                                        &\propto  \theta^y (1 - \theta)^{N - y}
                \end{align*}
                \]
                where \(y\) is the number of heads.
                <br><br>
                Next, we have to specify a <strong>prior</strong>. If we know nothing about the parameter, an 
                <strong>uninformative prior</strong> can be used:
                \[
                p(\theta) = \text{Unif }(\theta | 0, 1).
                \]
                However, "in this example", using beta distribution(See <a href="gamma.html">Gamma & Beta distribution </a>), we can represent the prior as follows:
                \[
                \begin{align*}
                p(\theta) = \text{Beta }(\theta | a, b) &= \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}\theta^{a-1}(1-\theta)^{b-1} \tag{1} \\\\
                                                        &\propto \theta^{a-1}(1-\theta)^{b-1}
                \end{align*}
                \]
                where \(a, b > 0\) are usually called hyper-parameters.(Our main parameter is \(\theta\).)
                <br>
                Note: If \(a = b = 1\), we get the uninformative prior. 
                <br><br>
                Using Bayes' rule, the <strong>posterior</strong> is proportional to the product of the likelihood and the prior:
                \[
                \begin{align*}
                p(\theta | \mathcal{D}) &\propto [\theta^{y}(1 - \theta)^{N-y}] \cdot [\theta^{a-1}(1-\theta)^{b-1}] \\\\
                                        &\propto \text{Beta }(\theta | a+y, \, b+N-y) \\\\
                                        &= \frac{\Gamma(a+b+N)}{\Gamma(a+y)\Gamma(b+N-y)}\theta^{a+y-1}(1-\theta)^{b+N-y-1}. \tag{2}
                \end{align*}
                \]
                Here, the posterior has the same functional form as the prior. Thus, the beta distribution is the <strong>conjugate prior</strong> for the 
                binomial distribution. 
                <br><br>
                Once we got the posterior distribution, for example, we can use <strong>posterior mean</strong>, \(\bar{\theta}\) as a point estimate of \(\theta\):
                \[
                \begin{align*}
                \bar{\theta} = \mathbb{E }[\theta | \mathcal{D}] &= \frac{a+y}{(a+y) + (b+N-y)} \\\\
                                                                 &= \frac{a+y}{a+b+N}.
                \end{align*}
                \]
                Note: 
                By adjusting hyper-parameters \(a\) and \(b\), we can control the influence of the prior on the posterior.
                <br>
                If \(a\) and \(b\) are small, the posterior mean will closely reflect the data: 
                \[
                \bar{\theta} \approx  \frac{y}{N} = \hat{\theta}_{MLE}
                \]
                while if \(a\) and \(b\) are large, the posterior mean will be more influenced by the prior.
                <br><br>
                Often we need to check the <strong>standard error</strong> of our estimate, which is the posterior standard deviation:
                \[
                \begin{align*}
                \text{SE }(\theta) &= \sqrt{\text{Var }[\theta | \mathcal{D}]} \\\\
                                   &= \sqrt{\frac{(a+y)(b+N-y)}{(a+b+N)^2(a+b+N+1)}}
                \end{align*}
                \]
                Here, if \(N \gg a, b\), we can simplify the <strong>posterior variance</strong> as follows:
                \[
                \begin{align*}
                \text{Var }[\theta | \mathcal{D}] &\approx \frac{y(N-y)}{(N)^2 N} \\\\
                                                  &= \frac{y}{N^2} - \frac{y^2}{N^3} \\\\
                                                  &= \frac{\hat{\theta}(1 - \hat{\theta})}{N}
                \end{align*}
                \]
                where \(\hat{\theta} = \frac{y}{N}\) is the MLE.
                <br><br>
                Thus, the standard error is given by 
                \[
                \text{SE }(\theta) \approx \sqrt{\frac{\hat{\theta}(1 - \hat{\theta})}{N}}.
                \]
                <br>
                From (1) and (2), the <strong>marginal likelihood</strong> is given by the ratio of normalization constants(beta functions) 
                for the prior and posterior:
                \[
                p(\mathcal{D}) = \frac{B(a+y,\, b+N-y)}{B(a, b)}.
                \]
                Note: In general, computing the marginal likelihood is too expensive or impossible, but the conjugate prior allows us to get the 
                exact marginal likelihood easily. Otherwise, we have to introduce some approximation methods.
                <br><br>
                Finally, to make predictions for new observations, we use <strong>posterior predictive distribution</strong>:
                \[
                p(x_{new} | \mathcal{D}) = \int p(x_{new} | \theta) p(\theta | \mathcal{D}) d\theta.
                \]
                Again, like computing the marginal likelihood, it is difficult to compute posterior predictive distribution, but in this case, 
                we can get it easily due to the conjugate prior. 
                <br>
                For example, the probability of observing a head in the next coin toss is given by:
                \[
                \begin{align*}
                p(y_{new}=1 | \mathcal{D}) &= \int_0 ^1  p(y_{new}=1 | \theta) p(\theta | \mathcal{D}) d\theta \\\\
                                           &= \int_0 ^1 \theta \text{Beta }(\theta | a+y, \, b+N-y) d\theta \\\\
                                           &= \mathbb{E }[\theta|\mathcal{D}] \\\\
                                           &= \frac{a+y}{a+b+N}.
                \end{align*}
                \]
                Note: As you can see, the hyper-parameters \(a\) and \(b\) is critical in the whole process of our inference. In practice, setting up 
                hyper-parameters is one of the most challenging factor of the project. 
            </div>
            </p>
            </section>

            <section id="normal" class="section-content">
            <h2>Univariate Normal Distribution Model</h2>
            <p>
            Given the univariate normal distribution \(N(\mu, \sigma^2)\), there are three cases for our target posterior: 
            <ol style="padding-left: 40px;">
                <li>\(p(\mu | \mathcal{D}, \sigma^2) \): Variance \(\sigma^2\) is known.</li>
                <li>\(p(\sigma^2 | \mathcal{D}, \mu) \): Mean \(\mu\) is known.</li>
                <li>\(p(\mu, \sigma^2 | \mathcal{D})\): Both \(\sigma^2\) and \(\mu\) are unknown.</li>
            </ol>
            <br>
            Here, we only discuss the case 1 and case 2. 
            <div class="proof">
                <span class="proof-title">Example 2: Normal distribution model with known variance \(\sigma^2\)</span>
                In the case where \(\sigma^2\) is known constant, the <strong>likelihood</strong> for \(\mu\) is given by 
                \[
                \begin{align*}
                p(\mathcal{D}| \mu) &= \prod_{n=1}^N \frac{1}{\sqrt{2\pi \sigma^2}} \exp \Big(-\frac{(y_n - \mu)^2}{2\sigma^2}\Big)\\\\
                                    &= \Big(\frac{1}{\sqrt{2\pi \sigma^2}}\Big)^N \exp \Big(- \sum_{n=1}^N \frac{(y_n - \mu)^2}{2\sigma^2}\Big) \\\\
                                    &\propto \exp \Big(-\frac{1}{2\sigma^2} \sum_{n=1}^N (y_n - \mu)^2 \Big).
                \end{align*}
                \]
                The <strong>conjugate prior</strong> is another normal distribution:
                \[
                \begin{align*}
                p(\mu) &= N(\mu | \, \mu_0, \sigma_0^2) \\\\
                       &\propto \exp \Big(-\frac{(\mu - \mu_0)^2}{2\sigma_0^2} \Big).
                \end{align*}
                \]
                <br>
                Now, we can compute the <strong>posterior</strong> as follows:
                \[
                \begin{align*}
                p(\mu | \, \mathcal{D}, \sigma^2) &\propto \exp \Big(-\frac{1}{2\sigma^2} \sum_{n=1}^N (y_n - \mu)^2 \Big) \cdot \exp \Big(-\frac{(\mu - \mu_0)^2}{2\sigma_0^2}  \Big) \\\\
                                                  &= \exp \Big\{-\frac{1}{2}\Big(\frac{N}{\sigma^2}+\frac{1}{\sigma_0^2} \Big)\mu^2 +\Big(\frac{\sum y_n}{\sigma^2} +\frac{\mu_0}{\sigma_0^2} \Big)\mu 
                                                                + \Big(-\frac{1}{2\sigma^2} \sum_{n=1}^N y_n^2 - \frac{\mu_0^2}{2\sigma_0^2} \Big) \Big\}
                \end{align*}
                \]
                Since \(-\frac{1}{2\sigma^2} \sum_{n=1}^N y_n^2 - \frac{\mu_0^2}{2\sigma_0^2}\) is constant with respect to \(\mu\), we ignore it.
                <br><br>
                Let \(A = \Big(\frac{N}{\sigma^2}+\frac{1}{\sigma_0^2} \Big)\) and \(B =\Big(\frac{\sum y_n}{\sigma^2} +\frac{\mu_0}{\sigma_0^2} \Big)\), and completing the square, we get:
                \[
                \begin{align*}
                p(\mu | \, \mathcal{D}, \sigma^2) &\propto \exp \Big\{-\frac{1}{2}A\mu^2 + B\mu \Big\} \\\\
                                                  &= \exp \Big\{-\frac{1}{2}A \Big(\mu - \frac{B}{A} \Big)^2 + \frac{B^2}{2A}\Big\}.                     
                \end{align*}
                \]
                Since the term \(\frac{B^2}{2A}\) does not depend on \(\mu\), and \(-\frac{1}{2}A \Big(\mu - \frac{B}{A} \Big)^2\) is a quadratic form of an univariate normal distributuin, we conclude that
                \[
                \begin{align*}
                p(\mu | \, \mathcal{D}, \sigma^2)  = N(\mu | \, \mu_N, \sigma_N^2 )
                \end{align*}
                \]
                where the <strong>posterior variance</strong> is given by:
                \[
                \begin{align*}
                \sigma_N^2 &= \frac{1}{A} \\\\
                           &= \frac{\sigma^2 \sigma_0^2}{N\sigma_0^2 + \sigma^2}
                \end{align*}
                \]
                and the <strong>posterior mean</strong> is given by:
                \[
                \begin{align*}
                \mu_N &= \frac{B}{A} \\\\
                      &= \sigma_N^2 \Big(\frac{\mu_0}{\sigma_0^2} + \frac{N \bar{y}}{\sigma^2} \Big) \\\\
                      &= \frac{\sigma^2}{N\sigma_0^2 + \sigma^2}\mu_0 + \frac{N\sigma_0^2}{N\sigma_0^2 + \sigma^2}\bar{y} \\\\
                \end{align*}
                \]
                where \(\bar{y} = \frac{1}{N}\sum_{n=1}^N y_n\) is the empirical mean. 
                <br><br>
                Note: In this case, \(\mu_0\) and \(\sigma_0^2\) are hyper-parameters of the prior distribution. For example, as you can 
                see the form of the posterior mean \(\mu_N\), a small \(\sigma_0^2\) gives more weight to the prior mean \(\mu_0\), while 
                a large \(\sigma_0^2\) reduces the influence of the prior, making the posterior more rely on the data.
            </div>
            In machine learning, a typical application of this model is <strong>online learning</strong> that is a training approach where 
            the model is updated incrementally as new data becomes available rather than being trained on fixed data at once (which 
            is known as <strong>batch learning</strong>). Online learning is particularly useful in scenarios where data arrive in a stream 
            or is too large to be processed all at once. 
            <br><br>
            <div class="proof">
                <span class="proof-title">Example 3: Normal distribution model with known mean \(\mu\)</span>
                In the case where \(\mu\) is known constant, the <strong>likelihood</strong> for \(\sigma^2\) is given by 
                \[
                \begin{align*}
                p(\mathcal{D}| \sigma^2) &= \frac{1}{(2 \pi \sigma^2)^{\frac{N}{2}}} \exp \Big(-\frac{1}{2\sigma^2} \sum_{n=1}^N (y_n - \mu)^2 \Big) \\\\
                                         &\propto (\sigma^2)^{-\frac{N}{2}} \exp \Big(-\frac{1}{2\sigma^2} \sum_{n=1}^N (y_n - \mu)^2 \Big).            
                \end{align*}
                \]
                The "standard" <strong>conjugate prior</strong> is the <strong>inverse gamma distribution</strong>:
                \[
                \begin{align*}
                p(\sigma^2) &= \text{InvGamma }(\sigma^2 | \, a, b) \\\\
                            &= \frac{b^a}{\Gamma(a)}(\sigma^2)^{-(a+1)} \exp \Big(- \frac{b}{\sigma^2}\Big)
                \end{align*}
                \]
                where \(a > 0\) is the shape parameter and  \(b > 0\) is the scale parameter.
                <br><br>
                The <strong>posterior</strong> is given by:
                \[
                \begin{align*}
                p(\sigma^2 | \, \mathcal{D}, \mu) &\propto \Big\{ (\sigma^2)^{-\frac{N}{2}} \exp \Big(-\frac{1}{2\sigma^2} \sum_{n=1}^N (y_n - \mu)^2 \Big) \Big\}
                                                           \cdot \Big\{\frac{b^a}{\Gamma(a)}(\sigma^2)^{-(a+1)} \exp \Big(- \frac{b}{\sigma^2}\Big) \Big\} \\\\    
                                                  &= (\sigma^2)^{-(a+\frac{N}{2}+1)} \exp \Big\{ - \frac{1}{\sigma^2}\Big(b + \frac{1}{2} \sum_{n=1}^N (y_n - \mu)^2 \Big) \Big\}                         
                                                 
                \end{align*}
                \]
                Here, let \(\hat{a} = a + \frac{N}{2}\), and \(\hat{b} = b + \frac{1}{2} \sum_{n=1}^N (y_n - \mu)^2\).
                Thus,
                \[
                p(\sigma^2 | \, \mathcal{D}, \mu) = \text{InvGamma }(\sigma^2 | \, \hat{a}, \hat{b}).
                \]
                <br>
                Alternatively, we can choose <strong>scaled inverse chi-squared distribution</strong> as the conjugate prior:
                \[
                \begin{align*}
                p(\sigma^2) &= \text{ScaledInv- }\chi^2(\sigma^2 | \, \nu_0, \sigma_0^2) \\\\
                            &= \text{InvGamma }\Big(\sigma^2 | \, \frac{\nu_0}{2}, \frac{\nu_0 \sigma_0^2}{2}\Big) \\\\
                            &\propto (\sigma^2)^{-\frac{\nu_0}{2}-1} \exp \Big(- \frac{\nu_0 \sigma_0^2}{2\sigma^2}   \Big)
                \end{align*}
                \]
                where \(\nu\) is the degrees of freedom. 
                <br>
                Thus, the posterior is given by:
                \[
                p(\sigma^2 | \, \mathcal{D}, \mu) = \text{ScaledInv- }\chi^2 \Big(\sigma^2 | \, \nu_0 +N, \, \frac{\nu_0 \sigma_0^2 + \sum_{n=1}^N (y_n - \mu)^2}{\nu_0 + N}\Big).
                \]
                A benefit of using this form is that we can control the strength of the prior by the single hyper-parameter \(\nu_0\).
            </div>
            This is typical model for quality control and process monitoring because in industrial and manufacturing processes, the 
            mean of a quality characteristic might be known based on historical data. In machine learning, this kind of approach is 
            called <strong>anomaly detection</strong>. Anomalies are detected by comparing new datapoints to the baseline model.
            </p>
            </section>
        </div>
        <script src="/js/main.js"></script>
    </body>
</html>
    
    <footer>
    <div class="footer-content">
        <div class="footer-about">
            <h3>About MATH-CS COMPASS</h3>
            <p>Bridging the gap between pure mathematics and computer science applications.</p>
            <div class="contact-section">
                <strong><a href="https://docs.google.com/forms/d/e/1FAIpQLSf3_RNkegREvfCEIQznn6SCaf-iB5c-Doxn7Ymuu78Uf-PBjg/viewform" target="_blank">
                    Submit Your Questions & Feedback <i class="fas fa-external-link-alt"></i>
                </a></strong>
            </div>
        </div>
        <div class="footer-links">
            <h3>Quick Links</h3>
            <ul>
                
                <li><a href="../../../index.html">Home</a></li>
                <li>
                <a href="../Linear_algebra/linear_algebra.html">
                    <span class="nav-number">I</span>
                    <span>Linear Algebra to Algebraic Foundations</span>
                </a>
                </li>
                <li>
                <a href="../Calculus/calculus.html">
                    <span class="nav-number">II</span>
                    <span>Calculus to Optimization & Analysis</span>
                </a>
                </li>
                <li>
                <a href="../Probability/probability.html">
                    <span class="nav-number">III</span>
                    <span>Probability & Statistics</span>
                </a>
                </li>
                <li>
                <a href="../Discrete/discrete_math.html">
                    <span class="nav-number">IV</span>
                    <span>Discrete Mathematics & Algorithms</span>
                </a>
                </li>
                <li>
                <a href="../Machine_learning/ml.html">
                    <span class="nav-number">V</span>
                    <span>Machine Learning</span>
                </a>
                </li>
            </ul>
        </div>
        <div class="footer-social">
            <h3>Connect With Me</h3>
            <p>
                <a href="https://x.com/MathCSCompass?t=Zi8nyfzszeFw8QizbJwnNg&s=09" target="_blank">
                    <i class="fab fa-x-twitter"></i> X (Twitter)
                </a>
            </p>
            <p>
                <a href="https://www.facebook.com/share/15uPaBWgQN/" target="_blank">
                    <i class="fab fa-facebook"></i> Facebook
                </a>
            </p>
      
            <h3>MATH-CS COMPASS App</h3>
            <div class="app-download-footer">
                <a href="https://play.google.com/store/apps/details?id=io.github.yusukey01.twa" target="_blank" rel="noopener">
                    <img src="../../../images/google-play-badge.png" 
                        alt="Get it on Google Play" 
                        class="google-play-badge">
                </a>
            </div>
            
        </div>

    </div>
    <div class="footer-bottom">
        <p>&copy; 2024-2025 MATH-CS COMPASS. All rights reserved.</p>
    </div>
</footer>
    <script src="../../../js/main.js?v=1.0.2"></script>
    <script src="../../../js/search.js?v=1.0.2"></script>
    

    <button id="go-to-top-btn" class="go-to-top-btn" title="Go to top">
        <i class="fas fa-arrow-up"></i>
    </button>
    <script>
        // Register the service worker
        if ('serviceWorker' in navigator) {
          window.addEventListener('load', () => {
            navigator.serviceWorker.register('/js/service-worker.js')
              .then(registration => {
                console.log('ServiceWorker registration successful with scope: ', registration.scope);
              })
              .catch(error => {
                console.log('ServiceWorker registration failed: ', error);
              });
          });
        }
      </script>
</body>
</html>