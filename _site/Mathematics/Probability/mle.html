<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    
    <title>Maximum Likelihood Estimation - MATH-CS COMPASS</title>
    
    
    <script>
    window.MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']]
      }
    };
    </script>
    <script id="MathJax-script" defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="preload" href="https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Main-Regular.woff" as="font" type="font/woff2" crossorigin>
    <link rel="preload" href="https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Math-Italic.woff" as="font" type="font/woff2" crossorigin>
    
    
    

    <link rel="stylesheet" href="../../../css/styles.css?v=1.0.2">
  
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css">

    <link rel="dns-prefetch" href="https://cdn.jsdelivr.net">
    <link rel="dns-prefetch" href="https://cdnjs.cloudflare.com">
    
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Learn about point estimators, and maximum likelihood estimation(MLE).">
    <meta name="google-site-verification" content="IieEujypYW38caEVe8ymjczouNl56yxAy27iPztcfRA" />
    

    <link rel="manifest" href="/manifest.json">

    <!-- Theme color for Chrome, Android, and other browsers -->
    <meta name="theme-color" content="#007bff">
  
    <!-- iOS specific tags -->
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <meta name="apple-mobile-web-app-title" content="MATH-CS COMPASS">
    <meta name="mobile-web-app-capable" content="yes">

    <!-- Icon references -->
    <link rel="icon" type="image/png" sizes="1280x1280" href="/images/maskable_icon_x1280.png?v=1.0.2">
    <link rel="icon" type="image/png" sizes="512x512" href="/images//maskable_icon_x512.png?v=1.0.2">
    
    <!-- Apple Touch Icons -->
    <link rel="apple-touch-icon" href="/images/maskable_icon_x1280.png?v=1.0.2">
    <link rel="apple-touch-icon" href="/images/maskable_icon_x512.png?v=1.0.2">

    <!-- For Windows -->
    <meta name="msapplication-TileImage" content="/images/test1.png?v=1.0.2">
    <meta name="msapplication-TileColor" content="#007bff">

    <!-- Schema.org Structured Data for AI Optimization -->
    

    <!-- Article Schema for content pages -->
    
    <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "Article",
      "headline": "Maximum Likelihood Estimation",
      "description": "Learn about point estimators, and maximum likelihood estimation(MLE).",
      "datePublished": "2024-01-01",
      "dateModified": "2025-01-01",
      "author": {
        "@type": "Person",
        "name": "Yusuke Yokota",
        "jobTitle": "Mathematics & Computer Science Educator",
        "knowsAbout": [
          "Linear Algebra",
          "Machine Learning",
          "Computer Science",
          "Mathematics"
        ]
      },
      "publisher": {
        "@type": "Organization",
        "name": "MATH-CS COMPASS",
        "url": "https://math-cs-compass.com",
        "logo": {
          "@type": "ImageObject",
          "url": "https://math-cs-compass.com/images/maskable_icon_x512.png"
        }
      },
      "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "https://math-cs-compass.com/Mathematics/Probability/mle.html"
      },
      "about": [
        
        { "@type": "Thing", "name": "Probability" },
        { "@type": "Thing", "name": "Statistics" }
        
      ],
      "educationalLevel": "university",
      "learningResourceType": "tutorial",
      "educationalUse": "instruction"
    }
    </script>
    
</head>

<body>
    <header>
    <nav class="navbar">
        <!-- Logo as Home Button -->
        <div class="logo-home">
            <a href="../../../index.html" 
               class="logo-link " 
               title="Home - Math-CS Compass">
                <img src="../../images/icon_x512.png" 
                     alt="Math-CS Compass Logo" 
                     class="nav-logo">
            </a>
        </div>

        <div class="search-container">
            <input type="text" id="search-input" placeholder="Search keywords...">
            <button id="search-button" aria-label="Search" title="Search website"><i class="fas fa-search"></i></button>
        </div>
        
        <div class="menu-toggle">
            <i class="fas fa-bars"></i>
        </div>
        
        <ul class="nav-links">
            <li><a href="../Linear_algebra/linear_algebra.html" >I - Linear Algebra to Algebraic Foundations</a></li>
            
            <li><a href="../Calculus/calculus.html" >II - Calculus to Optimization & Analysis</a></li>
            
            <li><a href="../Probability/probability.html" class="active">III - Probability & Statistics</a></li>
            
            <li><a href="../Discrete/discrete_math.html" >IV - Discrete Mathematics & Algorithms</a></li>
            
            <li><a href="../Machine_learning/ml.html" >V - Machine Learning</a></li>
        </ul>
    </nav>
</header>
    
    <!DOCTYPE html>
<html>
    <body>
        <!-- Meta script for mle.html -->
        <script type="application/ld+json">
        {
        "@context": "https://schema.org",
        "@type": "LearningResource",
        "name": "Maximum Likelihood Estimation",
        "description": "Learn about point estimators, and maximum likelihood estimation(MLE).",
        "learningResourceType": "lesson",
        "educationalUse": "instruction",
        "educationalLevel": "university",
        "interactivityType": "active",
        "author": {
            "@type": "Person",
            "name": "Yusuke Yokota",
            "jobTitle": "Mathematics & Computer Science Educator"
        },
        "publisher": {
            "@type": "Organization",
            "name": "MATH-CS COMPASS",
            "url": "https://math-cs-compass.com"
        },
        "about": [
            { "@type": "Thing", "name": "Maximum Likelihood Estimation" },
            { "@type": "Thing", "name": "MLE" },
            { "@type": "Thing", "name": "Point Estimators" },
            { "@type": "Thing", "name": "Likelihood Functions" },
            { "@type": "Thing", "name": "Parameter Estimation" },
            { "@type": "Thing", "name": "Bias" },
            { "@type": "Thing", "name": "Variance" },
            { "@type": "Thing", "name": "Mean Square Error" },
            { "@type": "Thing", "name": "MSE" },
            { "@type": "Thing", "name": "Standard Error" },
            { "@type": "Thing", "name": "Sample Mean" },
            { "@type": "Thing", "name": "Population Parameter" },
            { "@type": "Thing", "name": "Log-Likelihood" },
            { "@type": "Thing", "name": "Optimization" },
            { "@type": "Thing", "name": "Binomial Distribution" },
            { "@type": "Thing", "name": "Normal Distribution" },
            { "@type": "Thing", "name": "Sample Proportion" },
            { "@type": "Thing", "name": "Unbiased Estimator" },
            { "@type": "Thing", "name": "Joint PDF" },
            { "@type": "Thing", "name": "Independent and Identically Distributed" },
            { "@type": "Thing", "name": "IID" }
        ],
        "teaches": [
            "Maximum likelihood estimation theory",
            "Point estimator properties",
            "Parameter estimation techniques",
            "Statistical optimization methods"
        ],
        "isPartOf": {
            "@type": "Course",
            "name": "Probability & Statistics",
            "description": "Explore fundamental concepts of probability and statistics essential for machine learning",
            "provider": {
            "@type": "Organization",
            "name": "MATH-CS COMPASS",
            "url": "https://math-cs-compass.com"
            },
            "instructor": {
            "@type": "Person",
            "name": "Yusuke Yokota",
            "jobTitle": "Mathematics & Computer Science Educator"
            },
            "courseCode": "III",
            "hasCourseInstance": {
            "@type": "CourseInstance",
            "courseMode": "online",
            "courseWorkload": "PT2H30M",
            "instructor": {
                "@type": "Person",
                "name": "Yusuke Yokota"
            }
            },
            "offers": {
            "@type": "Offer",
            "price": "0",
            "priceCurrency": "USD",
            "availability": "https://schema.org/InStock",
            "category": "free"
            }
        }
        }
        </script>
        <div class="hero-section">
            <h1 class="webpage-name">Maximum Likelihood Estimation
            </h1>
        </div>

        
  <div class="quick-jump-container">
    <button type="button" id="quick-jump-toggle" class="quick-jump-toggle">
      <i class="fas fa-list"></i> <strong>Section III Navigation</strong>
    </button>
    <div id="quick-jump-menu" class="quick-jump-menu">
      <h3>Probability & Statistics</h3>
      <div class="quick-jump-links">

         
        <a href="probability.html">‚Üê Back to Section III Overview</a>
        <hr style="margin: 8px 0; border: 1px solid #ddd;">
        
        
        <a href="basic.html" >Part 1: Basic Probability Ideas</a>
        <a href="random_variables.html" >Part 2: Random Variables</a>
        <a href="gamma.html" >Part 3: Gamma & Beta Distribution</a>
        <a href="gaussian.html" >Part 4: Normal (Gaussian) Distribution</a>
        <a href="student.html" >Part 5: Student's t-Distribution</a>
        <a href="covariance.html" >Part 6: Covariance</a>
        <a href="correlation.html" >Part 7: Correlation</a>
        <a href="mvn.html" >Part 8: Multivariate Distributions</a>
        <a href="mle.html" class="current-page">Part 9: Maximum Likelihood Estimation</a>
        <a href="hypothesis_testing.html" >Part 10: Statistical Inference & Hypothesis Testing</a>
        <a href="linear_regression.html" >Part 11: Linear Regression</a>
        <a href="entropy.html" >Part 12: Entropy</a>
        <a href="convergence.html" >Part 13: Convergence</a>
        <a href="bayesian.html" >Part 14: Intro to Bayesian Statistics</a>
        <a href="expfamily.html" >Part 15: The Exponential Family</a>
        <a href="fisher_info.html" >Part 16: Fisher Information Matrix</a>
        <a href="decision_theory.html" >Part 17: Bayesian Decision Theory</a>
        <a href="markov.html" >Part 18: Markov Chains</a>
        <a href="monte_carlo.html" >Part 19: Monte Carlo Methods</a>
        <a href="importance_sampling.html" >Part 20: Importance Sampling</a>
        <a href="gaussian_process.html" >Part 21: Gaussian Processes</a>
      </div>
    </div>
</div>


        <div class="topic-nav">
            <a href="#point">Point Estimators</a>
            <a href="#lf">Likelihood Functions</a>
            <a href="#mle">Maximum Likelihood Estimation</a>
            <a href="#ex1">Example 1: Binomial Distribution \(X \sim b(n, p) \)</a>
            <a href="#ex2">Example 2: Normal Distribution</a>
        </div> 

        <div class="container">  
           
            <section id="point" class="section-content">
            <h2>Point Estimators</h2>
            <p>
            In practice, it is hard or impossible to find a population parameter \(\theta\). Instead, we estimate the unknown parameter by 
            statistic computations from sample data. The estimated parameter from the sample data is called a <strong>point estimator</strong> denoted 
            by \(\hat{\theta}\). 
            <br>For example, to estimate the population mean \(\mu = \theta\), we compute a sample mean
            \[
            \bar{X} = \frac{1}{n}\sum_{i = 1}^n  X_i = \hat{\theta}
            \]
            The point estimator is also a random variable, and then a <strong>function</strong> of sampled random variables \(X_1, X_2, \cdots, X_n\). 
            <br><br>
            Once we obtained the estimator, it is natural that we want to know how much it is close to the true parameter. There are two 
            factors we have to be concerned about:
            <ol style="padding-left: 40px;">
                <li>\(\text{Bias }(\hat{\theta}) = \mathbb{E}(\hat{\theta}) - \theta\)</li>
                The <strong>bias</strong> measures the average accuracy of the estimator. 
                <li>\(\text{Var }(\hat{\theta}) = \mathbb{E}[\hat{\theta} - \mathbb{E}(\hat{\theta})]^2\)</li>
                The <strong>variance</strong> measures the reliability(precision) of the estimator. 
            </ol>
            <br>
            If both bias and variance are low enough, the estimator must be acceptable as an approximation of the population parameter. 
            <div class="theorem">
                <span class="theorem-title">Mean square error(MSE):</span>
                \[
                \begin{align*}
                \text{MSE }(\hat{\theta}) &= \mathbb{E }(\hat{\theta} - \theta)^2 \\\\
                                          &= \text{Var }(\hat{\theta}) + [\text{Bias }(\hat{\theta})]^2
                \end{align*}
                \] 
            </div>
            <div class="proof">
                \[
                \begin{align*}
                \text{MSE }(\hat{\theta}) &= \mathbb{E }(\hat{\theta} - \theta)^2 \\\\
                                          &= \mathbb{E }[\hat{\theta} - \theta + \mathbb{E }(\hat{\theta}) - \mathbb{E }(\hat{\theta})]^2 \\\\
                                          &= \mathbb{E }[\hat{\theta} - \mathbb{E }(\hat{\theta})]^2 
                                             + 2 \mathbb{E }[\hat{\theta} - \mathbb{E }(\hat{\theta})][\mathbb{E }(\hat{\theta})- \theta]
                                             + [\mathbb{E }(\hat{\theta})- \theta]^2 \\\\
                                         &= \mathbb{E }[\hat{\theta} - \mathbb{E }(\hat{\theta})]^2 
                                             + 2 [\mathbb{E }(\hat{\theta}) - \mathbb{E }(\hat{\theta})][\mathbb{E }(\hat{\theta})- \theta]
                                            + [\mathbb{E }(\hat{\theta})- \theta]^2 \\\\   
                                         &=  \text{Var }(\hat{\theta}) + 0 + [\text{Bias }(\hat{\theta})]^2
                \end{align*}
                \]
                Note: <strong>The population parameter is a fixed value.</strong> Thus, 
                \(\mathbb{E }\{[\mathbb{E }(\hat{\theta})- \theta]\} = [\mathbb{E }(\hat{\theta})- \theta] \) because 
                \(\mathbb{E }(constant) = constant\).
            </div>
            The MSE serves as a criterion for comparing estimators, enabling us to identify the most suitable one. Once an estimator 
            is selected, its precision in approximating the population parameter is typically assessed using the <strong>standard error (SE)</strong>, which 
            represents the <strong>standard deviation</strong> of the estimator's sampling distribution.
            <br><br>
            For example, since \(\text{Var }(\bar{X}) = \frac{\sigma^2}{n}\), the standard error of the mean (SEM) is given by
            \[
            \text{SE }(\bar{X}) = \sqrt{\text{Var }(\bar{X})} = \frac{\sigma}{\sqrt{n}}.
            \]
            </p>
            </section>

            <section id="lf" class="section-content">
            <h2>Likelihood Functions</h2>
            <p>
            Suppose observations \(X_1, X_2, \cdots, X_n\) are i.i.d. random variables. The "observed" values of these random variables are 
            denoted by \(x_1, x_2, \cdots, x_n\) respectively. Then the joint p.d.f. or p.m.f. of \(X_1, X_2, \cdots, X_n\) is given by 
            \[
            f(x_1, x_2, \cdots, x_n | \theta) = \prod_{i = 1}^n f(x_i|\theta) 
            \]
            where \(\theta\) is some unknown parameter.
            <br><br>
            we call this <strong>likelihood function</strong> of \(\theta\) for observed  \(x_1, x_2, \cdots, x_n\) and denote it by 
            \(L(\theta | x_1, x_2, \cdots, x_n)\), or simply \(L(\theta)\): 
            \[
            \underbrace{L(\theta | x_1, x_2, \cdots, x_n)}_{\text{After sampling}}  = \underbrace{\prod_{i = 1}^n f(x_i|\theta)}_{\text{Before sampling}}
            \]
            </p>
            </section>

            <section id="mle" class="section-content">
            <h2>Maximum Likelihood Estimation</h2>
            <p>
                In machine learning, model fitting (or training) is the process of estimating unknown 
                parameters \(\pmb{\theta} = (\theta_1, \theta_2, \cdots, \theta_k) \) from sample data 
                \(\mathcal{D} = \{\mathbf{x_1}, \mathbf{x_2}, \cdots, \mathbf{x_n}\}\), which can be represented by an optimization problem of the form 
                \[
                \pmb{\hat{\theta}} = \arg \min_{\pmb{\theta}} \mathcal{L} (\pmb{\theta})
                \]
                where \(\mathcal{L} (\pmb{\theta})\) is a loss function(or objective function).
                <br><br>
                The most common approach for the optimization problem is <strong>maximum likelihood estimation (MLE)</strong>:
                \[
                \pmb{\hat{\theta}_{MLE}} = \arg \max_{(\pmb{\theta})} L(\pmb{\theta})
                \]
                where \(L(\pmb{\theta}) \) is a likelihood function of \(\pmb{\theta}\) for sample data \(\mathcal{D}\).
                If \(L(\pmb{\theta})\) is differentiable function of \(\pmb{\theta}\), then \(\pmb{\hat{\theta}_{MLE}}\) can be calculated by solving the 
                following equation: 
                \[
                \begin{align*}
                &\nabla_{\pmb{\theta}} \ln L(\pmb{\theta}) = \nabla_{\pmb{\theta}} \ln \prod_{i = 1}^n f(\mathbf{x_i}|\pmb{\theta}) = 0 \\\\
                &\Longrightarrow \nabla_{\pmb{\theta}} \ln L(\pmb{\theta}) = \sum_{i=1}^ n \nabla_{\pmb{\theta}} \ln  f(\mathbf{x_i}|\pmb{\theta}) = 0
                \end{align*}
                \]
                Note: In practice, it is efficient to work with the <strong>log-likelihood function</strong> because we can 
                compute it additions instead of multiplications. 
            </p>
            </section>

            <section id="ex1" class="section-content">
                <h2>Example 1: Binomial Distribution \(X \sim b(n, p) \)</h2>
                <p>
                Consider flipping a coin \(n\) times and we got \(k\) Heads. We assume \(P(Head) = \theta\) and 
                \(P(Tail) = (1- \theta)\), where \(\theta \in [0, 1]\). Then 
                \[
                P(\mathcal{D} | \theta) = \theta^k (1-\theta)^{n-k}.
                \]
                To obtain \(\hat{\theta}_{MLE}\), we can solve: 
                \[
                \frac{d}{d\theta}[\ln \theta^k (1-\theta)^{n-k}] = 0
                \]
                \[
                \begin{align*}
                &\Longrightarrow \frac{d}{d\theta}[ k\ln (\theta) + (n-k)\ln (1-\theta)] = 0 \\\\
                &\Longrightarrow \frac{k}{\theta} - \frac{n-k}{1-\theta} = 0 \\\\
                &\Longrightarrow k(1 - \theta) - (n-k)\theta = 0 
                \end{align*}
                \]
                Therefore, 
                \[
                \hat{\theta}_{MLE} = \frac{k}{n}.
                \]
                This is equivalent to  the <strong>sample proportion</strong> \(\hat{p} = \frac{X}{n}\), which is used as the point estimator for 
                the population proportion \(p = \theta\). 
                <br><br>
                Note:
                \[
                \begin{align*}
                &\mathbb{E}[\hat{p}] = \frac{1}{n}\mathbb{E}[X] = \frac{1}{n}np = p \\\\
                &\text{Var }(\hat{p}) = \frac{1}{n^2}\text{Var }[X] = \frac{1}{n^2}np(1-p) = \frac{p(1-p)}{n}
                \end{align*}
                \]
                </p>
            </section>

            <section id="ex2" class="section-content">
                <h2>Example 2: Normal Distribution</h2>
                <p>
                Suppose \(\mathcal{D} = \{x_1, x_2, \cdots, x_n \}\) is from a normal distribution with p.d.f 
                \[
                f(x | \mu, \sigma^2) = \frac{1}{\sigma \sqrt{2\pi}}\exp \Big\{- \frac{(x - \mu)^2}{2\sigma^2}\Big\}.
                \]
                And its likelihood fuction is given by
                \[
                \begin{align*}
                L(\mu, \sigma^2) &= \prod_{i=1}^n [ f(x_i | \mu, \sigma^2)] \\\\
                                &= \Big(\frac{1}{\sigma \sqrt{2\pi}}\Big)^n \exp \Big\{-\frac{1}{2\sigma^2} \sum_{i=1}^n (x_i - \mu)^2 \Big\}
                \end{align*}
                \]
                The log-likelihood function is given by 
                \[
                \begin{align*}
                \ln L(\mu, \sigma^2) &= n \ln \Big(\frac{1}{\sigma \sqrt{2\pi}}\Big) -\frac{1}{2\sigma^2} \sum_{i=1}^n (x_i - \mu)^2 \\\\
                                    &= -n \ln (\sigma) - n \ln (\sqrt{2\pi})  -\frac{1}{2\sigma^2} \sum_{i=1}^n (x_i - \mu)^2  \\\\
                                    &= -\frac{n}{2} \ln (\sigma^2) - \frac{n}{2}\ln (2\pi)  -\frac{1}{2\sigma^2} \sum_{i=1}^n (x_i - \mu)^2 \tag{1} \\\\ 
                \end{align*}
                \]
                Setting the partial derivative of (1) with respect to \(\mu\) equal to zero: 
                \[
                \begin{align*}
                &\frac{\partial \ln L(\mu, \sigma^2) }{\partial \mu} = \frac{1}{\sigma^2} \sum_{i=1}^n (x_i - \mu) = 0 \\\\
                &\Longrightarrow \sum_{i=1}^n (x_i) - n\mu = 0
                \end{align*}
                \]
                Thus, 
                \[
                \hat{\mu}_{MLE} = \frac{1}{n} \sum_{i=1}^n x_i = \bar{x} \tag{2}
                \]
                Similarly, setting the partial derivative of (1) with respect to \(\sigma^2\) equal to zero and substituting (2) in the equation:
                \[
                \begin{align*}
                &\frac{\partial \ln L(\mu, \sigma^2) }{\partial \sigma^2} = -\frac{n}{2\sigma^2}+ \frac{1}{2\sigma^4}\sum_{i=1}^n (x_i - \bar{x})^2  = 0 \\\\
                &\Longrightarrow -n \sigma^2 + \sum_{i=1}^n (x_i - \bar{x})^2 = 0
                \end{align*}
                \]
                Thus, 
                \[
                \hat{\sigma^2}_{MLE} = \frac{1}{n}\sum_{i=1}^n (x_i - \bar{x})^2
                \]
                Recall that the variance is biased, so we use \(s^2 = \frac{1}{n-1} \sum_{i=1}^n (x_i - \bar{x})^2\) for the sample variance.
                </p>
            </section>
        </div>
        <script src="/js/main.js"></script>
    </body>
</html>
    
    <footer>
    <div class="footer-content">
        <div class="footer-about">
            <h3>About MATH-CS COMPASS</h3>
            <p>Bridging the gap between pure mathematics and computer science applications.</p>
            <div class="contact-section">
                <strong><a href="https://docs.google.com/forms/d/e/1FAIpQLSf3_RNkegREvfCEIQznn6SCaf-iB5c-Doxn7Ymuu78Uf-PBjg/viewform" target="_blank">
                    Submit Your Questions & Feedback <i class="fas fa-external-link-alt"></i>
                </a></strong>
            </div>
        </div>
        <div class="footer-links">
            <h3>Quick Links</h3>
            <ul>
                
                <li><a href="../../../index.html">Home</a></li>
                <li>
                <a href="../Linear_algebra/linear_algebra.html">
                    <span class="nav-number">I</span>
                    <span>Linear Algebra to Algebraic Foundations</span>
                </a>
                </li>
                <li>
                <a href="../Calculus/calculus.html">
                    <span class="nav-number">II</span>
                    <span>Calculus to Optimization & Analysis</span>
                </a>
                </li>
                <li>
                <a href="../Probability/probability.html">
                    <span class="nav-number">III</span>
                    <span>Probability & Statistics</span>
                </a>
                </li>
                <li>
                <a href="../Discrete/discrete_math.html">
                    <span class="nav-number">IV</span>
                    <span>Discrete Mathematics & Algorithms</span>
                </a>
                </li>
                <li>
                <a href="../Machine_learning/ml.html">
                    <span class="nav-number">V</span>
                    <span>Machine Learning</span>
                </a>
                </li>
            </ul>
        </div>
        <div class="footer-social">
            <h3>Connect With Me</h3>
            <p>
                <a href="https://x.com/MathCSCompass?t=Zi8nyfzszeFw8QizbJwnNg&s=09" target="_blank">
                    <i class="fab fa-x-twitter"></i> X (Twitter)
                </a>
            </p>
            <p>
                <a href="https://www.facebook.com/share/15uPaBWgQN/" target="_blank">
                    <i class="fab fa-facebook"></i> Facebook
                </a>
            </p>
      
            <h3>MATH-CS COMPASS App</h3>
            <div class="app-download-footer">
                <a href="https://play.google.com/store/apps/details?id=io.github.yusukey01.twa" target="_blank" rel="noopener">
                    <img src="../../../images/google-play-badge.png" 
                        alt="Get it on Google Play" 
                        class="google-play-badge">
                </a>
            </div>
            
        </div>

    </div>
    <div class="footer-bottom">
        <p>&copy; 2024-2025 MATH-CS COMPASS. All rights reserved.</p>
    </div>
</footer>
    <script src="../../../js/main.js?v=1.0.2"></script>
    <script src="../../../js/search.js?v=1.0.2"></script>
    

    <button id="go-to-top-btn" class="go-to-top-btn" title="Go to top">
        <i class="fas fa-arrow-up"></i>
    </button>
    <script>
        // Register the service worker
        if ('serviceWorker' in navigator) {
          window.addEventListener('load', () => {
            navigator.serviceWorker.register('/js/service-worker.js')
              .then(registration => {
                console.log('ServiceWorker registration successful with scope: ', registration.scope);
              })
              .catch(error => {
                console.log('ServiceWorker registration failed: ', error);
              });
          });
        }
      </script>
</body>
</html>