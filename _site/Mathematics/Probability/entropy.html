<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    
    <title>Entropy - MATH-CS COMPASS</title>
    
    
    <script>
    window.MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']]
      }
    };
    </script>
    <script id="MathJax-script" defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="preload" href="https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Main-Regular.woff" as="font" type="font/woff2" crossorigin>
    <link rel="preload" href="https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Math-Italic.woff" as="font" type="font/woff2" crossorigin>
    
    
    

    <link rel="stylesheet" href="../../../css/styles.css?v=1.0.2">
  
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css">

    <link rel="dns-prefetch" href="https://cdn.jsdelivr.net">
    <link rel="dns-prefetch" href="https://cdnjs.cloudflare.com">
    
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Learn about entropy in information theory including KL divergence and mutual information.">
    <meta name="google-site-verification" content="IieEujypYW38caEVe8ymjczouNl56yxAy27iPztcfRA" />
    

    <link rel="manifest" href="/manifest.json">

    <!-- Theme color for Chrome, Android, and other browsers -->
    <meta name="theme-color" content="#007bff">
  
    <!-- iOS specific tags -->
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <meta name="apple-mobile-web-app-title" content="MATH-CS COMPASS">
    <meta name="mobile-web-app-capable" content="yes">

    <!-- Icon references -->
    <link rel="icon" type="image/png" sizes="1280x1280" href="/images/maskable_icon_x1280.png?v=1.0.2">
    <link rel="icon" type="image/png" sizes="512x512" href="/images//maskable_icon_x512.png?v=1.0.2">
    
    <!-- Apple Touch Icons -->
    <link rel="apple-touch-icon" href="/images/maskable_icon_x1280.png?v=1.0.2">
    <link rel="apple-touch-icon" href="/images/maskable_icon_x512.png?v=1.0.2">

    <!-- For Windows -->
    <meta name="msapplication-TileImage" content="/images/test1.png?v=1.0.2">
    <meta name="msapplication-TileColor" content="#007bff">

    <!-- Schema.org Structured Data for AI Optimization -->
    

    <!-- Article Schema for content pages -->
    
    <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "Article",
      "headline": "Entropy",
      "description": "Learn about entropy in information theory including KL divergence and mutual information.",
      "datePublished": "2024-01-01",
      "dateModified": "2025-01-01",
      "author": {
        "@type": "Person",
        "name": "Yusuke Yokota",
        "jobTitle": "Mathematics & Computer Science Educator",
        "knowsAbout": [
          "Linear Algebra",
          "Machine Learning",
          "Computer Science",
          "Mathematics"
        ]
      },
      "publisher": {
        "@type": "Organization",
        "name": "MATH-CS COMPASS",
        "url": "https://yusukey01.github.io",
        "logo": {
          "@type": "ImageObject",
          "url": "https://yusukey01.github.io/images/maskable_icon_x512.png"
        }
      },
      "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "https://yusukey01.github.io/Mathematics/Probability/entropy.html"
      },
      "about": [
        
        { "@type": "Thing", "name": "Probability" },
        { "@type": "Thing", "name": "Statistics" }
        
      ],
      "educationalLevel": "university",
      "learningResourceType": "tutorial",
      "educationalUse": "instruction"
    }
    </script>
    
</head>

<body>
    <header>
    <nav class="navbar">
        <!-- Logo as Home Button -->
        <div class="logo-home">
            <a href="../../../index.html" 
               class="logo-link " 
               title="Home - Math-CS Compass">
                <img src="../../images/icon_x512.png" 
                     alt="Math-CS Compass Logo" 
                     class="nav-logo">
            </a>
        </div>

        <div class="search-container">
            <input type="text" id="search-input" placeholder="Search keywords...">
            <button id="search-button" aria-label="Search" title="Search website"><i class="fas fa-search"></i></button>
        </div>
        
        <div class="menu-toggle">
            <i class="fas fa-bars"></i>
        </div>
        
        <ul class="nav-links">
            <li><a href="../Linear_algebra/linear_algebra.html" >I - Linear Algebra to Algebraic Foundations</a></li>
            
            <li><a href="../Calculus/calculus.html" >II - Calculus to Optimization & Analysis</a></li>
            
            <li><a href="../Probability/probability.html" class="active">III - Probability & Statistics</a></li>
            
            <li><a href="../Discrete/discrete_math.html" >IV - Discrete Mathematics & Algorithms</a></li>
            
            <li><a href="../Machine_learning/ml.html" >V - Machine Learning</a></li>
        </ul>
    </nav>
</header>
    
    <!DOCTYPE html>
<html>
    <body>
        <!-- Meta script for entropy.html -->
        <script type="application/ld+json">
        {
        "@context": "https://schema.org",
        "@type": "LearningResource",
        "name": "Entropy",
        "description": "Learn about entropy in information theory including KL divergence and mutual information.",
        "learningResourceType": "lesson",
        "educationalUse": "instruction",
        "educationalLevel": "university",
        "interactivityType": "active",
        "author": {
            "@type": "Person",
            "name": "Yusuke Yokota",
            "jobTitle": "Mathematics & Computer Science Educator"
        },
        "publisher": {
            "@type": "Organization",
            "name": "MATH-CS COMPASS",
            "url": "https://yusukey01.github.io"
        },
        "about": [
            { "@type": "Thing", "name": "Entropy" },
            { "@type": "Thing", "name": "Information Theory" },
            { "@type": "Thing", "name": "Joint Entropy" },
            { "@type": "Thing", "name": "Conditional Entropy" },
            { "@type": "Thing", "name": "Cross Entropy" },
            { "@type": "Thing", "name": "KL Divergence" },
            { "@type": "Thing", "name": "Kullback-Leibler Divergence" },
            { "@type": "Thing", "name": "Relative Entropy" },
            { "@type": "Thing", "name": "Information Gain" },
            { "@type": "Thing", "name": "Mutual Information" },
            { "@type": "Thing", "name": "Self-Information" },
            { "@type": "Thing", "name": "Cross-Entropy Loss" },
            { "@type": "Thing", "name": "Gibbs Inequality" },
            { "@type": "Thing", "name": "Log Sum Inequality" },
            { "@type": "Thing", "name": "Jensen's Inequality" },
            { "@type": "Thing", "name": "Chain Rule of Entropy" }
        ],
        "teaches": [
            "Information theory fundamentals",
            "Entropy calculations and applications",
            "KL divergence and mutual information",
            "Cross-entropy in machine learning"
        ],
        "isPartOf": {
            "@type": "Course",
            "name": "Probability & Statistics",
            "description": "Explore fundamental concepts of probability and statistics essential for machine learning",
            "provider": {
            "@type": "Organization",
            "name": "MATH-CS COMPASS",
            "url": "https://yusukey01.github.io"
            },
            "instructor": {
            "@type": "Person",
            "name": "Yusuke Yokota",
            "jobTitle": "Mathematics & Computer Science Educator"
            },
            "courseCode": "III",
            "hasCourseInstance": {
            "@type": "CourseInstance",
            "courseMode": "online",
            "courseWorkload": "PT2H30M",
            "instructor": {
                "@type": "Person",
                "name": "Yusuke Yokota"
            }
            },
            "offers": {
            "@type": "Offer",
            "price": "0",
            "priceCurrency": "USD",
            "availability": "https://schema.org/InStock",
            "category": "free"
            }
        }
        }
        </script>
        <div class="hero-section">
            <h1 class="webpage-name">Entropy
            </h1>
        </div>

        
  <div class="quick-jump-container">
    <button type="button" id="quick-jump-toggle" class="quick-jump-toggle">
      <i class="fas fa-list"></i> <strong>Section III Navigation</strong>
    </button>
    <div id="quick-jump-menu" class="quick-jump-menu">
      <h3>Probability & Statistics</h3>
      <div class="quick-jump-links">

         
        <a href="probability.html">‚Üê Back to Section III Overview</a>
        <hr style="margin: 8px 0; border: 1px solid #ddd;">
        
        
        <a href="basic.html" >Part 1: Basic Probability Ideas</a>
        <a href="random_variables.html" >Part 2: Random Variables</a>
        <a href="gamma.html" >Part 3: Gamma & Beta Distribution</a>
        <a href="gaussian.html" >Part 4: Normal (Gaussian) Distribution</a>
        <a href="student.html" >Part 5: Student's t-Distribution</a>
        <a href="covariance.html" >Part 6: Covariance</a>
        <a href="correlation.html" >Part 7: Correlation</a>
        <a href="mvn.html" >Part 8: Multivariate Distributions</a>
        <a href="mle.html" >Part 9: Maximum Likelihood Estimation</a>
        <a href="hypothesis_testing.html" >Part 10: Statistical Inference & Hypothesis Testing</a>
        <a href="linear_regression.html" >Part 11: Linear Regression</a>
        <a href="entropy.html" class="current-page">Part 12: Entropy</a>
        <a href="convergence.html" >Part 13: Convergence</a>
        <a href="bayesian.html" >Part 14: Intro to Bayesian Statistics</a>
        <a href="expfamily.html" >Part 15: The Exponential Family</a>
        <a href="fisher_info.html" >Part 16: Fisher Information Matrix</a>
        <a href="decision_theory.html" >Part 17: Bayesian Decision Theory</a>
        <a href="markov.html" >Part 18: Markov Chains</a>
        <a href="monte_carlo.html" >Part 19: Monte Carlo Methods</a>
        <a href="importance_sampling.html" >Part 20: Importance Sampling</a>
        <a href="gaussian_process.html" >Part 21: Gaussian Processes</a>
      </div>
    </div>
</div>


        <div class="topic-nav">
            <a href="#entropy">Entropy</a>
            <a href="#joint">Joint Entropy</a>
            <a href="#conditional">Conditional Entropy</a>
            <a href="#cross">Cross Entropy</a>
            <a href="#kl">KL Divergence (Relative Entropy, Information Gain)</a>
            <a href="#mi">Mutual Information (MI)</a>     
        </div> 

        <div class="container">  
           
            <section id="entropy" class="section-content">
            <h2>Entropy</h2>
            <p>
            Consider you play a number guessing game. You have to guess a natural number between 1 and 100. If I tell you that the number is not 1, then 
            you still have 99 possible choices. So, this information is not valuable. In this case, we can say that the <strong>information content</strong> 
            of this clue is low. However, if I tell you that the number is divisible by 11, the number of choices is reduced to 9 only and thus the information 
            content of this clue is much higher. 
            <br><br>
            This example illustrates that information content (or <strong>self-information</strong>) measures the "uncertainty" associated with an 
            event. The lower the probability of an event \(p(E)\), the higher its information content, \(I(E)\). So, we can define the information content as follows:
            \[
            I(E) = - \log p(E) \geq 0.
            \]
            Note: If two events \(A\) and \(B\) are independent, the information content is <strong>additive</strong>:  
            \[
            \begin{align*}
            I(A, B) &= - \log(p(A)\cdot p(B)) \\\\
                    &= - (\log p(A) + \log p(B)) \\\\
                    &= I(A) + I(B).
            \end{align*}
            \]
            This is the main reason why we use the logarithm. We can convert the multiplication of probabilities into the 
            addition of information contents. 
            <br><br>
            We extend this concept by quantifying the "expected value" of the information content across all possible outcomes of a 
            discrete random variable \(X\).
            <br>
            The <strong>entropy</strong> of a discrete random variable \(X\) with distribution \(p\) over 
            \(n\) states is defined as:
            \[
            \begin{align*}
            \mathbb{H}(X) &= - \sum_{k=1}^n p(X = k) \log_2 p(X = k) \\\\
                          &= \mathbb{E }_X [- \log_2 p(X)]
            \end{align*}
            \]
            Note: The choice of logarithmic base determines the units of entropy. In this case, the units is <strong>bits</strong>. 
            Also, often we use the natural logarithm (base \(e\))with the units <strong>nats</strong>.
            </p>
            </section>

            <section id="joint" class="section-content">
            <h2>Joint Entropy</h2>
            <p>
            The <strong>joint entropy</strong> between two random variables \(X\) and \(Y\) is defined as
            \[
            \mathbb{H}(X, Y) = - \sum_{x, y} p(x, y) \log p(x, y).
            \]
            If \(X\) and \(Y\) are independent, \(\mathbb{H}(X, Y) =  \mathbb{H}(X) + \mathbb{H}(Y)\). Otherwise, the sum is 
            larger than \(\mathbb{H}(X, Y)\). Also, if \(Y\) is a deterministic function of \(X\), then \(\mathbb{H}(X, Y) = \mathbb{H}(X)\). Thus, 
            \[
            \mathbb{H}(X) + \mathbb{H}(Y) \geq \mathbb{H}(X, Y) \geq \max\{\mathbb{H}(X), \mathbb{H}(Y)\} \geq 0.
            \]
            This is true for more than two random variables. 
            </p>
            </section>

            <section id="conditional" class="section-content">
            <h2>Conditional Entropy</h2>
            <p>
            The <strong>conditional entropy</strong> of \(Y\) given \(X\) is the uncertainty we have in \(Y\) after 
            seeing \(X\), averaged over possible values for \(X\):
            \[
            \mathbb{H}(Y | X) = \mathbb{E }_{p(X)}[ \mathbb{H}(p(Y | X))]
            \]
            <div class="proof">
                <span class="proof-title"> \(\mathbb{H}(Y | X)  = \mathbb{H}(X, Y) -  \mathbb{H}(X)\):</span>
                \[
                \begin{align*}
                \mathbb{H}(Y | X) &=  \mathbb{E }_{p(X)}[ \mathbb{H}(p(Y | X))] \\\\
                                  &=  \sum_{x} p(x) \mathbb{H}(p(Y | X = x)) \\\\
                                  &= - \sum_{x} p(x) \sum_{y} p(y | x) \log p(y | x) \\\\\
                                  &= - \sum_{x, y} p(x, y) \log p(y | x) \\\\
                                  &= - \sum_{x, y} p(x, y) \log \frac{p(x,y)}{p(x)}\\\\
                                  &= - \sum_{x, y} p(x, y) \log p(x,y) + \sum_{x} p(x) \log p(x) \\\\
                                  &= \mathbb{H}(X, Y) -  \mathbb{H}(X) \tag{1}
                \end{align*}
                \]
            </div>
            Note: If \(Y\) can be completely determined by \(X\), \(\mathbb{H}(Y | X)  = 0\). 
            Also, if both \(X\) and \(Y\) are independent each other,  \(\mathbb{H}(Y | X)  =  \mathbb{H}(Y) \).
            <br><br>
            By (1), \(\mathbb{H}(X, Y) = \mathbb{H}(X) + \mathbb{H}(Y | X) \). This implies the <strong>chain rule</strong> of entropy: 
            \[
            \mathbb{H}(X_1, X_2, \cdots, X_n) = \sum_{i =1}^n \mathbb{H}(X_i |X_1, \cdots,  X_{i-1}).
            \]
            </p>
            </section>

            <section id="cross" class="section-content">
            <h2>Cross Entropy</h2>
            <p>
            The <strong>cross entropy</strong> of a distribution \(q\) relative to a distribution \(p\) is defined as
            \[
            \mathbb{H}(p, q) = - \sum_{k=1}^n p_k \log  q_k.
            \]
            In machine learning, especially in the context of  <a href="../Machine_learning/intro_classification.html"><strong>classification</strong></a> problems and 
            <a href="../Machine_learning/neural_networks.html"><strong>neural networks</strong></a>, often we use the 
            cross entropy as a loss function. It measures the performance of a model whose output is a probability distribution.
            Or, it helps in training models by penalizing the difference between the true labels and the predicted probabilities.
            Generally, the smaller the cross entropy loss, the better the predictions are. 
            <br><br>
            For example, consider a <strong>binary classification</strong>(n =2). The true label \(y \in \{0, 1\}\) is represented by 
            a distribution \(p = [y, 1-y]\), and the predicted probability \(\hat{y} \in (0, 1)\) is represented by Bernoulli 
            distribution \(q = [\hat{y}, 1 - \hat{y}]\). Then we obtain a <strong>cross-entropy loss function</strong>:
            \[
            \begin{align*}
            \mathcal{L}(y, \hat{y}) &= - [y \log (\hat{y}) + (1 - y) \log (1 - \hat{y})] \\\\
                                    &= - y \log (\hat{y}) - (1 - y) \log (1 - \hat{y}) \\
            \end{align*}
            \]
            Let's say the true label is 1, and then the loss function becomes \(- \log (\hat{y})\). 
            <br>
            If \(\hat{y}\) is close to 1 (the model is confident and correct), the penalty becomes small. On the other hand, 
            if \(\hat{y}\) is close to 0 (the model is confident and wrong), the penalty becomes large. So, the higher penalty(loss) 
            indicates the model need to be improved and forces the optimization algorithm to adjust parameters for reducing future error. 
            </p>
            </section>

            <section id="kl" class="section-content">
            <h2>KL Divergence (Relative Entropy, Information Gain)</h2>
            <p>
            It is important to define a distance metric to measure "how similar" two distributions \(p\) and \(q\) are. However, 
            instead, we can define a <strong>divergence measure</strong> \(D(p,q)\), which only requires \(D(p, q) \geq 0\) with 
            equality if and only if \(p = q\). Here, we introduce the <strong>Kullback-Leibler divergence</strong> between two 
            distributions \(p\) and \(q\): 
            \[
            \begin{align*}
            D_{\mathbb{KL}}(p \| q) &= \sum_{k=1}^n p_k \log \frac{p_k}{q_k} \\\\
                                   &= \sum_{k=1}^n p_k \log p_k - \sum_{k=1}^n p_k \log q_k \\\\
                                   &= - \mathbb{H}(p) + \mathbb{H}(p, q) \tag{2}
            \end{align*}
            \]
            where \(p\) and \(q\) are defined on the same sample space \(\mathcal{X}\).
            <br>
            For example, \(D_{\mathbb{KL}}(p \| q)\) can measure how much an estimated distribution \(q\) is different from 
            a true distribution \(p\). If the estimation of \(p\) by \(q\) is "good," the KL divergence will be close to zero. 
            <br><br>
            By the expression (2), the cross entropy can be written as:
            \[
            \mathbb{H}(p, q) = \mathbb{H}(p) +  D_{\mathbb{KL}}(p \| q).
            \]
            So, if \(p(x)\) is fixed, minimizing the cross entropy is equivalent to minimizing KL divergence. 
            <br><br>
            <div class="theorem">
                <span class="theorem-title">Theorem 1: Non-negativity of KL Divergence</span>
                \[
                D_{\mathbb{KL}}(p \| q)  \geq 0
                \]
                where \(p\) and \(q\) are discrete distributions defined on the same sample space \(\mathcal{X}\) and with 
                equality if and only if \(p = q\).
                <br>
                Or, the entropy of \(p\) is less than or equal to its cross entropy with any other distribution \(q\):
                \[
                \begin{align*}
                &\sum_{k=1}^n p_k \log p_k - \sum_{k=1}^n p_k \log q_k \geq 0 \\\\
                &\Longrightarrow  - \sum_{k=1}^n p_k \log p_k \leq  - \sum_{k=1}^n p_k \log q_k.
                \end{align*}
                \]
                This is called <strong>Gibbs' inequality</strong>.     
            </div>
            There are many ways to prove this theorem, here we introduce <strong>log sum inequality</strong> to prove Theorem 1.
            <div class="theorem">
                <span class="theorem-title">Theorem 2: Log Sum Inequality</span>
                For nonnegative numbers \(a_1, a_2, \cdots, a_n\) and \(b_1, b_2, \cdots, b_n\), 
                \[
                \sum_{k=1}^n a_k \log \frac{a_k}{b_k} \geq \Big(\sum_{k=1}^n a_k \Big) \log \frac{\sum_{k=1}^n a_k}{\sum_{k=1}^n b_k} \tag{3}
                \]
                with equality if and only if \(\frac{a_k}{b_k} = constant\).
            </div>

            <div class="proof">
                <span class="proof-title">Proof of Theorem 2:</span>
                Assume without loss of generality that \(a_k >0\) and \(b_k >0\). Consider a function \(f(x) = x \log x\). Since 
                \(f'(x) = \log x + 1\) and  \(f''(x) = \frac{1}{x}\), the function is a convex function for all \(x > 0\).
                <br>
                Let \(\lambda_i = \frac{b_k}{\sum_{k=1}^n b_k}\), then the left side of inequality (3) becomes
                \[
                \begin{align*}
                \sum_{k=1}^n a_k \log \frac{a_k}{b_k}
                &= \sum_{k=1}^n b_k \frac{a_k}{b_k} \log \frac{a_k}{b_k} \\\\
                &= \sum_{k=1}^n b_k \frac{\sum_{k=1}^n b_k}{\sum_{k=1}^n b_k}f(\frac{a_k}{b_k}) \\\\
                &= \Big(\sum_{k=1}^n b_k \Big) \sum_{k=1}^n \lambda_k f(\frac{a_k}{b_k})\\\\
                \end{align*}
                \]
                Here, by <strong>Jensen's inequality</strong>,
                \[
                \begin{align*}
                \Big(\sum_{k=1}^n b_k \Big) \sum_{k=1}^n \lambda_k f(\frac{a_k}{b_k})
                                                     &\geq \Big(\sum_{k=1}^n b_k \Big) f\Big(\sum_{k=1}^n \lambda_k \frac{a_k}{b_k} \Big) \\\\
                                                      &= \Big(\sum_{k=1}^n b_k \Big) f\Big( \frac{\sum_{k=1}^n a_k}{\sum_{k=1}^n b_k} \Big) \\\\
                                                      &=  \Big(\sum_{k=1}^n a_k \Big) \log \frac{\sum_{k=1}^n a_k}{\sum_{k=1}^n b_k}
                \end{align*}
                \]
                Therefore, 
                \[
                \sum_{k=1}^n a_k \log \frac{a_k}{b_k} \geq \Big(\sum_{k=1}^n a_k \Big) \log \frac{\sum_{k=1}^n a_k}{\sum_{k=1}^n b_k}. 
                \]
            </div>
            <div class="theorem">
                <span class="theorem-title">Theorem 3: Jensen's inequality</span>
                If \(\phi\) is <strong>convex</strong> on an open interval \(I\), and \(X\) is a random variable whose <strong>support</strong> is 
                contained in \(I\), and has a finite expected value, then
                \[
                \phi (\mathbb{E }[X]) \leq \mathbb{E }[\phi (X)].
                \]
                Note: The support of a discrete random variable \(X\) to be the points in the space of \(X\) which has 
                positive probability.
                <br><br>
                Note: In the above proof, we used the following alternative finite form:
                \[
                f\Big(\sum_{k=1}^n \lambda_k x_k \Big) \leq \sum_{k=1}^n \lambda_k f(x_k) 
                \]
                where \(\lambda_k \geq 0\) and \(\sum_{k=1}^n \lambda_k = 1\).
            </div>
            Finally, we can easily prove non-negativity of KL divergence using log sum inequality. 
            <div class="proof">
                <span class="proof-title">Proof of Theorem 1:</span>
                Suppose \(\sum_{k=1}^n p_k = \sum_{k=1}^n q_k = 1\) and \(p_k, \, q_k \geq 0\).
                By the definition of LK divergence, 
                \[
                D_{\mathbb{KL}}(p \| q) = \sum_{k=1}^n p_k \log \frac{p_k}{q_k}.
                \]
                Using Log sum inequality with \(a_k = p_k\) and \(b_k = q_k\), 
                \[
                \sum_{k=1}^n p_k \log \frac{p_k}{q_k} \geq \Big(\sum_{k=1}^n p_k \Big) \log \frac{\sum_{k=1}^n p_k}{\sum_{k=1}^n q_k}.
                \]
                Since \(\sum_{k=1}^n p_k = \sum_{k=1}^n q_k = 1\), 
                \[
                \sum_{k=1}^n p_k \log \frac{p_k}{q_k} \geq \Big(\sum_{k=1}^n p_k \Big) \log 1 = 0
                \]
                with equality if and only if \(\forall k, \, p_k = q_k\).
            </div>
            <br>
            Again, KL divergence is NOT a metric. KL divergence is not symmetric and does not satisfy the triangle inequality.
            </p>
            </section>

            <section id="mi" class="section-content">
            <h2>Mutual Information (MI)</h2>
            <p>
            To measure mutual dependency of two random variables, we measure similarity of their distributions. 
            The <strong>mutual information</strong> between two random variable \(X\) and \(Y\) is defined as:
            \[
            \begin{align*}
            \mathbb{I }(X ; Y) &=  D_{\mathbb{KL}}(p(x,y) \| p(x)p(y)) \\\\
                               &= \sum_{y \in Y} \sum_{x \in X} p(x, y) \log \frac{p(x,y)}{p(x)p(y)} \geq 0.
            \end{align*}
            \]
            Or, using entropies: 
            \[
            \begin{align*}
            \mathbb{I }(X ; Y)  &= \sum_{y \in Y} \sum_{x \in X} p(x, y) \log p(x,y) 
                                   - \sum_{y \in Y} \sum_{x \in X} p(x, y) \log p(x) - \sum_{y \in Y} \sum_{x \in X} p(x, y) \log p(y)  \\\\
                                &= \mathbb{H}(X) + \mathbb{H}(Y) -  \mathbb{H}(X, Y)
            \end{align*}
            \]
            Using the chain rule for entropy: \(\mathbb{H}(X, Y) = \mathbb{H}(Y| X) + \mathbb{H}(X) \,\),
            we obtain following expressions:
            \[
            \begin{align*}
            \mathbb{I }(X ; Y) &= \mathbb{H}(X) + \mathbb{H}(Y) -  \mathbb{H}(X, Y) \\\\
                               &= \mathbb{H}(Y) -  \mathbb{H}(Y | X) \\\\
                               &= \mathbb{H}(X) -  \mathbb{H}(X | Y).
            \end{align*}
            \]
            <br>
            \[
            \begin{align*}
            \mathbb{I }(X ; Y) &= \mathbb{H}(X) - \mathbb{H}(X | Y) \\\\
                               &= \mathbb{H}(X, Y) -  \mathbb{H}(X | Y) -  \mathbb{H}(Y | X).
            \end{align*}
            \]
            </p>
            </section>
        </div>   
        <script src="/js/main.js"></script>   
    </body>
</html>
    
    <footer>
    <div class="footer-content">
        <div class="footer-about">
            <h3>About MATH-CS COMPASS</h3>
            <p>Bridging the gap between pure mathematics and computer science applications.</p>
            <div class="contact-section">
                <strong><a href="https://docs.google.com/forms/d/e/1FAIpQLSf3_RNkegREvfCEIQznn6SCaf-iB5c-Doxn7Ymuu78Uf-PBjg/viewform" target="_blank">
                    Submit Your Questions & Feedback <i class="fas fa-external-link-alt"></i>
                </a></strong>
            </div>
        </div>
        <div class="footer-links">
            <h3>Quick Links</h3>
            <ul>
                
                <li><a href="../../../index.html">Home</a></li>
                <li>
                <a href="../Linear_algebra/linear_algebra.html">
                    <span class="nav-number">I</span>
                    <span>Linear Algebra to Algebraic Foundations</span>
                </a>
                </li>
                <li>
                <a href="../Calculus/calculus.html">
                    <span class="nav-number">II</span>
                    <span>Calculus to Optimization & Analysis</span>
                </a>
                </li>
                <li>
                <a href="../Probability/probability.html">
                    <span class="nav-number">III</span>
                    <span>Probability & Statistics</span>
                </a>
                </li>
                <li>
                <a href="../Discrete/discrete_math.html">
                    <span class="nav-number">IV</span>
                    <span>Discrete Mathematics & Algorithms</span>
                </a>
                </li>
                <li>
                <a href="../Machine_learning/ml.html">
                    <span class="nav-number">V</span>
                    <span>Machine Learning</span>
                </a>
                </li>
            </ul>
        </div>
        <div class="footer-social">
            <h3>Connect With Me</h3>
            <p>
                <a href="https://x.com/MathCSCompass?t=Zi8nyfzszeFw8QizbJwnNg&s=09" target="_blank">
                    <i class="fab fa-x-twitter"></i> X (Twitter)
                </a>
            </p>
            <p>
                <a href="https://www.facebook.com/share/15uPaBWgQN/" target="_blank">
                    <i class="fab fa-facebook"></i> Facebook
                </a>
            </p>
      
            <h3>MATH-CS COMPASS App</h3>
            <div class="app-download-footer">
                <a href="https://play.google.com/store/apps/details?id=io.github.yusukey01.twa" target="_blank" rel="noopener">
                    <img src="../../../images/google-play-badge.png" 
                        alt="Get it on Google Play" 
                        class="google-play-badge">
                </a>
            </div>
            
        </div>

    </div>
    <div class="footer-bottom">
        <p>&copy; 2024-2025 MATH-CS COMPASS. All rights reserved.</p>
    </div>
</footer>
    <script src="../../../js/main.js?v=1.0.2"></script>
    <script src="../../../js/search.js?v=1.0.2"></script>
    

    <button id="go-to-top-btn" class="go-to-top-btn" title="Go to top">
        <i class="fas fa-arrow-up"></i>
    </button>
    <script>
        // Register the service worker
        if ('serviceWorker' in navigator) {
          window.addEventListener('load', () => {
            navigator.serviceWorker.register('/js/service-worker.js')
              .then(registration => {
                console.log('ServiceWorker registration successful with scope: ', registration.scope);
              })
              .catch(error => {
                console.log('ServiceWorker registration failed: ', error);
              });
          });
        }
      </script>
</body>
</html>