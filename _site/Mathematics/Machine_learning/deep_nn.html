<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    
    <title>Intro to Deep Neural Networks - MATH-CS COMPASS</title>
    
    
    <script>
    window.MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']]
      }
    };
    </script>
    <script id="MathJax-script" defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="preload" href="https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Main-Regular.woff" as="font" type="font/woff2" crossorigin>
    <link rel="preload" href="https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Math-Italic.woff" as="font" type="font/woff2" crossorigin>
    
    
    

    <link rel="stylesheet" href="../../../css/styles.css?v=1.0.2">
  
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css">

    <link rel="dns-prefetch" href="https://cdn.jsdelivr.net">
    <link rel="dns-prefetch" href="https://cdnjs.cloudflare.com">
    
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Learn about deep neural networks basics via Attention mechanisms and Transformer architecture.">
    <meta name="google-site-verification" content="IieEujypYW38caEVe8ymjczouNl56yxAy27iPztcfRA" />
    

    <link rel="manifest" href="/manifest.json">

    <!-- Theme color for Chrome, Android, and other browsers -->
    <meta name="theme-color" content="#007bff">
  
    <!-- iOS specific tags -->
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <meta name="apple-mobile-web-app-title" content="MATH-CS COMPASS">
    <meta name="mobile-web-app-capable" content="yes">

    <!-- Icon references -->
    <link rel="icon" type="image/png" sizes="1280x1280" href="/images/maskable_icon_x1280.png?v=1.0.2">
    <link rel="icon" type="image/png" sizes="512x512" href="/images//maskable_icon_x512.png?v=1.0.2">
    
    <!-- Apple Touch Icons -->
    <link rel="apple-touch-icon" href="/images/maskable_icon_x1280.png?v=1.0.2">
    <link rel="apple-touch-icon" href="/images/maskable_icon_x512.png?v=1.0.2">

    <!-- For Windows -->
    <meta name="msapplication-TileImage" content="/images/test1.png?v=1.0.2">
    <meta name="msapplication-TileColor" content="#007bff">

    <!-- Schema.org Structured Data for AI Optimization -->
    

    <!-- Article Schema for content pages -->
    
    <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "Article",
      "headline": "Intro to Deep Neural Networks",
      "description": "Learn about deep neural networks basics via Attention mechanisms and Transformer architecture.",
      "datePublished": "2024-01-01",
      "dateModified": "2025-01-01",
      "author": {
        "@type": "Person",
        "name": "Yusuke Yokota",
        "jobTitle": "Mathematics & Computer Science Educator",
        "knowsAbout": [
          "Linear Algebra",
          "Machine Learning",
          "Computer Science",
          "Mathematics"
        ]
      },
      "publisher": {
        "@type": "Organization",
        "name": "MATH-CS COMPASS",
        "url": "https://math-cs-compass.com",
        "logo": {
          "@type": "ImageObject",
          "url": "https://math-cs-compass.com/images/maskable_icon_x512.png"
        }
      },
      "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "https://math-cs-compass.com/Mathematics/Machine_learning/deep_nn.html"
      },
      "about": [
        
        { "@type": "Thing", "name": "Machine Learning" },
        { "@type": "Thing", "name": "Artificial Intelligence" }
        
      ],
      "educationalLevel": "university",
      "learningResourceType": "tutorial",
      "educationalUse": "instruction"
    }
    </script>
    
</head>

<body>
    <header>
    <nav class="navbar">
        <!-- Logo as Home Button -->
        <div class="logo-home">
            <a href="../../../index.html" 
               class="logo-link " 
               title="Home - Math-CS Compass">
                <img src="../../images/icon_x512.png" 
                     alt="Math-CS Compass Logo" 
                     class="nav-logo">
            </a>
        </div>

        <div class="search-container">
            <input type="text" id="search-input" placeholder="Search keywords...">
            <button id="search-button" aria-label="Search" title="Search website"><i class="fas fa-search"></i></button>
        </div>
        
        <div class="menu-toggle">
            <i class="fas fa-bars"></i>
        </div>
        
        <ul class="nav-links">
            <li><a href="../Linear_algebra/linear_algebra.html" >I - Linear Algebra to Algebraic Foundations</a></li>
            
            <li><a href="../Calculus/calculus.html" >II - Calculus to Optimization & Analysis</a></li>
            
            <li><a href="../Probability/probability.html" >III - Probability & Statistics</a></li>
            
            <li><a href="../Discrete/discrete_math.html" >IV - Discrete Mathematics & Algorithms</a></li>
            
            <li><a href="../Machine_learning/ml.html" class="active">V - Machine Learning</a></li>
        </ul>
    </nav>
</header>
    
    <!DOCTYPE html>
<html>
    <body> 
        <!-- LearningResource Schema for Deep Neural Networks -->
        <script type="application/ld+json">
        {
        "@context": "https://schema.org",
        "@type": "LearningResource",
        "name": "Intro to Deep Neural Networks",
        "description": "Learn about deep neural networks basics via Attention mechanisms and Transformer architecture",
        "learningResourceType": "lesson",
        "educationalUse": "instruction",
        "educationalLevel": "university",
        "interactivityType": "active",
        "author": {
            "@type": "Person",
            "name": "Yusuke Yokota",
            "jobTitle": "Mathematics & Computer Science Educator",
            "knowsAbout": [
            "Deep Learning",
            "Neural Networks",
            "Transformer Architecture",
            "Attention Mechanisms",
            "Machine Learning"
            ]
        },
        "publisher": {
            "@type": "Organization",
            "name": "MATH-CS COMPASS",
            "url": "https://math-cs-compass.com"
        },
        "about": [
            { "@type": "Thing", "name": "Deep Neural Networks" },
            { "@type": "Thing", "name": "Convolutional Neural Networks" },
            { "@type": "Thing", "name": "Transformer Architecture" },
            { "@type": "Thing", "name": "Attention Mechanisms" },
            { "@type": "Thing", "name": "Self-Attention" },
            { "@type": "Thing", "name": "Multi-Head Attention" },
            { "@type": "Thing", "name": "Positional Encoding" },
            { "@type": "Thing", "name": "ResNet" },
            { "@type": "Thing", "name": "Layer Normalization" }
        ],
        "teaches": [
            "Convolutional Neural Networks (CNNs)",
            "Residual connections and ResNet architecture",
            "Layer normalization techniques",
            "Attention mechanisms and self-attention",
            "Multi-head attention implementation",
            "Positional encoding for sequence models",
            "Transformer architecture components"
        ],
        "isPartOf": {
            "@type": "Course",
            "name": "Machine Learning",
            "description": "Explore machine learning ideas and applications with mathematical foundations",
            "provider": {
            "@type": "Organization",
            "name": "MATH-CS COMPASS",
            "url": "https://math-cs-compass.com"
            },
            "instructor": {
            "@type": "Person",
            "name": "Yusuke Yokota",
            "jobTitle": "Mathematics & Computer Science Educator"
            },
            "courseCode": "V",
            "hasCourseInstance": {
            "@type": "CourseInstance",
            "courseMode": "online",
            "courseWorkload": "PT5H",
            "instructor": {
                "@type": "Person",
                "name": "Yusuke Yokota"
            }
            },
            "offers": {
            "@type": "Offer",
            "price": "0",
            "priceCurrency": "USD",
            "availability": "https://schema.org/InStock",
            "category": "free"
            }
        }
        }
        </script>

        <!-- WebApplication Schema for Interactive Demo -->
        <script type="application/ld+json">
        {
        "@context": "https://schema.org",
        "@type": "WebApplication",
        "name": "Deep Neural Networks Interactive Demo",
        "description": "Interactive demonstration of deep neural network concepts with real-time transformer visualization",
        "applicationCategory": "EducationalApplication",
        "operatingSystem": "Web Browser",
        "url": "https://math-cs-compass.com/Mathematics/Machine_learning/deep_nn.html",
        "author": {
            "@type": "Person",
            "name": "Yusuke Yokota"
        },
        "applicationSubCategory": "Deep Learning Visualization",
        "featureList": [
            "Interactive transformer architecture visualization",
            "Attention mechanism demonstration",
            "Multi-head attention visualization",
            "Positional encoding illustration",
            "Real-time neural network exploration"
        ],
        "isAccessibleForFree": true,
        "educationalUse": "instruction",
        "educationalLevel": "university",
        "about": [
            { "@type": "Thing", "name": "Deep Learning Visualization" },
            { "@type": "Thing", "name": "Transformer Demo" },
            { "@type": "Thing", "name": "Attention Mechanism Demo" }
        ]
        }
        </script>
        <div class="hero-section">
            <h1 class="webpage-name">Intro to Deep Neural Networks</h1>
        </div>

        
  <div class="quick-jump-container">
    <button type="button" id="quick-jump-toggle" class="quick-jump-toggle">
      <i class="fas fa-list"></i> <strong>Section V Navigation</strong>
    </button>
    <div id="quick-jump-menu" class="quick-jump-menu">
      <h3>Machine Learning</h3>
      <div class="quick-jump-links">

        
        <a href="ml.html">← Back to Section V Overview</a>
        <hr style="margin: 8px 0; border: 1px solid #ddd;">
        
        
        <a href="intro_ml.html" >Part 1: Intro to Machine Learning</a>
        <a href="regression.html" >Part 2: Regularized Regression</a>
        <a href="intro_classification.html" >Part 3: Intro to Classification</a>
        <a href="neural_networks.html" >Part 4: Neural Networks Basics</a>
        <a href="autodiff.html" >Part 5: Automatic Differentiation</a>
        <a href="svm.html" >Part 6: Support Vector Machine (SVM)</a>
        <a href="pca.html" >Part 7: Principal Component Analysis (PCA) & Autoencoders</a>
        <a href="clustering.html" >Part 8: Clustering</a>
        <a href="deep_nn.html" class="current-page">Part 9: Intro to Deep Neural Networks</a>
        <a href="intro_RL.html" >Part 10: Intro to Reinforcement Learning</a>
        
      </div>
    </div>
</div>


        <div class="topic-nav">
            <a href="#CNN">Convolutional Neural Networks (CNNs)</a>
            <a href="#ResNet">ResNet: Residual Connections</a>
            <a href="#normalization">Layer Normalization</a>
            <a href="#attention">Attention Mechanisms</a>
            <a href="#self">Self-Attention</a>
            <a href="#multihead">Multi-Head Attention</a>
            <a href="#position">Positional Encoding</a>
            <a href="#transformer">Transformers</a>
            <a href="#demo">Demo</a>
        </div>

        <div class="container">  

            <section id="CNN" class="section-content">
                <h2>Convolutional Neural Networks (CNNs)</h2>
                <p>
                    <strong>Deep neural networks (DNNs)</strong> have undergone remarkable evolution over the past few decades. From early image 
                    classifiers to today's cutting-edge language and multimodal models, the field has progressed through a series of foundational innovations. 
                    Broadly speaking, modern DNNs are typically <strong>feedforward networks</strong>, where information flows in a single direction — from input 
                    to output — without any loops or recurrence. This design simplifies computations, making them faster and more stable.
                </p>


                <p>
                    In the 1990s, <strong>Convolutional Neural Networks (CNNs)</strong> emerged as an architecture for processing grid-like data such as images. 
                    A CNN uses <em>convolutional layers</em> that apply learnable filters across local spatial regions. Mathematically, a 2D convolution operation is given by:
                    \[
                    (f * x)(i, j) = \sum_m \sum_n f(m, n) \cdot x(i - m, j - n)
                    \]
                    where \(f\) is the filter (or kernel) and \(x\) is the input image. This operation captures local spatial patterns while sharing parameters across the image.
                </p>

                <p>
                    The breakthrough came in 2012, when <strong>AlexNet</strong> won the ImageNet competition. It stacked convolution and pooling layers, used the ReLU nonlinearity \( \text{ReLU}(x) = \max(0, x) \), 
                    and applied dropout to reduce overfitting. AlexNet demonstrated that deep CNNs trained on GPUs could vastly outperform traditional vision pipelines.
                </p>

                <p>
                    This success led to the deeper architecture, <strong>ResNet</strong>(2015), which introduced <strong>residual connections</strong>.
                    Such a model advanced image recognition and set the stage for modern vision architectures.
                </p>
            </section>

            <section id="ResNet" class="section-content">
                <h2>ResNet: Residual Connections</h2>
                <p>
                    <strong>ResNet</strong> is a feedforward architecture in which each layer uses a <strong>residual block</strong> of the form:
                    \[
                    \mathcal{F}_{\ell}' (\mathbf{x}) = \mathcal{F}_{\ell} (\mathbf{x}) + \mathbf{x}
                    \]
                    where \(\mathcal{F}_{\ell}\) is a nonlinear transformation at layer \(\ell\), and the input \(\mathbf{x}\) is added back directly.
                </p>

                <p>
                    The activations at the final layer \(L\) can be expressed recursively in terms of an earlier layer \(\ell\):
                    \[
                    \mathbf{z}_L = \mathbf{z}_{\ell} + \sum_{i = \ell}^{L - 1} \mathcal{F}_i (\mathbf{z}_i; \mathbf{\theta}_i),
                    \]
                    where \(\mathbf{\theta}_i\) denotes the parameters of the transformation at layer \(i\).
                </p>

                <p>
                    This formulation mitigates the <strong>vanishing gradient problem</strong> in deep neural networks, enabling the 
                    successful training of very deep models.
                </p>
                <p>
                    Applying the chain rule, the gradient of the loss \(\mathcal{L}\) with respect to the parameters at layer \(\ell\) becomes:
                    \[
                    \begin{align*}
                    \frac{\partial \mathcal{L}}{\partial \mathbf{\theta}_{\ell}} 
                        &= \frac{\partial \mathbf{z}_{\ell}}{\partial \mathbf{\theta}_{\ell}} \frac{\partial \mathcal{L}}{\partial \mathbf{z}_{\ell}} \\\\
                        &= \frac{\partial \mathbf{z}_{\ell}}{\partial \mathbf{\theta}_{\ell}} \frac{\partial \mathcal{L}}{\partial \mathbf{z}_{L}} \frac{\partial \mathbf{z}_L}{\partial \mathbf{z}_{\ell}} \\\\
                        &= \frac{\partial \mathbf{z}_{\ell}}{\partial \mathbf{\theta}_{\ell}} \frac{\partial \mathcal{L}}{\partial \mathbf{z}_{L}} 
                                \left(1 + \sum_{i = \ell}^{L-1} \frac{\partial \mathcal{F}_i (\mathbf{z}_i ; \mathbf{\theta}_i)}{\partial \mathbf{z}_{\ell}}\right) \\\\
                        &= \frac{\partial \mathbf{z}_{\ell}}{\partial \mathbf{\theta}_{\ell}} \frac{\partial \mathcal{L}}{\partial \mathbf{z}_{L}} + \text{other terms}
                    \end{align*}
                    \]
                    The <em>“other terms”</em> in the gradient capture indirect paths through deeper residual blocks and reflect how updates propagate backward through the network.
                    These terms may diminish in magnitude in very deep networks, but they are generally not zero and play a critical role in learning refined representations.
                    Residual networks avoid the vanishing gradient problem not by eliminating these terms, but by ensuring they are <strong>not the only path</strong> for gradient flow.
                </p>
            </section>

            <section id="normalization" class="section-content">
                <h2>Layer Normalization</h2>
                <p>
                    Let \(z_i\) be the \(i\)'th element of a tensor. For example, in 2d images, the index \(i\) has four components, 
                    indicating batch, height, width, and channel:
                    \[
                    i = (i_N, i_H, i_W, i_C).
                    \]
                    For each index \(z_i\), the mean and variance are computed by pooling statistics across other dimensions of the tensor 
                    but not across examples in the batch:
                    \[
                    \begin{align*}
                    &\mu_i = \frac{1}{\| \mathcal{S}_i\|} \sum_{k \in \mathcal{S}_i} z_k \\\\
                    &\sigma_i = \sqrt{\frac{1}{\| \mathcal{S}_i\|}\sum_{k \in \mathcal{S}_i} (z_k - \mu_i)^2 + \epsilon}
                    \end{align*}
                    \]
                    where \(\mathcal{S}_i\) is the set of elements over which statistics are computed for index \(i\), 
                    and \(\epsilon\) is a small constant for numerical stability. 
                </p>
                <p>
                    The normalized and scaled output is given by:
                    \[
                    \begin{align*}
                    &\hat{z}_i = \frac{(z_i - \mu_i)}{\sigma_i} \\\\
                    &\tilde{z_i} = \gamma_c \hat{z}_i + \beta_c
                    \end{align*}
                    \]
                    where \(c\) is the channel index corresponding to \(i\), and \(\gamma_c, \beta_c\) are learnable 
                    parameters for scaling and shifting.
                </p>
                <p>
                    Unlike <strong>batch normalization</strong>, which pools across the batch dimension, <strong>layer normalization</strong> pools only 
                    over feature dimensions for each input example. This makes it especially effective in settings where batch-level statistics are 
                    unstable or unavailable — such as in autoregressive models, small batch sizes, or variable-length sequences.
                </p>
            </section>


            <section id="attention" class="section-content">
                <h2>Attention Mechanisms</h2>
                <p>
                    Recurrent architectures process sequences step-by-step, which limits parallelism and makes modeling 
                    long-range dependencies challenging. For example, imagine reading a sentence and trying to understand 
                    what each word refers to. When processing the word "it," you automatically look back at previous words 
                    to determine its meaning. <strong>Attention mechanisms</strong> formalize this intuitive process by computing 
                    a weighted sum of all input feature vectors, enabling the model to dynamically focus on the most relevant 
                    parts of the sequence regardless of distance.
                </p>

                <p>
                    In traditional feedforward neural networks, each layer computes hidden activations as a linear transformation of input activations followed by a nonlinearity:
                    \[
                    \mathbf{Z} = \varphi (\mathbf{X}\mathbf{W}) \in \mathbb{R}^{m \times v'}
                    \]
                    where \(\mathbf{X} \in \mathbb{R}^{m \times v}\) is a matrix of input (or hidden) feature vectors, and 
                    \(\mathbf{W} \in \mathbb{R}^{v \times v'}\) is a fixed weight matrix learned during training.
                </p>

                <p>
                    This formulation uses the same set of weights \(\mathbf{W}\) for every input position. To allow more flexibility, 
                    we can instead make the weights input-dependent. That is, we can allow:
                    \[
                    \mathbf{Z} = \varphi (\mathbf{X}\mathbf{W}(\mathbf{X})),
                    \]
                    where the transformation matrix \(\mathbf{W}(\mathbf{X})\) varies depending on the input itself.
                    This allows the model to dynamically adjust its computations based on the input context.
                    Such input-dependent multiplicative interactions are central to <strong>attention mechanisms</strong>.
                </p>

                <p>
                    In more general settings, the input-dependent transformation matrix can be computed from separate sets of learned representations called 
                    queries, keys, and values. This leads to a broader formulation of attention:
                    \[
                    \mathbf{Z} = \varphi (\mathbf{V}\mathbf{W}(\mathbf{Q}, \mathbf{K}))
                    \]
                    where:
                    <ul style="padding-left: 40px;">
                        <li>\(\mathbf{Q} \in \mathbb{R}^{m \times q}\) is a set of <strong>queries</strong></li>
                        <li>\(\mathbf{K} \in \mathbb{R}^{m \times q}\) is a set of <strong>keys</strong></li>
                        <li>\(\mathbf{V} \in \mathbb{R}^{m \times v}\) is a set of <strong>values</strong></li>
                    </ul>
                    <br>
                    The transformation matrix \(\mathbf{W}(\mathbf{Q}, \mathbf{K})\) represents a learned or computed relationship between queries and keys.
                    Note: In practice, these matrices are often obtained by applying learned linear projections to the input sequence.
                </p>

                
                 <p>
                    In attention, the core idea is to compute each output vector \(\mathbf{z}_j\) as a weighted sum over all value vectors \(\mathbf{v}_i\), 
                    where the weights \(\alpha_{ij}\) are determined by a similarity between the query \(\mathbf{q}_j\) and each key \(\mathbf{k}_i\):
                    \[
                    \mathbf{z}_j = \sum_{i}\alpha_{ij}\mathbf{v}_i
                    \]
                    where \(0 \leq \alpha_{ij} \leq 1\) and \(\sum_i \alpha_{ij} = 1\). This allows the model to selectively attend to relevant parts of the input.
                </p>
                
                <p>
                    A common similarity function is the inner product between query and key vectors. If 
                    \(\mathbf{q}, \mathbf{k} \in \mathbb{R}^d\) are independent random variables with components that have zero mean and unit variance, 
                    then their inner product \(\mathbf{q}^\top \mathbf{k}\) has zero mean and variance \(d\). 
                    To prevent large variance from pushing the softmax into saturation regions where gradients vanish, we normalize it by \(\sqrt{d}\):
                    \[
                    a(\mathbf{q}, \mathbf{k}) = \frac{\mathbf{q}^\top \mathbf{k}}{\sqrt{d}}.
                    \]
                    This is known as <strong>scaled dot-product attention score</strong>, where the scale factor 
                    \(\frac{1}{\sqrt{d}}\) normalizes the variance so that it remains stable(i.e., \(= 1\)) as the dimension \(d\) increases.  
                    This is used to compute <strong>attention weights</strong> \(\alpha_{ij}\) after applying the softmax function.
                </p>
               
                <p>
                    In practice, we compute attention over minibatches of \(n\) query vectors. Let:
                    \[
                    \mathbf{Q} \in \mathbb{R}^{n \times d}, \quad \mathbf{K} \in \mathbb{R}^{m \times d}, \quad \mathbf{V} \in \mathbb{R}^{m \times v}
                    \]
                    Then the attention-weighted output is computed as:
                    \[
                    \text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{softmax}\left(\frac{\mathbf{Q} \mathbf{K}^\top}{\sqrt{d}}\right) \mathbf{V} \in \mathbb{R}^{n \times v}
                    \]
                    where the softmax is applied row-wise to normalize each row of the \(n \times m\) similarity matrix.
                </p>

           </section>

            <section id="self" class="section-content">  
            <h2>Self-Attention</h2>  
            <p>  
                <strong>Self-attention</strong> is a mechanism that allows each position \(i\) in a sequence to attend to all other positions 
                in the same sequence. Given a sequence of input tokens \(\mathbf{x}_1, \cdots, \mathbf{x}_n \in \mathbb{R}^d\),
                a sequence of outputs is given by:
                \[
                \mathbf{y}_i = \text{Attention}(\mathbf{q}_i, (\mathbf{x}_1, \mathbf{x}_1), \cdots, (\mathbf{x}_n, \mathbf{x}_n )) \in \mathbb{R}^d
                \]
                where the query is \(q_i\) and the keys and values are \(\mathbf{x}_1, \cdots \mathbf{x}_n\).
            </p>
            <p>
                This mechanism captures contextual relationships between tokens by allowing information to flow from any position to any other in a single layer.
                During training, all the outputs are known. Thus the model can evaluate \(\mathbf{y}_i\) <strong>in parallel</strong>, which 
                overcomes the sequential bottleneck of past DNNs. 
            </p>

            </section>  

            <section id="multihead" class="section-content">
                <h2>Multi-Head Attention</h2>
                <p>
                    While a single self-attention mechanism can capture dependencies between tokens, using only one set of projections 
                    may limit the model's expressiveness. <strong>Multi-head attention (MHA)</strong> addresses this by computing 
                    multiple self-attention operations in parallel using different learned projections, called "heads." 
                </p>

                <p>
                    Formally, given an input matrix \( X \in \mathbb{R}^{n \times d} \), each attention head applies its own learned projections to compute queries, keys, and values:
                    \[
                    \text{head}_i = \text{Attention}\left(\mathbf{W}_i^{(q)} \mathbf{q}, \{\mathbf{W}_i^{(k)} \mathbf{k}_j, \mathbf{W}_i^{(v)}\mathbf{v}_j\}\right) \in \mathbb{R}^{p_v},
                    \]
                    where \( \mathbf{W}_i^{(q)}, \mathbf{W}_i^{(k)}, \mathbf{W}_i^{(v)} \in \mathbb{R}^{p_v \times d} \) are projection matrices specific to head \( i \).
                </p>

                <p>
                    All heads are computed in parallel and their outputs are concatenated and linearly projected to produce the final output:
                    \[
                    \text{MHA}(\mathbf{X}) = \text{MHA}\left(\mathbf{q}, \{\mathbf{k}_j, \mathbf{v}_j\}\right) 
                        = \mathbf{W}_o  \begin{bmatrix} 
                                    \text{head}_1 \\
                                    \vdots \\
                                    \text{head}_h\\
                                \end{bmatrix}.
                    \]
                    where \( \mathbf{W}_o \in \mathbb{R}^{d \times hp_v} \) is a learned output projection matrix.
                </p>

                <p>
                    By using multiple heads, the model can attend to information from different representation subspaces and capture a richer set of relationships in the data. 
                    Each head may focus on different positions or interaction patterns, improving both performance and interpretability.
                </p>
            </section>

            
            <section id="position" class="section-content"> 
                <h2>Positional Encoding</h2> 
                <p> 
                    The standard self-attention is permutation invariant, which means the model ignores the order of tokens in an 
                    input sequence. To address this, <strong>positional encoding</strong> is added to the input embeddings to inject information about 
                    the position of each token. A widely used method is to define fixed, deterministic positional encodings using 
                    sine and cosine functions of varying frequencies. 
                </p> 

                <p> 
                    The positional encoding vector for position \( i \) and dimension \( j \) is defined as follows: 
                    \[ 
                    p_{i, 2j} = \sin \left( \frac{i}{C^{2j/d}} \right) 
                    \]
                    \[ 
                    p_{i, 2j+1} = \cos \left( \frac{i}{C^{2j/d}} \right) 
                    \] 
                    where: 
                    <ul style="padding-left: 40px;"> 
                        <li>\( i \) is the position index in the sequence, starting from 0,</li> 
                        <li>\( j \) is the dimension index in the positional encoding vector,</li> 
                        <li>\( d \) is the total dimension of the model (e.g., 512),</li> 
                        <li>\( C \) is a constant (usually set to 10,000) to control the frequency scale.</li> 
                    </ul> 
                </p>

                 <p> 
                    This formulation ensures that each dimension of the positional encoding corresponds to a sinusoid of a 
                    different frequency. As a result, any two positions will have a unique positional encoding vector, and the 
                    relative position between tokens can be represented as a linear function of the encodings. 
                </p> 

                <p> 
                    <strong>Example:</strong> Suppose the model dimension is \( d = 4 \), and we compute the positional encoding for 
                    position \( i = 2 \). Then we compute each component of the encoding as:
                    \[ 
                    \begin{align*}
                    &p_{2,0} = \sin \left( \frac{2}{10,000^{0/4}} \right) = \sin(2) \\\\      
                    &p_{2,1} = \cos \left( \frac{2}{10,000^{0/4}} \right) = \cos(2) \\\\
                    &p_{2,2} = \sin \left( \frac{2}{10,000^{2/4}} \right) = \sin \left( \frac{2}{100} \right) \\\\
                    &p_{2,3} = \cos \left( \frac{2}{10,000^{2/4}} \right) = \cos \left( \frac{2}{100} \right) 
                    \end{align*}
                    \] 
                    </p> 

                    <p> 
                        This results in the positional encoding vector for position 2 as: 
                        \[ 
                        \mathbf{p}_2 = \left[ \sin(2),\ \cos(2),\ \sin(0.02),\ \cos(0.02) \right].
                        \]
                    </p> 

                     <p> These encodings are added element-wise to the token embeddings at each position, allowing the model to 
                        incorporate position information without relying on sequence-aware architectures like CNNs. 
                        Moreover, the use of periodic functions enables the model to potentially generalize to sequences longer 
                        than those seen during training. 
                    </p> 
                </section>

            <section id="transformer" class="section-content">
                <h2>Transformers</h2>


                <p> 
                    The <strong>Transformer</strong> architecture, introduced by Vaswani et al. (2017), replaces recurrence and convolution with a mechanism based entirely on self-attention. 
                    In both the encoder and decoder, the core components are multi-head self-attention layers, feedforward networks, residual connections, and layer normalization. 
                    The design enables full parallelization across sequence positions during training, and allows the model to capture long-range dependencies through direct pairwise 
                    interactions between tokens. By using learned attention weights that vary depending on the input, the model dynamically determines which parts of the sequence to emphasize 
                    at each layer.
                </p>

                <p>
                    Transformer-based models form the basis of many state-of-the-art systems across domains such as text, vision, and multimodal learning. 
                    In particular, <strong>large language models (LLMs)</strong> such as the <strong>GPT</strong> family, developed by OpenAI, are built on 
                    stacked Transformer decoder blocks trained on massive text corpora. These models leverage the scalability and expressiveness of the 
                    Transformer to generate coherent and context-aware output across a wide range of tasks.
                </p>

                <div class="pseudocode">
                    <span class="pseudocode-title">Transformer Encoder & Decoder</span>
                    <strong>EncoderBlock(X)</strong>
                    &emsp;&emsp;Z = LayerNorm(MHA(Q =X, K = X, V = X) + X)
                    &emsp;&emsp;E = LayerNorm(FeedForward(Z) + Z)
                    &emsp;&emsp;<strong>return</strong> E
                    <br>
                    <strong>Encoder(X, N)</strong>
                    &emsp;&emsp;E = POS(Embed(X))
                    &emsp;&emsp;for \(n\) in range (N)
                    &emsp;&emsp;&emsp;&emsp; E = EncoderBlock(E)
                    &emsp;&emsp;<strong>return</strong> E
                    <br>
                    <strong>DecoderBlock(Y, E)</strong>
                    &emsp;&emsp;Z = LayerNorm(MHA(Q =Y, K = Y, V = Y) + Y)
                    &emsp;&emsp;Z' = LayerNorm(MHA(Q = Z, K = E, V = E) + Z)
                    &emsp;&emsp;D = LayerNorm(FeedForward(Z') + Z')
                    &emsp;&emsp;<strong>return</strong> D
                    <br>
                    <strong>Decoder(Y, E, N)</strong>
                    &emsp;&emsp;D = POS(Embed(Y))
                    &emsp;&emsp;for \(n\) in range (N)
                    &emsp;&emsp;&emsp;&emsp; D = DecoderBlock(D, E)
                    &emsp;&emsp;<strong>return</strong> D
                </div>
                <p>
                    Note:
                    <ul style="padding-left: 40px;">
                        <li><strong>MHA(...)</strong> : the Multi-head self-attention. Each token attends to every other token.</li>
                        <li><strong>+ (...)</strong> : residual connection. The input is added back to the output of attention. </li>
                        <li><strong>LayerNorm(...)</strong> : Layer normalization, which is applied after the residual connection to stabilize training.</li>
                        <li><strong>FeedForward(...)</strong> : A position-wise fully connected feedforward network is applied to each token vector independently.</li>
                        <li><strong>Embed(...)</strong> : Each token is mapped to a high-dimensional embedding vector.</li>
                        <li><strong>POS(...)</strong> : Positional encoding is added to embeddings to inject token order information.</li>
                        <li><strong>N</strong>: the number of copies of the block</li>
                    </ul>
                </p>
            </section>

            <section id="demo" class="section-content">
                <h2>Demo</h2>
                 <div id="transformer_demo"></div>
            </section>


        </div>
        <script src="/js/main.js"></script>  
        <script src="/js/deep_nn.js"></script>  
    </body>
</html>
    
    <footer>
    <div class="footer-content">
        <div class="footer-about">
            <h3>About MATH-CS COMPASS</h3>
            <p>Bridging the gap between pure mathematics and computer science applications.</p>
            <div class="contact-section">
                <strong><a href="https://docs.google.com/forms/d/e/1FAIpQLSf3_RNkegREvfCEIQznn6SCaf-iB5c-Doxn7Ymuu78Uf-PBjg/viewform" target="_blank">
                    Submit Your Questions & Feedback <i class="fas fa-external-link-alt"></i>
                </a></strong>
            </div>
        </div>
        <div class="footer-links">
            <h3>Quick Links</h3>
            <ul>
                
                <li><a href="../../../index.html">Home</a></li>
                <li>
                <a href="../Linear_algebra/linear_algebra.html">
                    <span class="nav-number">I</span>
                    <span>Linear Algebra to Algebraic Foundations</span>
                </a>
                </li>
                <li>
                <a href="../Calculus/calculus.html">
                    <span class="nav-number">II</span>
                    <span>Calculus to Optimization & Analysis</span>
                </a>
                </li>
                <li>
                <a href="../Probability/probability.html">
                    <span class="nav-number">III</span>
                    <span>Probability & Statistics</span>
                </a>
                </li>
                <li>
                <a href="../Discrete/discrete_math.html">
                    <span class="nav-number">IV</span>
                    <span>Discrete Mathematics & Algorithms</span>
                </a>
                </li>
                <li>
                <a href="../Machine_learning/ml.html">
                    <span class="nav-number">V</span>
                    <span>Machine Learning</span>
                </a>
                </li>
            </ul>
        </div>
        <div class="footer-social">
            <h3>Connect With Me</h3>
            <p>
                <a href="https://x.com/MathCSCompass?t=Zi8nyfzszeFw8QizbJwnNg&s=09" target="_blank">
                    <i class="fab fa-x-twitter"></i> X (Twitter)
                </a>
            </p>
            <p>
                <a href="https://www.facebook.com/share/15uPaBWgQN/" target="_blank">
                    <i class="fab fa-facebook"></i> Facebook
                </a>
            </p>
      
            <h3>MATH-CS COMPASS App</h3>
            <div class="app-download-footer">
                <a href="https://play.google.com/store/apps/details?id=io.github.yusukey01.twa" target="_blank" rel="noopener">
                    <img src="../../../images/google-play-badge.png" 
                        alt="Get it on Google Play" 
                        class="google-play-badge">
                </a>
            </div>
            
        </div>

    </div>
    <div class="footer-bottom">
        <p>&copy; 2024-2025 MATH-CS COMPASS. All rights reserved.</p>
    </div>
</footer>
    <script src="../../../js/main.js?v=1.0.2"></script>
    <script src="../../../js/search.js?v=1.0.2"></script>
    

    <button id="go-to-top-btn" class="go-to-top-btn" title="Go to top">
        <i class="fas fa-arrow-up"></i>
    </button>
    <script>
        // Register the service worker
        if ('serviceWorker' in navigator) {
          window.addEventListener('load', () => {
            navigator.serviceWorker.register('/js/service-worker.js')
              .then(registration => {
                console.log('ServiceWorker registration successful with scope: ', registration.scope);
              })
              .catch(error => {
                console.log('ServiceWorker registration failed: ', error);
              });
          });
        }
      </script>
</body>
</html>