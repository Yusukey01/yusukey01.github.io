<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    
    <title>Support Vector Machine (SVM) - MATH-CS COMPASS</title>
    
    
    <script>
    window.MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']]
      }
    };
    </script>
    <script id="MathJax-script" defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="preload" href="https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Main-Regular.woff" as="font" type="font/woff2" crossorigin>
    <link rel="preload" href="https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Math-Italic.woff" as="font" type="font/woff2" crossorigin>
    
    
    

    <link rel="stylesheet" href="../../../css/styles.css?v=1.0.2">
  
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css">

    <link rel="dns-prefetch" href="https://cdn.jsdelivr.net">
    <link rel="dns-prefetch" href="https://cdnjs.cloudflare.com">
    
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Learn about the support vector machine (SVM) basics.">
    <meta name="google-site-verification" content="IieEujypYW38caEVe8ymjczouNl56yxAy27iPztcfRA" />
    

    <link rel="manifest" href="/manifest.json">

    <!-- Theme color for Chrome, Android, and other browsers -->
    <meta name="theme-color" content="#007bff">
  
    <!-- iOS specific tags -->
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <meta name="apple-mobile-web-app-title" content="MATH-CS COMPASS">
    <meta name="mobile-web-app-capable" content="yes">

    <!-- Icon references -->
    <link rel="icon" type="image/png" sizes="1280x1280" href="/images/maskable_icon_x1280.png?v=1.0.2">
    <link rel="icon" type="image/png" sizes="512x512" href="/images//maskable_icon_x512.png?v=1.0.2">
    
    <!-- Apple Touch Icons -->
    <link rel="apple-touch-icon" href="/images/maskable_icon_x1280.png?v=1.0.2">
    <link rel="apple-touch-icon" href="/images/maskable_icon_x512.png?v=1.0.2">

    <!-- For Windows -->
    <meta name="msapplication-TileImage" content="/images/test1.png?v=1.0.2">
    <meta name="msapplication-TileColor" content="#007bff">

    <!-- Schema.org Structured Data for AI Optimization -->
    

    <!-- Article Schema for content pages -->
    
    <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "Article",
      "headline": "Support Vector Machine (SVM)",
      "description": "Learn about the support vector machine (SVM) basics.",
      "datePublished": "2024-01-01",
      "dateModified": "2025-01-01",
      "author": {
        "@type": "Person",
        "name": "Yusuke Yokota",
        "jobTitle": "Mathematics & Computer Science Educator",
        "knowsAbout": [
          "Linear Algebra",
          "Machine Learning",
          "Computer Science",
          "Mathematics"
        ]
      },
      "publisher": {
        "@type": "Organization",
        "name": "MATH-CS COMPASS",
        "url": "https://yusukey01.github.io",
        "logo": {
          "@type": "ImageObject",
          "url": "https://yusukey01.github.io/images/maskable_icon_x512.png"
        }
      },
      "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "https://yusukey01.github.io/Mathematics/Machine_learning/svm.html"
      },
      "about": [
        
        { "@type": "Thing", "name": "Machine Learning" },
        { "@type": "Thing", "name": "Artificial Intelligence" }
        
      ],
      "educationalLevel": "university",
      "learningResourceType": "tutorial",
      "educationalUse": "instruction"
    }
    </script>
    
</head>

<body>
    <header>
    <nav class="navbar">
        <!-- Logo as Home Button -->
        <div class="logo-home">
            <a href="../../../index.html" 
               class="logo-link " 
               title="Home - Math-CS Compass">
                <img src="../../images/icon_x512.png" 
                     alt="Math-CS Compass Logo" 
                     class="nav-logo">
            </a>
        </div>

        <div class="search-container">
            <input type="text" id="search-input" placeholder="Search keywords...">
            <button id="search-button" aria-label="Search" title="Search website"><i class="fas fa-search"></i></button>
        </div>
        
        <div class="menu-toggle">
            <i class="fas fa-bars"></i>
        </div>
        
        <ul class="nav-links">
            <li><a href="../Linear_algebra/linear_algebra.html" >I - Linear Algebra to Algebraic Foundations</a></li>
            
            <li><a href="../Calculus/calculus.html" >II - Calculus to Optimization & Analysis</a></li>
            
            <li><a href="../Probability/probability.html" >III - Probability & Statistics</a></li>
            
            <li><a href="../Discrete/discrete_math.html" >IV - Discrete Mathematics & Algorithms</a></li>
            
            <li><a href="../Machine_learning/ml.html" class="active">V - Machine Learning</a></li>
        </ul>
    </nav>
</header>
    
    <!DOCTYPE html>
<html>
    <body> 
        <!-- LearningResource Schema for Content Pages -->
        <script type="application/ld+json">
        {
        "@context": "https://schema.org",
        "@type": "LearningResource",
        "name": "Support Vector Machine (SVM)",
        "description": "Learn about the support vector machine (SVM) basics.",
        "learningResourceType": "lesson",
        "educationalUse": "instruction",
        "educationalLevel": "university",
        "interactivityType": "active",
        "author": {
            "@type": "Person",
            "name": "Yusuke Yokota",
            "jobTitle": "Mathematics & Computer Science Educator"
        },
        "publisher": {
            "@type": "Organization",
            "name": "MATH-CS COMPASS",
            "url": "https://yusukey01.github.io"
        },
        "about": [
            
            { "@type": "Thing", "name": "Support Vector Machine" },
            { "@type": "Thing", "name": "SVM" },
            { "@type": "Thing", "name": "Kernel Methods" },
            { "@type": "Thing", "name": "Margin Classification" },
            { "@type": "Thing", "name": "Soft Margin" },
            { "@type": "Thing", "name": "KKT Conditions" },
            { "@type": "Thing", "name": "Dual Optimization" }
            
        ],
        "teaches": [
            "Mathematical concepts",
            "Practical applications",
            "Problem-solving techniques"
        ],
        "isPartOf": {
            "@type": "Course",
            "name": "Machine Learning",
            "description": "Explore machine learning ideas and applications with mathematical foundations",
            "provider": {
            "@type": "Organization",
            "name": "MATH-CS COMPASS",
            "url": "https://yusukey01.github.io"
            },
            "instructor": {
            "@type": "Person",
            "name": "Yusuke Yokota",
            "jobTitle": "Mathematics & Computer Science Educator"
            },
            "courseCode": "V",
            "hasCourseInstance": {
            "@type": "CourseInstance",
            "courseMode": "online",
            "courseWorkload": "PT5H",
            "instructor": {
                "@type": "Person",
                "name": "Yusuke Yokota"
            }
            },
            "offers": {
            "@type": "Offer",
            "price": "0",
            "priceCurrency": "USD",
            "availability": "https://schema.org/InStock",
            "category": "free"
            }
        }
        }
        </script>
        <!-- WebApplication Schema for Interactive Demos -->
        <script type="application/ld+json">
        {
        "@context": "https://schema.org",
        "@type": "WebApplication",
        "name": "Support Vector Machine (SVM) Interactive Demo",
        "description": "Interactive demonstration of support vector machine (svm) concepts",
        "applicationCategory": "EducationalApplication",
        "operatingSystem": "Web Browser",
        "url": "https://yusukey01.github.io/Mathematics/Machine_learning/svm.html",
        "author": {
            "@type": "Person",
            "name": "Yusuke Yokota"
        },
        "applicationSubCategory": "Mathematical Visualization",
        "featureList": [
            "Interactive mathematical visualization",
            "Real-time parameter adjustment",
            "Educational demonstrations"
        ],
        "isAccessibleForFree": true,
        "educationalUse": "instruction",
        "educationalLevel": "university"
        }
        </script>
        <div class="hero-section">
            <h1 class="webpage-name"> Support Vector Machine (SVM)</h1>
        </div>

        
  <div class="quick-jump-container">
    <button type="button" id="quick-jump-toggle" class="quick-jump-toggle">
      <i class="fas fa-list"></i> <strong>Section V Navigation</strong>
    </button>
    <div id="quick-jump-menu" class="quick-jump-menu">
      <h3>Machine Learning</h3>
      <div class="quick-jump-links">

        
        <a href="ml.html">← Back to Section V Overview</a>
        <hr style="margin: 8px 0; border: 1px solid #ddd;">
        
        
        <a href="intro_ml.html" >Part 1: Intro to Machine Learning</a>
        <a href="regression.html" >Part 2: Regularized Regression</a>
        <a href="intro_classification.html" >Part 3: Intro to Classification</a>
        <a href="neural_networks.html" >Part 4: Neural Networks Basics</a>
        <a href="autodiff.html" >Part 5: Automatic Differentiation</a>
        <a href="svm.html" class="current-page">Part 6: Support Vector Machine (SVM)</a>
        <a href="pca.html" >Part 7: Principal Component Analysis (PCA) & Autoencoders</a>
        <a href="clustering.html" >Part 8: Clustering</a>
        <a href="deep_nn.html" >Part 9: Intro to Deep Neural Networks</a>
        <a href="intro_RL.html" >Part 10: Intro to Reinforcement Learning</a>
        
      </div>
    </div>
</div>

        
        <div class="topic-nav">
            <a href="#svm">Support Vector Machine</a>
            <a href="#smc">Soft Margin Constraints</a>
            <a href="#demo">SVM Demo</a>
        </div> 

        <div class="container">  

            <section id="svm" class="section-content">
                <h2>Support Vector Machine</h2>
                <p>
                    So far in both regression and classification problems, we have used <strong>probabilistic predictors</strong>. Here, 
                    we learn a <strong>non-probabilistic</strong> method, which is known as <strong>Support Vector Machines (SVMs)</strong>.
                </p>
                <p>
                    This margin-based approach often leads to strong generalization, especially in high dimensional spaces with limited 
                    samples. By employing the <strong>kernel trick</strong> (e.g., RBF kernel), SVMs can implicitly map inputs 
                    into higher dimensional feature spaces, enabling the classification of nonlinearly separable data without explicitly 
                    computing those transformations. Despite the rise of deep learning, SVMs remain valuable for small- to medium-sized 
                    datasets, offering robustness and interpretability through a sparse set of support vectors.
                </p>
                <p>
                    In binary classification \(h(x) = \text{sign }(f(x))\), the decision boundary (hyperplane) is given by:
                    \[
                    f(x) = w^\top x + w_0 \tag{1}
                    \]
                    To obtain robust solution, we would like to <strong>maximize the margin between data points and the decision boundary</strong>.

                    <div style="text-align: center;">
                    <img src="images/svm_boundary.png" alt="complexity"  class="responsive-image">
                    </div>
                    To express the distance of the closest point to the decision boundary, 
                    we introduce the orthogonal projection of any point \(x\) onto the boundary \(f(x)\):
                    \[
                    x = \text{proj}_{f(x)}x  + r \frac{w}{\| w \|} \tag{2}
                    \]
                    where \(r\) is the distance from \(x\) to the decision boundary whose normal vector is \(w\).
                </p>

                <p>
                    Substitute Expression (2) for the decision boundary (1):
                    \[
                    \begin{align*}
                    f(x) &= w^\top \left(\text{proj}_{f(x)}x  + r \frac{w}{\| w \|} + w_0 \right) \\\\
                         &= w^\top \text{proj}_{f(x)}x + w_0 + r \| w \|.
                    \end{align*}
                    \]
                    Since \(f(\text{proj}_{f(x)}x ) = 0\), 
                    \[
                    r = \frac{f(x)}{\| w \|}
                    \]
                    and each point must be on the correct side of the decision boundary:
                    \[
                    f(x_n)\tilde{y}_n > 0.
                    \]
                    Therefore, our objective is represented by:
                    \[
                    \max_{w, w_0} \frac{1}{\| w \|} \min_{n = 1}^N \left[\tilde{y}_n \left(w^\top x_n + w_0 \right)\right].
                    \]
                    Moreover, define a scale factor \(k\) subject to \(\tilde{y}_n f_n =1\), but the scaling does not change the 
                    distance of points to the decision boundary (it will be canceled out by \(\frac{1}{\| w \|}\) ).
                </p>
                <p>
                    Note that \(\max \frac{1}{\| w \|}\) is the same as \(\min \| w \|^2\). Thus our objective becomes:
                    \[
                    \min_{w, w_0} \frac{1}{2}\| w \|^2 
                    \]
                    subjec to 
                    \[
                    \tilde{y}_n (w^\top x_n + w_0) \geq 1, \quad n = 1 : N.
                    \]
                    (Factor of \(\frac{1}{2}\) is for just convenience.)
                </p>
                    
                <p>
                    This constrained optimization problem, known as the <strong>primal problem</strong>, is difficult to solve directly. However, 
                    since this is the convex optimization problem, there exists its <a href="../Calculus/duality.html"><strong>dual problem</strong></a> 
                    using Lagrange multipliers, which often has computational advantages and enables the kernel trick.
                </p>

                <p>
                    Let \(\alpha \in \mathbb{R}^N\) be the dual variables corresponding to Lagrange multipliers:
                    \[
                    \mathcal{L}(w, w_0, \alpha) = \frac{1}{2}w^\top w - \sum_{n =1}^N \alpha_n (\tilde{y}_n (w^\top x_n + w_0) -1) \tag{3}
                    \]
                    To optimize this, we need a stationary point that satisfies:
                    \[
                    (\hat{w}, \hat{w_0}, \hat{\alpha}) = \min_{w, w_0} \max_{\alpha} \mathcal{L}(w, w_0, \alpha).
                    \]
                    Taking derivatives wrt \(w\) and \(w_0\) and seting to zero:
                    \[
                    \begin{align*}
                    \nabla_w \mathcal{L}(w, w_0, \alpha)  &= w - \sum_{n =1}^N \alpha_n \tilde{y}_n x_n  \\\\
                    \frac{\partial \mathcal{L}(w, w_0, \alpha)}{\partial w_0} &= - \sum_{n =1}^N \alpha_n \tilde{y}_n.
                     \end{align*}
                    \]
                    Then we have:
                    \[
                    \begin{align*}
                    \hat{w} &= \sum_{n =1}^N \hat{\alpha_n} \tilde{y}_n x_n  \\\\
                    0 &= \sum_{n =1}^N \hat{\alpha_n} \tilde{y}_n.
                    \end{align*}
                    \]
                    Substitute these for the Lagrangian (3), we obtain
                    \[
                    \begin{align*}
                     \mathcal{L}(\hat{w}, \hat{w_0}, \alpha) &= \frac{1}{2}\hat{w}^\top \hat{w} - \sum_{n =1}^N \alpha_n \tilde{y}_n \hat{w}^\top x_n
                                                               - \sum_{n =1}^N \alpha_n \tilde{y}_n w_0 + \sum_{n =1}^N  \alpha_n  \\\\
                                                             &= \frac{1}{2}\hat{w}^\top \hat{w} - \hat{w}^\top \hat{w} - 0 + \sum_{n =1}^N  \alpha_n \\\\
                                                             &= - \frac{1}{2} \sum_{i =1}^N \sum_{j = 1}^N \alpha_i \alpha_j \tilde{y}_i \tilde{y}_j x_i^\top x_j 
                                                                + \sum_{n =1}^N \alpha_n.
                    \end{align*}
                    \]
                    Therefore the dual problem is:
                    \[
                    \max_{\alpha}  \mathcal{L}(\hat{w}, \hat{w_0}, \alpha) 
                    \]
                    subject to 
                    \[
                    \sum_{n =1}^N \alpha_n \tilde{y}_n = 0 \text{ and }  \alpha_n \geq 0, \quad n = 1 : N. 
                    \]
                    Also, the solution must satisfy the <a href="../Calculus/constrained_opt.html"><strong>KKT conditions</strong></a>:
                    \[
                    \begin{align*}
                     \alpha_n &\geq 0 \text{ (Dual feasibility) }\\\\
                    \tilde{y}_n f(x_n) -1 &\geq 0   \text{ (Primal feasibility) }\\\\
                    \alpha_n (\tilde{y}_n f(x_n) -1 ) &= 0  \text{ (Complementary slackness) }
                    \end{align*}
                    \]
                    Thus, for each data point, either 
                    \[
                    \alpha_n = 0 \, \text{ or } \, \tilde{y}_n (\hat{w}^\top x_n + \hat{w}_0) = 1
                    \]
                    must hold. The points meet the second condition are called <strong>support vectors</strong>.
                </p>

                <p>
                    Let the set of support vectors  be \(\mathcal{S}\). To perform prediction, we use: 
                    \[
                    \begin{align*}
                    f(x ; \hat{w}, \hat{w}_0) &= \hat{w}^\top x + \hat{w}_0 \\\\
                                              &= \sum_{n \in \mathcal{S}} \alpha_n \tilde{y}_n x_n^\top x + \hat{w}_0.
                    \end{align*}
                    \]
                    By the <a href="intro_classification.html"><strong>kernel trick</strong></a>, 
                    \[
                    f(x) = \sum_{n \in \mathcal{S}} \alpha_n \tilde{y}_n \mathcal{K}(x_n, x) + \hat{w}_0
                    \]
                    Also, 
                    \[
                    \begin{align*}
                    \hat{w}_0 &= \frac{1}{|\mathcal{S}|} \sum_{n \in \mathcal{S}} \left(\tilde{y}_n - \hat{w}^\top x_n \right) \\\\
                              &= \frac{1}{|\mathcal{S}|} \sum_{n \in \mathcal{S}} \left(\tilde{y}_n - \sum_{m \in \mathcal{S}} \alpha_m \tilde{y}_m x_m^\top x_n \right).
                    \end{align*}
                    \]
                    By the <strong>kernel trick</strong>, 
                    \[
                    \frac{1}{|\mathcal{S}|} \sum_{n \in \mathcal{S}} \left(\tilde{y}_n - \sum_{m \in \mathcal{S}} \alpha_m \tilde{y}_m \mathcal{K}(x_m, x_n) \right).
                    \]
                </p>
            </section>
            
             <section id="smc" class="section-content">
                <h2>Soft Margin Constraints</h2>
                <p>
                    Consider the case where the data is NOT linearly separable. In this case there is not feasible solution in which 
                    \(\forall n, \, \tilde{y}_n f_n \geq 1\). By introducing <a href="../Calculus/constrained_opt.html"><strong>slack variables</strong></a> \(\xi_n \geq 0\), we relax
                    the constraints \(\tilde{y}_n f_n \geq 0\) with <strong>soft margin constraints</strong>:
                    \[
                    \tilde{y}_n f_n \geq 1 - \xi_n
                    \]
                    Thus our objective becomes: 
                    \[
                    \min_{w, w_0, \xi} \frac{1}{2} \| w \|^2 + C \sum_{n =1}^N \xi_n 
                    \]
                    subject to 
                    \[
                    \xi_n \geq 0, \quad \tilde{y}_n (x_n^\top w + w_0) \geq 1 - \xi_n
                    \]
                    where \(C \geq 0\) is a hyper parameter that controls how many data points are allowed to violate the margin.
                </p>
                <p>
                    The Lagrangian becomes:
                    \[
                    \begin{align*}
                    &\mathcal{L}(w, w_0, \alpha, \xi, \mu)  \\\\
                    &= \frac{1}{2} w^\top w + C \sum_{n =1}^N \xi_n 
                            - \sum_{n = 1}^N \alpha_n (\tilde{y}_n(w^\top x_n + w_0) -1 + \xi_n)
                            - \sum_{n = 1}^N \mu_n \xi_n
                    \end{align*}
                    \]
                    where \(\alpha_n \geq 0, \text{ and } \mu_n \geq 0\) are lagrange multipliers.
                </p>
                <p>
                By optimizing out \(w, \, w_0, \text{ and } \xi\), we obtain:
                \[
                 \mathcal{L}(\alpha) = \sum_{i = 1}^N \alpha_i -\frac{1}{2}\sum_{i =1}^N \sum_{j = 1}^N \alpha_i \alpha_j \tilde{y}_i \tilde{y}_j x_i^\top x_j.
                 \]
                 This dual form is equivalent to the hard margin version, but constraints are different. By the KKT conditions:
                \[
                0 \leq \alpha_n \leq C
                \]
                \[
                \sum_{n=1}^N \alpha_n \tilde{y}_n = 0
                \]
                 These constraints imply: 
                 <ul style="padding-left: 60px;">
                    <li>If \(\alpha_n = 0\), the point is ignored.</li>
                    <li>If \(0 < \alpha_n < C\), the point is on the margin. (\(\xi_n = 0\))</li>
                    <li>If \(\alpha_n = C\), the point is inside the margin. In this case,</li>
                        <ul style="padding-left: 70px;">
                            <li>If \(\xi_n \leq 1\), the point is correctly classified.</li>
                            <li>If \(\xi_n > 1\), the point is misclassified.</li>
                        </ul>
                 </ul>
                </p>
                
            </section>

            <section id="demo" class="section-content">
                <h2>SVM Demo</h2>
                 <div id="svm_visualizer"></div>
                 <p>
                    This interactive demo visualizes the SVM algorithm in action. You can observe how the algorithm finds the optimal 
                    decision boundary that maximizes the margin between two classes while respecting the soft margin constraints.
                </p>

                <h3>Understanding the Visualization</h3>
                <p>
                    The visualization displays several key components of the SVM:
                </p>
                <ul style="padding-left: 40px;">
                    <li><strong>Training Data Points</strong>: Solid circles represent training samples with labels \(\tilde{y} \in \{-1, +1\}\)</li>
                    <li><strong>Test Data Points</strong>: Semi-transparent squares show test samples for evaluating generalization</li>
                    <li><strong>Decision Boundary</strong>: The dark line (linear kernel) or contour (RBF kernel) where \(f(x) = w^\top x + w_0 = 0\)</li>
                    <li><strong>Margin Boundaries</strong>: Dashed lines showing where \(\tilde{y}_n f(x_n) = 1\)</li>
                    <li><strong>Support Vectors</strong>: Points with golden borders that satisfy \(\alpha_n > 0\) and define the decision boundary</li>
                    <li><strong>Slack Variables</strong>: Dotted circles around support vectors visualize \(\xi_n\) for points violating the margin</li>
                </ul>
                <br>
                <h3>Interactive Controls</h3>
                <p>
                    Experiment with different parameters to understand their effects:
                </p>
                <ul style="padding-left: 40px;">
                    <li><strong>Kernel Type</strong>: 
                        <ul style="padding-left: 40px;">
                            <li><em>Linear</em>: Uses \(\mathcal{K}(x_i, x_j) = x_i^\top x_j\) for linearly separable data</li>
                            <li><em>RBF (Gaussian)</em>: Uses \(\mathcal{K}(x_i, x_j) = \exp(-\gamma \|x_i - x_j\|^2)\) for non-linear boundaries</li>
                        </ul>
                    </li>
                    <li><strong>Regularization Parameter C</strong>: Controls the trade-off between maximizing the margin and minimizing classification errors.
                        As shown in the soft margin formulation: \(\min_{w, w_0, \xi} \frac{1}{2} \| w \|^2 + C \sum_{n =1}^N \xi_n\)
                        <ul style="padding-left: 40px;">
                            <li>Low C: Prioritizes wider margins, allows more margin violations</li>
                            <li>High C: Prioritizes correct classification, may lead to overfitting</li>
                        </ul>
                    </li>
                    <li><strong>RBF Gamma (γ)</strong>: Controls the influence radius of support vectors in RBF kernel
                        <ul style="padding-left: 40px;">
                            <li>Low γ: Smooth decision boundaries, each point influences a wider region</li>
                            <li>High γ: Complex boundaries, localized influence around each support vector</li>
                        </ul>
                    </li>
                    <li><strong>Learning Rate & Iterations</strong>: SGD optimization parameters for training convergence</li>
                </ul>
                <br>
                <h3>Training Process</h3>
                <p>
                    The demo uses Stochastic Gradient Descent (SGD) to optimize the SVM objective. For the RBF kernel, it employs 
                    Random Fourier Features to approximate the kernel function, making the non-linear SVM computationally efficient.
                    Click "Train SVM" to watch the algorithm iteratively:
                </p>
                <ol style="padding-left: 40px;">
                    <li>Update weights to minimize the hinge loss: \(\max(0, 1 - \tilde{y}_n f(x_n))\)</li>
                    <li>Apply regularization to control model complexity</li>
                    <li>Identify support vectors that satisfy the KKT conditions</li>
                    <li>Converge to the optimal decision boundary</li>
                </ol>
                <br>
                <h3>Observing KKT Conditions</h3>
                <p>
                    After training, the demo verifies the Karush-Kuhn-Tucker (KKT) conditions for each point:
                </p>
                <ul style="padding-left: 40px;">
                    <li>For non-support vectors (\(\alpha_n = 0\)): Must have \(\tilde{y}_n f(x_n) \geq 1\)</li>
                    <li>For support vectors on the margin (\(0 < \alpha_n < C\)): Must have \(\tilde{y}_n f(x_n) = 1\)</li>
                    <li>For support vectors inside/beyond margin (\(\alpha_n = C\)): May have \(\tilde{y}_n f(x_n) < 1\)</li>
                </ul>
                <p>
                    The percentage of points satisfying these conditions indicates how well the optimization has converged.
                </p>

                <h3>Experiment Suggestions</h3>
                <p>
                    Try these experiments to deepen your understanding:
                </p>
                <ol style="padding-left: 40px;">
                    <li>Generate linearly separable data and compare linear vs RBF kernels</li>
                    <li>Create circular data patterns to see where linear kernels fail</li>
                    <li>Adjust C to observe the trade-off between margin width and training accuracy</li>
                    <li>For RBF kernel, vary γ to see how decision boundaries become more complex</li>
                    <li>Watch how the number of support vectors changes with different parameters</li>
                </ol>
            </section>

        </div>
        <script src="/js/main.js"></script> 
        <script src="/js/svm_visualizer.js"></script> 
    </body>
</html>
    
    <footer>
    <div class="footer-content">
        <div class="footer-about">
            <h3>About MATH-CS COMPASS</h3>
            <p>Bridging the gap between pure mathematics and computer science applications.</p>
            <div class="contact-section">
                <strong><a href="https://docs.google.com/forms/d/e/1FAIpQLSf3_RNkegREvfCEIQznn6SCaf-iB5c-Doxn7Ymuu78Uf-PBjg/viewform" target="_blank">
                    Submit Your Questions & Feedback <i class="fas fa-external-link-alt"></i>
                </a></strong>
            </div>
        </div>
        <div class="footer-links">
            <h3>Quick Links</h3>
            <ul>
                
                <li><a href="../../../index.html">Home</a></li>
                <li>
                <a href="../Linear_algebra/linear_algebra.html">
                    <span class="nav-number">I</span>
                    <span>Linear Algebra to Algebraic Foundations</span>
                </a>
                </li>
                <li>
                <a href="../Calculus/calculus.html">
                    <span class="nav-number">II</span>
                    <span>Calculus to Optimization & Analysis</span>
                </a>
                </li>
                <li>
                <a href="../Probability/probability.html">
                    <span class="nav-number">III</span>
                    <span>Probability & Statistics</span>
                </a>
                </li>
                <li>
                <a href="../Discrete/discrete_math.html">
                    <span class="nav-number">IV</span>
                    <span>Discrete Mathematics & Algorithms</span>
                </a>
                </li>
                <li>
                <a href="../Machine_learning/ml.html">
                    <span class="nav-number">V</span>
                    <span>Machine Learning</span>
                </a>
                </li>
            </ul>
        </div>
        <div class="footer-social">
            <h3>Connect With Me</h3>
            <p>
                <a href="https://x.com/MathCSCompass?t=Zi8nyfzszeFw8QizbJwnNg&s=09" target="_blank">
                    <i class="fab fa-x-twitter"></i> X (Twitter)
                </a>
            </p>
            <p>
                <a href="https://www.facebook.com/share/15uPaBWgQN/" target="_blank">
                    <i class="fab fa-facebook"></i> Facebook
                </a>
            </p>
      
            <h3>MATH-CS COMPASS App</h3>
            <div class="app-download-footer">
                <a href="https://play.google.com/store/apps/details?id=io.github.yusukey01.twa" target="_blank" rel="noopener">
                    <img src="../../../images/google-play-badge.png" 
                        alt="Get it on Google Play" 
                        class="google-play-badge">
                </a>
            </div>
            
        </div>

    </div>
    <div class="footer-bottom">
        <p>&copy; 2024-2025 MATH-CS COMPASS. All rights reserved.</p>
    </div>
</footer>
    <script src="../../../js/main.js?v=1.0.2"></script>
    <script src="../../../js/search.js?v=1.0.2"></script>
    

    <button id="go-to-top-btn" class="go-to-top-btn" title="Go to top">
        <i class="fas fa-arrow-up"></i>
    </button>
    <script>
        // Register the service worker
        if ('serviceWorker' in navigator) {
          window.addEventListener('load', () => {
            navigator.serviceWorker.register('/js/service-worker.js')
              .then(registration => {
                console.log('ServiceWorker registration successful with scope: ', registration.scope);
              })
              .catch(error => {
                console.log('ServiceWorker registration failed: ', error);
              });
          });
        }
      </script>
</body>
</html>