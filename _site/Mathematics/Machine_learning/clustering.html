<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    
    <title>Clustering - MATH-CS COMPASS</title>
    
    
    <script>
    window.MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']]
      }
    };
    </script>
    <script id="MathJax-script" defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="preload" href="https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Main-Regular.woff" as="font" type="font/woff2" crossorigin>
    <link rel="preload" href="https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Math-Italic.woff" as="font" type="font/woff2" crossorigin>
    
    
    

    <link rel="stylesheet" href="../../../css/styles.css?v=1.0.2">
  
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css">

    <link rel="dns-prefetch" href="https://cdn.jsdelivr.net">
    <link rel="dns-prefetch" href="https://cdnjs.cloudflare.com">
    
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Learn about clustering basics via K means clustering and spectral clustering.">
    <meta name="google-site-verification" content="IieEujypYW38caEVe8ymjczouNl56yxAy27iPztcfRA" />
    

    <link rel="manifest" href="/manifest.json">

    <!-- Theme color for Chrome, Android, and other browsers -->
    <meta name="theme-color" content="#007bff">
  
    <!-- iOS specific tags -->
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <meta name="apple-mobile-web-app-title" content="MATH-CS COMPASS">
    <meta name="mobile-web-app-capable" content="yes">

    <!-- Icon references -->
    <link rel="icon" type="image/png" sizes="1280x1280" href="/images/maskable_icon_x1280.png?v=1.0.2">
    <link rel="icon" type="image/png" sizes="512x512" href="/images//maskable_icon_x512.png?v=1.0.2">
    
    <!-- Apple Touch Icons -->
    <link rel="apple-touch-icon" href="/images/maskable_icon_x1280.png?v=1.0.2">
    <link rel="apple-touch-icon" href="/images/maskable_icon_x512.png?v=1.0.2">

    <!-- For Windows -->
    <meta name="msapplication-TileImage" content="/images/test1.png?v=1.0.2">
    <meta name="msapplication-TileColor" content="#007bff">

    <!-- Schema.org Structured Data for AI Optimization -->
    

    <!-- Article Schema for content pages -->
    
    <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "Article",
      "headline": "Clustering",
      "description": "Learn about clustering basics via K means clustering and spectral clustering.",
      "datePublished": "2024-01-01",
      "dateModified": "2025-01-01",
      "author": {
        "@type": "Person",
        "name": "Yusuke Yokota",
        "jobTitle": "Mathematics & Computer Science Educator",
        "knowsAbout": [
          "Linear Algebra",
          "Machine Learning",
          "Computer Science",
          "Mathematics"
        ]
      },
      "publisher": {
        "@type": "Organization",
        "name": "MATH-CS COMPASS",
        "url": "https://math-cs-compass.com",
        "logo": {
          "@type": "ImageObject",
          "url": "https://math-cs-compass.com/images/maskable_icon_x512.png"
        }
      },
      "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "https://math-cs-compass.com/Mathematics/Machine_learning/clustering.html"
      },
      "about": [
        
        { "@type": "Thing", "name": "Machine Learning" },
        { "@type": "Thing", "name": "Artificial Intelligence" }
        
      ],
      "educationalLevel": "university",
      "learningResourceType": "tutorial",
      "educationalUse": "instruction"
    }
    </script>
    
</head>

<body>
    <header>
    <nav class="navbar">
        <!-- Logo as Home Button -->
        <div class="logo-home">
            <a href="../../../index.html" 
               class="logo-link " 
               title="Home - Math-CS Compass">
                <img src="../../images/icon_x512.png" 
                     alt="Math-CS Compass Logo" 
                     class="nav-logo">
            </a>
        </div>

        <div class="search-container">
            <input type="text" id="search-input" placeholder="Search keywords...">
            <button id="search-button" aria-label="Search" title="Search website"><i class="fas fa-search"></i></button>
        </div>
        
        <div class="menu-toggle">
            <i class="fas fa-bars"></i>
        </div>
        
        <ul class="nav-links">
            <li><a href="../Linear_algebra/linear_algebra.html" >I - Linear Algebra to Algebraic Foundations</a></li>
            
            <li><a href="../Calculus/calculus.html" >II - Calculus to Optimization & Analysis</a></li>
            
            <li><a href="../Probability/probability.html" >III - Probability & Statistics</a></li>
            
            <li><a href="../Discrete/discrete_math.html" >IV - Discrete Mathematics & Algorithms</a></li>
            
            <li><a href="../Machine_learning/ml.html" class="active">V - Machine Learning</a></li>
        </ul>
    </nav>
</header>
    
    <!DOCTYPE html>
<html>
    <body> 
        <!-- LearningResource Schema for Clustering -->
        <script type="application/ld+json">
        {
        "@context": "https://schema.org",
        "@type": "LearningResource",
        "name": "Clustering",
        "description": "Learn about clustering basics via K means clustering and spectral clustering",
        "learningResourceType": "lesson",
        "educationalUse": "instruction",
        "educationalLevel": "university",
        "interactivityType": "active",
        "author": {
            "@type": "Person",
            "name": "Yusuke Yokota",
            "jobTitle": "Mathematics & Computer Science Educator",
            "knowsAbout": [
            "Clustering Algorithms",
            "Unsupervised Learning",
            "K-means Clustering",
            "Spectral Clustering",
            "Graph Theory",
            "Machine Learning"
            ]
        },
        "publisher": {
            "@type": "Organization",
            "name": "MATH-CS COMPASS",
            "url": "https://math-cs-compass.com"
        },
        "about": [
            { "@type": "Thing", "name": "Clustering" },
            { "@type": "Thing", "name": "K-means Clustering" },
            { "@type": "Thing", "name": "Spectral Clustering" },
            { "@type": "Thing", "name": "Vector Quantization" },
            { "@type": "Thing", "name": "Unsupervised Learning" },
            { "@type": "Thing", "name": "Graph Laplacian" },
            { "@type": "Thing", "name": "Dirichlet Energy" },
            { "@type": "Thing", "name": "K-means++ Initialization" },
            { "@type": "Thing", "name": "One-hot Encoding" }
        ],
        "teaches": [
            "K-means clustering algorithm and implementation",
            "Vector quantization for data compression",
            "Spectral clustering for non-convex data structures",
            "Graph Laplacian theory and applications",
            "Normalized graph Laplacian techniques",
            "K-means++ initialization strategy",
            "Eigenvector analysis for clustering"
        ],
        "isPartOf": {
            "@type": "Course",
            "name": "Machine Learning",
            "description": "Explore machine learning ideas and applications with mathematical foundations",
            "provider": {
            "@type": "Organization",
            "name": "MATH-CS COMPASS",
            "url": "https://math-cs-compass.com"
            },
            "instructor": {
            "@type": "Person",
            "name": "Yusuke Yokota",
            "jobTitle": "Mathematics & Computer Science Educator"
            },
            "courseCode": "V",
            "hasCourseInstance": {
            "@type": "CourseInstance",
            "courseMode": "online",
            "courseWorkload": "PT5H",
            "instructor": {
                "@type": "Person",
                "name": "Yusuke Yokota"
            }
            },
            "offers": {
            "@type": "Offer",
            "price": "0",
            "priceCurrency": "USD",
            "availability": "https://schema.org/InStock",
            "category": "free"
            }
        }
        }
        </script>

        <!-- WebApplication Schema for Interactive Demo -->
        <script type="application/ld+json">
        {
        "@context": "https://schema.org",
        "@type": "WebApplication",
        "name": "Clustering Interactive Demo",
        "description": "Interactive demonstration of K-means and spectral clustering algorithms with real-time visualization",
        "applicationCategory": "EducationalApplication",
        "operatingSystem": "Web Browser",
        "url": "https://math-cs-compass.com/Mathematics/Machine_learning/clustering.html",
        "author": {
            "@type": "Person",
            "name": "Yusuke Yokota"
        },
        "applicationSubCategory": "Clustering Algorithm Visualization",
        "featureList": [
            "Interactive K-means clustering visualization",
            "Spectral clustering demonstration",
            "Real-time cluster center updates",
            "K-means++ initialization comparison",
            "Algorithm convergence visualization",
            "Multiple dataset generation options"
        ],
        "isAccessibleForFree": true,
        "educationalUse": "instruction",
        "educationalLevel": "university",
        "about": [
            { "@type": "Thing", "name": "K-means Clustering Demo" },
            { "@type": "Thing", "name": "Spectral Clustering Visualization" },
            { "@type": "Thing", "name": "Unsupervised Learning Demo" }
        ]
        }
        </script>
        <div class="hero-section">
            <h1 class="webpage-name">Clustering</h1>
        </div>

        
  <div class="quick-jump-container">
    <button type="button" id="quick-jump-toggle" class="quick-jump-toggle">
      <i class="fas fa-list"></i> <strong>Section V Navigation</strong>
    </button>
    <div id="quick-jump-menu" class="quick-jump-menu">
      <h3>Machine Learning</h3>
      <div class="quick-jump-links">

        
        <a href="ml.html">← Back to Section V Overview</a>
        <hr style="margin: 8px 0; border: 1px solid #ddd;">
        
        
        <a href="intro_ml.html" >Part 1: Intro to Machine Learning</a>
        <a href="regression.html" >Part 2: Regularized Regression</a>
        <a href="intro_classification.html" >Part 3: Intro to Classification</a>
        <a href="neural_networks.html" >Part 4: Neural Networks Basics</a>
        <a href="autodiff.html" >Part 5: Automatic Differentiation</a>
        <a href="svm.html" >Part 6: Support Vector Machine (SVM)</a>
        <a href="pca.html" >Part 7: Principal Component Analysis (PCA) & Autoencoders</a>
        <a href="clustering.html" class="current-page">Part 8: Clustering</a>
        <a href="deep_nn.html" >Part 9: Intro to Deep Neural Networks</a>
        <a href="intro_RL.html" >Part 10: Intro to Reinforcement Learning</a>
        
      </div>
    </div>
</div>


        <div class="topic-nav">
            <a href="#intro">Introduction</a>
            <a href="#K-means">K-means Clustering</a>
            <a href="#vq">Vector Quantization (VQ)</a>
            <a href="#sp">Spectral Clustering</a>
            <a href="#demo">Demo</a>
        </div> 

        <div class="container">  

            <section id="intro" class="section-content">
                <h2>Introduction</h2>
                <p>
                    <strong>Clustering</strong> is one of the core tasks in <strong>unsupervised learning</strong>. Unlike supervised learning, where models learn from labeled data, 
                    unsupervised learning aims to uncover hidden patterns or structures in data without any predefined labels. Among these tasks, clustering 
                    is particularly important — it groups data points into clusters based on their similarity, revealing the underlying structure of the data.
                </p>

                <p>
                    In previous sections, we explored techniques like <strong>Principal Component Analysis (PCA)</strong> and <strong>autoencoders</strong>, which 
                    reduce the dimensionality of high-dimensional data while preserving meaningful information. These dimensionality reduction methods helped us uncover 
                    compact representations of data, often referred to as <strong>latent features</strong>.
                </p>

                <p>
                    Now that we have the tools to represent complex data in a lower-dimensional space, we turn to the next natural question: 
                    <strong>Can we identify meaningful groupings within this reduced space?</strong> This is the goal of clustering, which allows us to segment 
                    data into coherent groups — for example, grouping similar images, organizing unlabeled documents, or detecting communities in social networks.
                </p>
      
            </section>

            <section id="K-means" class="section-content">
                <h2>K-means Clustering</h2>
                <p>
                    We begin with <strong>K-means clustering</strong>, one of the most widely used and intuitive algorithms.
                </p>
                <p>
                    We assume that there are \(K\) <strong>cluster centers</strong> \(\mu_k \in \mathbb{R}^D\), for \(k = 1, \cdots, K\). 
                    Each data point \(x_n \in \mathbb{R}^D\) is assigned to its nearest center:
                    \[
                    z_n^* = \arg \min_k \| x_n - \mu_k \|_2^2.
                    \]
                </p>

                <p>
                    Given the assignments, the cluster centers are updated as the mean of points in each cluster:
                    \[
                    \mu_k = \frac{1}{N_k} \sum_{n : z_n =k} x_n, \,\text{where } N_k = \sum_{n=1}^N \mathbb{I}[z_n = k].
                    \]
                </p>

                <p>
                    These two steps — assignment and update — are repeated until convergence. 
                    This process can be viewed as minimizing a cost function known as the <strong>distortion</strong>, which 
                    is equivalent to the squared Frobenius norm of the reconstruction error: 
                    \[
                    \begin{align*}
                    J(M, Z) &= \sum_{n = 1}^N \| x_n - \mu_{z_n} \|_2^2 \\\\
                            &= \| X - ZM^\top \|_F^2
                    \end{align*}
                    \] 
                    where \(X \in \mathbb{R}^{N \times D}\), \(Z \in \{0, 1\} ^{N \times K}\), and \(M \in \mathbb{R}^{D \times K}\) contains 
                    the cluster centers \(\mu_k\) in its columns. 
                    <br>
                    Note: \(Z\) is the <strong>one-hot encoding</strong>(or <strong>dummy encoding</strong>) matrix whose entries are 
                    \[
                    Z_{nk} =  \begin{cases}
                                1, &\text{ if \(x_n\) is assigned to cluster \(k\) } \\
                                0, &\text{ otherwise}.
                                \end{cases}
                    \]
                </p>    
            </section>

             <section id="vq" class="section-content">
                <h2>Vector Quantization (VQ)</h2>
                <p>
                    A key idea of <strong>vector quantization</strong> is to compress data by replacing each high-dimensional vector 
                    \(x_n \in \mathbb{R}^D\) with a discrete symbol \(z_n \in \{1, \cdots, K\}\), which is an index into a 
                    <strong>codebook</strong> of \(K\) prototype vectors, \(\mu_k \in \mathbb{R}^D\).
                </p>    
                <p>
                    Each data point is encoded by finding the index of the closest prototype using Euclidean distance:
                    \[
                    z_n := f_e (x_n) = \arg \min_k \| x_n - \mu_k \|^2.
                    \]
                </p>
                <p>
                    The corresponding decoder simply maps the index back to the prototype:
                    \[
                    f_d(k) = \mu_k.
                    \]
                    The reconstruction of each data point is therefore \(\hat{x}_n = f_d(z_n) = \mu_{z_n}\).
                </p>
                <p>
                    The quality of a codebook is measured using the <strong>reconstruction error</strong> (also called distortion):
                    \[
                    \begin{align*}
                    J &= \frac{1}{N} \sum_{n =1}^N \| x_n - f_d (f_e (x_n))\|^2  \\\\
                      &= \frac{1}{N} \sum_{n=1}^N \|x_n - \mu_{z_n} \|^2.
                    \end{align*}
                    \]
                    Indeed, this is equivalent to the cost function minimized by the <strong>K-means algorithm</strong>, which can be interpreted 
                    as a special case of vector quantization where the codebook is learned by minimizing distortion via iterative updates. VQ is 
                    more focused on signal compression and encoding, whereas K-means is usually employed for data analysis and clustering tasks.
                </p>
                
            </section>

            <section id="sp" class="section-content">
                <h2>Spectral Clustering</h2>
                <p>
                    Traditional clustering methods like K-means assume clusters are linearly separable and spherical in shape. 
                    However, many real-world datasets exhibit complex, non-convex structures that are not well-captured by distance alone. 
                    <strong>Spectral clustering</strong> addresses this limitation by transforming the data into a new space that reflects 
                    the connectivity structure of the data, often represented as a <strong>graph</strong>.
                </p>

                <p>
                    The idea is to build a <strong>similarity graph</strong> where each data point is a node, and edges encode pairwise similarities. 
                    Let \(\mathbf{W} \in \mathbb{R}^{N \times N}\) be a symmetric <strong>weight matrix</strong> for a graph such that \(w_{ij} = w_{ji} \geq 0\) measures 
                    the similarity between points \(i\) and \(j\). The <strong>degree</strong> of node \(i\) is defined as:
                    \[
                    d_i = \sum_{j=1}^N w_{ij},
                    \]
                    and we define the <strong>degree matrix</strong> \(\mathbf{D} \in \mathbb{R}^{N \times N}\) as a diagonal matrix with entries \(D_{ii} = d_i\).
                </p>

                <p>
                    The fundamental object in spectral clustering is the <strong>graph Laplacian</strong>, defined as:
                    \[
                    \mathbf{L} = \mathbf{D} - \mathbf{W}.
                    \]
                    So, the elements of \(\mathbf{L}\) are given by 
                    \[
                     \begin{cases}
                        d_i &\text{if \(i = j\)} \\
                        -w_{ij} &\text{if } i \neq j \text{ and } w_{ij} \neq 0\\
                        0 &\text{Otherwise}.
                    \end{cases}
                    \]
                    The graph Laplacian captures the structure of the graph in a way that supports clustering via <strong>eigenvector analysis</strong>.
                </p>

                <p>
                    <div class="theorem">
                        <span class="theorem-title">Theorem:</span>
                        Let \(G\) be an undirected (possibly weighted) graph with Laplacian \(\mathbf{L} = \mathbf{D} - \mathbf{W}\). Then the number of connected 
                        components of \(G\) is equal to the multiplicity \(k\) of the eigenvalue 0 of \(\mathbf{L}\). Moreover, the eigenspace corresponding 
                        to eigenvalue 0 is spanned by the indicator vectors \(\mathbf{1}_{S_1}, \ldots, \mathbf{1}_{S_k}\), where each \(S_j\) is a 
                        connected component of \(G\).
                    </div>
                    This makes spectral methods especially powerful for separating well-connected groups in a graph.

                     <div class="proof">
                        <span class="proof-title">Proof:</span>
                        Let \(G = (V, E)\) be an undirected (possibly weighted) graph with Laplacian \(\mathbf{L} = \mathbf{D} - \mathbf{W}\), 
                        where \(\mathbf{W}\) is a symmetric weight matrix and \(\mathbf{D}\) is the diagonal degree matrix \(D_{ii} = \sum_j w_{ij}\).

                        <br><br>
                        For any vector \(\boldsymbol{f} \in \mathbb{R}^N\), the quadratic form of the Laplacian is:
                        \[
                        \boldsymbol{f}^\top \mathbf{L} \boldsymbol{f} = \boldsymbol{f}^\top (\mathbf{D} - \mathbf{W})\boldsymbol{f}
                        = \sum_i D_{ii} f_i^2 - \sum_{i,j} w_{ij} f_i f_j.
                        \]

                        Since \(D_{ii} = \sum_j w_{ij}\), we can write:
                        \[
                        \sum_i D_{ii} f_i^2 = \sum_{i,j} w_{ij} f_i^2.
                        \]

                        So the full expression becomes:
                        \[
                        \boldsymbol{f}^\top \mathbf{L} \boldsymbol{f} = \sum_{i,j} w_{ij} f_i^2 - \sum_{i,j} w_{ij} f_i f_j \tag{1}
                        \]

                        Now, since \(W\) is symmetric, \(w_{ij} = w_{ji}\), then:
                        \[
                        \begin{align*}
                        \sum_{i,j} w_{ij} f_i^2 &= \frac{1}{2} \sum_{i,j} w_{ij}f_i^2 + \frac{1}{2} \sum_{i,j} w_{ji}f_j^2 \\\\
                                                &= \frac{1}{2} \sum_{i,j} w_{ij}(f_i^2 + f_j^2).
                        \end{align*}
                        \]

                        Plugging this into the expression (1):
                        \[
                        \begin{align*}
                        \boldsymbol{f}^\top \mathbf{L} \boldsymbol{f} &= \frac{1}{2} \sum_{i,j} w_{ij}(f_i^2 + f_j^2) - \sum_{i,j} w_{ij} f_i f_j \\\\
                                                              &= \frac{1}{2} \sum_{i,j} w_{ij}(f_i^2 + f_j^2 - 2f_i f_j) \\\\
                                                              &= \boxed{\frac{1}{2} \sum_{i,j} w_{ij} (f_i - f_j)^2.} \tag{1}
                         \end{align*}
                        \]

                        <br>
                        <strong>Case 1: \(k = 1\)</strong><br>
                        Suppose \(\boldsymbol{f} \in \mathbb{R}^n\) is an eigenvector corresponding to eigenvalue 0, i.e., \(\mathbf{L} \boldsymbol{f} = 0\). Then:
                        \[
                        \boldsymbol{f}^\top \mathbf{L} \boldsymbol{f} = 0 \quad \Rightarrow \quad \sum_{i,j} w_{ij}(f_i - f_j)^2 = 0.
                        \]

                        Since \(w_{ij} \geq 0\), each term \((f_i - f_j)^2\) must be zero wherever \(w_{ij} > 0\). That is, for every edge \((i,j) \in E\), we have:
                        \[
                        f_i = f_j.
                        \]

                        Because the graph is connected, there is a path between any two vertices. Applying the above equality recursively along paths in the graph implies that:
                        \[
                        f_i = f_j \quad \text{for all } i,j.
                        \]

                        Therefore, \(\boldsymbol{f}\) must be constant on all vertices — i.e., \(\boldsymbol{f} \in \text{span}\{\mathbf{1}\}\). Hence, the nullspace of \(\mathbf{L}\) 
                        is one-dimensional and spanned by the constant vector \(\mathbf{1} = [1, 1, \dots, 1]^\top\). This confirms that the eigenvalue 0 has multiplicity 1.
                        <br><br>

                        <strong>Case 2: \(k > 1\)</strong><br>
                        Then \(\mathbf{W}\) and \(\mathbf{L}\) can be written in block diagonal form, where each block corresponds to one connected component. 
                        Specifically,
                        \[
                        \mathbf{L} = \begin{bmatrix}
                                        L_1 & 0 & \cdots & 0 \\
                                        0 & L_2 & \cdots & 0 \\
                                        \vdots & \vdots & \ddots & \vdots \\
                                        0 & 0 & \cdots & L_k
                                    \end{bmatrix},
                        \]
                        where each \(L_i\) is the Laplacian of the \(i\)-th connected component. 
                        From Case 1, each \(L_i\) has nullspace spanned by the constant vector \(\mathbf{1}_{S_i}\) 
                        (the indicator vector on component \(S_i\)). Therefore, the full nullspace of \(\mathbf{L}\) is spanned 
                        by \(\mathbf{1}_{S_1}, \ldots, \mathbf{1}_{S_k}\), and the eigenvalue 0 has multiplicity \(k\).
                     </div>
                </p>

                <p>
                    We can consider the graph Laplacian as a <a href="../Linear_algebra/linear_transformation.html"><strong>linear operator</strong></a> 
                    that acts as a discrete analogue of a differential operator, computing finite differences of \(\boldsymbol{f}\) at each node:
                    \[
                    (\boldsymbol{L}\boldsymbol{f})(i) = \sum_{j \in \text{nbr}_i} W_{ij} [\boldsymbol{f}_i - \boldsymbol{f}(j)]
                    \]
                    where \(\text{nbr}_i\) is the set of neighbors of node \(i\).
                </p>

                <p>
                    Note that the Laplacian matrix \(\mathbf{L}\) is <strong>symmetric positive semi-definite</strong> because \(w_{ij} \geq 0\), and thus
                    \(\boldsymbol{f}^\top \mathbf{L} \boldsymbol{f} \geq 0\) for all \(\boldsymbol{f} \in \mathbb{R}^N\). The expression (1) in the above proof 
                    is called the 
                    <strong>Laplacian quadratic form</strong>, or the <strong>Dirichlet energy</strong> of \(\boldsymbol{f}\):
                    \[
                    \boldsymbol{f}^\top \mathbf{L} \boldsymbol{f} = \frac{1}{2} \sum_{i,j} w_{ij}(f_i - f_j)^2.
                    \]
                    It measures the <strong>smoothness</strong> of the function \(\boldsymbol{f}\) on the graph: it becomes small when adjacent nodes 
                    (i.e., those with \(w_{ij} > 0\)) have similar values. 
                </p>

                <p>
                    Moreover, since \(\mathbf{L}\) is symmetric and positive semi-definite, it admits an <a href="../Linear_algebra/symmetry.html"><strong>orthogonal diagonalization</strong></a> (also called a <strong>spectral decomposition</strong>):
                    \[
                    \mathbf{L} = \mathbf{V}\boldsymbol{\Lambda}\mathbf{V}^\top,
                    \quad
                    0 = \lambda_1 \le \lambda_2 \le \cdots \le \lambda_N,
                    \]
                    where \(\boldsymbol{\Lambda}\) contains the real, non-negative eigenvalues \(\lambda_i\) of \(  \mathbf{L}\), and 
                    the columns of \(\mathbf{V}\) form an orthonormal basis of the corresponding eigenvectors.
                </p>

                <p>
                    The smallest eigenvalues correspond to the smoothest variations of a function on the graph. 
                    This property forms the mathematical foundation of <strong>spectral clustering</strong>, 
                    which uses the eigenvectors of \(\mathbf{L}\) (or its normalized form) to embed data points into a lower-dimensional space that reveals cluster structure.
                </p>

                <p>
                    The <strong>Dirichlet energy</strong> thus plays a central role in <strong>spectral graph theory</strong>. 
                    In <strong>spectral clustering</strong>, minimizing this energy reveals groupings where nodes are 
                    densely connected within clusters but sparsely connected across clusters — 
                    making it a powerful tool for uncovering natural structures in complex data.
                </p>
                <br>
                <p>
                    In practice, graphs are often exhibit irregular structure: 
                    nodes may have highly varying degrees, and clusters are not perfectly block-separated. 
                    This means the raw graph Laplacian \(\mathbf{L} = \mathbf{D} - \mathbf{W}\) can be dominated by high-degree nodes, 
                    leading to unbalanced or misleading spectral embeddings.
                </p>

                <p>
                    To address this, we use the <strong>normalized graph Laplacian</strong>, which compensates for 
                    differences in node connectivity and ensures that each node contributes more equally to the 
                    spectral structure: 
                    \[
                    \begin{align*}
                    \mathbf{L}_{\text{sym}} &= \mathbf{D}^{-\frac{1}{2}} \mathbf{L} \mathbf{D}^{-\frac{1}{2}} \\\\
                                   &= \mathbf{I} - \mathbf{D}^{-\frac{1}{2}} \mathbf{W} \mathbf{D}^{-\frac{1}{2}}
                    \end{align*}
                    \]
                    where \(\mathbf{D}\) is the diagonal degree matrix and \(\mathbf{W}\) is the symmetric weight (similarity) matrix. In this case, 
                    for the normalized graph Laplacian \(\mathbf{L}_{\text{sym}}\), the eigenspace of zero eigenvalue is spanned by 
                    \[
                    \mathbf{D}^{\frac{1}{2}} \mathbf{1}_{S_k},
                    \]
                    where \(\mathbf{1}_{S_k}\) are the indicator vectors of the connected components.
                </p>

                <p>
                    The <strong>spectral clustering algorithm</strong> proceeds as follows:
                    <div class="theorem">
                    <ol style="padding-left: 40px;">
                        <li>Compute the symmetric normalized Laplacian \(\mathbf{L}_{\text{sym}} = \mathbf{I} - \mathbf{D}^{-\frac{1}{2}} \mathbf{W} \mathbf{D}^{-\frac{1}{2}}\).</li>
                        <li>Find the smallest \(K\) eigenvectors of \(\mathbf{L}_{\text{sym}}\) and stack them column-wise into a matrix \(\mathbf{U} \in \mathbb{R}^{N \times K}\).</li>
                        <li>Normalize each row of \(\mathbf{U}\) to have unit norm, forming a matrix \(\mathbf{T} \in \mathbb{R}^{N \times K}\): 
                            \[
                            t_{ij} = \frac{u_{ij}}{\sqrt{\sum_k u_{ik}^2}}.
                            \]
                        </li>
                        <li>Apply K-means clustering to the rows of \(\mathbf{T}\).</li>
                        <li>Assign the original data point \(x_i\) to cluster \(k\) if row \(i\) of \(\mathbf{T}\) is assigned to cluster \(k\).</li>
                    </ol>
                    </div>
                </p>   
                <p>
                    See also: 
                    <a href="../Linear_algebra/graph_laplacian.html"><strong>Graph Laplacians and Spectral Methods</strong></a>.
                </p>                       
            </section>

             <section id="demo" class="section-content">
                <h2>Demo</h2>
                <div id="clustering_visualizer"></div>
                <p>
                    <strong>Note:</strong> In random initialization, we simply pick \(K\) data points uniformly at random to serve as the initial cluster centers. 
                    While this method is fast, it can be highly sensitive to outliers or imbalanced data distributions, often leading to poor cluster quality 
                    or slow convergence. To address this, the <strong>K-means++ initialization</strong> strategy is widely used in practice. 
                    It selects initial centers more carefully by favoring points that are far apart from already chosen centers, significantly improving the stability and performance of K-means clustering and often leading to better local optima.
                </p>

                 <div class="theorem">
                        <span class="theorem-title">Algorithm: K-means++ Initialization</span>
                <ol style="padding-left: 40px;">
                    <li>Choose the first center \(\mu_1\) uniformly at random from the dataset \(\{x_1, \ldots, x_N\}\).</li>
                    <li>For each remaining point \(x_i\), compute its squared distance to the nearest selected center:
                        \[
                        D(x_i) = \min_{1 \leq j \leq m} \|x_i - \mu_j\|_2^2
                        \]
                        where \(\mu_j\) is one of the centers already chosen.
                    </li>
                    <li>Choose the next center \(\mu_{m+1}\) from the data points with probability proportional to \(D(x_i)\):
                        \[
                        \Pr(x_i \text{ is chosen}) = \frac{D(x_i)}{\sum_j D(x_j)}
                        \]
                    </li>
                    <li>Repeat steps 2-3 until \(K\) centers have been selected.</li>
                    <li>Proceed with the standard K-means algorithm using these \(K\) initial centers.</li>
                </ol>
                 </div>

            </section>


        </div>
        <script src="/js/main.js"></script>  
        <script src="/js/clustering_visualizer.js"></script>  
    </body>
</html>
    
    <footer>
    <div class="footer-content">
        <div class="footer-about">
            <h3>About MATH-CS COMPASS</h3>
            <p>Bridging the gap between pure mathematics and computer science applications.</p>
            <div class="contact-section">
                <strong><a href="https://docs.google.com/forms/d/e/1FAIpQLSf3_RNkegREvfCEIQznn6SCaf-iB5c-Doxn7Ymuu78Uf-PBjg/viewform" target="_blank">
                    Submit Your Questions & Feedback <i class="fas fa-external-link-alt"></i>
                </a></strong>
            </div>
        </div>
        <div class="footer-links">
            <h3>Quick Links</h3>
            <ul>
                
                <li><a href="../../../index.html">Home</a></li>
                <li>
                <a href="../Linear_algebra/linear_algebra.html">
                    <span class="nav-number">I</span>
                    <span>Linear Algebra to Algebraic Foundations</span>
                </a>
                </li>
                <li>
                <a href="../Calculus/calculus.html">
                    <span class="nav-number">II</span>
                    <span>Calculus to Optimization & Analysis</span>
                </a>
                </li>
                <li>
                <a href="../Probability/probability.html">
                    <span class="nav-number">III</span>
                    <span>Probability & Statistics</span>
                </a>
                </li>
                <li>
                <a href="../Discrete/discrete_math.html">
                    <span class="nav-number">IV</span>
                    <span>Discrete Mathematics & Algorithms</span>
                </a>
                </li>
                <li>
                <a href="../Machine_learning/ml.html">
                    <span class="nav-number">V</span>
                    <span>Machine Learning</span>
                </a>
                </li>
            </ul>
        </div>
        <div class="footer-social">
            <h3>Connect With Me</h3>
            <p>
                <a href="https://x.com/MathCSCompass?t=Zi8nyfzszeFw8QizbJwnNg&s=09" target="_blank">
                    <i class="fab fa-x-twitter"></i> X (Twitter)
                </a>
            </p>
            <p>
                <a href="https://www.facebook.com/share/15uPaBWgQN/" target="_blank">
                    <i class="fab fa-facebook"></i> Facebook
                </a>
            </p>
      
            <h3>MATH-CS COMPASS App</h3>
            <div class="app-download-footer">
                <a href="https://play.google.com/store/apps/details?id=io.github.yusukey01.twa" target="_blank" rel="noopener">
                    <img src="../../../images/google-play-badge.png" 
                        alt="Get it on Google Play" 
                        class="google-play-badge">
                </a>
            </div>
            
        </div>

    </div>
    <div class="footer-bottom">
        <p>&copy; 2024-2025 MATH-CS COMPASS. All rights reserved.</p>
    </div>
</footer>
    <script src="../../../js/main.js?v=1.0.2"></script>
    <script src="../../../js/search.js?v=1.0.2"></script>
    

    <button id="go-to-top-btn" class="go-to-top-btn" title="Go to top">
        <i class="fas fa-arrow-up"></i>
    </button>
    <script>
        // Register the service worker
        if ('serviceWorker' in navigator) {
          window.addEventListener('load', () => {
            navigator.serviceWorker.register('/js/service-worker.js')
              .then(registration => {
                console.log('ServiceWorker registration successful with scope: ', registration.scope);
              })
              .catch(error => {
                console.log('ServiceWorker registration failed: ', error);
              });
          });
        }
      </script>
</body>
</html>