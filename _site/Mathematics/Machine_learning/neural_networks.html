<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    
    <title>Neural Networks Basics - MATH-CS COMPASS</title>
    
    
    <script>
    window.MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']]
      }
    };
    </script>
    <script id="MathJax-script" defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="preload" href="https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Main-Regular.woff" as="font" type="font/woff2" crossorigin>
    <link rel="preload" href="https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Math-Italic.woff" as="font" type="font/woff2" crossorigin>
    
    
    

    <link rel="stylesheet" href="../../../css/styles.css?v=1.0.2">
  
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css">

    <link rel="dns-prefetch" href="https://cdn.jsdelivr.net">
    <link rel="dns-prefetch" href="https://cdnjs.cloudflare.com">
    
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Learn about Neural Networks basics with the MLP example.">
    <meta name="google-site-verification" content="IieEujypYW38caEVe8ymjczouNl56yxAy27iPztcfRA" />
    

    <link rel="manifest" href="/manifest.json">

    <!-- Theme color for Chrome, Android, and other browsers -->
    <meta name="theme-color" content="#007bff">
  
    <!-- iOS specific tags -->
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <meta name="apple-mobile-web-app-title" content="MATH-CS COMPASS">
    <meta name="mobile-web-app-capable" content="yes">

    <!-- Icon references -->
    <link rel="icon" type="image/png" sizes="1280x1280" href="/images/maskable_icon_x1280.png?v=1.0.2">
    <link rel="icon" type="image/png" sizes="512x512" href="/images//maskable_icon_x512.png?v=1.0.2">
    
    <!-- Apple Touch Icons -->
    <link rel="apple-touch-icon" href="/images/maskable_icon_x1280.png?v=1.0.2">
    <link rel="apple-touch-icon" href="/images/maskable_icon_x512.png?v=1.0.2">

    <!-- For Windows -->
    <meta name="msapplication-TileImage" content="/images/test1.png?v=1.0.2">
    <meta name="msapplication-TileColor" content="#007bff">

    <!-- Schema.org Structured Data for AI Optimization -->
    

    <!-- Article Schema for content pages -->
    
    <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "Article",
      "headline": "Neural Networks Basics",
      "description": "Learn about Neural Networks basics with the MLP example.",
      "datePublished": "2024-01-01",
      "dateModified": "2025-01-01",
      "author": {
        "@type": "Person",
        "name": "Yusuke Yokota",
        "jobTitle": "Mathematics & Computer Science Educator",
        "knowsAbout": [
          "Linear Algebra",
          "Machine Learning",
          "Computer Science",
          "Mathematics"
        ]
      },
      "publisher": {
        "@type": "Organization",
        "name": "MATH-CS COMPASS",
        "url": "https://yusukey01.github.io",
        "logo": {
          "@type": "ImageObject",
          "url": "https://yusukey01.github.io/images/maskable_icon_x512.png"
        }
      },
      "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "https://yusukey01.github.io/Mathematics/Machine_learning/neural_networks.html"
      },
      "about": [
        
        { "@type": "Thing", "name": "Machine Learning" },
        { "@type": "Thing", "name": "Artificial Intelligence" }
        
      ],
      "educationalLevel": "university",
      "learningResourceType": "tutorial",
      "educationalUse": "instruction"
    }
    </script>
    
</head>

<body>
    <header>
    <nav class="navbar">
        <!-- Logo as Home Button -->
        <div class="logo-home">
            <a href="../../../index.html" 
               class="logo-link " 
               title="Home - Math-CS Compass">
                <img src="../../images/icon_x512.png" 
                     alt="Math-CS Compass Logo" 
                     class="nav-logo">
            </a>
        </div>

        <div class="search-container">
            <input type="text" id="search-input" placeholder="Search keywords...">
            <button id="search-button" aria-label="Search" title="Search website"><i class="fas fa-search"></i></button>
        </div>
        
        <div class="menu-toggle">
            <i class="fas fa-bars"></i>
        </div>
        
        <ul class="nav-links">
            <li><a href="../Linear_algebra/linear_algebra.html" >I - Linear Algebra to Algebraic Foundations</a></li>
            
            <li><a href="../Calculus/calculus.html" >II - Calculus to Optimization & Analysis</a></li>
            
            <li><a href="../Probability/probability.html" >III - Probability & Statistics</a></li>
            
            <li><a href="../Discrete/discrete_math.html" >IV - Discrete Mathematics & Algorithms</a></li>
            
            <li><a href="../Machine_learning/ml.html" class="active">V - Machine Learning</a></li>
        </ul>
    </nav>
</header>
    
    <!DOCTYPE html>
<html>
    <body> 
        <!-- LearningResource Schema for Content Pages -->
        <script type="application/ld+json">
        {
        "@context": "https://schema.org",
        "@type": "LearningResource",
        "name": "Neural Networks Basics",
        "description": "Learn about Neural Networks basics with the MLP example.",
        "learningResourceType": "lesson",
        "educationalUse": "instruction",
        "educationalLevel": "university",
        "interactivityType": "active",
        "author": {
            "@type": "Person",
            "name": "Yusuke Yokota",
            "jobTitle": "Mathematics & Computer Science Educator"
        },
        "publisher": {
            "@type": "Organization",
            "name": "MATH-CS COMPASS",
            "url": "https://yusukey01.github.io"
        },
        "about": [
            
            { "@type": "Thing", "name": "Neural Networks" },
            { "@type": "Thing", "name": "Multilayer Perceptron" },
            { "@type": "Thing", "name": "MLP" },
            { "@type": "Thing", "name": "Backpropagation" },
            { "@type": "Thing", "name": "Activation Functions" },
            { "@type": "Thing", "name": "Deep Learning" },
            { "@type": "Thing", "name": "Feedforward Networks" }
            
        ],
        "teaches": [
            "Mathematical concepts",
            "Practical applications",
            "Problem-solving techniques"
        ],
        "isPartOf": {
            "@type": "Course",
            "name": "Machine Learning",
            "description": "Explore machine learning ideas and applications with mathematical foundations",
            "provider": {
            "@type": "Organization",
            "name": "MATH-CS COMPASS",
            "url": "https://yusukey01.github.io"
            },
            "instructor": {
            "@type": "Person",
            "name": "Yusuke Yokota",
            "jobTitle": "Mathematics & Computer Science Educator"
            },
            "courseCode": "V",
            "hasCourseInstance": {
            "@type": "CourseInstance",
            "courseMode": "online",
            "courseWorkload": "PT5H",
            "instructor": {
                "@type": "Person",
                "name": "Yusuke Yokota"
            }
            },
            "offers": {
            "@type": "Offer",
            "price": "0",
            "priceCurrency": "USD",
            "availability": "https://schema.org/InStock",
            "category": "free"
            }
        }
        }
        </script>
        <!-- WebApplication Schema for Interactive Demos -->
        <script type="application/ld+json">
        {
        "@context": "https://schema.org",
        "@type": "WebApplication",
        "name": "Neural Networks Basics Interactive Demo",
        "description": "Interactive demonstration of neural networks basics concepts",
        "applicationCategory": "EducationalApplication",
        "operatingSystem": "Web Browser",
        "url": "https://yusukey01.github.io/Mathematics/Machine_learning/neural_networks.html",
        "author": {
            "@type": "Person",
            "name": "Yusuke Yokota"
        },
        "applicationSubCategory": "Mathematical Visualization",
        "featureList": [
            "Interactive mathematical visualization",
            "Real-time parameter adjustment",
            "Educational demonstrations"
        ],
        "isAccessibleForFree": true,
        "educationalUse": "instruction",
        "educationalLevel": "university"
        }
        </script>
        <div class="hero-section">
            <h1 class="webpage-name"> Neural Networks Basics</h1>
        </div>

        
  <div class="quick-jump-container">
    <button type="button" id="quick-jump-toggle" class="quick-jump-toggle">
      <i class="fas fa-list"></i> <strong>Section V Navigation</strong>
    </button>
    <div id="quick-jump-menu" class="quick-jump-menu">
      <h3>Machine Learning</h3>
      <div class="quick-jump-links">

        
        <a href="ml.html">← Back to Section V Overview</a>
        <hr style="margin: 8px 0; border: 1px solid #ddd;">
        
        
        <a href="intro_ml.html" >Part 1: Intro to Machine Learning</a>
        <a href="regression.html" >Part 2: Regularized Regression</a>
        <a href="intro_classification.html" >Part 3: Intro to Classification</a>
        <a href="neural_networks.html" class="current-page">Part 4: Neural Networks Basics</a>
        <a href="autodiff.html" >Part 5: Automatic Differentiation</a>
        <a href="svm.html" >Part 6: Support Vector Machine (SVM)</a>
        <a href="pca.html" >Part 7: Principal Component Analysis (PCA) & Autoencoders</a>
        <a href="clustering.html" >Part 8: Clustering</a>
        <a href="deep_nn.html" >Part 9: Intro to Deep Neural Networks</a>
        <a href="intro_RL.html" >Part 10: Intro to Reinforcement Learning</a>
        
      </div>
    </div>
</div>


        <div class="topic-nav">
            <a href="#MLP">Multilayer Perceptron (MLP)</a>
            <a href="#activation">Activation Functions</a>
            <a href="#learning">Learning in Neural Networks</a>
            <a href="#demo">Neural Networks Demo</a>
            <a href="#GPU">Development of Deep Learning</a>    
        </div> 

        <div class="container">  

            <section id="MLP" class="section-content">
                <h2>Multilayer Perceptron (MLP)</h2>
                <p> 
                    The key idea of <strong>deep neural networks (DNNs)</strong> is "composing" a vast number of simple functions to 
                    make a huge complex function. In this section, we focus on a specific type of DNN known as the <strong>multilayer perceptron (MLP)</strong>, 
                    also referred to as a <strong>feedforward neural network (FFNN)</strong>.
                </p>

                <p>
                    An MLP defines a composite function of the form: 
                    \[
                    f(x ; \theta) = f_L (f_{L-1}(\cdots(f_1(x))\cdots))
                    \]
                
                    where each component function \( f_\ell(x) = f(x; \theta_\ell) \) represents the transformation at 
                    <strong>layer</strong> \( \ell \,\), \( x \in \mathbb{R}^D \) is an input vector with \( D \) features, and 
                    \(\theta\) is a collection of parameters(weights and biases):
                    \[
                    \theta = \{ \theta_\ell \}_{\ell=1}^L \text{ ,where } \theta_\ell = \{ W^{(\ell)}, b^{(\ell)} \}.
                    \]
                </p>
                <p>
                     Each layer is assumed to be <strong>differentiable</strong> and consists of two operations: an <strong>affine transformation</strong> followed 
                     by a <strong>non-linear differentiable activation function</strong> \( g_\ell : \mathbb{R} \to \mathbb{R}\). An MLP consists of an <strong>input layer</strong>, 
                     one or more <strong>hidden layers</strong>, and an <strong>output layer</strong>.
                </p>

                <p>
                    We define the hidden units \(z^{(\ell)}\) at each layer \(\ell\) passed  elementwise through the activation:
                    \[
                    z^{(\ell)} = g_{\ell}(b^{(\ell)} + W^{(\ell)}z^{(\ell -1)}) = g_{\ell}(a^{(\ell)})
                    \]
                    where \(a^{(\ell)}\) is called the <strong>pre-activations</strong>, and the output of the newtwork is denoted by 
                    \(\hat{y} = h_\theta(x) = g_L(a^{(L)})\).
                </p>

                 <p>
                    <strong>Note:</strong> Input data is typically stored as an \( N \times D \) <strong>design matrix</strong>, where each 
                    row corresponds to a data point and each column to a feature. This is referred to as <strong>structured data</strong> or 
                    <strong>tabular data</strong>. In contrast, for <strong>unstructured data</strong> such as images or text, different architectures 
                    are used:
                </p>

                <ul style="padding-left: 40px;">
                    <li><strong>Convolutional Neural Networks (CNNs)</strong> for images</li>
                    <li><strong>Recurrent Neural Networks (RNNs)</strong> and <strong>Transformers</strong> for sequential data (e.g. text)</li>
                </ul>

                <p>
                    In particular, modern <strong>Large Language Models (LLMs)</strong> such as GPT are based on the transformer architecture and 
                    have replaced RNNs in many natural language processing tasks.
                </p>
            </section>

            <section id="activation" class="section-content">
                <h2>Activation Functions</h2>
                <p>
                    Without a <strong>non-linear</strong> activation function, a neural network composed of multiple layers would reduce to a 
                    single linear transformation:
                    \[
                    f(x ; \theta) = \theta^{(L)} \theta^{(L-1)}  \cdots \theta^{(2)} \theta^{(1)} x.
                    \]
                    This composition is still <strong>linear</strong> in \( x \), and therefore incapable of representing non-linear decision boundaries. 
                    Non-linear activation functions are necessary to break this linearity and allow networks to approximate arbitrary functions.
                </p>
                <p>
                    Historically, a common choice was the <strong>sigmoid (logistic)</strong> activation function:
                    \[
                    \sigma(a) = \frac{1}{1+e^{-a}}.
                    \]
                    However, sigmoid functions saturate for large positive or negative inputs: \( \sigma(a) \to 1 \) as \( a \to +\infty \), and \( \sigma(a) \to 0 \) as \( a \to -\infty \). 
                    In these regions, the gradient \(\sigma'(a) = \sigma(a)(1-\sigma(a))\) becomes very small (approaching zero), leading to the <strong>vanishing gradient problem</strong>. 
                    During backpropagation, gradients are multiplied layer by layer, so very small gradients in deeper layers become exponentially smaller as they propagate backward, making 
                    learning slow or unstable in deep networks.

                </p>
                <p>
                    To address this, modern networks often use the <strong>Rectified Linear Unit (ReLU)</strong>:
                    \[
                    g(a) = \max(0, a) = a \mathbb{I}(a>0)
                    \]
                    ReLU's derivative is 1 for positive inputs and 0 for negative inputs, avoiding the exponential decay of gradients that occurs with sigmoid functions. 
                    ReLU introduces non-linearity while preserving gradient magnitude for positive inputs. It is computationally simple and helps maintain gradient flow 
                    during training, which is why it is now a standard choice in modern neural networks architectures.
                </p>
                <p>
                    <strong>Note:</strong> While ReLU solves the vanishing gradient problem, it can suffer from the "dying ReLU" problem where neurons become inactive (always output zero) and 
                    stop learning. This occurs when neurons consistently receive negative inputs, causing their gradients to be permanently zero.
                </p>
            </section>

            <section id="learning" class="section-content">
                <h2>Learning in Neural Networks</h2>

                <p> 
                    Training the network is finding parameters \( \theta = \{ \theta_\ell \}_{\ell=1}^L \), where 
                    \( \theta_\ell = \{ W^{(\ell)}, b^{(\ell)} \} \) that minimize the empirical risk (average loss over all training data):
                    \[
                    J(\theta) = \frac{1}{N} \sum_i \mathcal{L}(y_i, \hat{y_i})
                    \]
                    where \(\hat{y_i} = h_{\theta}(x_i)\) is the network's prediction.
                </p>
                <p>
                    For the binary classification, a common choice can be
                    <a href="../Probability/entropy.html"><strong>binary cross-entropy</strong></a>:
                    \[
                     \mathcal{L}(y, \hat{y}) = -y \log(\hat{y}) - (1-y) \log(1-\hat{y}) 
                    \]
                </p>
                <p>
                    The optimization is done by performing a <a href="../Calculus/gradient.html"><strong>gradient-based optimization method </strong></a> which iteratively updates 
                    parameters in the direction of negative gradient:
                    \[
                    \theta \leftarrow \theta - \alpha \nabla_{\theta} J(\theta)
                    \]
                    where \(\alpha\) is the learning rate. 
                </p>

                <p>
                    Our demo employs <strong>mini-batch gradient descent</strong>, which computes gradients on a small random subset of the data at 
                    each iteration. This provides a good balance between computational efficiency and gradient quality, often leading to faster convergence 
                    and better generalization compared to using the entire dataset at once.
                </p>

                <p>
                    The gradients are computed efficiently by the <a href="../Calculus/jacobian.html"><strong>backpropagation</strong></a> 
                    algorithm. Backprop is an efficient application of the chain rule starting from the gradient of the loss w.r.t the 
                    output and working backwards. The algorithm computes all gradients in just two passes through the network:
                </p>
                
                <div class="pseudocode">
                    <span class="pseudocode-title">Algorithm: BACKPROPAGATION</span>
                    <strong>Consider the MLP with K layers</strong>
                    <strong>Input:</strong> \(x \in \mathbb{R}^D\)
                    <strong>//Forward Pass</strong>
                    &emsp;\(x_1 = x\); 
                    // \(f_k\) is an activation with the previous output: \(x_k\) and the parameters for this layer: \(\theta_k\) 
                    &emsp;<strong>for \(k = 1 : K\) do</strong>
                    &emsp;&emsp;&emsp; \(x_{k+1} = f_k(x_k, \theta_k)\); 
                    <strong>//Backward Pass</strong>
                    &emsp;\(u_{K+1} = 1\); //Gradient of \mathcal{L} wrt itself is 1
                    &emsp;<strong>for \(k = K : 1\) do</strong>
                    &emsp;&emsp;&emsp; \(g_k = u_{k+1}^\top \frac{\partial f_k (x_k, \theta_k)}{\partial \theta_k}\); //Gradient of the loss wrt \(\theta_k\)
                    &emsp;&emsp;&emsp; \(u_k^\top = u_{k+1}^\top \frac{\partial f_k (x_k, \theta_k)}{\partial x_k}\); //Gradient of the loss wrt \(x_k\)
                    &emsp;Output:  
                    &emsp;&emsp;&emsp;\(\mathcal{L} = x_{K+1}\);  //Loss value (computed in forward pass)
                    &emsp;&emsp;&emsp;\(\nabla_x \mathcal{L} = u_1\); //Gradient wrt the input
                    &emsp;&emsp;&emsp;\(\{\nabla_{\theta} \mathcal{L} = g_k\}_{k=1}^K\); ///Gradients wrt the parameters
                </div>    
              
            </section>

            <section id="demo" class="section-content">
                <h2>Neural Networks Demo</h2>
                <div id="neural_network_visualizer"></div>
                <p>
                    This interactive demo showcases how a simple neural network can learn to classify non-linear patterns. 
                    You can generate datasets, tweak model parameters, and visualize the training process in real time.
                </p>

                <ul style="padding-left: 40px;">
                    <li><strong>Model Architecture:</strong>
                    <ul style="padding-left: 40px;">
                        <li>2 input features (\(x_1\) and \(x_2\))</li>
                        <li>1 hidden layer with ReLU activation (adjustable number of units)</li>
                        <li>1 output unit with sigmoid activation for binary classification</li>
                    </ul>
                    </li>
                    <li><strong>Forward Pass:</strong> 
                    The network computes predictions by applying matrix operations and non-linear activations. Selecting a 
                    demo point shows a step-by-step computation.
                    </li>
                    <li><strong>Training:</strong>
                    The network is trained using <strong>mini-batch gradient descent with backpropagation</strong> to minimize 
                    binary cross-entropy loss. Each iteration uses a small, randomly sampled subset of the training data to 
                    update weights.
                    <ul style="padding-left: 40px;">
                        <li>Faster and more stable than full-batch training</li>
                        <li>Helps escape flat regions and saddle points</li>
                        <li>More closely mirrors how real-world neural networks are trained</li>
                    </ul>
                    </li>
                    <li><strong>Training Optimizations:</strong>
                    <ul style="padding-left: 40px;">
                        <li>Dynamic learning rate adjustment</li>
                        <li><strong>Gradient clipping</strong>: Prevents instability due to the <strong>exploding gradients</strong> by scaling them when their norm exceeds a threshold.</li>
                        <li>Early stopping when performance stabilizes</li>
                        <li>\(\ell_2\) regularization (λ) to reduce overfitting</li>
                    </ul>
                    </li>
                    <li><strong>Visualizations:</strong>
                    <ul style="padding-left: 40px;">
                        <li>Color-coded data points for training and test sets</li>
                        <li>Decision boundary (green) shows where prediction = 0.5</li>
                        <li>Probability contours reveal model confidence</li>
                        <li>Dynamic network graph and forward pass breakdown</li>
                    </ul>
                    </li>
                </ul>
                <br>
                <h2>Try Adjusting:</h2>
                <ul style="padding-left: 40px;">
                    <li><strong>Hidden Units:</strong> More neurons allow for more complex decision boundaries</li>
                    <li><strong>Regularization (λ):</strong> Helps prevent overfitting by discouraging large weights</li>
                    <li><strong>Learning Rate:</strong> Controls how quickly the model updates</li>
                    <li><strong>Max Iterations:</strong> Sets how long the training runs before stopping</li>
                </ul>
            </section>

            <section id="GPU" class="section-content">
  <h2>Development of Deep Learning</h2>

  <p>
    The modern revolution in deep learning has been driven not only by algorithmic advances, but also by dramatic improvements in hardware—especially the rise of 
    <strong>graphics processing units (GPUs)</strong>. Originally designed to accelerate <strong>matrix-vector computations</strong> for real-time rendering in 
    video games, GPUs turned out to be ideally suited for the linear algebra operations at the heart of neural networks.
  </p>

  <p>
    In the early 2010s, researchers discovered that GPUs could speed up deep learning training by orders of magnitude compared to traditional CPUs. This enabled the 
    training of large neural networks on <strong>large labeled datasets</strong>, like ImageNet, which led to breakthroughs in computer vision, <strong>speech recognition</strong> 
    (converting spoken language to text), and broader <strong>natural language processing (NLP)</strong> tasks such as translation, summarization, and question answering. 
    Today, GPUs are a core component in AI research and development, alongside other fields such as scientific computing, complex simulations, and even cryptocurrency mining.
  </p>

  <p>
    Zooming out further, GPUs themselves rely on foundational advances in <strong>semiconductor</strong> technology. Semiconductors are materials whose conductivity can 
    be precisely controlled, making them the backbone of all modern electronics—from GPUs and CPUs to memory chips and mobile devices. By using advanced fabrication 
    techniques and nanometer-scale engineering, manufacturers can pack billions of <strong>transistors</strong> (the basic units of computation) onto a single chip.
    This density of computation enables the incredible power of today's hardware and fuels the era of <strong>foundation models</strong>, including <strong>large language models (LLMs)</strong> 
    such as GPT and BERT.
  </p>


</section>


        </div>
        <script src="/js/main.js"></script> 
        <script src="/js/intro_nn.js"></script>
    </body>
</html>
    
    <footer>
    <div class="footer-content">
        <div class="footer-about">
            <h3>About MATH-CS COMPASS</h3>
            <p>Bridging the gap between pure mathematics and computer science applications.</p>
            <div class="contact-section">
                <strong><a href="https://docs.google.com/forms/d/e/1FAIpQLSf3_RNkegREvfCEIQznn6SCaf-iB5c-Doxn7Ymuu78Uf-PBjg/viewform" target="_blank">
                    Submit Your Questions & Feedback <i class="fas fa-external-link-alt"></i>
                </a></strong>
            </div>
        </div>
        <div class="footer-links">
            <h3>Quick Links</h3>
            <ul>
                
                <li><a href="../../../index.html">Home</a></li>
                <li>
                <a href="../Linear_algebra/linear_algebra.html">
                    <span class="nav-number">I</span>
                    <span>Linear Algebra to Algebraic Foundations</span>
                </a>
                </li>
                <li>
                <a href="../Calculus/calculus.html">
                    <span class="nav-number">II</span>
                    <span>Calculus to Optimization & Analysis</span>
                </a>
                </li>
                <li>
                <a href="../Probability/probability.html">
                    <span class="nav-number">III</span>
                    <span>Probability & Statistics</span>
                </a>
                </li>
                <li>
                <a href="../Discrete/discrete_math.html">
                    <span class="nav-number">IV</span>
                    <span>Discrete Mathematics & Algorithms</span>
                </a>
                </li>
                <li>
                <a href="../Machine_learning/ml.html">
                    <span class="nav-number">V</span>
                    <span>Machine Learning</span>
                </a>
                </li>
            </ul>
        </div>
        <div class="footer-social">
            <h3>Connect With Me</h3>
            <p>
                <a href="https://x.com/MathCSCompass?t=Zi8nyfzszeFw8QizbJwnNg&s=09" target="_blank">
                    <i class="fab fa-x-twitter"></i> X (Twitter)
                </a>
            </p>
            <p>
                <a href="https://www.facebook.com/share/15uPaBWgQN/" target="_blank">
                    <i class="fab fa-facebook"></i> Facebook
                </a>
            </p>
      
            <h3>MATH-CS COMPASS App</h3>
            <div class="app-download-footer">
                <a href="https://play.google.com/store/apps/details?id=io.github.yusukey01.twa" target="_blank" rel="noopener">
                    <img src="../../../images/google-play-badge.png" 
                        alt="Get it on Google Play" 
                        class="google-play-badge">
                </a>
            </div>
            
        </div>

    </div>
    <div class="footer-bottom">
        <p>&copy; 2024-2025 MATH-CS COMPASS. All rights reserved.</p>
    </div>
</footer>
    <script src="../../../js/main.js?v=1.0.2"></script>
    <script src="../../../js/search.js?v=1.0.2"></script>
    

    <button id="go-to-top-btn" class="go-to-top-btn" title="Go to top">
        <i class="fas fa-arrow-up"></i>
    </button>
    <script>
        // Register the service worker
        if ('serviceWorker' in navigator) {
          window.addEventListener('load', () => {
            navigator.serviceWorker.register('/js/service-worker.js')
              .then(registration => {
                console.log('ServiceWorker registration successful with scope: ', registration.scope);
              })
              .catch(error => {
                console.log('ServiceWorker registration failed: ', error);
              });
          });
        }
      </script>
</body>
</html>