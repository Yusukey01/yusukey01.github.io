<!DOCTYPE html>
<html>
    <head> 
        <title>Trace</title>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <link rel="stylesheet" href="../styles.css"> 
    </head>
    <body> 
        <h1>Trace</h1>
        <blockquote>
            The <strong>trace</strong> of an \(n \times n\) matrix \(A\) is the sum of the diagonal entries of \(A\) and
            denoted by \(\text{tr }(A)\).
            \[\text{tr }(A) =\sum_{k=1}^n a_{kk} \]
            We list some properties of the trace. 
            <ol>
                <li>\(\text{tr }(cA) = c (\text{tr }A) \quad , c \in \mathbb{R}\)</li>
                <li>\(\text{tr }(A+B) = \text{tr }A + \text{tr }B  \quad , B \in \mathbb{R}^{n \times n}\)</li>
                <li>\(\text{tr }A^T = \text{tr }A \)</li> <br>
                <li>\(\text{tr }(A^TB) = \sum_{i=1}^m\sum_{j=1}^n a_{ij}b_{ij}\)</li> <br>
                <li>\(\text{tr }(AB) = \text{tr }(BA) \)</li>
                Note: \(A\) can be \(m \times n\) and  \(B\) can be \(n \times m\). More generally, <br>
                \(\text{tr }(ABC) = \text{tr }(CAB) = \text{tr }(BCA) \): <strong>cyclic permutation property</strong> <br><br>
                <li>\(\text{tr }A =\sum_{i=1}^n \lambda_i \) where \(\lambda_1, \cdots, \lambda_n\) are the eigenvalues of \(A\)</li>
            </ol>
            For example, consider a quadratic form \(x^TAx \in \mathbb{R}\). <a href="symmetry.html"><strong> Check: Part 9</strong></a>
            <br>Let \(x\) be a vector in \(\mathbb{R}^n\) by Property 1 and 5, 
            \[x^TAx = \text{tr }(x^TAx) = \text{tr }(xx^TA) = (x^Tx)\text{tr }(A)\]
            <br><br>
            Property 4 is actually the definition of the <strong>inner product</strong> for matrices. 
            <a href="orthogonality.html"><strong>Check: Part 7 </strong></a>
            <br>We call it the <strong>Frobenius inner product</strong>.  For matrices \(A, B \in \mathbb{R}^{m \times n}\),
            \[\left\langle A, B \right\rangle_F = \text{tr }(A^TB) = \sum_{i=1}^m\sum_{j=1}^n a_{ij}b_{ij}\]
            Also, like the Euclidean(\(l_2\)) norm \(\|v\|_2 = \sqrt{v \cdot v} = v^Tv\) of vector, we define 
            the <strong>Frobenius norm</strong> of a "matrix" \(A\):
            \[\| A \|_F = \sqrt{\left\langle A, A \right\rangle_F} = \sqrt{\text{tr }(A^TA)}\] 
        </blockquote>

        <h1>Normes</h1>
        <blockquote>
            Norms play a key role in <strong>normalization</strong> and <strong>regularization</strong>. Depending on the
            application, we choose an appropriate norm. In machine learning, norms are fundational for training stability of models. 
            <br>Norms are used to scale data to a "common" range. It ensures that every single feature contributes
            "equally" to the training of the model and reduces sensitivity to the scale of each feature. This
            <strong>normalization</strong> process is important in the "distance(= norm)" based models such as 
            <strong>support vector machines(SVMs)</strong> and <strong>\(k\)-nearest neighbor(k-NN)</strong>. 
            Also, <strong>regularization</strong> process uses norms to "penalize" the model complexity, which avoids 
            overfitting and then helps the model <strong>generalize</strong> to new data.

            <br><br>Another popular norm of a matrix is the <strong>nuclear norm(trace norm)</strong>. 
            <br>Given a matrix \(A \in \mathbb{R}^{m \times n}\), then the nuclear norm of \(A\) is defined as the sum of its
            <strong>singular values</strong>.
            \[\| A\|_{\ast} = \sum_{i} \sigma_i \geq 0\]
            <div class="theorem">
                <span class="theorem-title">Theorem 1:</span>  
                Let a matrix \(A = U\Sigma V^T\) by SVD. Then
                \[\| A\|_{\ast} = \sum_{i} \sigma_i = \text{tr }(\Sigma) = \text{tr }(\sqrt{A^TA})\].
            </div>
            <div class="proof">
                <span class="proof-title">Proof:</span>
                Suppose \(A = U\Sigma V^T\) by the singular value decomposition. 
                Since\(A^TA = (V\Sigma U^T)(U\Sigma V^T) = V\Sigma^2 V^T\), 
                \[\sqrt{A^TA} = V\Sigma V^T\]
                Then by the cyclic permutation property of trace, 
                \[\text{tr }(\sqrt{A^TA}) = \text{tr }(V\Sigma V^T) = \text{tr }(\Sigma V^T V) = \text{tr }(\Sigma ) \]
            </div>
            <br>
            Also, the <strong>induced norm</strong> of \(A\) is defined as
            \[\| A \|_p = \max_{x \neq 0} \frac{\| Ax \|_p}{\| x \|_p} = \max_{\| x \| =1} \| Ax \|_P \tag{1}\]
            Typically, when \(p =2\), we call it the <strong>spectral norm</strong> of \(A\), which is just
            the largest singular value of \(A\).
            \[\| A \|_2 = \max_{i} \sigma_i  = \sqrt{\lambda_{\max} (A^TA)}\] 
            where \(\lambda_{max}\) is the largest eigenvalue of \(A\). 
            <br>The spectral norm is commonly used to control the <strong>Lipschiz continuity</strong> of
            functions in the <strong>neural network</strong> in order to prevent <strong>vanishing or exploding
            gradients</strong>.
            <br><br>
            \(\| x \|_p \) in (1) is called the <strong>\(p\)-norm</strong> of the vector \(x\). The Euclidean norm is 
            a specific case of \(p\)-norm where \(p =2\). In general, 
            \[\| x \|_p = (\sum_{i =1} ^n |x_i|^p)^{\frac{1}{p}} \quad , p \geq 1\]
            When \(p =1\), we call it <strong>Manhattan norm</strong>, which is commonly used in the grid-based
            environments. Also, if we need <strong>sparsity</strong> in models such as Lasso regression, the Manhattan
            norm is used as a regularizer. 
            <br>Moreover, when \(p\) approachies \(\infty\), we define the <strong>maximum norm</strong>: 
            \[\| x \|_\infty = \lim_{p \to \infty} \| x \|_p = \max_{i} |x_i|\]
            In numerical analysis, the maximum norm is used for "worst-case" error analysis. Similarly,
            in machine learning, it is used when we want to minimize the worst error rather than the total
            error across sample deta.
        </blockquote>

        <h1>Intorduction to Metric Spaces</h1>
        <blockquote>
            Remember, in <a href="vectorspaces.html"><strong>Part 5</strong></a>, we learned about vector spaces and 
            norms, which measure "distance" between points(vectors) in \(\mathbb{R}^n\). Although this concept may seem
            "general" as it extends beyond the familiar 3D space, the vector space with norms is actually a specific
            case of a more general mathematical structure called a <strong>metric space</strong>. As an introduction to
            <strong>mathematical analysis</strong>, we define the metric space \((M, d)\) as follows. <br><br>
            Suppose \(M\) is a set and \(d\) is a real function defined on the Cartesian product
            \(M \times M\). Then \(d\) is called a <strong>metric</strong> on \(M\) if and only if <br>
            \(\forall a, b, c \in M\),
            <ol>
                <li>Positivity: \(d(a,b) \geq 0\) with equality iff \(a = b\)</li>
                <li>Symmetry: \(d(a,b) = d(b,a)\)</li>
                <li>The triangle inequality \(d(a,b) \leq d(a,c) + d(c, b)\) </li>
            </ol>
            Often we just say "the matric space \(M\)" if the metric \(d\) is obvious.
            <br>Note: Suppose \(X\) and \(Y\) are sets. Then <strong>Cartesian product </strong>of \(X\) and \(Y\)
            is a set of "ordered" pair 
            \[\{(x, y) | x \in X, \, y\in Y\}\] 
            <br><br>
            Now, you can see that a vector space with a norm d(x,y) = \(\|x-y\|\)is a metric space. We call it 
            a <strong>normed vector space</strong>. Particularly, in linear algebra, we have worked within
            the framework of <strong>inner product spaces</strong> whose norm is defined by the inner product
            \[d(u, v) = \|u-v\|= \sqrt{\langle u-v, u-v \rangle} \]
            (The most familiar examle can be <strong>Euclidean Space</strong> \(\mathbb{R}^n\).)
            Also, when we use matrix norms such as the Frobenius norm, someone might call the space 
            <strong>matrix space</strong> even though mathematically, matrices form a vector space. 

        </blockquote>




        <a href="../index.html">Back to Home </a>
        <br> <a href="linear_algebra.html">Back to Linear Algebra </a>
    </body>
</html>