<!DOCTYPE html>
<html>
    <head> 
        <title>Linear Transformation</title>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <link rel="stylesheet" href="../styles.css"> 
    </head>
    <body> 
        <h1>Linear Transformation</h1>
        <blockquote>
            A transformation (or mapping) \(T: \mathbb{R}^n  \rightarrow \mathbb{R}^m \) is <strong> linear</strong> if<br> 
            \(\forall u, v \in\mathbb{R}^m,\) and any scalars \(c\), the following two properties hold: 
            \[T(u+v) = T(u) +T(v) \]
            \[T(cu) = cT(u)\]
           <br>
           For example, consider a matrix \(A \in \mathbb{R}^{m \times 3}\) and a vector \(x \in \mathbb{R}^3 \). 
           Then, the matrix-vector product \(Ax\) is a linear transformation \(T: x \mapsto Ax\).  
           To verify this, let \(u, v \in \mathbb{R}^3 \) and \(c\) be a scalar. Let's check the two conditions: 
           <br><br>Vector addition:
           \[A(u+v) = 
                \begin{bmatrix} a_1 & a_2 & a_3 \end{bmatrix}
                \begin{bmatrix} u_1 + v_1 \\ u_2 + v_3 \\ u_3 + v_3 \end{bmatrix}
                = (u_1 + v_1)a_1 + (u_2 + v_2)a_2 + (u_3 + v_3)a_3
                = (u_1a_1 + u_2a_2 + u_3a_3) + (v_1a_1 + v_2a_2 + v_3a_3)
                = Au + Av
           \]
           <br>Scalar multiplication:
           \[A(cu) = 
                \begin{bmatrix} a_1 & a_2 & a_3 \end{bmatrix}
                \begin{bmatrix} cu_1\\ cu_2 \\ cu_3 \end{bmatrix}
                = c(u_1a_1)+c(u_2a_2)+c(u_3a_3)
                = c(u_1a_1 + u_2a_2 + u_3a_3)
                = c(Au).
           \]
           <br>In general, the operations of vector addition and scalar multiplication are preserved 
           under linear transformations. 
           <br>In addition, if a transformation \(T\) is linear, then 
           \[T(0)=0\]
           Also, for any scalar \(a, b\) and \(u, v\) is in the domain of \(T\), 
           \[T(au + bv)=aT(u)+bT(v)\]
           <br><br>
           You probably learned following concepts somewhere in terms of functions \(f(x): \mathbb{R} \to  \mathbb{R}\). 
           Now, we would like to extend the definition to higher dimensions.
           <br><br>
           A mapping \(T: \mathbb{R}^n \to  \mathbb{R}^m\) is said to be <strong>onto(surjective)</strong> \(\mathbb{R}^m\) if
           \[\forall b \in \mathbb{R}^m,  \exists \, x \in \mathbb{R}^n \text{s.t } T(x) = b.\]  
           Equivalently, the range of \(T\) (the set of all outputs) is equal to the codomain \(\mathbb{R}^m\). 
           <div class="theorem">
                <span class="theorem-title">Theorem 1:</span>
                In a matrix transformation(<strong>"linear"</strong> transformation) \(T: \mathbb{R}^n  \rightarrow \mathbb{R}^m \), 
                <p style="text-align: center;">\(T\) maps \(\mathbb{R}^n\) onto \(\mathbb{R}^m \) iff the columns of a matrix \(A\) <strong>span</strong> \(\mathbb{R}^m\).</p>
           </div><br>
           Note: This means that for each \(b \in \mathbb{R}^m\), the equation \(Ax =b \) has at lest one solution.
           <br><br>
           A mapping \(T: \mathbb{R}^n \to  \mathbb{R}^m\) is said to be <strong>one-to-one(injective)</strong> if
           \[\forall b \in \mathbb{R}^m,  \forall u, v \in \mathbb{R}^n,  T(u) = T(v) = b \Rightarrow u = v. \]
           Equivalently, for each \(b \in \mathbb{R}^m\), \(T(x) = b\) has either a unique solution or no solution at all.
           <div class="theorem">
                <span class="theorem-title">Theorem 2:</span>
                In a matrix transformation(<strong>"linear"</strong> transformation) \(T: \mathbb{R}^n  \rightarrow \mathbb{R}^m \),
                <p style="text-align: center;">\(T\) is one-to-one iff the columns of a matrix \(A\) are <strong>linearly independent</strong>
                    (or, the homogeneous equation \(Ax =0\) has only the trivial solution).</p>
           </div>
           Note: A linear transformation \(T\) is one-to-one iff \(\, T(x)=0\) has only the trivial solution(\(x = 0\)).<br>
           Since \(T\) is linear, \(T(0)=0\) and \( \forall u, v \in \mathbb{R}^n\),
           \[T(u-v)=T(u)-T(v) =b -b =0\]
        </blockquote>

        <h1>Linearity in Mathematics</h1>
        <blockquote>
           Essentially, we are not only tallking about vectors and matrices. The linear transformation is one of the most fundamental idea in mathematics. 
           You have seen this essential conceptA multiple times outside Linear Algebra. Let's briefly observe some examples.
           <br><br>
           A function \(f(x) = mx\) is linear transformation \(f:\mathbb{R} \rightarrow \mathbb{R}\).
           \[f(ax+by)=m(ax+by)=a(mx)+b(my) = af(x) + bf(y)\]
           Note: the grapth of fanctions that have linearity must pass through the origin 
           since lienar transformations must satisfy \(T(0) =0\).
           <br>
           If you have just started to learn calculus, you might know that
           \[\frac{d}{dx}(aX(t)+bY(t)) =a\frac{d}{dx}(X(t))+b\frac{d}{dx}(Y(t)).\]
           For example, 
           \[\frac{d}{dx}(5x^3 +4x^2) = 15x^2 + 8x  =5\frac{d}{dx}(x^3)+4\frac{d}{dx}(x^2)\]
           so the differential is just a <strong>linear operator</strong>. (actually, the integration too.)
           <br><br>
           Have you learned statistics? The expected value also has linearity. For any random variables \(X\) and \(Y\), and constants \(a\) and \(b\)
           \[\mathbb{E}[aX+bY]=a\mathbb{E}[X]+b\mathbb{E}[X].\]
           You can see the <strong>linearity</strong> everywhere in mathematics and this is why "linear" algebra is poweful.
        </blockquote>

        <h1>Matrix Multiplication</h1>
        <blockquote>
            Suppose \(A\) is an \(m \times n\) matrix and \(B\) is an \(n \times p\) matrix , then the product \(AB\)
            is the \(m \times p\) matrix 
            \[
            AB = 
            \begin{bmatrix} Ab_1 & Ab_2 & \cdots & Ab_p \end{bmatrix} 
            \]
            where \(b_1, b_2, \cdots, b_p\) are columns of \(B\).
            <br><br>
            So, each column of \(AB\) is a linear combination of the columns of A with weights from the corresponding 
            column of \(B\). Let's see an example: 
            \[
            AB = 
            \begin{bmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \\ 7 & 8 & 9 \end{bmatrix}
            \begin{bmatrix} -1 & -2 \\ 0 & -3 \\ -4 & 0 \end{bmatrix}
            =
            \begin{bmatrix} (-1+0-12) & (-2-6+0) \\ (-4+0-24) & (-8-15+0) \\ (-7+0-36) & (-14-24+0) \end{bmatrix}
            =
            \begin{bmatrix} -13 & -8 \\ -28 & -23 \\ -43 & -38 \end{bmatrix}
            \]
            You can see the \((i,j)\)-entry in \(AB\) is 
            \[(AB)_{ij} = \sum_{k=1}^n a_{ik}b_{kj}.\]
            Also, note that in the example, \(BA\) is <strong>NOT</strong> defined because the nummber of columns of \(B\) does not
            match the number of rows of \(A\). This implies in general, <strong>\(AB \neq BA\)</strong> even the size of matrices match each other.
            Similarly, \(AB = AC\) does NOT guarantee \(B = C\), and \(AB = 0\) does NOT imply \(A=0\) or \(B=0\) in general.
            <br><br>
            Why is not matrix multiplication like just entrywise computation such as sums and scalar 
            multiples of matrices? The key idea is a linear transformation. 
            <strong>The matrix multiplication is composition of linear transformations.</strong>
            Let's say a vector \(x\) is multiplied by a matrix \(B\), which means it maps \(x\) into the new vector
            \(Bx\). Next, a matrix \(A\) multiplies the vector \(Bx\). Then we get the resulting vector \(A(Bx)\). 
            Since conposition of "functions" is associative, 
            \[(AB)x = A(Bx)\]
            which allows us to calculate \(Bx\) (matrix \(\times\) vector) first, instead of \(AB\) (matrix \(\times\) matrix). 
            This is significant in the numerical computation. For example, if the size of matrices A, B, C, D, and E are one million by one million
            and we want to compute \(ABCDEx\), where \(x\) is a column vector, we should calculate it from right most side\(Ex\) that is a new column "vector"
            then we can avoid huge matrix multiplications. In the end, \(A(BCDEx)\) is also just a matrix-vector product.
                 
        </blockquote> 

        <a href="../index.html">Back to Home </a>
        <br> <a href="linear_algebra.html">Back to Linear Algebra </a>
    </body>
</html>