<!DOCTYPE html>
<html>
    <head> 
        <title>Orthogonality</title>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <link rel="stylesheet" href="../styles.css"> 
        <meta name="viewport" content="width=device-width, initial-scale=1">
    </head>
    <body> 
        <h1>Inner Product & Orthogonality in \(\mathbb{R}^n\) </h1>
        <blockquote>
            The <strong>inner product</strong> of two vectors \(u, v \in \mathbb{R}^n \) is a scalar defined as:
            \[
            u \cdot v = u^Tv = u_1v_1 + \cdots + u_nv_n.
            \]
            <br>
            To extend the concept of "length" to higher dimensions, we define the <strong>norm</strong> of 
            a vector \(v \in \mathbb{R}^n\) as the nonnegative scalar:
            \[
            \|v\| = \sqrt{v \cdot v}.
            \]
            This is also called <strong>Euclidean(\(L_2\)) norm</strong>.
            <br>
            A vector with a length of 1 is called a <strong>unit vector</strong>, represented as 
            \[
            u = \frac{v}{\|v\|}
            \]
            The unit vector maintains the "direction" of the original vector. This process is known as <strong>normalization</strong>, which
            is widely used in applications like machine learning.
            <br>
            Now we can define the <strong>distance</strong> between two vectors
            \(u, v \in \mathbb{R}^n \) as
            \[
            \text{dist }(u, v) = \|u - v\|.
            \] 
            This is known as the <strong>Euclidean distance</strong>, which measures the difference or similarity between data points.
            <br>
            To explore orthogonality geometrically, consider the squared distances:
            \[\begin{align*}
            \|u - v\|^2 &= (u-v) \cdot (u-v) \\\\
            &= u \cdots (u-v) + (-v) \cdot (u - v) \\\\
            &= \|u\|^2 - 2u \cdot v + \|v\|^2
            \end{align*}
            \] 
            \[
            \begin{align*}
            \|u - (-v)\|^2 &= (u+v) \cdot (u+v) \\\\
            &= u \cdots (u+v) + (v) \cdot (u + v) \\\\
            &= \|u\|^2 + 2u \cdot v + \|v\|^2
            \end{align*}
            \]
            (Note: \(dist(u, v) \text{ and } dist(u, -v)\) are perpendicular iff they are the same.)
            Thus, if \(u \cdot v = 0\), the terms resemble the Pythagorean theorem:
            \[
            \|u  +v \|^2 = \|u\|^2 + \|v\|^2
            \]
            <br>
            We define the general orthogonality:
            <br>
            Two vectors \(u, v \in \mathbb{R}^n \) are <strong>orthogonal</strong> to each other if their inner product satisfies
            \[u \cdot v =0.\]
            <br>
            If a vector \(v\) is orthogonal to <strong>every</strong> vector in \(W \subseteq \mathbb{R}^n\), then the vector \(v\) is said to be
            orthogonal to \(W\). Also, the set of all vectors that are orthogonal to \(W\) is called the <strong>orthogonal complement</strong>
            of \(W\), denoted by \(W^{\perp}\).
            <br><br>
            This orthogonality provides a useful relationship between the column space and null space of a matrix:
            <div class="theorem">
                <span class="theorem-title">Theorem 1:</span>   
                Let \(A \in \mathbb{R}^{m \times n}\). Then, 
                \[(\text{Row } A)^{\perp} = \text{Nul } A \quad \text{and} \quad (\text{Col } A)^{\perp} = \text{Nul } A^T\] 
            </div>
            <div class="proof">
                <span class="proof-title">Proof:</span>
                If \(x \in \text{Nul } A\), then \(Ax =0\), which means that \(x\) is orthogonal to each row of \(A\) 
                because the inner product of each row of \(A\) and \(x\) are zero. Then, since the rows of \(A\) span the row space of \(A\),
                \(x\) is orthogonal to \(\text{Row } A \). Also, if \(x\) is orthogonal to \(\text{Row } A \), then \(x\) is orthogonal every row of \(A\)
                and thus \(Ax =0\), or \(x \in \text{Nul } A\). Therefore, \((\text{Row } A)^{\perp} = \text{Nul } A \).
                <br>Next, since the first equation is true for any matrix, we consider for \(A^T\). Since \(\text{Row } A^T = \text{Col } A\), 
                we conclude that \((\text{Col } A)^{\perp} = \text{Nul } A^T \). 
            </div>
        </blockquote>

        <h1>The Orthogonal Set</h1>
        <blockquote>
            A set of vectors \(\{v_1, v_2, \cdots, v_p\}\) in \(\mathbb{R}^n\)  is called an <strong>orthogonal set</strong>
            if \(v_i \cdot v_j =0\), \(i \neq j\). Additionally, an <strong>orthogonal basis</strong> for \(W \subseteq \mathbb{R}^n\) is
            a basis for \(W\) that forms an orthogonal set. 
            <div class="theorem">
                <span class="theorem-title">Theorem 2:</span>    
                Let \(\{v_1, v_2, \cdots, v_p\}\) be an orthogonal basis for \(W \subseteq \mathbb{R}^n\). For each
                \(y \in W\), the weights in the linear combination \(y = c_1v_1 + \cdots + c_pv_p\) are given by
                \[c_j = \frac{y \cdot v_j}{v_j \cdot v_j} \qquad (j = 1, \cdots, p)\]
            </div>
            <div class="proof">
                <span class="proof-title">Proof:</span>
                Since \(v_1\) is orthogonal to all other vectors in the set, 
                \
                [y \cdot v_1 = (c_1v_1 + \cdots + c_pv_p) \cdot v_1 
                                = c_1(v_1 \cdot v_1) + \cdots + c_p(v_p \cdot v_1)
                                = c_1(v_1 \cdot v_1).
                \]
                By definition of the basis, \(v_1\) is nonzero, \(v_1 \cdot v_1\) must be nonzero, and 
                then we can solve this equation for \(c_1\). Similarly, we can complute \(y \cdot v_j\) 
                and solve for \(c_j \quad (j = 2, \cdots, p)\).
            </div>
            A set \(\{u_1, \cdots, u_p\}\) is said to be an <strong>orthonormal set</strong> if it is an orthogonal set of unit vectors. If \(W\)
            is the subspace spanned by such a set, then \(\{u_1, \cdots, u_p\}\)  is an <strong>orthonormal basis</strong> for \(W\). 
            Matrices with orthonormal columns are central to many practical applications due to their unique properties. 
            <div class="theorem">
                <span class="theorem-title">Theorem 3:</span>
                An matrix \(U \in \mathbb{R}^{m \times n} \) has orthonormal columns iff \(U^TU = I\).   
            </div>
            <div class="proof">
                <span class="proof-title">Proof:</span>
                \[
                 U^TU = \begin{bmatrix} u_1^T \\ u_2^T \\  \vdots \\ u_n^T \end{bmatrix}
                        \begin{bmatrix} u_1 & u_2 & \cdots & u_n \end{bmatrix}
                        = \begin{bmatrix} u_1^Tu_1 & u_1^Tu_2 & \cdots & u_1^Tu_n \\
                                        u_2^Tu_1 & u_2^Tu_2 & \cdots & u_2^Tu_n \\
                                        \vdots   &  \vdots  & \ddots & \vdots \\
                                        u_n^Tu_1 & u_n^Tu_2 & \cdots & u_n^Tu_n 
                        \end{bmatrix}
                \]
                The columns of \(U\) are orthogonal iff 
                \[
                u_i^Tu_j = 0, i \neq j \quad (i, j = 1, \cdots, n)
                \]
                and, the columns of \(U\) have unit length iff
                \[
                u_i^Tu_i = 1, (i = 1, \cdots, n).
                \] 
                Thus, \(U^TU = I\) and the converse is also true.
            </div>
            <div class="theorem">
                <span class="theorem-title">Theorem 4:</span>
                Let \(U \in \mathbb{R}^{m \times n} \) with orthonormal columns, and let \(x, y \in \mathbb{R}^n\).
                Then
                    <ol>
                        <li>\(\| Ux \| = \| x \| \)</li>
                        <li>\((Ux) \cdot (Uy) = x \cdot y \)</li>
                        <li>\((Ux) \cdot (Uy) = 0\) iff \(x \cdot y = 0\)</li>
                    </ol>
            </div>
            <div class="proof">
                <span class="proof-title">Proof:</span>
                For property 1, 
                \[
                 \| Ux \|^2 = (Ux)^T(Ux) = x^TU^TUx = x^TIx = x^Tx \Longrightarrow \| Ux \| = \| x \|
                 \]
                For property 2, 
                \[
                (Ux) \cdot (Uy) = (Ux)^T(Uy) = x^TU^TUy = x^TIy = x^Ty = x \cdot y \tag{1}
                \]
                Note: We can also prove properties 1 and 3 from the equation (1).   
            </div>
            These results are particularly significant for <strong>square</strong> matrices.
            we define a square invertible matrix \(U\) as an <strong>orthogonal matrix</strong> if 
            \[U^{-1} = U^T.\]
            By Theorem 3, this matrix \(U\) has <strong>orthonormal</strong> columns(The name is misnomer...). 
            Notably, the rows of \(U\) form an orthonormal basis of \(\mathbb{R}^n\).
            <br><br>
            For example, the 2D rotation matrix  is an orthogonal matrix. 
            \[
            R_{\theta} = \begin{bmatrix} \cos\theta & - \sin\theta \\ \sin\theta & \cos\theta \end{bmatrix}
            \]
            Verify:
            \[
            R_{\theta}^TR_{\theta} = R_{\theta}^{-1}R_{\theta}
                                    = \begin{bmatrix} \cos\theta &  \sin\theta \\ -\sin\theta & \cos\theta \end{bmatrix}
                                        \begin{bmatrix} \cos\theta & - \sin\theta \\ \sin\theta & \cos\theta \end{bmatrix}
                                    =  \begin{bmatrix} \cos^2 \theta + \sin^2 \theta & 0 \\ 0 & \sin^2 \theta + \cos^2 \theta \end{bmatrix}
                                    = I
            \]
            Also, \[\det R_{\theta} = \cos^2 \theta + \sin^2 \theta = 1\] 
            This property generalizes to any orthogonal matrix \(U\):
            \[
            1 = \det I = \det ( U^TU )= \det U^T \det U = \det U \det U = (\det U)^2
            \]
            (Note: Since \(U\) is a square matrix, \(\det U^T = \det U\).)
            <br>Thus, for any orthogonal matrix \(U\),
            \[
            \det U = \pm 1
            \]
            Lastly, consider the eigenvalues of eigenvalues of \(R_{\theta}\).
            \[
            \det (U - \lambda ) = (\cos\theta - \lambda)^2 - \sin^2 \theta = \lambda^2 -2 \lambda \cos\theta +1
            \]
            Solve the characteristic equation:
            \[
            \lambda^2 -2 \lambda\cos\theta +1 = 0 
            \Longrightarrow
            \lambda = \frac{2\cos\theta \pm 2\sqrt{(\cos^2\theta -1)}i}{2} = \cos\theta \pm i \sin\theta
            \]
            Then, 
            \[
            |\lambda | = 1
            \]
             
            <br>Let \(x\) be the nonzero eigenvector correspoding to the eigenvalue of the orthogonal matrix \(U\), then 
            \[Ux = \lambda x  \Longrightarrow \| Ux \| = |\lambda | \| x \| \Longrightarrow \| x \| = |\lambda | \| x \| \]
            Since \(\| x \| \neq 0 \), we conclude that \(|\lambda | = 1\ \).
            <br>
            (Note: \(\| Ux \| =  \| x \|\) is also true in the case \(x\) is a <strong>complex vector</strong>. For complex vectors, an orthogonal matrix
            generalizes to a <strong>unitary matrix</strong> s.t. \(U^*U = I\) where \(U^*\) is the <strong>conjugate transpose</strong>
            of \(U\). Then you can prove this fact like we did in Theorem 4. So, the orthogonal matrix is just a special case of 
            a <strong>"U"</strong>nitary matrix with real entries.)
        </blockquote>

        <h1>The Orthogonal Projection</h1>
        <blockquote>
            Given a nonzero vector \(u \in \mathbb{R}^n\). We often need to decompose a vector \(y\in \mathbb{R}^n\)
            into two vectors: 
            \[
            y= \hat{y} + z \tag{2}
            \] 
            where \(\hat{y} = \alpha u\) is a multiple of \(u\) for some scalar \(\alpha\) and the vector \(z\) is orthogonal to \(u\).
            <br>
            For any scalar \(\alpha\), let \(z = y - \alpha u\) so that (2) holds. Then \(y - \hat{y}\) is 
            <strong>orthogonal</strong> to \(u\) iff
            \[
            (y - \alpha u) \cdot u = y \cdot u - \alpha (u \cdot u) = 0
            \]
            \[
            \Longrightarrow \alpha  = \frac{y \cdot u}{u \cdot u}
            \]
            Thus, the <strong>orthogonal projection of \(y\) onto \(u\) </strong> is
            \[
            \hat{y} = \frac{y \cdot u}{u \cdot u} u
            \]
            <div class="theorem">
                <span class="theorem-title">Theorem 5: Orthogonal Decomposition</span>
                Let \(W \subseteq \mathbb{R}^n\). Then \(\forall y \in \mathbb{R}^n\) can be written uniquely in the form
                \[y= \hat{y} + z\]
                where \(\hat{y} \in W\) and \(z \in W^{\perp}\). If \(\{u_1, \cdots, u_p\}\) is any orthogonal basis of \(W\),
                then 
                \[
                \hat{y} = \frac{y \cdot u_1}{u_1 \cdot u_1} u_1 + \cdots + \frac{y \cdot u_p}{u_p \cdot u_p} u_p \tag{3}
                \] 
                and \[z = y - \hat{y}.\]
                Note: (3) is called the <strong>orthogonal projection of \(y\) onto \(W\) </strong> denoted by \(\text{proj}_W y\).
            </div>
            <div class="proof">
                <span class="proof-title">Proof:</span>
                Let \(\{u_1, \cdots, u_p\}\) is any orthogonal basis of \(W\) and 
                \(\hat{y} = \frac{y \cdot u_1}{u_1 \cdot u_1} u_1 + \cdots + \frac{y \cdot u_p}{u_p \cdot u_p} u_p\).
                Since \(\hat{y}\) is a linear combination of \(u_1, \cdots u_p\), \(\quad \hat{y} \in W\). 
                <br>Let \(z = y - \hat{y}\). Because \(u_i\) is orthogonal to each \(u_j\) in the basis for \(W\) with \(i \neq j\),
                \[
                z \cdot u_i = (y -\hat{y}) \cdot u_i = y \cdot u_i - (\frac{y \cdot u_i}{u_i \cdot u_i})u_i \cdot u_i - 0 - \cdots - 0
                \]
                \[= y \cdot u_i - y \cdot u_i = 0\]
                Hence \(z\) is orthogonal to each \(u_i\) in the basis for \(W\). Therefore, \(z\) is orthogonal to every vector in \(W\).
                In other words, \(z \in W^{\perp}\).
                <br>Next, suppose \(y\) can also be written as \(y = \hat{y_1} + z_1\) where \(y_1 \in W \text{ and } z_1 \in W^{\perp}\).
                Then \[\hat{y} +z = \hat{y_1} + z_1 \Longrightarrow \hat{y} - \hat{y_1} = z_1 - z \]
                This equation implies that the vector \((\hat{y} - \hat{y_1})\) is in both \(W\) and \(W^{\perp}\). Thus,
                since the only vector that can be  in both \(W\) and \(W^{\perp}\) is the zero vector, \((\hat{y} - \hat{y_1}) = 0\). 
                Therefore, \(\hat{y} = \hat{y_1} \text{ and } z = z_1\).
            </div>

            The orthogonal projection of \(y\) onto \(W\) is <strong>the best approximation to \(y\) by elements of W</strong>. 
            This is significant in practical applications. 
            <div class="theorem">
                <span class="theorem-title">Theorem 6: Best Approximation</span>
                Let \(W \subseteq \mathbb{R}^n\) and \(y \in \mathbb{R}^n\). Also let \(\hat{y}\) be 
                the orthogonal projection of \(y\) onto \(W\). Then \(\hat{y}\) is the closest point in \(W\) to \(y\) 
                such that  \[\| y - \hat{y} \| < \| y - v \| \tag{4}\] for all \(v \in W\) distinct from \(\hat{y}\).
            </div>
            
            <div class="proof">
                <span class="proof-title">Proof:</span>
                Choose \(v \in W\) distinct from \(\hat{y} \in W\). Then \((\hat{y} - v) \in W\). By the Orthogonal Decomposition
                Theorem, \((y - \hat{y})\) is orthogonal to \(W\). Moreover,  \((y - \hat{y})\) is orthogonal to \((\hat{y} - v) \in W\)
                Consider \[y - v = (y - \hat{y}) + (\hat{y} -v). \] 
                By the Pythagorean Theorem, 
                \[\|y - v\|^2 = \|y - \hat{y}\|^2 + \|\hat{y} -v \|^2  > 0\]
                Thus, we can conclude \(\| y - \hat{y} \| < \| y - v \|\).
            </div>
            The formula (3) becomes much simpler when the basis for \(W\) is an <strong>orthonormal</strong> set as follows:
            <br>If \(\{u_1, \cdots, u_p\}\) is an orthonormal basis for \(W \subseteq \mathbb{R}^n\), then
            \[
            \text{proj}_W y = (y \cdot u_1)u_1 + \cdots + (y \cdot u_p)u_p
            \]
            If \(U = [u_1, \cdots, u_p]\), then
            \[
            \forall y \in \mathbb{R}^n, \qquad \text{proj}_W y = UU^Ty
            \]
            because each weight can be written as \(u_1^Ty, \cdots, u_p^Ty\), which are the entries of \(U^Ty\).
            <br>
            Besides having orthonormal columns, if \(U\) is an \(n \times n\) <strong>square</strong> matrix
            (\(U\) is the <strong>orthogonal</strong> matrix),
            \[
            \forall y \in \mathbb{R}^n, \qquad \text{proj}_W y = UU^Ty = Iy = y.
            \]
            This implies the column space \(W\) is equal to \(\mathbb{R}^n\).
        </blockquote>

        <h1>Gram-Schmidt Algorithm</h1>
        <blockquote>
            The <Strong>Gram-Schmidt algorithm</Strong> generates an orthogonal (or orthonormal) basis for <strong>any</strong>
            nonzero subspace of \(\mathbb{R}^n\). Let us explore this process through a simple example. 
            <br><br>
            Consider the basis \({x_1, x_2, x_3}\) for a nonzero subspace \(W\) of \(\mathbb{R}^3\), where:
            \(x_1 = \begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix}, \quad 
            x_2 =\begin{bmatrix} 0 \\ 1 \\ 0 \end{bmatrix}, \text{ and } 
            x_3 = \begin{bmatrix} 0 \\ 0 \\ 1\end{bmatrix} \)
            <br>
            Let \(v_1 = x_1\) and \(W_1 = Span \{v_1\}\). Then 
            \[
            v_2 = x_2 - \text{proj}_{W_1} x_2 
                = x_2 - \frac{x_2 \cdot v_1} {v_1 \cdot v_1}v_1
                = \begin{bmatrix} 0 \\ 1 \\ 0 \end{bmatrix} - \frac{2}{14}\begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix}
                = \begin{bmatrix} \frac{-1}{7} \\ \frac{5}{7} \\ \frac{-3}{7} \end{bmatrix}
            \]
            <br>Next, \(W_2 = Span \{v_1, v_2\}\) and then
            \[
            v_3 = x_3 - \text{proj}_{W_2} x_3 
                = x_3 - \frac{x_3 \cdot v_1} {v_1 \cdot v_1}v_1 - \frac{x_3 \cdot v_2} {v_2 \cdot v_2}v_2
                = \begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix} - \frac{3}{14}\begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix}
                    - \frac{\frac{-3}{7}}{\frac{5}{7}}\begin{bmatrix} \frac{-1}{7} \\ \frac{5}{7} \\ \frac{-3}{7} \end{bmatrix}
                = \begin{bmatrix} \frac{-3}{10} \\ 0 \\ \frac{1}{10} \end{bmatrix}
            \]
            Thus, the orthogonal basis for \(W\) is \(W_3 = Span\{v_1, v_2, v_3\}\). 
            <br>
            Finally, we construct an orthonormal basis for \(W\) by nomalizing each \(v_k\):
            \[
            \{u_1 = \begin{bmatrix} \frac{1}{\sqrt{14}} \\ \frac{2}{\sqrt{14}} \\ \frac{3}{\sqrt{14}} \end{bmatrix}, \quad
              u_2 = \begin{bmatrix} \frac{-1}{\sqrt{35}} \\ \frac{5}{\sqrt{35}} \\ \frac{-3}{\sqrt{35}} \end{bmatrix}, \quad
              u_3 = \begin{bmatrix} \frac{-3}{\sqrt{10}} \\ \ 0 \\ \frac{1}{\sqrt{10}} \end{bmatrix} \}
            \]
            The Gram-Schmidt algorithm at the \(k\)th step:
            \[
            v_k = x_k - \sum_{j=1}^{k-1} (x_k \cdot u_j)u_j  ,\qquad u_k = \frac{v_k}{\| v_k \|}
            \]
        </blockquote>

        <h1>QR Factorization</h1>
        <blockquote>
            The <strong>QR factorization</strong> is a practical decomposition of matrices with linearly independent columns.
            It is widely used in numerical applications, such as eigenvalue estimation.
            (Note:Solving the characteristic polynomial is often impossible.)
            <br><br>
            Let \(A\) be an \(m \times n\) matrix with linearly independent columns. Then A can be factorized as
            \(A = QR\), where \(Q\) is an \(m \times n\) matrix with columns forming an orthonormal basis for \(Col A\)
            and \(R\) is an \(n \times n\) upper triangular invertible matrix with positive diagonal entries. 
            <br><br>
            Consider \(A = \begin{bmatrix} 1 & 0 & 0 \\ 2 & 1 & 0 \\ 3 & 0 & 1 \end{bmatrix}\) again. 
            By the GS algorithm, we have already obtained \(Q\) from the previous example:
            \[Q = \begin{bmatrix} \frac{1}{\sqrt{14}} &  \frac{-1}{\sqrt{35}} & \frac{-3}{\sqrt{10}} \\ 
                                \frac{2}{\sqrt{14}} &  \frac{5}{\sqrt{35}} &  0  \\
                                \frac{3}{\sqrt{14}} &  \frac{-3}{\sqrt{35}} & \frac{1}{\sqrt{10}}
                \end{bmatrix}\]
            Since the columns of \(Q\) are orthonormal, \(Q^TQ=I\). Then, \(Q^TA = Q^TQR = IR = R\).
            \[
            R = Q^TA = \begin{bmatrix}
                        \frac{1}{\sqrt{14}}  & \frac{2}{\sqrt{14}}  & \frac{3}{\sqrt{14}} \\
                        \frac{-1}{\sqrt{35}} & \frac{5}{\sqrt{35}} & \frac{-3}{\sqrt{35}} \\
                        \frac{-3}{\sqrt{10}} & \ 0                 & \frac{1}{\sqrt{10}} 
                        \end{bmatrix}
                        \begin{bmatrix} 1 & 0 & 0 \\ 2 & 1 & 0 \\ 3 & 0 & 1 \end{bmatrix}
                    = \begin{bmatrix}
                        \frac{14}{\sqrt{14}} & \frac{2}{\sqrt{14}} & \frac{3}{\sqrt{14}} \\
                        0                   & \frac{5}{\sqrt{35}} & \frac{-3}{\sqrt{35}} \\
                        0                   &  0                  & \frac{1}{\sqrt{10}} 
                        \end{bmatrix}
            \]  
        </blockquote>

        <a href="../index.html">Back to Home </a>
        <br> <a href="linear_algebra.html">Back to Linear Algebra </a>
    </body>
</html>