<!DOCTYPE html>
<html>
    <head> 
        <title>Stochastic Matrix</title>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <link rel="stylesheet" href="../styles.css"> 
        <meta name="viewport" content="width=device-width, initial-scale=1">
    </head>
    <body> 
        <h1>Stochastic Matrix</h1>
        <blockquote>
            A <strong>probability vector</strong> \(x \in \mathbb{R}^n\) is a vetor with nonnegative entries that add up 
            to 1, and A <strong>stochastic matrix</strong> \(P \in \mathbb{R}^{n \times n}\) is a square matrix whose columns 
            are probability vectors.
            <br><br>
            A <a href="../Probability/markov.html"><strong>Markov chain</strong></a> is a sequence of probability 
            vectors \(x_0, x_1, x_2, \cdots \) tohether with a stochastic matrix \(P\) 
            such that 
            \[
            x_1 = P x_0, \quad x_2 = P x_1, \quad x_3 = P x_2, \quad \cdots.
            \]
            So, the Markov chain is explained by the first-order difference equation:
            \[
            x_{k+1} = P x_k \quad \text{for } k = 0, 1, 2, \cdots.
            \]
            Here, \(x_k\) is called a <strong>state vector</strong> and we have:
            \[
            x_k = P^k x_0  \quad \text{for } k = 0, 1, 2, \cdots.
            \]

            <div class="proof">
                <span class="proof-title">Example: </span>
                Consider the following two states:
                <ul>
                    <li> State 1: a student is sick</li>
                    <li> State 2: a student is not sick</li>
                </ul>
                We obserbed an initial state distribution:
                \[
                x_0 = \begin{bmatrix} 0.1 \\ 0.9 \end{bmatrix}
                \]
                which means now, 10 % of students are 90 % are not.
                <br><br>
                Moreover, we assume the following conditions:
                <ul>
                    <li> 70% of sick students recover the next day and 30 % remain sick.</li>
                    <li> 5 % of not sick students become sick the next day and 95 % remain not sick</li>
                </ul>
                So, our stochastic matrix can be written as 
                \[
                P = \begin{bmatrix} 0.3 & 0.05 \\ 0.7 & 0.95 \end{bmatrix}.
                \]
                Then, 
                \[
                x_1 = P x_0 = \begin{bmatrix} 0.3 & 0.05 \\ 0.7 & 0.95 \end{bmatrix} \begin{bmatrix} 0.1 \\ 0.9 \end{bmatrix} = \begin{bmatrix} 0.075 \\ 0.925 \end{bmatrix}
                \]
                This means that on the next day, approximately 7.5 % students are expected to be sick and 92.5 % are not.
                <br><br>
                We can keep going this process:
                \[
                x_2 = P x_1 = \begin{bmatrix} 0.3 & 0.05 \\ 0.7 & 0.95 \end{bmatrix} \begin{bmatrix} 0.075 \\ 0.925 \end{bmatrix} = \begin{bmatrix} 0.06875 \\ 0.93125 \end{bmatrix}
                \]
                \[
                x_3 = \begin{bmatrix} 0.0671875 \\ 0.9328125 \end{bmatrix}
                \]
                and so on. 
            </div>
        </blockquote>

        <h1>Steady-state Vector</h1>
        <blockquote>
            A <strong>steady-state vector</strong> \(q\) for a stochastic matrix \(P\) is defined as 
            \[
            P q = q.
            \]
            In statistics, we also call it <strong>stationary distribution</strong>.
            <br><br>
            Indeed, the Markov chain, a sequence of vectors \(\{x_k : k = 1, 2, \cdots\}\) converges to the unique steady-state 
            vector \(q\) as \(k \to \infty\). Importantly, the initial state does not effect on the long-term behavior of the 
            Markov chain. 
            <br><br>
            In our example, 
            \[
            \begin{align*}
            & P q = q \\\\
            &\begin{bmatrix} 0.3 & 0.05 \\ 0.7 & 0.95 \end{bmatrix} \begin{bmatrix} q_1 \\ q_2 \end{bmatrix} = \begin{bmatrix} q_1 \\ q_2 \end{bmatrix}\\\\
            & q_1 + q_2 = 1 \\\\
            &\Longrightarrow q = \begin{bmatrix} \frac{1}{15} \\ \frac{14}{15} \end{bmatrix}
            \end{align*}
            \]
            which means that in the long run, about 6.67 % of the students will be sick and 93.33 % will be not sick.
            <br><br>
            The eigenvalues of the stochastic matrix \(P\) provide insight into how fast the Markov chain converges to 
            the stationary distribution. In our example: 
            \[
            \begin{align*}
            &\det(P - \lambda I) = 0 \\\\
            &\Longrightarrow \lambda^2 - 1.25\lambda + 0.25 = 0 \\\\
            &\Longrightarrow (\lambda -1)(\lambda -0.25) = 0 \\\\
            &\Longrightarrow \lambda_1 = 1, \quad \lambda_2 = 0.25.
            \end{align*}
            \]
            Here, \(\lambda_1 = 1\) always appears because the vector whoes all entries are 1 is always the eigenvector of \(P\).
            The error in the state distribution after \(n\) steps is dominated by the term \(\lambda_2^2 = (0.25)^2\). Thus, the 
            convergence rate of the Markov chain toward the stationary distribution is exponential, with each additional step reducing 
            the error roughly by a factor of \(0.25\).
        </blockquote>

        <a href="../../index.html">Back to Home </a>
        <br> <a href="linear_algebra.html">Back to Linear Algebra </a>
    </body>
</html>