<!DOCTYPE html>
<html>
    <head> 
        <title>Stochastic Matrix</title>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <link rel="stylesheet" href="../styles.css"> 
        <meta name="viewport" content="width=device-width, initial-scale=1">
    </head>
    <body> 
        <h1>Stochastic Matrix</h1>
        <blockquote>
            A <strong>probability vector</strong> \(x \in \mathbb{R}^n\) is a vetor with nonnegative entries that add up 
            to 1, and A <strong>stochastic matrix</strong>(or <strong>transition matrix</strong>) \(P \in \mathbb{R}^{n \times n}\) is 
            a square matrix whose "columns" are probability vectors. 
            <br><br>
            Note: In this page, we will use the <strong>column-stochastic matrix</strong>, but we can also use the 
            <strong>row-stochastic matrix</strong>. Essentially, the choice between a row-stochastic and a column-stochastic 
            matrix is a matter of convention and convenience, and both formulations are equivalent up to taking a transpose. 
            Using the row-stochastic matrix is often natural in Markov chains because each row directly tells 
            you the probabilities of transitioning from a given state to all other states. In linear algebra context, due to 
            the form of \(Ax = b\), the column-stochastic matrix can be chosen more often. (Also, many programming environments 
            default to column vectors.)
            <br><br>
            A <a href="../Probability/markov.html"><strong>Markov chain</strong></a> is a sequence of probability 
            vectors \(x_0, x_1, x_2, \cdots \) tohether with a stochastic matrix \(P\) 
            such that 
            \[
            x_1 = P x_0, \quad x_2 = P x_1, \quad x_3 = P x_2, \quad \cdots.
            \]
            So, the Markov chain is explained by the first-order difference equation:
            \[
            x_{k+1} = P x_k \quad \text{for } k = 0, 1, 2, \cdots.
            \]
            Here, \(x_k\) is called a <strong>state vector</strong> and we have:
            \[
            x_k = P^k x_0  \quad \text{for } k = 0, 1, 2, \cdots.
            \]

            <div class="proof">
                <span class="proof-title">Example: </span>
                Consider the following two states:
                <ul>
                    <li> State 1: a student is sick</li>
                    <li> State 2: a student is not sick</li>
                </ul>
                We obserbed an initial state distribution:
                \[
                x_0 = \begin{bmatrix} 0.1 \\ 0.9 \end{bmatrix}
                \]
                which means now, 10 % of students are 90 % are not.
                <br><br>
                Moreover, we assume the following conditions:
                <ul>
                    <li> 70% of sick students recover the next day and 30 % remain sick.</li>
                    <li> 5 % of not sick students become sick the next day and 95 % remain not sick</li>
                </ul>
                So, our stochastic matrix can be written as 
                \[
                P = \begin{bmatrix} 0.3 & 0.05 \\ 0.7 & 0.95 \end{bmatrix}.
                \]
                Then, 
                \[
                x_1 = P x_0 = \begin{bmatrix} 0.3 & 0.05 \\ 0.7 & 0.95 \end{bmatrix} \begin{bmatrix} 0.1 \\ 0.9 \end{bmatrix} = \begin{bmatrix} 0.075 \\ 0.925 \end{bmatrix}
                \]
                This means that on the next day, approximately 7.5 % students are expected to be sick and 92.5 % are not.
                <br><br>
                We can keep going this process:
                \[
                \begin{align*}
                &x_2 = P x_1 = \begin{bmatrix} 0.3 & 0.05 \\ 0.7 & 0.95 \end{bmatrix} \begin{bmatrix} 0.075 \\ 0.925 \end{bmatrix} = \begin{bmatrix} 0.06875 \\ 0.93125 \end{bmatrix} \\\\
                &x_3 = P x_2 = \begin{bmatrix} 0.3 & 0.05 \\ 0.7 & 0.95 \end{bmatrix} \begin{bmatrix} 0.06875 \\ 0.93125 \end{bmatrix} = \begin{bmatrix} 0.0671875 \\ 0.9328125 \end{bmatrix}
                \end{align*}
                \]
                and so on. 
            </div>
        </blockquote>

        <h1>Steady-state Vector</h1>
        <blockquote>
            A <strong>steady-state vector</strong> \(q\) for a stochastic matrix \(P\) is defined as 
            \[
            P q = q.
            \]
            In statistics, we also call it <strong>stationary distribution</strong>. 
            <br><br>
            The Markov chain, a sequence of vectors \(\{x_k : k = 1, 2, \cdots\}\) converges to the unique steady-state 
            vector \(q\) as \(k \to \infty\). Importantly, the initial state does not effect on the long-term behavior of the 
            Markov chain. The reason for having this property is related to the fact that the steady-state vector \(q\) is 
            an <strong>eigenvector</strong> of the transition matrix corresponding to the <strong>eigenvalue 1</strong>.
            <br><br>
            For any matrix \(A\), the <strong>spectral radius</strong> \(\rho(A)\) is defined by the maximum absolute value of its eigenvalues. It satisfies
            \[
            \rho(A) \leq \| A \|
            \]
            for any submultiplicative matrix norm \(\| \cdot \| \).
            <br><br>
            Remember, by definition, every column of a column-stochastic matrix sums to 1, which means the maximum absolute 
            column sum (<strong>1-norm</strong>, \(\| P \|_1\)) is always 1. Thus:
            \[
            \rho(P) \leq \| P \|_1 = 1
            \]
            (For the row-stochastic matrix, we use the <strong>infinity norm</strong> \(\| P \|_{\infty} =1\).)
            <br>
            Since we already know that 1 is an eigenvalue of \(P\), the spectral radius must be exactly 1, and no eigenvalue 
            can have greater than 1. 
            <br><br>
            Now, suppose \(P\) has a <strong>spectral decomposition</strong>(or a similar representation when not strictly 
            diagonalizable) such that:
            \[
            P = v_1 w_1^\top + \sum_{i = 2}^n \lambda_i v_i w_i^\top
            \]
            where \(v_1\) corresponds to the eigenvalue 1 and all other eigenvalues \(\lambda_i\) satisfy \(\|\lambda_i \| < 1\).
            <br><br>
            If we compute the \(n\)-step transition, we have:
            \[
            P^n = v_1 w_1^\top + \sum_{i = 2}^n \lambda_i^n v_i w_i^\top
            \]
            This means that the contribution from all components other than the one corresponding to \(\lambda_1 =1\) decays 
            exponentially fast. Therefore, the convergence of the state distribution to the stationary distribution is primarily due to 
            the spectral properties of the transition matrix.
            <br>
            <div class="proof">
                <span class="proof-title">Example: </span>
                In our example, 
                \[
                \begin{align*}
                & P q = q \\\\
                &\begin{bmatrix} 0.3 & 0.05 \\ 0.7 & 0.95 \end{bmatrix} \begin{bmatrix} q_1 \\ q_2 \end{bmatrix} = \begin{bmatrix} q_1 \\ q_2 \end{bmatrix}\\\\
                & q_1 + q_2 = 1 \\\\
                &\Longrightarrow q = \begin{bmatrix} \frac{1}{15} \\ \frac{14}{15} \end{bmatrix}
                \end{align*}
                \]
                which means that in the long run, about 6.67 % of the students will be sick and about 93.33 % will be not sick.
                <br><br>
                Find eigenvalues:
                \[
                \begin{align*}
                &\det(P - \lambda I) = 0 \\\\
                &\Longrightarrow \lambda^2 - 1.25\lambda + 0.25 = 0 \\\\
                &\Longrightarrow (\lambda -1)(\lambda -0.25) = 0 \\\\
                &\Longrightarrow \lambda_1 = 1, \quad \lambda_2 = 0.25.
                \end{align*}
                \]
                The error in the state distribution after \(n\) steps is dominated by the term \(\lambda_2^n = (0.25)^n\). Thus, the 
                convergence rate of the Markov chain toward the stationary distribution is exponential, with each additional step reducing 
                the error roughly by a factor of \(0.25\):
                <br>
                \[
                e_n \approx e_0 (0.25)^n
                \]
                where \(e_0\) is the initial error.
            </div>
        </blockquote>

        <a href="../../index.html">Back to Home </a>
        <br> <a href="linear_algebra.html">Back to Linear Algebra </a>
    </body>
</html>