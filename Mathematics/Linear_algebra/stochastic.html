---
layout: default
title: Stochastic Matrix
topic_id: linalg-13
level: detail
uses_math: true
uses_python: false
---
<!DOCTYPE html>
<html>
    <body>
        {% include learning_resource_schema.html topic_id=page.topic_id %}
        
        <div class="hero-section">
            <h1 class="webpage-name">Stochastic Matrix</h1>
        </div>

        {% include section_navigation.html %}

        <div class="topic-nav">
            <a href="#sto">Stochastic Matrix</a>
            <a href="#ssv">Steady-State Vector</a>
        </div> 

        <div class="container">  
           
            <section id="sto" class="section-content">
                <h2>Stochastic Matrix</h2>

                 <p>
                    Many real-world systems evolve probabilistically: a customer's next purchase depends on their current 
                    preferences, a web user's next click depends on the current page, and weather tomorrow depends on 
                    conditions today. To model such processes using linear algebra, we need matrices whose columns (or rows) 
                    represent probability distributions. These are called <strong>stochastic matrices</strong>, and they 
                    form the algebraic backbone of <strong>Markov chains</strong>.
                </p>

                <div class="theorem">
                    <span class="theorem-title">Definition: Stochastic Matrix & Probability Vector</span>
                    <p>
                        A <strong>probability vector</strong> \(x \in \mathbb{R}^n\) is a vector with nonnegative entries that sum 
                        to 1:
                        \[
                        x_i \geq 0 \text{ for all } i, \quad \sum_{i=1}^n x_i = 1.
                        \]
                        A <strong>stochastic matrix</strong> (or <strong>transition matrix</strong>) \(P \in \mathbb{R}^{n \times n}\) is 
                        a square matrix whose columns are probability vectors. That is, \(P_{ij} \geq 0\) for all \(i, j\), 
                        and \(\sum_{i=1}^n P_{ij} = 1\) for each column \(j\).
                    </p>
                </div>
                
                <p>
                    In this page, we will use the <strong>column-stochastic matrix</strong>, but we can also use the 
                    <strong>row-stochastic matrix</strong>. Essentially, the choice between a row-stochastic and a column-stochastic 
                    matrix is a matter of convention and convenience, and both formulations are equivalent up to taking a transpose. 
                    Using the row-stochastic matrix is often natural in Markov chains because each row directly tells 
                    you the probabilities of transitioning from a given state to all other states. In linear algebra context, due to 
                    the form of \(Ax = b\), the column-stochastic matrix can be chosen more often. (Also, many programming environments 
                    default to column vectors.)
                </p>

                <div class="insight-box">
                    <h3>Insight: Row vs Column Convention in ML Frameworks</h3>
                    <p>
                        In practice, most machine learning libraries (scikit-learn, PyTorch, TensorFlow) use 
                        <strong>row-stochastic</strong> convention, where each row of the transition matrix sums to 1. 
                        This is because data is typically stored with samples as rows. For instance, in a softmax 
                        layer of a neural network, the output for each input sample is a probability vector forming 
                        a row of the output matrix. When reading research papers, always check which convention is used — 
                        the mathematics is equivalent (related by transposition), but confusing the two leads to incorrect 
                        matrix multiplications.
                    </p>
                </div>
               
                <p>
                    Remember, a <a href="../Probability/markov.html"><strong>Markov chain</strong></a> is a sequence of probability 
                    vectors \(x_0, x_1, x_2, \cdots \) together with a stochastic matrix \(P\) 
                    such that 
                    \[
                    x_1 = P x_0, \quad x_2 = P x_1, \quad x_3 = P x_2, \quad \cdots.
                    \]
                    So, the Markov chain is explained by the first-order difference equation:
                    \[
                    x_{k+1} = P x_k \quad \text{for } k = 0, 1, 2, \cdots.
                    \]
                    Here, \(x_k\) is called a <strong>state vector</strong> and we have:
                    \[
                    x_k = P^k x_0  \quad \text{for } k = 0, 1, 2, \cdots.
                    \]
                </p>
                
                <div class="proof">
                    <span class="proof-title">Example: </span>
                    <p>
                        Consider the following two states:
                    </p>
                    <ul>
                        <li> State 1: a student is sick</li>
                        <li> State 2: a student is not sick</li>
                    </ul><br>
                    <p>
                        We observed an initial state distribution:
                        \[
                        x_0 = \begin{bmatrix} 0.1 \\ 0.9 \end{bmatrix}
                        \]
                        which means that currently, 10% of students are sick and 90% are not.
                    </p>
                    <p>
                        Moreover, we assume the following conditions:
                    </p>
                    <ul>
                        <li> 70% of sick students recover the next day, and 30% remain sick.</li>
                        <li> 5% of healthy students become sick the next day, and 95% remain healthy.</li>
                    </ul><br>
                    <p>
                        So, our stochastic matrix can be written as 
                        \[
                        P = \begin{bmatrix} 0.3 & 0.05 \\ 0.7 & 0.95 \end{bmatrix}.
                        \]
                        Then, 
                        \[
                        x_1 = P x_0 = \begin{bmatrix} 0.3 & 0.05 \\ 0.7 & 0.95 \end{bmatrix} \begin{bmatrix} 0.1 \\ 0.9 \end{bmatrix} = \begin{bmatrix} 0.075 \\ 0.925 \end{bmatrix}
                        \]
                        This means that on the next day, approximately 7.5% students are expected to be sick and 92.5% are not.
                    </p>
                    <p>
                        We can continue this process:
                        \[
                        \begin{align*}
                        &x_2 = P x_1 = \begin{bmatrix} 0.3 & 0.05 \\ 0.7 & 0.95 \end{bmatrix} \begin{bmatrix} 0.075 \\ 0.925 \end{bmatrix} = \begin{bmatrix} 0.06875 \\ 0.93125 \end{bmatrix} \\\\
                        &x_3 = P x_2 = \begin{bmatrix} 0.3 & 0.05 \\ 0.7 & 0.95 \end{bmatrix} \begin{bmatrix} 0.06875 \\ 0.93125 \end{bmatrix} = \begin{bmatrix} 0.0671875 \\ 0.9328125 \end{bmatrix}
                        \end{align*}
                        \]
                        and so on. 
                    </p>
                </div>

            </section>

            <section id="ssv" class="section-content">
                <h2>Steady-State Vector</h2>

                <div class="theorem">
                    <span class="theorem-title">Definition: Steady-State Vector</span>
                     <p>
                        A <strong>steady-state vector</strong> \(q\) for a stochastic matrix \(P\) is defined as 
                        \[
                        P q = q.
                        \]
                        In statistics, we also call it <strong>stationary distribution</strong>. 
                    </p>
                </div>
               
                <p>
                    The Markov chain, a sequence of vectors \(\{x_k : k = 1, 2, \cdots\}\) converges to the unique steady-state 
                    vector \(q\) as \(k \to \infty\). Importantly, the initial state does not effect on the long-term behavior of the 
                    Markov chain. 
                </p>

                <p>
                    In short, the reason why it happens is related to the fact that the steady-state vector \(q\) is 
                    an <strong>eigenvector</strong> of the stochastic matrix corresponding to the largest eigenvalue \(\lambda_1 = 1\). Let's 
                    dig a little deeper. 
                </p>

                <div class="theorem">
                    <span class="theorem-title">Definition: Spectral Radius</span>
                      <p>
                        For any matrix \(A\), the <strong>spectral radius</strong> \(\rho(A)\) is defined by the maximum absolute value 
                        of its eigenvalues. It satisfies
                        \[
                        \rho(A) \leq \| A \|
                        \]
                        for any submultiplicative matrix norm \(\| \cdot \| \).
                    </p>
                </div>
    
                <p>
                    Remember, by definition, every column of a column-stochastic matrix sums to 1, which means the maximum absolute 
                    column sum (<a href="trace.html"><strong>1-norm</strong></a>, \(\| P \|_1\)) is always 1. Thus:
                    \[
                    \rho(P) \leq \| P \|_1 = 1
                    \]
                    (For the row-stochastic matrix, we use the <strong>infinity norm</strong> \(\| P \|_{\infty} = 1\).)
                </p>
                <p>
                    Since we already know that 1 is an eigenvalue of \(P\), the spectral radius must be exactly 1, and no eigenvalue 
                    can have greater than 1. This means that as \(k \to \infty\), the contribution from all components other than the 
                    one corresponding to \(\lambda_1 = 1\) decays exponentially fast. Therefore, any initial distribution converges to the 
                    stationary distribution.
                </p>

                <div class="proof">
                    <span class="proof-title">Example: </span>
                    <p>
                        Revisiting our example, 
                        \[
                        \begin{align*}
                        & P q = q \\\\
                        &\begin{bmatrix} 0.3 & 0.05 \\ 0.7 & 0.95 \end{bmatrix} \begin{bmatrix} q_1 \\ q_2 \end{bmatrix} = \begin{bmatrix} q_1 \\ q_2 \end{bmatrix}\\\\
                        & q_1 + q_2 = 1 \\\\
                        &\Longrightarrow q = \begin{bmatrix} \frac{1}{15} \\ \frac{14}{15} \end{bmatrix}
                        \end{align*}
                        \]
                        which means that in the long run, about 6.67 % of the students will be sick and about 93.33 % will be not sick.
                    </p>
                    <p>
                        Find eigenvalues and corresponding eigenvectors:
                        \[
                        \begin{align*}
                        &\det(P - \lambda I) = 0 \\\\
                        &\Longrightarrow \lambda^2 - 1.25\lambda + 0.25 = 0 \\\\
                        &\Longrightarrow (\lambda -1)(\lambda -0.25) = 0 \\\\
                        &\Longrightarrow \lambda_1 = 1, \quad \lambda_2 = 0.25.
                        \end{align*}
                        \]
                        For \(\lambda_1 = 1\), solving \((P -I)v_1 = 0\), we obtain the corresponding eigenvector:
                        \[
                        v_1 = \begin{bmatrix} 1 \\ 14 \end{bmatrix}.
                        \]
                        (Scaling by \(\frac{1}{15}\), we can get the stationary distribution \(q = \frac{1}{15}v_1\)).
                    </p>
                    <p>
                        For \(\lambda_2 = 0.25\), solving \((P -0.25I)v_2 = 0\), we obtain the corresponding eigenvector:
                        \[
                        v_2 =  \begin{bmatrix} - 1 \\  1 \end{bmatrix}.
                        \]
                        So, 
                        \[
                        V = \begin{bmatrix} 1 & - 1 \\ 14 & 1 \end{bmatrix}, \quad V^{-1} = \frac{1}{15} \begin{bmatrix}  1 & 1 \\ -14 & 1 \end{bmatrix}, 
                        \]
                        and 
                        \[
                        D = \begin{bmatrix} \lambda_1 & 0\\ 0 & \lambda_2 \end{bmatrix} = \begin{bmatrix} 1 & 0 \\ 0 & 0.25 \end{bmatrix}
                        \]
                    </p>
                    <p>
                        Thus, the transition matrix \(P\) after \(k\) steps can be written as: 
                        \[
                        \begin{align*}
                        P^k &= V D^k V^{-1} \\\\
                            &= \begin{bmatrix} 1 & - 1 \\ 14 & 1 \end{bmatrix} 
                            \begin{bmatrix} 1^k & 0 \\ 0 & (0.25)^k \end{bmatrix} 
                            \frac{1}{15}\begin{bmatrix} 1 & 1 \\ -14 & 1 \end{bmatrix}.
                        \end{align*}
                        \]
                    </p>
                    <p>
                        The error in the state distribution after \(k\) steps is dominated by the term \(\lambda_2^k = (0.25)^k\). Thus, the 
                        convergence rate of the Markov chain toward the stationary distribution is exponential, with each additional step reducing 
                        the error roughly by a factor of \(0.25\):
                        
                        \[
                        e_k \approx e_0 (0.25)^k
                        \]
                        where \(e_0\) is the initial error.
                    </p>
                </div>

                 <p>
                    In this example, \(P\) is <a href="eigenvectors.html"><strong>diagonalizable</strong></a>, which allowed us 
                    to express \(P^k\) in closed form and read off the convergence rate directly from the eigenvalues. In general, 
                    stochastic matrices are not always diagonalizable, but the convergence result still holds under mild 
                    conditions (irreducibility and aperiodicity) via the Perron-Frobenius theorem.
                </p>

                <p>
                    A particularly well-behaved class of stochastic matrices arises when both the columns <em>and</em> 
                    the rows are probability vectors.
                </p>

                <div class="theorem">
                    <span class="theorem-title">Definition: Doubly Stochastic Matrix</span>
                     <p>
                        A nonnegative matrix \(A\) is said to be <strong>doubly stochastic</strong> if both the sum of each row 
                        and the sum of each column equal 1. That is, \(A\) is both row-stochastic and column-stochastic.
                    </p>
                </div>
                
                <div class="proof">
                    <span class="proof-title">Example: Doubly Stochastic Matrix (\(2 \times 2\))</span>
                    <p>
                        For \(n = 2\), the doubly stochastic constraint forces the matrix to be symmetric. 
                        Consider the following doubly stochastic matrix for \(0 < t \leq 1\):
                        \[
                        A = \begin{bmatrix} 1 - t & t \\  t & 1 - t \end{bmatrix}.
                        \]
                        The trace is \(\text{tr}(A) = 2 - 2t\). Since every stochastic matrix has eigenvalue 
                        \(\lambda_1 = 1\) with corresponding eigenvector \(v_1 = \begin{bmatrix} 1 \\ 1 \end{bmatrix}\), 
                        we can find the second eigenvalue from
                        \[
                        \lambda_1 + \lambda_2 = \text{tr}(A) = 2 - 2t \quad \Longrightarrow \quad \lambda_2 = 1 - 2t.
                        \]
                    </p>
                    <p>
                        Since \(A\) is symmetric and the two eigenvalues are distinct (for \(t \neq 0\)), the 
                        <a href="orthogonality.html">spectral theorem</a> guarantees that the eigenvectors are orthogonal. 
                        Thus \(v_2 = \begin{bmatrix} 1 \\ -1 \end{bmatrix}\), and the eigendecomposition is
                        \[
                        A = \frac{1}{2}\begin{bmatrix} 1 & 1 \\ 1 & -1 \end{bmatrix} 
                        \begin{bmatrix} 1  & 0 \\ 0 & 1-2t \end{bmatrix} 
                        \begin{bmatrix} 1 & 1 \\ 1 & -1 \end{bmatrix}.
                        \]
                    </p>
                    <p>
                        Note that the eigenvector matrix \(V = \begin{bmatrix} 1 & 1 \\ 1 & -1 \end{bmatrix}\) satisfies 
                        \(V^{-1} = \frac{1}{2}V\), since the columns are orthogonal with squared norm 2.
                    </p>
                </div>

                <div class="insight-box">
                    <h3>Insight: Doubly Stochastic Matrices in Machine Learning</h3>
                    <p>
                        Doubly stochastic matrices appear throughout machine learning. The <strong>Sinkhorn-Knopp algorithm</strong> 
                        iteratively normalizes rows and columns of a nonnegative matrix to produce a doubly stochastic 
                        matrix, and forms the basis of <strong>optimal transport</strong> computations used in Wasserstein 
                        GANs, domain adaptation, and distribution matching. In <strong>spectral clustering</strong>, the 
                        normalized graph Laplacian produces a doubly stochastic structure, and the steady-state distribution 
                        of a doubly stochastic matrix is always the uniform distribution \(q = \frac{1}{n}\mathbf{1}\) — a 
                        fact that simplifies the analysis of random walks on graphs.
                    </p>
                </div>
            </section>
        </div>
       
        <script src="/js/main.js"></script>
    </body>
</html>