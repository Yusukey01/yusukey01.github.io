---
layout: default
title: Woodbury Matrix Identity
topic_id: linalg-12
level: detail
uses_math: true
uses_python: false
---
<!DOCTYPE html>
<html>
    <body>
        {% include learning_resource_schema.html topic_id=page.topic_id %}
       
        <div class="hero-section">
            <h1 class="webpage-name">Woodbury Matrix Identity
            </h1>
        </div>

        {% include section_navigation.html %}

        <div class="topic-nav">
            <a href="#wmi">Woodbury Matrix Identity</a>
        </div> 

        <div class="container">  
           
            <section id="wmi" class="section-content">
                <h2>Woodbury Matrix Identity</h2>
                <p>
                    The <strong>Woodbury Matrix Identity</strong> is a fundamental result in matrix algebra that allows us to compute the 
                    inverse of a matrix after a <strong>rank-k update</strong>. In the fields of Machine Learning, Optimization, and 
                    Signal Processing, we often encounter situations where a large matrix \(A\) is modified by adding a product of 
                    smaller matrices \(UBV\). 
                </p>

                <div class="theorem">
                    <span class="theorem-title">Theorem 1: Woodbury matrix identity</span>  
                    <p>
                        \[
                        (A + UBV)^{-1} = A^{-1} - A^{-1} U (B^{-1} + VA^{-1}U)^{-1} VA^{-1}
                        \]
                        where \(A \in \mathbb{R}^{n \times n}\), \(U \in \mathbb{R}^{n \times k}\), 
                        \(V \in \mathbb{R}^{k \times n}\), and \(B \in \mathbb{R}^{k \times k}\).
                     </p>                 
                </div>

                <div class="proof">
                    <span class="proof-title">Proof:</span>
                    <p>
                        \[
                        \begin{align*}
                        &(A + UBV)\{A^{-1} - A^{-1} U (B^{-1} + VA^{-1}U)^{-1} VA^{-1}\} \\\\
                        &= I + UBVA^{-1} -U(B^{-1} + VA^{-1}U)^{-1}VA^{-1} -UBVA^{-1}U(B^{-1} + VA^{-1}U)^{-1}VA^{-1}\\\\
                        &= I + UBVA^{-1} -U \{(B^{-1} + VA^{-1}U)^{-1} + BVA^{-1}U(B^{-1} + VA^{-1}U)^{-1}\}VA^{-1} \\\\
                        &= I + UBVA^{-1} -U \{(I + BVA^{-1}U)(B^{-1} + VA^{-1}U)^{-1}\}VA^{-1} \\\\
                        &= I + UBVA^{-1} -U \{(BB^{-1} + BVA^{-1}U)(B^{-1} + VA^{-1}U)^{-1}\}VA^{-1} \\\\
                        &= I + UBVA^{-1} -UB \{(B^{-1} + VA^{-1}U)(B^{-1} + VA^{-1}U)^{-1}\}VA^{-1} \\\\
                        &= I + UBVA^{-1} -UBVA^{-1} \\\\
                        &= I 
                        \end{align*}
                        \]
                    </p>   
                </div>

                <div class="insight-box">
                    <h3>The "Efficiency" Perspective:</h3>
                    <p>
                         Directly inverting an \(n \times n\) matrix costs \(O(n^3)\) operations. However, if the update is "low-rank" (where \(k \ll n\)), 
                         the Woodbury identity allows us to update the inverse using only \(O(n^2)\) operations, provided we already know \(A^{-1}\). 
                         This is the mathematical backbone of <strong>online learning</strong> and <strong>recursive estimation</strong>.
                    </p>
                </div>

                <p>
                    When \(k = 1\), the matrices \(U\) and \(V\) reduce to vectors \(u\) and \(v\), and we obtain the <strong>Sherman-Morrison formula</strong>. 
                    This special case is widely used for rank-1 updates.
                </p>

                <div class="theorem">
                    <span class="theorem-title">Theorem 2: Sherman-Morrison formula</span> 
                    <p>
                        Suppose \(A \in \mathbb{R}^{n \times n}\) is invertible, and \(u, v \in \mathbb{R}^n\).
                        The matrix \((A + uv^T)\) is invertible if and only if \(1 + v^T A^{-1} u \neq 0\). In this case:
                        \[
                        (A + uv^T)^{-1} = A^{-1} - \frac{A^{-1} uv^T A^{-1}}{1 + v^T A^{-1} u}.
                        \]
                        Similarly, for a rank-1 reduction:
                        \[
                        (A - uv^T)^{-1} = A^{-1} + \frac{A^{-1} uv^T A^{-1}}{1 - v^T A^{-1} u}.
                        \]
                    </p>              
                </div>

                <div class="proof">
                    <span class="proof-title">Proof:</span>
                    
                    <p>
                        To derive the Sherman-Morrison formula, we treat the rank-1 update as a specific case of the Woodbury 
                        identity with the following parameters:
                    </p>
                    
                    <ul style="padding-left: 40px;">
                        <li>\(k = 1\) (The update space is 1-dimensional)</li>
                        <li>\(U = u \in \mathbb{R}^{n \times 1}\) (Column vector)</li>
                        <li>\(V = v^T \in \mathbb{R}^{1 \times n}\) (Row vector)</li>
                        <li>\(B = 1 \in \mathbb{R}^{1 \times 1}\) (Scalar)</li>
                    </ul>
                    
                    <p>
                        With these substitutions:
                        \[
                        \begin{align*}
                        UBV &= u \cdot 1 \cdot v^T = uv^T \\\\
                        B^{-1} &= 1^{-1} = 1 \\\\
                        VA^{-1}U &= v^T A^{-1} u \quad \text{(scalar)}
                        \end{align*}
                        \]
                        
                        Substituting into the Woodbury identity:
                        \[
                        \begin{align*}
                        (A + UBV)^{-1} &= A^{-1} - A^{-1} U (B^{-1} + VA^{-1}U)^{-1} VA^{-1} \\\\
                        (A + uv^T)^{-1} &= A^{-1} - A^{-1} u (1 + v^T A^{-1} u)^{-1} v^T A^{-1} \\\\
                                        &= A^{-1} - A^{-1} u \cdot \frac{1}{1 + v^T A^{-1} u} \cdot v^T A^{-1} \\\\
                                        &= A^{-1} - \frac{A^{-1} u v^T A^{-1}}{1 + v^T A^{-1} u}
                        \end{align*}
                        \]
                        This confirms that the rank-1 update requires only <strong>matrix-vector products</strong> and <strong>outer products</strong>, 
                        maintaining a computational complexity of \(O(n^2)\).
                    </p>
                </div>
                
            </section>
        </div>
        <script src="/js/main.js"></script>
    </body>
</html>