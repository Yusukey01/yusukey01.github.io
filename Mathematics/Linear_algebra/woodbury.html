---
layout: default
title: Efficient Matrix Updates
topic_id: linalg-12
level: detail
uses_math: true
uses_python: false
---
<!DOCTYPE html>
<html>
    <body>
        {% include learning_resource_schema.html topic_id=page.topic_id %}
       
        <div class="hero-section">
            <h1 class="webpage-name">Efficient Matrix Updates</h1>
        </div>

        {% include section_navigation.html %}

        <div class="topic-nav">
            <a href="#intro">Introduction</a>
            <a href="#wmi">Woodbury Matrix Identity</a>
            <a href="#determinant-lemma">Matrix Determinant Lemma</a>
            <a href="#bfgs-connection">Case Study: BFGS and Rank-2 Updates</a>
        </div> 

        <div class="container">  

            <section id="intro" class="section-content">
                <h2>Introduction</h2>
                <p>
                    The <strong>Woodbury Matrix Identity</strong> is a powerful tool in matrix algebra that allows us to compute the 
                    inverse of a matrix after it has been modified by a <strong>low-rank update</strong>. In many real-world applications, 
                    we don't just solve a system once; we update it as new information arrives.
                </p>

                <div class="insight-box">
                    <h3>The "Efficiency" Perspective:</h3>
                    <p>
                        Directly inverting an \(n \times n\) matrix from scratch typically costs \(O(n^3)\) operations. 
                        However, the Woodbury identity is designed for <strong>incremental updates</strong>. 
                        If we already possess the inverse \(A^{-1}\) and perform a "low-rank" update (where \(k \ll n\)), 
                        the cost to find the new inverse is reduced to approximately <strong>\(O(n^2 k)\)</strong> operations.
                    </p>
                    <p>
                        For a small, fixed \(k\) (such as a rank-1 update), this effectively shifts the computational complexity from cubic to 
                        quadratic. This efficiency is the mathematical backbone of <strong>online learning</strong>, <strong>Kalman filters</strong>, 
                        and <strong>recursive least squares (RLS)</strong>.
                    </p>
                </div>
            </section>
           
            <section id="wmi" class="section-content">
                <h2>Woodbury Matrix Identity</h2>
                <p>
                    The <strong>Woodbury Matrix Identity</strong> is a fundamental result in matrix algebra that allows us to compute the 
                    inverse of a matrix after a <strong>rank-k update</strong>. In the fields of Machine Learning, Optimization, and 
                    Signal Processing, we often encounter situations where a large matrix \(A\) is modified by adding a product of 
                    smaller matrices \(UBV\). 
                </p>

                <div class="theorem">
                    <span class="theorem-title">Theorem 1: Woodbury matrix identity</span>  
                    <p>
                        \[
                        (A + UBV)^{-1} = A^{-1} - A^{-1} U (B^{-1} + VA^{-1}U)^{-1} VA^{-1}
                        \]
                        where \(A \in \mathbb{R}^{n \times n}\), \(U \in \mathbb{R}^{n \times k}\), 
                        \(V \in \mathbb{R}^{k \times n}\), and \(B \in \mathbb{R}^{k \times k}\).
                     </p>                 
                </div>

                <div class="proof">
                    <span class="proof-title">Proof:</span>
                    <p>
                        \[
                        \begin{align*}
                        &(A + UBV)\{A^{-1} - A^{-1} U (B^{-1} + VA^{-1}U)^{-1} VA^{-1}\} \\\\
                        &= I + UBVA^{-1} -U(B^{-1} + VA^{-1}U)^{-1}VA^{-1} -UBVA^{-1}U(B^{-1} + VA^{-1}U)^{-1}VA^{-1}\\\\
                        &= I + UBVA^{-1} -U \{(B^{-1} + VA^{-1}U)^{-1} + BVA^{-1}U(B^{-1} + VA^{-1}U)^{-1}\}VA^{-1} \\\\
                        &= I + UBVA^{-1} -U \{(I + BVA^{-1}U)(B^{-1} + VA^{-1}U)^{-1}\}VA^{-1} \\\\
                        &= I + UBVA^{-1} -U \{(BB^{-1} + BVA^{-1}U)(B^{-1} + VA^{-1}U)^{-1}\}VA^{-1} \\\\
                        &= I + UBVA^{-1} -UB \{(B^{-1} + VA^{-1}U)(B^{-1} + VA^{-1}U)^{-1}\}VA^{-1} \\\\
                        &= I + UBVA^{-1} -UBVA^{-1} \\\\
                        &= I 
                        \end{align*}
                        \]
                    </p>   
                </div>

                <p>
                    When \(k = 1\), the matrices \(U\) and \(V\) reduce to vectors \(u\) and \(v\), and we obtain the <strong>Sherman-Morrison formula</strong>. 
                    This special case is widely used for rank-1 updates.
                </p>

                <div class="theorem">
                    <span class="theorem-title">Theorem 2: Sherman-Morrison formula</span> 
                    <p>
                        Suppose \(A \in \mathbb{R}^{n \times n}\) is invertible, and \(u, v \in \mathbb{R}^n\).
                        The matrix \((A + uv^T)\) is invertible if and only if \(1 + v^T A^{-1} u \neq 0\). In this case:
                        \[
                        (A + uv^T)^{-1} = A^{-1} - \frac{A^{-1} uv^T A^{-1}}{1 + v^T A^{-1} u}.
                        \]
                        Similarly, for a rank-1 reduction:
                        \[
                        (A - uv^T)^{-1} = A^{-1} + \frac{A^{-1} uv^T A^{-1}}{1 - v^T A^{-1} u}.
                        \]
                    </p>              
                </div>

                <div class="proof">
                    <span class="proof-title">Proof:</span>
                    
                    <p>
                        To derive the Sherman-Morrison formula, we treat the rank-1 update as a specific case of the Woodbury 
                        identity with the following parameters:
                    </p>
                    
                    <ul style="padding-left: 40px;">
                        <li>\(k = 1\) (The update space is 1-dimensional)</li>
                        <li>\(U = u \in \mathbb{R}^{n \times 1}\) (Column vector)</li>
                        <li>\(V = v^T \in \mathbb{R}^{1 \times n}\) (Row vector)</li>
                        <li>\(B = 1 \in \mathbb{R}^{1 \times 1}\) (Scalar)</li>
                    </ul>
                    
                    <p>
                        With these substitutions:
                        \[
                        \begin{align*}
                        UBV &= u \cdot 1 \cdot v^T = uv^T \\\\
                        B^{-1} &= 1^{-1} = 1 \\\\
                        VA^{-1}U &= v^T A^{-1} u \quad \text{(scalar)}
                        \end{align*}
                        \]
                        
                        Substituting into the Woodbury identity:
                        \[
                        \begin{align*}
                        (A + UBV)^{-1} &= A^{-1} - A^{-1} U (B^{-1} + VA^{-1}U)^{-1} VA^{-1} \\\\
                        (A + uv^T)^{-1} &= A^{-1} - A^{-1} u (1 + v^T A^{-1} u)^{-1} v^T A^{-1} \\\\
                                        &= A^{-1} - A^{-1} u \cdot \frac{1}{1 + v^T A^{-1} u} \cdot v^T A^{-1} \\\\
                                        &= A^{-1} - \frac{A^{-1} u v^T A^{-1}}{1 + v^T A^{-1} u}
                        \end{align*}
                        \]
                        This confirms that the rank-1 update requires only <strong>matrix-vector products</strong> and <strong>outer products</strong>, 
                        maintaining a computational complexity of \(O(n^2)\).
                    </p>
                </div>  
            </section>

            <section id="determinant-lemma" class="section-content">
                <h2>Matrix Determinant Lemma</h2>
                <p>
                    The <strong>Matrix Determinant Lemma</strong> provides a formula for computing the determinant of a matrix after a low-rank update. Just like the Woodbury identity for inverses, this lemma is essential for maintaining computational efficiency, particularly when evaluating likelihoods in statistical models.
                </p>

                <div class="theorem">
                    <span class="theorem-title">Theorem 3: Matrix Determinant Lemma</span> 
                    <p>
                        Suppose \(A \in \mathbb{R}^{n \times n}\) and \(B \in \mathbb{R}^{k \times k}\) are invertible matrices, and \(U \in \mathbb{R}^{n \times k}, V \in \mathbb{R}^{k \times n}\). 
                        The determinant of the updated matrix is given by:
                        \[
                        \det(A + UBV) = \det(B^{-1} + VA^{-1}U) \det(B) \det(A)
                        \]
                        In the special case of a <strong>rank-1 update</strong> (\(k=1\)) where \(u, v \in \mathbb{R}^n\):
                        \[ 
                        \det(A + uv^\top) = (1 + v^\top A^{-1} u) \det(A) 
                        \]
                    </p>
                </div>

                <div class="insight-box">
                    <h3>Computational & Numerical Insight:</h3>
                    <p>
                        <strong>Efficiency:</strong> For \(k=1\), the term \((1 + v^\top A^{-1} u)\) is a scalar. If \(A^{-1}\) and \(\det(A)\) are known, the update requires only \(O(n^2)\) operations (matrix-vector multiplication), avoiding the \(O(n^3)\) cost of a full determinant calculation.
                    </p>
                    <p>
                        <strong>Numerical Stability:</strong> In practical machine learning applications (e.g., Gaussian Processes), we often work with the <strong>log-determinant</strong> to prevent numerical overflow/underflow:
                        \[ \ln |\det(A + uv^\top)| = \ln |\det(A)| + \ln |1 + v^\top A^{-1} u| \]
                    </p>
                </div>
            </section>

            <section id="bfgs-connection" class="section-content">
                <h2>Case Study: BFGS and Rank-2 Updates</h2>
                <p>
                    The <strong>BFGS algorithm</strong> (the most popular quasi-Newton method) provides a perfect example of the Woodbury 
                    identity in action. It updates the inverse Hessian approximation \(C_{t} \approx B_{t}^{-1}\) by adding a 
                    <strong>symmetric rank-2 matrix</strong>.
                </p>

                <div class="insight-box">
                    <h3>Mathematical Rigor: Why Sherman-Morrison?</h3>
                    <p>
                        The BFGS update formula is defined as:
                        \[ 
                        B_{t+1} = B_t + \frac{y_t y_t^\top}{y_t^\top s_t} - \frac{B_t s_t s_t^\top B_t}{s_t^\top B_t s_t} 
                        \]
                        By applying the Sherman-Morrison formula <strong>recursively</strong> (first to the rank-1 addition, then to the rank-1 subtraction), 
                        we derive the update for the inverse \(C_{t+1}\).
                    </p>
                    <ul style="padding-left: 25px;">
                        <li><strong>Preservation of Structure:</strong><br>
                            This process ensures that if \(C_t\) is symmetric and positive definite, \(C_{t+1}\) remains so (provided \(y_t^\top s_t > 0\)).
                        </li>
                        <li><strong>Efficiency:</strong><br>
                            It transforms an \(O(n^3)\) matrix inversion into a sequence of \(O(n^2)\) outer products and matrix-vector multiplications.
                        </li>
                    </ul>
                </div>         

                <p>
                    <a href="../Calculus/newton.html#BFGS">
                        <strong>View Full Derivation: From Rank-1 to BFGS Rank-2 â†’ Section II Part 8</strong>
                    </a>
                </p>
            </section>
        </div>
        <script src="/js/main.js"></script>
    </body>
</html>