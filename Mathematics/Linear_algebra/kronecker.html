---
layout: default
title: Kronecker Product
topic_id: linalg-11
level: detail
uses_math: true
uses_python: false
---
<!DOCTYPE html>
<html>
    <body> 
        {% include learning_resource_schema.html topic_id=page.topic_id %}
        
        <div class="hero-section">
            <h1 class="webpage-name">Kronecker Product & Tensor</h1>
        </div>

        {% include section_navigation.html %}

        <div class="topic-nav">
            <a href="#vec">Vectorization</a>
            <a href="#k_pro">Kronecker Product</a>
            <a href="#tensor">Tensor</a>
        </div> 

        <div class="container">  
           
            <section id="vec" class="section-content">
                <h2>Vectorization</h2>

                <p>
                    When working with matrices in optimization and statistics, we often need to treat a matrix as a single vector. 
                    For instance, to take derivatives with respect to all entries simultaneously or to express matrix equations as 
                    standard linear systems. The <strong>vectorization</strong> operation provides exactly this conversion.
                </p>

                <div class="theorem">
                    <span class="theorem-title">Definition: Vectorization</span>
                    <p>
                        The <strong>vectorization</strong> of a matrix \(A \in \mathbb{R}^{m \times n}\), denoted \(\text{vec}(A)\), 
                        stacks the columns of \(A\) into a single <strong>column vector </strong> in \(\mathbb{R}^{mn}\):
                        \[
                        \text{vec}(A) = \begin{bmatrix} a_1 \\ a_2 \\ \vdots \\ a_n \end{bmatrix}
                        \]
                        where \(a_j \in \mathbb{R}^m\) is the \(j\)th column of \(A\).
                    </p>
                </div>
               
                <p>
                    For example, let \(x \in \mathbb{R}^n \), and consider outer product \(xx^\top  \in \mathbb{R}^{n \times n}\).
                    \[
                    \begin{align*}
                    \text{vec }(xx^\top) &= \begin{bmatrix}x_1x_1 \\ x_2x_1 \\ \vdots \\ x_nx_1 \\ x_1x_2 \\ x_2x_2 \\ \vdots \\ x_nx_2 \\ \vdots \\
                                    x_1x_n \\ x_2x_n \\ \vdots \\ x_nx_n 
                                    \end{bmatrix} \\ \\
                                &= x \otimes x
                    \end{align*} 
                    \]
                </p>
                <p>
                    The notation \(\otimes\) denotes the <strong>Kronecker product</strong>, which we define formally in the next section. 
                    The identity \(\text{vec}(xx^\top) = x \otimes x\) is a special case of a more general relationship between 
                    vectorization and the Kronecker product.
                </p>

                <div class="insight-box">
                    <h3>Insight: Vectorization in Machine Learning</h3>
                    <p>
                        Vectorization is not merely a notational convenience — it is the algebraic basis for efficient 
                        computation in deep learning frameworks. Libraries such as PyTorch and TensorFlow internally reshape 
                        multi-dimensional parameter tensors into flat vectors when computing gradients.
                    </p>
                    <p>
                        Moreover, the identity \(\text{vec}(ABC) = (C^\top \otimes A)\,\text{vec}(B)\) is essential for solving 
                        <strong>Lyapunov stability problems</strong>. In control theory and AI safety, we use this to prove that 
                        a system (like a neural network or a robot) will always converge to a stable state without "blowing up," 
                        by transforming complex matrix equations into solvable linear systems.
                    </p>
                </div>
                
               <p>
                Conversely, we can reshape a vector back into a matrix. However, the result depends on the 
                <strong>memory layout convention</strong> used — an important practical distinction when moving between 
                mathematical formulas and code.
            </p>

            <div class="proof">
                <span class="proof-title">Example: Row-Major vs Column-Major Order</span>
                <p>
                    Consider the vector 
                    \[
                    a = \begin{bmatrix} 1 & 2 & 3 & 4 & 5 & 6 \end{bmatrix}^\top.
                    \]
                    Reshaping \(a\) into a \(2 \times 3\) matrix yields different results depending on the convention:
                </p>
                <p>
                    <strong>Row-major order</strong> (used by C, C++, and Python/NumPy by default): elements fill each row before moving to the next.
                    \[
                    A = \begin{bmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \end{bmatrix}
                    \]
                </p>
                <p>
                    <strong>Column-major order</strong> (used by Julia, MATLAB, R, and Fortran): elements fill each column before moving to the next.
                    \[
                    A = \begin{bmatrix} 1 & 3 & 5 \\ 2 & 4 & 6 \end{bmatrix}
                    \]
                </p>
                <p>
                    Note that the standard mathematical definition of \(\text{vec}(\cdot)\) follows <strong>column-major</strong> order. 
                    That is, \(\text{vec}(A)\) stacks columns, so the inverse operation (reshaping back) also fills by columns.
                </p>
            </div>

                <pre class="python-code">
                    import numpy as np

                    vector = np.array([1, 2, 3, 4, 5, 6])

                    #In Python, you can choose these two ways(default is row-major):

                    matrix_r = vector.reshape((2, 3), order='C') # C stands for "C" programming language
                    matrix_c = vector.reshape((2, 3), order='F') # F stands for "F"ortran programming language
                </pre>

            </section>

            <section id="k_pro" class="section-content">
                <h2>Kronecker Product</h2>

                <div class="theorem">
                    <span class="theorem-title">Definition: Kronecker product</span>
                    <p>
                        Let \(A \in \mathbb{R}^{m \times n}\) and \(B \in \mathbb{R}^{p \times q}\). In general, the <strong>Kronecker product</strong> 
                        \(A\otimes B\) is given by \((mp) \times (nq)\) matrix: 
                        \[
                        A\otimes B =  \begin{bmatrix} 
                                        a_{11}B & a_{12}B & \cdots & a_{1n} B \\
                                        a_{21}B & a_{22}B & \cdots & a_{2n} B \\
                                        \vdots & \vdots & \ddots & \vdots \\
                                        a_{m1}B & a_{m2}B & \cdots & a_{mn} B \\
                                        \end{bmatrix}.
                        \]
                        Each element \(a_{ij}\) of \(A\) is multiplied by the entire matrix \(B\) resulting in blocks of size \(p \times q\).
                    </p>
                </div>
                
                <div class="proof">
                    <span class="proof-title">Example:</span>
                    <p>
                        \[
                        \begin{bmatrix} 1 & 2  \\ 3 & 4 \\ \end{bmatrix}
                        \otimes
                        \begin{bmatrix} 5 & 6  \\ 7 & 8 \\ \end{bmatrix}
                        = 
                        \begin{bmatrix} 
                        5 & 6 & 10 & 12  \\
                        7 & 8 & 14 & 16  \\
                        15 & 18 & 20 & 24  \\
                        21 & 24 & 28 & 32  \\
                        \end{bmatrix}.
                        \]
                    </p>
                </div>   

                <div class="theorem">
                    <span class="theorem-title"> Useful Properties of the Kronecker Product</span> 
                    <ol style="padding-left: 40px;">
                        <li>Mixed-product property</li>
                            \[
                            (A \otimes B)(C \otimes D)  = (AC) \otimes (BD)
                            \]

                        <li>Transpose </li>
                            \[
                            (A \otimes B)^\top = A^\top \otimes B^\top
                            \]

                        <li>Inverse</li>
                            \[
                            (A \otimes B)^{-1} = A^{-1} \otimes B^{-1} 
                            \]

                        <li>Trace</li>
                            \[
                            \text{Tr }(A \otimes B) = \text{Tr }(A) \text{Tr }(B)
                            \]
                            
                        <li>Determinant</li>
                            \[
                            \det (A \otimes B) = \det(A)^n \det(B)^m
                            \]
                            where \(A \in \mathbb{R}^{m \times m}\), and \(B \in \mathbb{R}^{n \times n}\).

                        <li>Eigenvalues</li>
                            Suppose \(A \in \mathbb{R}^{m \times m}\) and \(B \in \mathbb{R}^{n \times n}\). Then 
                            \[
                            \text{Eigenvalues of } A \otimes B   = \lambda_i \mu_j
                            \]
                            where \(\lambda_i (i = 1, \cdots m)\) and \(\mu_j (j = 1, \cdots, n)\) are eigenvalues of \(A\) 
                            and \(B\) respectively.
                            <br>
                    </ol>
                </div>

                <p>
                    Now, we are ready to discuss <strong>tensors</strong>, an important concept in machine learning.
                </p>
            </section>
            
            <section id="tensor" class="section-content">
                <h2>Tensor</h2>
                <p>
                    A <strong>tensor</strong> is a generalization of a 2d array to more than 2 dimensions. So far we have seen 
                    the following tensors in mathematics. 
                </p>

                <ul style="padding-left: 40px;">
                    <li>A <strong>scalar</strong> is a 0-dimensional tensor (single number, \(a \in \mathbb{R}\)).</li>
                    <li>A <strong>vector</strong> is a 1-dimensional tensor (array of numbers, \(\vec{a} \in \mathbb{R}^n\)).</li>
                    <li>A <strong>matrix</strong> is a 2-dimensional tensor (table of numbers,  \(A \in \mathbb{R}^{m \times n}\)).</li>
                </ul>

                <p>
                    We can apply this notion to higher-order tensors. For example, the data of images are indeed, <strong>3-dimensional tensors</strong>, 
                    because real-world images usually include color information, which requires multiple <strong>channels</strong>. So, for any image \(I\), 
                    \[
                    I \in \mathbb{R}^{H \times W \times C}
                    \]
                    where \(H\) is height, \(W\) is width, and \(C\) is channels(e.g., RGB: \(C = 3\)) of the image. 
                </p>

                <div class="theorem">
                    <span class="theorem-title">Definition: Tensor</span>
                    <p>
                        Formally, a <strong>tensor</strong> is an element of a <strong>tensor product</strong> of vector spaces.
                        Consider vector spaces \(V\) and \(W\) over a field \(\mathbb{F}\). The tensor product of \(V\) and \(W\), denoted \(V \otimes W\) 
                        is a new vector space(<strong>tensor product space</strong>) whose elements are linear combinations of tensor products of vectors:
                        \[
                        v \otimes w  \quad \text{where } v \in V, \, w \in W.
                        \]
                        A <strong>rank-r tensor</strong>, \(T\) is an element of a tensor product of \(r\) vector spaces:
                        \[
                        T \in V_1 \otimes V_2 \otimes \cdots \otimes V_r.
                        \]
                    </p>
                </div>
                
                <p>
                    In this context, we can explain scalars, vectors, and matrices as following:
                </p>

                <ul style="padding-left: 40px;">
                    <li>A <strong>scalar</strong> is a rank-0 tensor, which is an element of \(\mathbb{F}\).</li>
                    <li>A <strong>vector</strong> is a rank-1 tensor, which is an element of the vector space \(V\) or \(W\).</li>
                    <li>A <strong>matrix</strong> is a rank-2 tensor, which is an element of the tensor product space \(V \otimes W\).</li>
                </ul><br>

                <p>
                    So, the <strong>Kronecker product</strong> is just a specific case of the tensor product applied to "matrices" and 
                    the tensor product is a more general mathematical operation that applies to <strong>multilinear maps</strong>.
                </p>
            </section>
        </div>
        <script src="/js/main.js"></script>
    </body>
</html>