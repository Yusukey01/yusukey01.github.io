<!DOCTYPE html>
<html>
    <head> 
        <title>Kronecker Product</title>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <link rel="stylesheet" href="../styles.css"> 
        <meta name="viewport" content="width=device-width, initial-scale=1">
    </head>
    <body> 
        <h1>Vectorization</h1>
        <blockquote>
            The <strong>vectorization</strong> operation, denoted by \(\text{vec }(\cdot)\), takes a matrix and stacks its columns 
            into a single <strong>column vector</strong>. 
            <br>
            For example, let \(x \in \mathbb{R}^n \), and consider outer product \(xx^\top  \in \mathbb{R}^{n \times n}\).
            \[
            \begin{align*}
            \text{vec }(xx^\top) &= \begin{bmatrix}x_1x_1 \\ x_2x_1 \\ \vdots \\ x_nx_1 \\ x_1x_2 \\ x_2x_2 \\ \vdots \\ x_nx_2 \\ \vdots \\
                            x_1x_n \\ x_2x_n \\ \vdots \\ x_nx_n 
                            \end{bmatrix} \\ \\
                          &= x \otimes x
            \end{align*} 
            \]
            The notation \(\otimes\) is called <strong>Kronecker product</strong>(more generally, <strong>tensor product</strong>). 
            This operation is widely used in fields like machine learning, statistics, and optimization for representing higher-order 
            interactions or matrix manipulations in a compact form.
            <br><br>
            Note: Conversely, we can reshape a vector into a matrix:
            <br>
            Consider a vector 
                \[
                a = \begin{bmatrix} 1 & 2 & 3 & 4 & 5 & 6 \end{bmatrix}^\top
                \]
            <ul>
            <li><strong>row-major order</strong> (e.g. used C, C++, and Python)</li>
                \[
                A = \begin{bmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \end{bmatrix}
                \]
            <li><strong>column-major order</strong> (e.g. used by Julia, Matlab, R, and Fortran)</li></li>
                \[
                A = \begin{bmatrix} 1 & 3 & 5 \\ 2 & 4 & 6 \end{bmatrix}
                \]
            </ul>
            <pre class="python-code">
                import numpy as np

                vector = np.array([1, 2, 3, 4, 5, 6])

                #In Python, you can choose these two ways(default is row-major):

                matrix_r = vector.reshape((2, 3), order='C') # C stands for "C" programming language
                matrix_c = vector.reshape((2, 3), order='F') # F stands for "F"ortran programming language
            </pre>
            <br>
        </blockquote>

        <h1>Kronecker Product</h1>
        <blockquote>
            Let \(A \in \mathbb{R^{m \times n}}\) and \(B \in \mathbb{R^{p \times q}}\). In general, the <strong>Kronecker product</strong> \(A\otimes B\) is given by 
            \((mp) \times (nq)\) matrix: 
            \[
            A\otimes B =  \begin{bmatrix} 
                              a_{11}B & a_{12}B & \cdots & a_{1n} B \\
                              a_{21}B & a_{22}B & \cdots & a_{2n} B \\
                              \vdots & \vdots & \ddots & \vdots \\
                              a_{m1}B & a_{m2}B & \cdots & a_{mm} B \\
                              \end{bmatrix}.
            \]
             Each element \(a_{ij}\) of \(A\) is multiplied by the entire matrix \(B\) resulting in blocks of size \(p \times q\).
            <br>
             For example,
             \[
             \begin{bmatrix} 1 & 2  \\ 3 & 4 \\ \end{bmatrix}
             \otimes
             \begin{bmatrix} 5 & 6  \\ 7 & 8 \\ \end{bmatrix}
             = 
             \begin{bmatrix} 
             5 & 6 & 10 & 12  \\
             7 & 8 & 14 & 16  \\
             15 & 18 & 20 & 24  \\
             21 & 24 & 28 & 32  \\
             \end{bmatrix}.
             \]
             <div class="theorem">
                <span class="theorem-title"> Useful Properties of the Kronecker Product</span> 
                <ol>
                    <li>Mixed-product property</li>
                        \[
                        (A \otimes B)(C \otimes D)  = (AC) \otimes (BD)
                        \]

                    <li>Transpose </li>
                        \[
                        (A \otimes B)^\top = A^\top \otimes B^\top
                        \]

                    <li>Inverse</li>
                        \[
                        (A \otimes B)^{-1} = A^{-1} \otimes B^{-1} 
                        \]

                    <li>Trace</li>
                        \[
                        \text{Tr }(A \otimes B) = \text{Tr }(A) \text{Tr }(B)
                        \]
                        
                    <li>Determinant</li>
                        \[
                        \det (A \otimes B) = \det(A)^n \det(B)^m
                        \]
                        where \(A \in \mathbb{R}^{m \times m}\), and \(B \in \mathbb{R}^{n \times n}\).

                    <li>Eigenvalues</li>
                        Suppose \(A \in \mathbb{R}^{m \times m}\) and \(B \in \mathbb{R}^{n \times n}\). Then 
                        \[
                        \text{Eigenvalues of } A \otimes B   = \lambda_i \mu_j
                        \]
                        where \(\lambda_i (i = 1, \cdots m)\) and \(\mu_j (j = 1, \cdots, n)\) are eigenvalues of \(A\) 
                        and \(B\) respectively.
                        <br>
                </ol>
             </div>
             Now, we are ready to discuss an important concept in machine learning. 
        </blockquote>

        <h1>Tensor</h1>
        <blockquote>
            A <strong>tensor</strong> is a generalization of a 2d array to more than 2 dimensions. So far we have seen 
            the following tensors in mathematics. 
            <ul>
                <li>A <strong>scalar</strong> is a 0-dimensional tensor (single number, \(a \in \mathbb{R}\)).</li>
                <li>A <strong>vector</strong> is a 1-dimensional tensor (array of numbers, \(\vec{a} \in \mathbb{R}^n\)).</li>
                <li>A <strong>matrix</strong> is a 2-dimensional tensor (table of numbers,  \(A \in \mathbb{R}^{m \times n}\)).</li>
            </ul>
            <br>
            We can apply this notion to higher-order tensors. For example, the data of images are indeed, <strong>3-dimensional tensor</strong>, 
            because real-world images usually include color information, which requires multiple <strong>channels</strong>. So, for any image \(I\), 
            \[
            I \in \mathbb{R}^{H \times W \times C}
            \]
            where \(H\) is height, \(W\) is width, and \(C\) is channels(e.g., RGB: \(C = 3\)) of the image. 

            <br><br>
            Formally, a <strong>tensor</strong> is an element of a <strong>tensor product</strong> of vector spaces.
            Consider vector spaces \(V\) and \(W\) over a field \(\mathbb{F}\). The tensor product </strong> of \(V\) and \(W\), denoted \(V \otimes W\)
             is a new vector space(<strong>tensor product space</strong>) whose elements are linear combinations of tensor products of vectors:
            \[
            v \otimes w  \quad \text{where } v \in V, \, w \in W.
            \]
            A <strong>rank-r tensor</strong>, \(T\) is an element of a tensor product of \(r\) vector spaces:
            \[
            T \in V_1 \otimes V_2 \otimes \cdots \otimes V_r.
            \]
            In this context, 
            <ul>
                <li>A <strong>scalar</strong> is a rank-0 tensor, which is an element of \(\mathbb{F}\).</li>
                <li>A <strong>vector</strong> is a rank-1 tensor, which is an element of the vector space \(V\) or \(W\).</li>
                <li>A <strong>matrix</strong> is a rank-2 tensor, which is an element of the tensor product space \(V \otimes W\).</li>
            </ul>
            <br>
            So, the <strong>Kronecker product</strong> is just a specific case of the tensor product applied to "matrices" and 
            the tensor product is a more general mathematical operation that applies to <strong>multilinear maps</strong>.
        </blockquote>
        <a href="../../index.html">Back to Home </a>
    <br> <a href="linear_algebra.html">Back to Linear Algebra </a>
</body>
</html>