---
layout: default
title: Linear Transformation
level: detail
description: Learn about the linear transformation, and matrix multiplication.
uses_math: true
uses_python: false
---
<!DOCTYPE html>
<html>
    <body>
        <!-- LearningResource Schema for Content Pages -->
        <script type="application/ld+json">
        {
        "@context": "https://schema.org",
        "@type": "LearningResource",
        "name": "{{ page.title }}",
        "description": "{{ page.description }}",
        "learningResourceType": "lesson",
        "educationalUse": "instruction",
        "educationalLevel": "university",
        "interactivityType": "{% if page.content contains 'Interactive Demo' or page.content contains 'demo' %}active{% else %}expositive{% endif %}",
        "author": {
            "@type": "Person",
            "name": "Yusuke Yokota",
            "jobTitle": "Mathematics & Computer Science Educator"
        },
        "publisher": {
            "@type": "Organization",
            "name": "MATH-CS COMPASS",
            "url": "https://math-cs-compass.com"
        },
        "about": [
            {% if page.url contains 'linear_transformation' %}
            { "@type": "Thing", "name": "Linear Transformation" },
            { "@type": "Thing", "name": "Matrix Multiplication" },
            { "@type": "Thing", "name": "Linear Mapping" },
            { "@type": "Thing", "name": "Surjective" },
            { "@type": "Thing", "name": "Injective" },
            { "@type": "Thing", "name": "One-to-One" },
            { "@type": "Thing", "name": "Onto" },
            { "@type": "Thing", "name": "Composition of Functions" },
            { "@type": "Thing", "name": "Linearity" }
            {% elsif page.url contains 'eigenvectors' %}
            { "@type": "Thing", "name": "Eigenvalues" },
            { "@type": "Thing", "name": "Eigenvectors" }
            {% elsif page.url contains 'symmetry' %}
            { "@type": "Thing", "name": "Symmetric Matrices" },
            { "@type": "Thing", "name": "Quadratic Forms" }
            {% else %}
            { "@type": "Thing", "name": "Linear Algebra" },
            { "@type": "Thing", "name": "Mathematics" }
            {% endif %}
        ],
        "teaches": [
            "Mathematical concepts",
            "Practical applications",
            "Problem-solving techniques"
        ],
        "isPartOf": {
            "@type": "Course",
            "name": "{% if page.url contains 'Machine_learning' %}Machine Learning{% elsif page.url contains 'Linear_algebra' %}Linear Algebra{% elsif page.url contains 'Calculus' %}Calculus to Optimization & Analysis{% elsif page.url contains 'Probability' %}Probability & Statistics{% elsif page.url contains 'Discrete' %}Discrete Mathematics & Algorithms{% endif %}",
            "description": "{% if page.url contains 'Machine_learning' %}Explore machine learning ideas and applications with mathematical foundations{% elsif page.url contains 'Linear_algebra' %}Explore the foundations of Linear Algebra, covering key concepts such as linear equations, vector spaces, eigenvalues, orthogonality, least squares, and stochastic matrices{% elsif page.url contains 'Calculus' %}Explore key calculus concepts essential for optimization, analysis, and machine learning{% elsif page.url contains 'Probability' %}Explore fundamental concepts of probability and statistics essential for machine learning{% elsif page.url contains 'Discrete' %}Explore the foundations of discrete mathematics and algorithms, covering graph theory, combinatorics, and the theory of computation{% endif %}",
            "provider": {
            "@type": "Organization",
            "name": "MATH-CS COMPASS",
            "url": "https://math-cs-compass.com"
            },
            "instructor": {
            "@type": "Person",
            "name": "Yusuke Yokota",
            "jobTitle": "Mathematics & Computer Science Educator"
            },
            "courseCode": "{% if page.url contains 'Machine_learning' %}V{% elsif page.url contains 'Linear_algebra' %}I{% elsif page.url contains 'Calculus' %}II{% elsif page.url contains 'Probability' %}III{% elsif page.url contains 'Discrete' %}IV{% endif %}",
            "hasCourseInstance": {
            "@type": "CourseInstance",
            "courseMode": "online",
            "courseWorkload": "{% if page.url contains 'Machine_learning' %}PT5H{% elsif page.url contains 'Linear_algebra' %}PT2H{% elsif page.url contains 'Calculus' %}PT3H{% elsif page.url contains 'Probability' %}PT2H30M{% elsif page.url contains 'Discrete' %}PT4H{% endif %}",
            "instructor": {
                "@type": "Person",
                "name": "Yusuke Yokota"
            }
            },
            "offers": {
            "@type": "Offer",
            "price": "0",
            "priceCurrency": "USD",
            "availability": "https://schema.org/InStock",
            "category": "free"
            }
        }
        }
        </script>
        <!-- WebApplication Schema for Interactive Demos -->
        <script type="application/ld+json">
        {
        "@context": "https://schema.org",
        "@type": "WebApplication",
        "name": "{{ page.title }} Interactive Demo",
        "description": "Interactive demonstration of {{ page.title | downcase }} concepts",
        "applicationCategory": "EducationalApplication",
        "operatingSystem": "Web Browser",
        "url": "https://math-cs-compass.com{{ page.url }}",
        "author": {
            "@type": "Person",
            "name": "Yusuke Yokota"
        },
        "applicationSubCategory": "Mathematical Visualization",
        "featureList": [
            "Interactive mathematical visualization",
            "Real-time parameter adjustment",
            "Educational demonstrations"
        ],
        "isAccessibleForFree": true,
        "educationalUse": "instruction",
        "educationalLevel": "university"
        }
        </script>
        <div class="hero-section">
            <h1 class="webpage-name">Linear Transformation
            </h1>
        </div>

        {% include section_navigation.html %}

        <div class="topic-nav">
            <a href="#lt">Linear Transformation</a>
            <a href="#linear">Linearity in Mathematics</a>
            <a href="#mult">Matrix Multiplication</a>
            <a href="#interactive">Interactive Linear Transformation Visualizer</a>
        </div> 

        <div class="container">  
           
            <section id="lt" class="section-content">
            <h2>Linear Transformation</h2>
            <p>
            <div class="theorem">
                <span class="theorem-title">Definition:</span>
                A <strong>transformation</strong> (or <strong>mapping</strong>) 
                \[
                T: \mathbb{R}^n  \rightarrow \mathbb{R}^m 
                \]
                is <strong> linear</strong> if \(\forall u, v \in\mathbb{R}^m,\) and any scalars \(c\), the 
                following two properties hold: 
                \[
                \begin{align*}
                &T(u+v) = T(u) +T(v) \\\\
                &T(cu) = cT(u) 
                \end{align*}
                \]
            </div>
            For example, consider a matrix \(A \in \mathbb{R}^{m \times 3}\) and a vector \(x \in \mathbb{R}^3 \). 
            Then, the matrix-vector product \(Ax\) is a linear transformation \(T: x \mapsto Ax\).  
            To verify this, let \(u, v \in \mathbb{R}^3 \) and \(c\) be a scalar. Let's check the two conditions: 
            <br>
            <br><strong>Vector addition</strong>:
            \[
            \begin{align*}
            &A(u+v) \\\\
            &= \begin{bmatrix} a_1 & a_2 & a_3 \end{bmatrix} 
                      \begin{bmatrix} u_1 + v_1 \\ u_2 + v_3 \\ u_3 + v_3 \end{bmatrix} \\\\
            &= (u_1 + v_1)a_1 + (u_2 + v_2)a_2 + (u_3 + v_3)a_3 \\\\
            &= (u_1a_1 + u_2a_2 + u_3a_3) + (v_1a_1 + v_2a_2 + v_3a_3) \\\\
            &= Au + Av
            \end{align*}
            \]
            <br><strong>Scalar multiplication</strong>:
            \[
            \begin{align*}
            &A(cu) \\\\
                &= \begin{bmatrix} a_1 & a_2 & a_3 \end{bmatrix}
                \begin{bmatrix} cu_1\\ cu_2 \\ cu_3 \end{bmatrix} \\\\
                &= c(u_1a_1)+c(u_2a_2)+c(u_3a_3) \\\\
                &= c(u_1a_1 + u_2a_2 + u_3a_3) \\\\
                &= c(Au).
            \end{align*}
            \]
            <br>In general, the operations of vector addition and scalar multiplication are preserved 
            under linear transformations. 
            <br>In addition, if a transformation \(T\) is linear, then 
            \[
            T(0)=0
            \]
            Also, for any scalar \(a, b\) and \(u, v\) is in the domain of \(T\), 
            \[
            T(au + bv)=aT(u)+bT(v)
            \]
            <br>
            You probably learned following concepts somewhere in terms of functions \(f(x): \mathbb{R} \to  \mathbb{R}\). 
            Now, we would like to extend the definition to higher dimensions.
            <br><br>
            A mapping \(T: \mathbb{R}^n \to  \mathbb{R}^m\) is said to be <strong>onto(surjective)</strong> \(\mathbb{R}^m\) if
            \[
            \begin{align*}
            &\forall b \in \mathbb{R}^m,   \\\\
            &\exists \, x \in \mathbb{R}^n \text{s.t } T(x) = b.
            \end{align*}
            \]  
            Equivalently, the range of \(T\) (the set of all outputs) is equal to the codomain \(\mathbb{R}^m\). 
            <div class="theorem">
                    <span class="theorem-title">Theorem 1:</span>
                    In a matrix transformation(<strong>"linear"</strong> transformation) \(T: \mathbb{R}^n  \rightarrow \mathbb{R}^m \), 
                    <p style="text-align: center;">\(T\) maps \(\mathbb{R}^n\) onto \(\mathbb{R}^m \) iff the columns of a matrix \(A\) <strong>span</strong> \(\mathbb{R}^m\).</p>
            </div>
            Note: This means that for each \(b \in \mathbb{R}^m\), the equation \(Ax =b \) has at lest one solution.
            <br><br>
            A mapping \(T: \mathbb{R}^n \to  \mathbb{R}^m\) is said to be <strong>one-to-one(injective)</strong> if
            \[
            \begin{align*}
            &\forall b \in \mathbb{R}^m,  \forall u, v \in \mathbb{R}^n, \\\\
            &T(u) = T(v) = b \Rightarrow u = v. 
            \end{align*}
            \]
            Equivalently, for each \(b \in \mathbb{R}^m\), \(T(x) = b\) has either a unique solution or no solution at all.
            <div class="theorem">
                    <span class="theorem-title">Theorem 2:</span>
                    In a matrix transformation(<strong>"linear"</strong> transformation) \(T: \mathbb{R}^n  \rightarrow \mathbb{R}^m \),
                    <p style="text-align: center;">\(T\) is one-to-one iff the columns of a matrix \(A\) are <strong>linearly independent</strong>
                        (or, the homogeneous equation \(Ax =0\) has only the trivial solution).</p>
            </div>
            Note: A linear transformation \(T\) is one-to-one iff \(\, T(x)=0\) has only the trivial solution(\(x = 0\)).<br>
                    Since \(T\) is linear, \(T(0)=0\). If \(T\) is one-to-one, clearly \(T(x)=0\) has <strong>only</strong> trivial solution.
                    Also, if \(T\) is not one-to-one, it is possible that for some different vectors \(u, v \in \mathbb{R}^n\), \( T(u) = T(v) = b\). Then since 
                    \(T\) is linear, \(T(u-v)=T(u)-T(v) =b - b =0\). The vector \(u-v\) is nonzero because \(u \neq v\). Thus
                    \(T(x)=0\) has more than one solution. 
            </p>
            </section>

            <section id="linear" class="section-content">
            <h2>Linearity in Mathematics</h2>
            <p>
            Essentially, we are not only talking about vectors and matrices. The linear transformation is one of the most fundamental ideas in mathematics. 
            You have encountered this essential concept multiple times outside Linear Algebra. Let's briefly observe a few examples.
            <br><br>
            A function \(f(x) = mx\) is linear transformation \(f:\mathbb{R} \rightarrow \mathbb{R}\).
            \[
            \begin{align*}
            f(ax+by)
            &= m(ax+by) \\\\
            &= a(mx)+b(my)  \\\\
            &= af(x) + bf(y)
            \end{align*}
            \]
            Note: the graph of fanctions that hold linearity must pass through the origin 
            since linear transformations must satisfy \(T(0) = 0\).
            <br><br>
            If you have just started to learn calculus, you might know that
            \[
            \begin{align*}
            &\frac{d}{dx}(aX(t)+bY(t)) \\\\
            &= a\frac{d}{dx}(X(t))+b\frac{d}{dx}(Y(t)).
            \end{align*}
            \]
            For example, 
            \[
            \begin{align*}
            &\frac{d}{dx}(5x^3 +4x^2) \\\\
            &= 15x^2 + 8x   \\\\
            &= 5\frac{d}{dx}(x^3)+4\frac{d}{dx}(x^2)
            \end{align*}
            \]
            so the differential is just a <strong>linear operator</strong>. (in fact, the integration too.)
            <br><br>
            Have you learned statistics? The expected value also holds linearity. For any random variables \(X\) and \(Y\), and constants \(a\) and \(b\)
            \[\mathbb{E}[aX+bY]=a\mathbb{E}[X]+b\mathbb{E}[Y].\]
            You can see the <strong>linearity</strong> everywhere in mathematics and this is why "linear" algebra is so powerful.
            </p>
            </section>

            <section id="mult" class="section-content">
            <h2>Matrix Multiplication</h2>
            <p>
            Suppose \(A\) is an \(m \times n\) matrix and \(B\) is an \(n \times p\) matrix. Then the product \(AB\)
            is the \(m \times p\) matrix 
            \[
            AB = 
            \begin{bmatrix} Ab_1 & Ab_2 & \cdots & Ab_p \end{bmatrix} 
            \]
            where \(b_1, b_2, \cdots, b_p\) are columns of \(B\).
            <br><br>
            So, each column of \(AB\) is a linear combination of the columns of A with weights from the corresponding 
            column of \(B\). Let's see an example: 
            \[
            \begin{align*}
            AB 
            &= 
            \begin{bmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \\ 7 & 8 & 9 \end{bmatrix}
            \begin{bmatrix} -1 & -2 \\ 0 & -3 \\ -4 & 0 \end{bmatrix} \\\\
            &=
            \begin{bmatrix} (-1+0-12) & (-2-6+0) \\ (-4+0-24) & (-8-15+0) \\ (-7+0-36) & (-14-24+0) \end{bmatrix} \\\\
            &=
            \begin{bmatrix} -13 & -8 \\ -28 & -23 \\ -43 & -38 \end{bmatrix}
            \end{align*}
            \]
            You can see the \((i,j)\)-entry in \(AB\) is 
            \[(AB)_{ij} = \sum_{k=1}^n a_{ik}b_{kj}.\]
            Also, note that in the example, \(BA\) is <strong>NOT</strong> defined because the number of columns of \(B\) does not
            match the number of rows of \(A\). This implies that in general, <strong>\(AB \neq BA\)</strong> even if the size of the matrices match each other.
            Similarly, \(AB = AC\) does NOT guarantee that \(B = C\), and \(AB = 0\) does NOT imply \(A=0\) or \(B=0\) in general.
            <br><br>
            You may wonder why matrix multiplication is not simply an entrywise operation, 
            like sums and scalar multiples of matrices. The key idea is a linear transformation. 
            <strong>Matrix multiplication is composition of linear transformations.</strong>
            <br><br>
            Let's say a vector \(x\) is multiplied by a matrix \(B\), which means it maps \(x\) into the new vector
            \(Bx\). Next, a matrix \(A\) multiplies the vector \(Bx\). Then we get the resulting vector \(A(Bx)\). 
            Since the conposition of functions is associative, 
            \[
            (AB)x = A(Bx)
            \]
            which allows us to calculate \(Bx\) (matrix \(\times\) vector) first, instead of \(AB\) (matrix \(\times\) matrix).
            <br><br> 
            This is significant in numerical computation. For example, if the size of matrices A, B, C, D, and E are one million by one million
            and we want to compute 
            \[
            ABCDEx,
            \]
            where \(x\) is a column vector, we should calculate it from the rightmost side\(Ex\), which gives a new column "vector."
            Then we can avoid huge matrix multiplications. Ultimately, computing \(A(BCDEx)\) is reduced to just a matrix-vector multiplication, which 
            avoids the computational complexity of performing the full matrix multiplication.
            </p>
            </section>

            <section id="interactive" class="section-content">
                <h2>Interactive Linear Transformation Visualizer</h2>
                <p>
                    Linear transformations are often best understood through visual intuition. In two dimensions, every linear transformation 
                    can be represented by a matrix multiplication that reshapes, rotates, reflects, or scales the plane.
                </p>
                <p>
                    The following interactive visualizer allows you to explore how different 2x2 matrices affect a 2D grid and various shapes. 
                    Try modifying the entries of the matrix and observe how the transformation changes the geometry.
                </p>
                <!-- Here, the React component will be rendered -->
                <div id="linear-transformation-visualizer"></div>
                <br>
                <p>For example, the above 90Â° clockwise rotation for the square shape is given by 
                    \[
                    \begin{align*}
                    &\begin{bmatrix}
                        0 & 1 \\ -1 & 0 
                    \end{bmatrix}
                    \begin{bmatrix}
                        1 & 5 & 5 & 1 \\ 5 & 5 & 1 & 1 
                    \end{bmatrix} \\\\
                    &=
                    \begin{bmatrix}
                    5 & 5 & 1 & 1 \\ -1 & -5 & -5 & -1 
                    \end{bmatrix}.
                    \end{align*}
                    \]
                </p>
            </section>

        </div>
        <script src="/js/main.js"></script>

        <!-- React -->
        <script src="/js/linear_transformation_visualizer.js"></script>
        <!-- Component script -->
        <script type="text/babel" src="/js/linear_transformation_visualizer.js"></script>

    </body>
</html>