---
layout: default
title: Determinants
level: detail
description: Learn about determinants, and invertible matrix theorems.
uses_math: true
uses_python: false
---
<!DOCTYPE html>
<html>
    <body>
        <!-- LearningResource Schema for Content Pages -->
        <script type="application/ld+json">
        {
        "@context": "https://schema.org",
        "@type": "LearningResource",
        "name": "{{ page.title }}",
        "description": "{{ page.description }}",
        "learningResourceType": "lesson",
        "educationalUse": "instruction",
        "educationalLevel": "university",
        "interactivityType": "{% if page.content contains 'Interactive Demo' or page.content contains 'demo' %}active{% else %}expositive{% endif %}",
        "author": {
            "@type": "Person",
            "name": "Yusuke Yokota",
            "jobTitle": "Mathematics & Computer Science Educator"
        },
        "publisher": {
            "@type": "Organization",
            "name": "MATH-CS COMPASS",
            "url": "https://yusukey01.github.io"
        },
        "about": [
            {% if page.url contains 'determinants' %}
            { "@type": "Thing", "name": "Determinants" },
            { "@type": "Thing", "name": "Matrix Determinant" },
            { "@type": "Thing", "name": "Cofactor Expansion" },
            { "@type": "Thing", "name": "Cramer's Rule" },
            { "@type": "Thing", "name": "Inverse Formula" },
            { "@type": "Thing", "name": "Invertible Matrix Theorem" },
            { "@type": "Thing", "name": "Adjugate Matrix" },
            { "@type": "Thing", "name": "Cofactor Matrix" }
            {% elsif page.url contains 'eigenvectors' %}
            { "@type": "Thing", "name": "Eigenvalues" },
            { "@type": "Thing", "name": "Eigenvectors" }
            {% elsif page.url contains 'symmetry' %}
            { "@type": "Thing", "name": "Symmetric Matrices" },
            { "@type": "Thing", "name": "Quadratic Forms" }
            {% else %}
            { "@type": "Thing", "name": "Linear Algebra" },
            { "@type": "Thing", "name": "Mathematics" }
            {% endif %}
        ],
        "teaches": [
            "Mathematical concepts",
            "Practical applications",
            "Problem-solving techniques"
        ],
        "isPartOf": {
            "@type": "Course",
            "name": "{% if page.url contains 'Machine_learning' %}Machine Learning{% elsif page.url contains 'Linear_algebra' %}Linear Algebra{% elsif page.url contains 'Calculus' %}Calculus to Optimization & Analysis{% elsif page.url contains 'Probability' %}Probability & Statistics{% elsif page.url contains 'Discrete' %}Discrete Mathematics & Algorithms{% endif %}",
            "description": "{% if page.url contains 'Machine_learning' %}Explore machine learning ideas and applications with mathematical foundations{% elsif page.url contains 'Linear_algebra' %}Explore the foundations of Linear Algebra, covering key concepts such as linear equations, vector spaces, eigenvalues, orthogonality, least squares, and stochastic matrices{% elsif page.url contains 'Calculus' %}Explore key calculus concepts essential for optimization, analysis, and machine learning{% elsif page.url contains 'Probability' %}Explore fundamental concepts of probability and statistics essential for machine learning{% elsif page.url contains 'Discrete' %}Explore the foundations of discrete mathematics and algorithms, covering graph theory, combinatorics, and the theory of computation{% endif %}",
            "provider": {
            "@type": "Organization",
            "name": "MATH-CS COMPASS",
            "url": "https://yusukey01.github.io"
            },
            "instructor": {
            "@type": "Person",
            "name": "Yusuke Yokota",
            "jobTitle": "Mathematics & Computer Science Educator"
            },
            "courseCode": "{% if page.url contains 'Machine_learning' %}V{% elsif page.url contains 'Linear_algebra' %}I{% elsif page.url contains 'Calculus' %}II{% elsif page.url contains 'Probability' %}III{% elsif page.url contains 'Discrete' %}IV{% endif %}",
            "hasCourseInstance": {
            "@type": "CourseInstance",
            "courseMode": "online",
            "courseWorkload": "{% if page.url contains 'Machine_learning' %}PT5H{% elsif page.url contains 'Linear_algebra' %}PT2H{% elsif page.url contains 'Calculus' %}PT3H{% elsif page.url contains 'Probability' %}PT2H30M{% elsif page.url contains 'Discrete' %}PT4H{% endif %}",
            "instructor": {
                "@type": "Person",
                "name": "Yusuke Yokota"
            }
            },
            "offers": {
            "@type": "Offer",
            "price": "0",
            "priceCurrency": "USD",
            "availability": "https://schema.org/InStock",
            "category": "free"
            }
        }
        }
        </script>
        <!-- WebApplication Schema for Interactive Demos -->
        <script type="application/ld+json">
        {
        "@context": "https://schema.org",
        "@type": "WebApplication",
        "name": "{{ page.title }} Interactive Demo",
        "description": "Interactive demonstration of {{ page.title | downcase }} concepts",
        "applicationCategory": "EducationalApplication",
        "operatingSystem": "Web Browser",
        "url": "https://yusukey01.github.io{{ page.url }}",
        "author": {
            "@type": "Person",
            "name": "Yusuke Yokota"
        },
        "applicationSubCategory": "Mathematical Visualization",
        "featureList": [
            "Interactive mathematical visualization",
            "Real-time parameter adjustment",
            "Educational demonstrations"
        ],
        "isAccessibleForFree": true,
        "educationalUse": "instruction",
        "educationalLevel": "university"
        }
        </script>
        <div class="hero-section">
            <h1 class="webpage-name">Determinants
            </h1>
        </div>

        {% include section_navigation.html %}
        
        <div class="topic-nav">
            <a href="#det">Determinants</a>
            <a href="#cramer">Cramer's Rule</a>
            <a href="#inv">Inverse Formula</a>
            <a href="#inv_m">Invertible Matrix Theorem</a>
        </div> 

        <div class="container">  
           
            <section id="det" class="section-content">
            <h2>Determinants</h2>
            <p>
            For \(n \geq 2\), the <strong>determinant</strong> of an \(n \times n\) matrix \(A\) is denined as
           \[
            \begin{align*}
            \det A  &= \sum_{j=1}^n (-1)^{1+j} a_{1j} \det A_{1j} \\\\
                    &= a_{11}\det A_{11}-a_{12}\det A_{12}+\cdots \\\\
                    &\quad +(-1)^{1+n}a_{1n}\det A_{1n}\\\\
                    &= a_{11}C_{11}+a_{12}C_{12} + \cdots + a_{1n}C_{1n}. 
            \end{align*}
            \]
            <br>
            Here, \(C_{ij}=(-1)^{i+j} \det A_{ij}\) is the \((i, j)\)-<strong>cofactor</strong> of \(A\). 
            <br><br>
            For example, consider the determinant of the matrix: 
            \[
            A = \begin{bmatrix} 3 & 2 & 5 \\ 7 & 5 & 4 \\ 0 & 1 & 0  \end{bmatrix}.
            \]
            The determinant of \(A\) can be computed using the cofactor expansion across the first row
            \[ 
            \begin{align*}
            \det A &= 3\det \begin{bmatrix} 5 & 4 \\ 1 & 0 \end{bmatrix} 
                    -2\det \begin{bmatrix} 7 & 4 \\ 0 & 0 \end{bmatrix} \\\\
                &\quad +5\det \begin{bmatrix} 7 & 5 \\ 0 & 1 \end{bmatrix} \\\\
                &= 3(-4)-2(0)+5(7) \\\\
                &= 23.
            \end{align*}
            \]
            <br>
            The <strong>cofactor expantion</strong> can be performed along any row or column.
            <br>
            In general, for an entry \(a_{ij}\) in a matrix \(A\)
            \[
             \text{Cofactor of } a_{ij} = C_{ij} = (-1)^{i+j} \det A_{ij}
            \]
            where \(A_{ij}\) is the \((n-1) \times (n-1)\) submatrix obtained by removing the \(i\)-th row and 
            \(j\)-th column of \(A\). 
            <br>
            In addition, the <strong>cofactor matrix</strong> of \(A\) is the \(n \times n\) matrirx where each 
            entry is the cofactor of the corresponding element of \(A\).
            \[
            \text{cofactor }(A) = \begin{bmatrix}
                                   C_{11} & C_{12} & \cdots & C_{1n}\\
                                   C_{21} & C_{22} & \cdots & C_{2n} \\
                                   \vdots & \vdots & \ddots & \vdots \\
                                   C_{n1} & C_{n2} & \cdots & C_{nn}
                                  \end{bmatrix}
            \]
            <br>
            For example, using the third row of \(A\): 
            \[
            \begin{align*}
            \det A 
                &= 0 -1\det \begin{bmatrix} 3 & 5 \\ 7 & 4 \end{bmatrix} + 0 \\\\
                &= -1(12-35) \\\\
                &= 23
            \end{align*}
            \]
            Alternatively, expanding down the first column of \(A\):
            \[
            \begin{align*}
            \det A 
                &= 3\det \begin{bmatrix} 5 & 4 \\ 1 & 0 \end{bmatrix} 
                -7\det \begin{bmatrix} 2 & 5 \\ 1 & 0 \end{bmatrix} \\\\
                &\quad  +0\det \begin{bmatrix} 2 & 5 \\ 5 & 4 \end{bmatrix} \\\\
                &= 3(-4)-7(-5)+0  \\\\
                &= 23.
            \end{align*}
            \]
            <br>
            These computations demonstrate an important property: the determinant of a <strong>triangular matrix</strong> is 
            the product of its diagonal entries. For example:
            \[
            \det \begin{bmatrix}
                1 & 7 & 5 & 4 & 2 \\
                0 & 2 & 9 & 2 & 3 \\
                0 & 0 & 3 & 5 & 7\\
                0 & 0 & 0 & 4 & 7\\
                0 & 0 & 0 & 0 & 5
                \end{bmatrix} = 1 \cdot 2 \cdot 3 \cdot 4 \cdot 5 = 120.
            \]
            Additionally, note that the <strong>transpose</strong> of a matrix does not affect its determinant.
            Thus, for any square matrix \(A\):
            \[
            \det A = \det A^T.
            \] 
            Consider the matrix \(A\) again. The transpose of \(A\) is: 
            \[
            A^T = \begin{bmatrix}  3 & 7 & 0 \\  2 & 5 & 1 \\  5 & 4 & 0 \end{bmatrix} \qquad
            \]
            Then 
            \[
            \begin{align*}
            \det A^T
            &=  3\det \begin{bmatrix} 5 & 1 \\ 4 & 0 \end{bmatrix} 
            -7\det \begin{bmatrix} 2 & 1 \\ 5 & 0 \end{bmatrix}
            +0\det \begin{bmatrix} 2 & 5 \\ 5 & 4 \end{bmatrix}\\\\
            &= 3(-4) -7(-5)+0 = 23 \\\\
            &= \det A.
            \end{align*}
            \]
            <br>
            This result is supported by the relationship between cofactors in \(A\) and \(A^T\). 
            The cofactor of \(a_{1j}\) in \(A\) is equal to the cofactor of \(a_{j1}\) in \(A^T\). 
            Therefore, the cofactor expansion across the first row of \(A\) is the same as the cofactor expansion
            down the first column of \(A^T\). This relationship holds for any row or column of any square matrix.
            <br><br>
            Determinants are <strong>multiplicative</strong>. For example, given:
            \[
            A = \begin{bmatrix} 1 & 2 \\ 8 & 9 \end{bmatrix}, \qquad 
            B = \begin{bmatrix} 5 & 7 \\ 4 & 6 \end{bmatrix},  \qquad
            AB = \begin{bmatrix} 13 & 19 \\ 76 & 110 \end{bmatrix}
            \]
            then
            \[
            \begin{align*}
            &\det A = 9-16=-7, \\\\
            &\det B = 30-28 =2, \\\\
            &\det AB = 1430-1444=-14 = (\det A)(\det B).
            \end{align*}
            \]
            Note: \(\det (A+B) \neq \det A + \det B\).
            <br><br>
            Finally, observe how elementary row operations affect determinants:
            \[
            \begin{align*}
            &\det \begin{bmatrix} a & b  \\ c & d \end{bmatrix} = ad-bc  \\\\
            &\det \begin{bmatrix} c & d  \\ a & b  \end{bmatrix} = cb-ad=-(ad-bc) \\\\\
            &\det \begin{bmatrix} a & b  \\ kc & kd \end{bmatrix} = kad-kbc = k(ad-bc) \\\\
            &\det \begin{bmatrix} a & b  \\ c+2k & d+2k \end{bmatrix} = a(d+2k)-b(c+2k) = ad-bc
            \end{align*}
            \]
            </p>
            </section>

            <section id="cramer" class="section-content">
            <h2>Cramer's Rule</h2>
            <p>
            <div class="theorem">
                <span class="theorem-title">Theorem 1: Cramer's Rule</span> 
                Let \(A\) be an invertible \(n \times n\) matrix. \(\forall b \in \mathbb{R}^n \), \(Ax = b\) has
                a solution \(x\) where the entries of \(x\) are given by:
                \[
                x_i = \frac{detA_i(b)}{\det A}, \qquad i = 1, 2, \cdots, n. \tag{1}
                \]
                Here, \(A_i(b)\) is the matrix obtained from \(A\) by replacing its \(i\)-th column with \(b\).
            </div>
            <div class="proof">
                <span class="proof-title">Proof:</span>
                Consider the \(n \times n\) identity matrix \(I\).
                Replace the \(i\)-th column of \(I\) with \(x\), giving a modified identity matrix \(I_i(x)\):
                \[
                I_i(x) = \begin{bmatrix} e_1 & e_2 & \cdots & x & \cdots & e_n \end{bmatrix}.
                \] 
                Multiplying \(A\) by \(I_i(x)\), we obtain:
                \[
                \begin{align*}
                AI_i(x) &= \begin{bmatrix} Ae_1 & Ae_2 & \cdots & Ax & \cdots & Ae_n \end{bmatrix} \\\\
                        &= \begin{bmatrix} a_1 & a_2 & \cdots & b & \cdots & a_n \end{bmatrix} \\\\
                        &= A_i(b)
                \end{align*}
                \]
                By the multiplicative property of determinants, 
                \[
                \begin{align*}
                &(\det A)(\det I_i(x))= \det A_i(b)  \\\\
                &\Longrightarrow (\det A)x_i = \det A_i(b).
                \end{align*}
                \]
                Since \(A\) is invertible \(\det A \neq 0\) and then dividing through by \(\det A\) yields (1).
            </div>
            </p>
            </section>

            <section id="inv" class="section-content">
            <h2>Inverse Formula</h2>
            <p>
            The Cramer's rule gives us the general inverse formula. 
            Note that both inverse formula and Cramer's rule are primarily useful in theoritical discussions. 
            <div class="theorem">
                <span class="theorem-title">Theorem 2:</span>  
                Let \(A\) be an invertible matrix. Then the inverse of \(A\) is given by
                \[
                A^{-1} = \frac{1}{\det A} \text{adj }(A)
                \]
                where \(\text{adj }A\) is the <strong>adjugate</strong> of \(A\), which 
                is the transpose of the cofactor matrix of \(A\).
                \[
                \begin{align*}
                \text{adj }(A) &= \text{cofactor }(A)^T \\\\
                               &= \begin{bmatrix}
                                   C_{11} & C_{21} & \cdots & C_{n1}\\
                                   C_{12} & C_{22} & \cdots & C_{n2} \\
                                   \vdots & \vdots & \ddots & \vdots \\
                                   C_{1n} & C_{2n} & \cdots & C_{nn}
                                  \end{bmatrix}.
                \end{align*}
                \]
            </div>
            For example, consider 
            \[
            A = \begin{bmatrix} -1 & 2 & 3 \\  2 & 1 & -4 \\  3 & 3 & 2 \end{bmatrix}.
            \]
            To get \(\text{adj }A\), we need the nine cofactors of \(A\):  
            \begin{align*}
            C_{11} &= +(2+12) = 14, & C_{12} &= -(4+12) = -16, & C_{13} &= +(6-3) = 3\\
            C_{21} &= -(4-9) = 5, & C_{22} &= +(-2-9) = -11, & C_{23} &= -(-3-6) = 9\\
            C_{31} &= +(-8-3) = -11, & C_{32} &= -(4-6) = 2, & C_{33} &= +(-1-4) = -5
            \end{align*}
            and \(\det A = -1(2+12)-2(4+12)+3(6-3) = -37\).
            To verify the determinant, we can compute 
            \[
            \begin{align*}
            (\text{adj }A)A &= \begin{bmatrix} 14 & 5 & -11 \\ -16 & -11 & 2 \\ 3 & 9 & -5 \end{bmatrix}
                                \begin{bmatrix} -1 & 2 & 3 \\  2 & 1 & -4 \\  3 & 3 & 2 \end{bmatrix} \\\\
                            &= -37I
            \end{align*}
            \]
            Thus, \(\det A = -37\), and 
            \[
            A^{-1} = \begin{bmatrix}
                    \frac{-14}{37} & \frac{-5}{37} & \frac{11}{37} \\
                    \frac{16}{37} & \frac{11}{37} & \frac{-2}{37} \\
                    \frac{-3}{37} & \frac{-9}{37} & \frac{5}{37}
                    \end{bmatrix}.
            \]
            </p>
            </section>

            
            <section id="inv_m" class="section-content">
            <h2>Invertible Matrix Theorem</h2>
            <p>
            We have seen many properties of invertible matrices. It is a good time to summarize the properties.
            <div class="theorem">
                <span class="theorem-title">Theorem 3: Invertible Matrix</span>  
                Let \(A\) be an \(n \times n\) matrix. Then the following statemets are logically equivalent.
                <br> 
                <ol style="padding-left: 40px;">
                    <li>\(A\) is invertible.</li>
                    <li>There is an \(n \times n\) matrix \(B\) s.t. \(AB = I\) and \(BA = I\).</li>
                    <li>\(Ax = 0\) has only trivial solution.</li>
                    <li>\(A\) has \(n\) pivot positions.</li>
                    <li>\(A\) is row equivalent to \(I_n\).</li>
                    <li>\(\forall b \in \mathbb{R}^n , Ax = b\) has at least one solution.</li>
                    <li>The column of \(A\) span \(\mathbb{R}^n\).</li>
                    <li>The linear transformation \(x \mapsto Ax\) maps \(\mathbb{R}^n\) onto \(\mathbb{R}^n\).</li>
                    <li>The columns of \(A\) form a linearly independent set.</li>
                    <li>The linear transformation \(x \mapsto Ax\) is one-to-one.</li>
                    <li>\(A^T\) is invertible.</li>
                    <li>\(\det A \neq 0\).</li>
                    <li>\(0\) is not an <a href="eigenvectors.html">eigenvalue</a> of \(A\).</li>
                    <li>\((\text{Col }A)^{\perp} = \{0\}.\) (See: <a href="symmetry.html">Symmetry</a>)</li>
                    <li>\((\text{Nul }A)^{\perp} = \mathbb{R}^n.\)  </li>
                    <li>\(\text{Row }A = \mathbb{R}^n.\)  </li>
                    <li> \(A\) has \(n\) nonzero singular values. </li>
                </ol>
            </div>
            </p>
            </section>
        </div>
        <script src="/js/main.js"></script>
    </body>
</html>