---
layout: default
title: Trace & Norms
level: detail
description: Learn about trace, norms, and introduction to Metric Spaces.
uses_math: true
uses_python: false
---
<!DOCTYPE html>
<html>
    <body>
        <!-- LearningResource Schema for Content Pages -->
        <script type="application/ld+json">
        {
        "@context": "https://schema.org",
        "@type": "LearningResource",
        "name": "{{ page.title }}",
        "description": "{{ page.description }}",
        "learningResourceType": "lesson",
        "educationalUse": "instruction",
        "educationalLevel": "university",
        "interactivityType": "{% if page.content contains 'Interactive Demo' or page.content contains 'demo' %}active{% else %}expositive{% endif %}",
        "author": {
            "@type": "Person",
            "name": "Yusuke Yokota",
            "jobTitle": "Mathematics & Computer Science Educator"
        },
        "publisher": {
            "@type": "Organization",
            "name": "MATH-CS COMPASS",
            "url": "https://yusukey01.github.io"
        },
        "about": [
            {% if page.url contains 'trace' %}
            { "@type": "Thing", "name": "Trace" },
            { "@type": "Thing", "name": "Matrix Norms" },
            { "@type": "Thing", "name": "Frobenius Norm" },
            { "@type": "Thing", "name": "Spectral Norm" },
            { "@type": "Thing", "name": "Nuclear Norm" },
            { "@type": "Thing", "name": "Metric Spaces" },
            { "@type": "Thing", "name": "p-Norms" },
            { "@type": "Thing", "name": "Mathematical Analysis" }
            {% elsif page.url contains 'eigenvectors' %}
            { "@type": "Thing", "name": "Eigenvalues" },
            { "@type": "Thing", "name": "Eigenvectors" }
            {% elsif page.url contains 'orthogonality' %}
            { "@type": "Thing", "name": "Orthogonality" },
            { "@type": "Thing", "name": "Orthogonal Projection" }
            {% else %}
            { "@type": "Thing", "name": "Linear Algebra" },
            { "@type": "Thing", "name": "Mathematics" }
            {% endif %}
        ],
        "teaches": [
            "Mathematical concepts",
            "Practical applications",
            "Problem-solving techniques"
        ],
        "isPartOf": {
            "@type": "Course",
            "name": "{% if page.url contains 'Machine_learning' %}Machine Learning{% elsif page.url contains 'Linear_algebra' %}Linear Algebra{% elsif page.url contains 'Calculus' %}Calculus to Optimization & Analysis{% elsif page.url contains 'Probability' %}Probability & Statistics{% elsif page.url contains 'Discrete' %}Discrete Mathematics & Algorithms{% endif %}",
            "description": "{% if page.url contains 'Machine_learning' %}Explore machine learning ideas and applications with mathematical foundations{% elsif page.url contains 'Linear_algebra' %}Explore the foundations of Linear Algebra, covering key concepts such as linear equations, vector spaces, eigenvalues, orthogonality, least squares, and stochastic matrices{% elsif page.url contains 'Calculus' %}Explore key calculus concepts essential for optimization, analysis, and machine learning{% elsif page.url contains 'Probability' %}Explore fundamental concepts of probability and statistics essential for machine learning{% elsif page.url contains 'Discrete' %}Explore the foundations of discrete mathematics and algorithms, covering graph theory, combinatorics, and the theory of computation{% endif %}",
            "provider": {
            "@type": "Organization",
            "name": "MATH-CS COMPASS",
            "url": "https://yusukey01.github.io"
            },
            "instructor": {
            "@type": "Person",
            "name": "Yusuke Yokota",
            "jobTitle": "Mathematics & Computer Science Educator"
            },
            "courseCode": "{% if page.url contains 'Machine_learning' %}V{% elsif page.url contains 'Linear_algebra' %}I{% elsif page.url contains 'Calculus' %}II{% elsif page.url contains 'Probability' %}III{% elsif page.url contains 'Discrete' %}IV{% endif %}",
            "hasCourseInstance": {
            "@type": "CourseInstance",
            "courseMode": "online",
            "courseWorkload": "{% if page.url contains 'Machine_learning' %}PT5H{% elsif page.url contains 'Linear_algebra' %}PT2H{% elsif page.url contains 'Calculus' %}PT3H{% elsif page.url contains 'Probability' %}PT2H30M{% elsif page.url contains 'Discrete' %}PT4H{% endif %}",
            "instructor": {
                "@type": "Person",
                "name": "Yusuke Yokota"
            }
            },
            "offers": {
            "@type": "Offer",
            "price": "0",
            "priceCurrency": "USD",
            "availability": "https://schema.org/InStock",
            "category": "free"
            }
        }
        }
        </script>
        <!-- WebApplication Schema for Interactive Demos -->
        <script type="application/ld+json">
        {
        "@context": "https://schema.org",
        "@type": "WebApplication",
        "name": "{{ page.title }} Interactive Demo",
        "description": "Interactive demonstration of {{ page.title | downcase }} concepts",
        "applicationCategory": "EducationalApplication",
        "operatingSystem": "Web Browser",
        "url": "https://yusukey01.github.io{{ page.url }}",
        "author": {
            "@type": "Person",
            "name": "Yusuke Yokota"
        },
        "applicationSubCategory": "Mathematical Visualization",
        "featureList": [
            "Interactive mathematical visualization",
            "Real-time parameter adjustment",
            "Educational demonstrations"
        ],
        "isAccessibleForFree": true,
        "educationalUse": "instruction",
        "educationalLevel": "university"
        }
        </script>
        <div class="hero-section">
            <h1 class="webpage-name">Trace & Norms
            </h1>
        </div>

        {% include section_navigation.html %}

        <div class="topic-nav">
            <a href="#tr">Trace</a>
            <a href="#norms">Norms</a>
            <a href="#ms">Introduction to Metric Spaces</a>
        </div> 

        <div class="container">  
           
            <section id="tr" class="section-content">
            <h2>Trace</h2>
            <p>
            The <strong>trace</strong> of an \(n \times n\) matrix \(A\) is the sum of the diagonal entries of \(A\) and
            denoted by \(\text{tr }(A)\).
            \[
            \text{tr }(A) =\sum_{k=1}^n a_{kk} 
            \]
            We list some properties of the trace. 
            <ol style="padding-left: 40px;">
                <li>\(\text{tr }(cA) = c (\text{tr }A) \quad , c \in \mathbb{R}\)</li>
                <li>\(\text{tr }(A+B) = \text{tr }A + \text{tr }B  \quad , B \in \mathbb{R}^{n \times n}\)</li>
                <li>\(\text{tr }A^T = \text{tr }A \)</li> <br>
                <li>\(\text{tr }(A^TB) = \sum_{i=1}^m\sum_{j=1}^n a_{ij}b_{ij}\)</li> <br>
                <li>\(\text{tr }(AB) = \text{tr }(BA) \)</li>
                Note: \(A\) can be \(m \times n\) and  \(B\) can be \(n \times m\). More generally, <br>
                \(\text{tr }(ABC) = \text{tr }(CAB) = \text{tr }(BCA) \): <strong>cyclic permutation property</strong> <br><br>
                <li>\(\text{tr }A =\sum_{i=1}^n \lambda_i \) where \(\lambda_1, \cdots, \lambda_n\) are the eigenvalues of \(A\)</li>
            </ol>
            For example, consider a <a href="symmetry.html"><strong>quadratic form</strong></a> \(x^TAx \in \mathbb{R}\). 
            <br>
            Let \(x\) be a vector in \(\mathbb{R}^n\). By Property 1 and 5, 
            \[
            \begin{align*}
            x^TAx = \text{tr }(x^TAx) &= \text{tr }(xx^TA) \\\\
                                      &= (x^Tx)\text{tr }(A)
            \end{align*}
            \]
            <br>
            Property 4 is actually the definition of the <a href="orthogonality.html"><strong>inner product</strong></a> for matrices. 
            <br>
            We call it the <strong>Frobenius inner product</strong>.  For matrices \(A, B \in \mathbb{R}^{m \times n}\),
            \[
            \left\langle A, B \right\rangle_F = \text{tr }(A^TB) = \sum_{i=1}^m\sum_{j=1}^n a_{ij}b_{ij}
            \]
            Also, like the Euclidean(\(l_2\)) norm \(\|v\|_2 = \sqrt{v \cdot v} = \sqrt{v^Tv}\) of vector, we define 
            the <strong>Frobenius norm</strong> of a "matrix" \(A\):
            \[
            \| A \|_F = \sqrt{\left\langle A, A \right\rangle_F} = \sqrt{\text{tr }(A^TA)}
            \] 
            </p>
            </section>

            <section id="norms" class="section-content">
            <h2>Norms</h2>
            <p>
            Norms play a key role in <strong>normalization</strong> and <strong>regularization</strong>. Depending on the
            application, we choose an appropriate norm. In machine learning, norms are fundamental for training stability of models. 
            Norms are used to scale data to a "common" range. It ensures that every single feature contributes
            "equally" to the training of the model and reduces sensitivity to the scale of each feature. This
            <strong>normalization</strong> process is important in the "distance(= norm)" based models such as 
            <a href="../Machine_learning/svm.html"><strong>support vector machines(SVMs)</strong></a> and <strong>\(k\)-nearest neighbor(k-NN)</strong>. 
            Also, <a href="../Machine_learning/regression.html"><strong>regularization process</strong></a> uses norms to "penalize" the model complexity, which avoids 
            overfitting and then helps the model <strong>generalize</strong> to new data.
            <br><br>
            Another popular norm of a matrix is the <strong>nuclear norm(trace norm)</strong>. 
            <br>Given a matrix \(A \in \mathbb{R}^{m \times n}\), then the nuclear norm of \(A\) is defined as the sum of its
            <strong>singular values</strong>.
            \[\| A\|_{\ast} = \sum_{i} \sigma_i \geq 0\]
            <div class="theorem">
                <span class="theorem-title">Theorem 1:</span>  
                Let a matrix \(A = U\Sigma V^T\) by SVD. Then
                \[
                \| A\|_{\ast} = \sum_{i} \sigma_i = \text{tr }(\Sigma) = \text{tr }(\sqrt{A^TA})
                \].
            </div>
            <div class="proof">
                <span class="proof-title">Proof:</span>
                Suppose \(A = U\Sigma V^T\) by the singular value decomposition. 
                Since \(A^TA = (V\Sigma U^T)(U\Sigma V^T) = V\Sigma^2 V^T\), 
                \[\sqrt{A^TA} = V\Sigma V^T\]
                Then by the cyclic permutation property of trace, 
                \[
                \begin{align*}
                \text{tr }(\sqrt{A^TA}) = \text{tr }(V\Sigma V^T) &= \text{tr }(\Sigma V^T V) \\\\
                                                                  &= \text{tr }(\Sigma )
                \end{align*}
                \]
            </div>
            <br>
            Also, the <strong>induced norm</strong> of \(A\) is defined as
            \[\| A \|_p = \max_{x \neq 0} \frac{\| Ax \|_p}{\| x \|_p} = \max_{\| x \| =1} \| Ax \|_p \tag{1}\]
            Typically, when \(p =2\), we call it the <strong>spectral norm</strong> of \(A\), which is just
            the largest singular value of \(A\).
            \[\| A \|_2 = \max_{i} \sigma_i  = \sqrt{\lambda_{\max} (A^TA)}\] 
            where \(\lambda_{\max}\) is the largest eigenvalue of \(A^TA\).
            <br>The spectral norm is commonly used to control the <strong>Lipschiz continuity</strong> of
            functions in the <a href="../Machine_learning/neural_networks.html"><strong>neural network</strong></a> in order to prevent <strong>vanishing or exploding
            gradients</strong>.
            <br><br>
            \(\| x \|_p \) in (1) is called the <strong>\(p\)-norm</strong> of the vector \(x\). The Euclidean norm is 
            a specific case of \(p\)-norm where \(p =2\). In general, 
            \[\| x \|_p = (\sum_{i =1} ^n |x_i|^p)^{\frac{1}{p}} \quad , p \geq 1\]
            When \(p =1\), we call it <strong>Manhattan norm</strong>, which is commonly used in the grid-based
            environments. Also, if we need <strong>sparsity</strong> in models such as Lasso regression, the Manhattan
            norm is used as a regularizer. 
            <br>Moreover, when \(p\) approaches \(\infty\), we define the <strong>maximum norm</strong>: 
            \[\| x \|_\infty = \lim_{p \to \infty} \| x \|_p = \max_{i} |x_i|\]
            In numerical analysis, the maximum norm is used for "worst-case" error analysis. Similarly,
            in machine learning, it is used when we want to minimize the worst error rather than the total
            error across sample data.
            </p>
            </section>

            <section id="ms" class="section-content">
            <h2>Introduction to Metric Spaces</h2>
            <p>
            We have learned about <a href="vectorspaces.html"><strong>vector spaces</strong></a> and 
            norms, which measure "distance" between points(vectors) in \(\mathbb{R}^n\). Although this concept may seem
            "general" as it extends beyond the familiar 3D space, the vector space with norms is actually a specific
            case of a more general mathematical structure called a <strong>metric space</strong>. 
            <br><br>
            Understanding this broader framework helps us see how the norms we use in linear algebra fit into the larger 
            picture of <strong>mathematical analysis</strong>. As an introduction to this perspective, we define the metric 
            space \((M, d)\) as follows.
        
            <div class="theorem">
                <span class="theorem-title">Metric Space \((M, d)\):</span> 
                Suppose \(M\) is a set and \(d\) is a real function defined on the Cartesian product
                \(M \times M\). Then \(d\) is called a distance function, or <strong>metric</strong> on \(M\) if and only if 
                \(\forall a, b, c \in M\),
                <ol style="padding-left: 40px;">
                    <li>Positivity: \(d(a,b) \geq 0\) with equality iff \(a = b\)</li>
                    <li>Symmetry: \(d(a,b) = d(b,a)\)</li>
                    <li>The triangle inequality \(d(a,b) \leq d(a,c) + d(c, b)\) </li>
                </ol>
            </div>

            Often we just say "the metric space \(M\)" if the metric \(d\) is obvious.
            <br>Note: Suppose \(X\) and \(Y\) are sets. Then <strong>Cartesian product </strong>of \(X\) and \(Y\)
            is a set of "ordered" pair 
            \[\{(x, y) | x \in X, \, y\in Y\}\] 
            <br>
            Now, you can see that any vector space equipped with a norm is a metric space. We call it 
            a <strong>normed vector space</strong>. More specifically, in linear algebra, we have worked within
            the framework of <strong>inner product spaces</strong> whose norm is defined by the inner product
            \[
            d(u, v) = \|u-v\|= \sqrt{\langle u-v, u-v \rangle} 
            \]
            The most familiar example of inner product space is the <strong>Euclidean space</strong> \(\mathbb{R}^n\).

            In <strong>machine learning</strong>, understanding how inner product spaces naturally extend to normed spaces and then to metric 
            spaces provides the mathematical foundation for defining <strong>similarity</strong> measures, regularization techniques, 
            and convergence criteria in optimization algorithms.
            </p>
            </section>
        </div>
        <script src="/js/main.js"></script>
    </body>
</html>