---
layout: default
title: Trace
level: detail
description: Learn about trace, norms, and intorduction to Metric Spaces.
---
<!DOCTYPE html>
<html>
    <body>
        <div class="hero-section">
            <h1 class="webpage-name">Trace
            </h1>
        </div>

        <div class="topic-nav">
            <a href="#tr">Trace</a>
            <a href="#normes">Normes</a>
            <a href="#ms">Intorduction to Metric Spaces</a>
        </div> 

        <div class="container">  
           
            <section id="tr" class="section-content">
            <h2>Trace</h2>
            <p>
            The <strong>trace</strong> of an \(n \times n\) matrix \(A\) is the sum of the diagonal entries of \(A\) and
            denoted by \(\text{tr }(A)\).
            \[
            \text{tr }(A) =\sum_{k=1}^n a_{kk} 
            \]
            We list some properties of the trace. 
            <ol style="padding-left: 40px;">
                <li>\(\text{tr }(cA) = c (\text{tr }A) \quad , c \in \mathbb{R}\)</li>
                <li>\(\text{tr }(A+B) = \text{tr }A + \text{tr }B  \quad , B \in \mathbb{R}^{n \times n}\)</li>
                <li>\(\text{tr }A^T = \text{tr }A \)</li> <br>
                <li>\(\text{tr }(A^TB) = \sum_{i=1}^m\sum_{j=1}^n a_{ij}b_{ij}\)</li> <br>
                <li>\(\text{tr }(AB) = \text{tr }(BA) \)</li>
                Note: \(A\) can be \(m \times n\) and  \(B\) can be \(n \times m\). More generally, <br>
                \(\text{tr }(ABC) = \text{tr }(CAB) = \text{tr }(BCA) \): <strong>cyclic permutation property</strong> <br><br>
                <li>\(\text{tr }A =\sum_{i=1}^n \lambda_i \) where \(\lambda_1, \cdots, \lambda_n\) are the eigenvalues of \(A\)</li>
            </ol>
            For example, consider a quadratic form \(x^TAx \in \mathbb{R}\). <a href="symmetry.html"><strong> Check: Symmetry</strong></a>
            <br>
            Let \(x\) be a vector in \(\mathbb{R}^n\) by Property 1 and 5, 
            \[
            x^TAx = \text{tr }(x^TAx) = \text{tr }(xx^TA) = (x^Tx)\text{tr }(A)
            \]
            <br>
            Property 4 is actually the definition of the <strong>inner product</strong> for matrices. 
            <a href="orthogonality.html"><strong>Check: orthogonality </strong></a>
            <br>We call it the <strong>Frobenius inner product</strong>.  For matrices \(A, B \in \mathbb{R}^{m \times n}\),
            \[
            \left\langle A, B \right\rangle_F = \text{tr }(A^TB) = \sum_{i=1}^m\sum_{j=1}^n a_{ij}b_{ij}
            \]
            Also, like the Euclidean(\(l_2\)) norm \(\|v\|_2 = \sqrt{v \cdot v} = v^Tv\) of vector, we define 
            the <strong>Frobenius norm</strong> of a "matrix" \(A\):
            \[
            \| A \|_F = \sqrt{\left\langle A, A \right\rangle_F} = \sqrt{\text{tr }(A^TA)}
            \] 
            </p>
            </section>

            <section id="normes" class="section-content">
            <h2>Norms</h2>
            <p>
            Norms play a key role in <strong>normalization</strong> and <strong>regularization</strong>. Depending on the
            application, we choose an appropriate norm. In machine learning, norms are fundational for training stability of models. 
            Norms are used to scale data to a "common" range. It ensures that every single feature contributes
            "equally" to the training of the model and reduces sensitivity to the scale of each feature. This
            <strong>normalization</strong> process is important in the "distance(= norm)" based models such as 
            <strong>support vector machines(SVMs)</strong> and <strong>\(k\)-nearest neighbor(k-NN)</strong>. 
            Also, <strong>regularization</strong> process uses norms to "penalize" the model complexity, which avoids 
            overfitting and then helps the model <strong>generalize</strong> to new data.
            <br><br>
            Another popular norm of a matrix is the <strong>nuclear norm(trace norm)</strong>. 
            <br>Given a matrix \(A \in \mathbb{R}^{m \times n}\), then the nuclear norm of \(A\) is defined as the sum of its
            <strong>singular values</strong>.
            \[\| A\|_{\ast} = \sum_{i} \sigma_i \geq 0\]
            <div class="theorem">
                <span class="theorem-title">Theorem 1:</span>  
                Let a matrix \(A = U\Sigma V^T\) by SVD. Then
                \[\| A\|_{\ast} = \sum_{i} \sigma_i = \text{tr }(\Sigma) = \text{tr }(\sqrt{A^TA})\].
            </div>
            <div class="proof">
                <span class="proof-title">Proof:</span>
                Suppose \(A = U\Sigma V^T\) by the singular value decomposition. 
                Since\(A^TA = (V\Sigma U^T)(U\Sigma V^T) = V\Sigma^2 V^T\), 
                \[\sqrt{A^TA} = V\Sigma V^T\]
                Then by the cyclic permutation property of trace, 
                \[\text{tr }(\sqrt{A^TA}) = \text{tr }(V\Sigma V^T) = \text{tr }(\Sigma V^T V) = \text{tr }(\Sigma ) \]
            </div>
            <br>
            Also, the <strong>induced norm</strong> of \(A\) is defined as
            \[\| A \|_p = \max_{x \neq 0} \frac{\| Ax \|_p}{\| x \|_p} = \max_{\| x \| =1} \| Ax \|_P \tag{1}\]
            Typically, when \(p =2\), we call it the <strong>spectral norm</strong> of \(A\), which is just
            the largest singular value of \(A\).
            \[\| A \|_2 = \max_{i} \sigma_i  = \sqrt{\lambda_{\max} (A^TA)}\] 
            where \(\lambda_{max}\) is the largest eigenvalue of \(A\). 
            <br>The spectral norm is commonly used to control the <strong>Lipschiz continuity</strong> of
            functions in the <strong>neural network</strong> in order to prevent <strong>vanishing or exploding
            gradients</strong>.
            <br><br>
            \(\| x \|_p \) in (1) is called the <strong>\(p\)-norm</strong> of the vector \(x\). The Euclidean norm is 
            a specific case of \(p\)-norm where \(p =2\). In general, 
            \[\| x \|_p = (\sum_{i =1} ^n |x_i|^p)^{\frac{1}{p}} \quad , p \geq 1\]
            When \(p =1\), we call it <strong>Manhattan norm</strong>, which is commonly used in the grid-based
            environments. Also, if we need <strong>sparsity</strong> in models such as Lasso regression, the Manhattan
            norm is used as a regularizer. 
            <br>Moreover, when \(p\) approachies \(\infty\), we define the <strong>maximum norm</strong>: 
            \[\| x \|_\infty = \lim_{p \to \infty} \| x \|_p = \max_{i} |x_i|\]
            In numerical analysis, the maximum norm is used for "worst-case" error analysis. Similarly,
            in machine learning, it is used when we want to minimize the worst error rather than the total
            error across sample deta.
            </p>
            </section>

            <section id="ms" class="section-content">
            <h2>Intorduction to Metric Spaces</h2>
            <p>
            we learned about <a href="vectorspaces.html"><strong>vector spaces</strong></a> and 
            norms, which measure "distance" between points(vectors) in \(\mathbb{R}^n\). Although this concept may seem
            "general" as it extends beyond the familiar 3D space, the vector space with norms is actually a specific
            case of a more general mathematical structure called a <strong>metric space</strong>. As an introduction to
            <strong>mathematical analysis</strong>, we define the metric space \((M, d)\) as follows.
            <div class="theorem">
                <span class="theorem-title">Metric Space \((M, d)\):</span> 
                Suppose \(M\) is a set and \(d\) is a real function defined on the Cartesian product
                \(M \times M\). Then \(d\) is called a distance function, or <strong>metric</strong> on \(M\) if and only if 
                \(\forall a, b, c \in M\),
                <ol style="padding-left: 40px;">
                    <li>Positivity: \(d(a,b) \geq 0\) with equality iff \(a = b\)</li>
                    <li>Symmetry: \(d(a,b) = d(b,a)\)</li>
                    <li>The triangle inequality \(d(a,b) \leq d(a,c) + d(c, b)\) </li>
                </ol>
            </div>
            Often we just say "the matric space \(M\)" if the metric \(d\) is obvious.
            <br>Note: Suppose \(X\) and \(Y\) are sets. Then <strong>Cartesian product </strong>of \(X\) and \(Y\)
            is a set of "ordered" pair 
            \[\{(x, y) | x \in X, \, y\in Y\}\] 
            <br>
            Now, you can see that a vector space with a norm is a metric space. We call it 
            a <strong>normed vector space</strong>. Particularly, in linear algebra, we have worked within
            the framework of <strong>inner product spaces</strong> whose norm is defined by the inner product
            \[d(u, v) = \|u-v\|= \sqrt{\langle u-v, u-v \rangle} \]
            The most familiar examle of inner product space is the <strong>Euclidean space</strong> \(\mathbb{R}^n\).
            In machine learning, metric spaces are used to defince the distance, or <strong>similarity</strong> between
            data points. 
            </p>
            </section>
        </div>
        <script src="/js/main.js"></script>
    </body>
</html>