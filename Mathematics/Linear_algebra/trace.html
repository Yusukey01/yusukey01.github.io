---
layout: default
title: Trace & Norms
topic_id: linalg-10
level: detail
uses_math: true
uses_python: false
---
<!DOCTYPE html>
<html>
    <body>
        {% include learning_resource_schema.html topic_id=page.topic_id %}
        
        <div class="hero-section">
            <h1 class="webpage-name">Trace & Norms</h1>
        </div>

        {% include section_navigation.html %}

        <div class="topic-nav">
            <a href="#tr">Trace</a>
            <a href="#norms">Norms</a>
            <a href="#ms">Introduction to Metric Spaces</a>
        </div> 

        <div class="container">  
           
            <section id="tr" class="section-content">
                <h2>Trace</h2>

                <p>
                    The trace is a fundamental scalar function that associates a single number with a square matrix. 
                    Despite its simple definition as the sum of diagonal entries, the trace has deep connections to 
                    eigenvalues, matrix norms, and appears throughout optimization and machine learning. Its cyclic 
                    permutation property makes it invaluable for manipulating matrix expressions.
                </p>

                <div class="theorem">
                    <span class="theorem-title">Definition: Trace</span>
                    <p>
                        The <strong>trace</strong> of an \(n \times n\) matrix \(A\) is the sum of the diagonal entries of \(A\) and
                        denoted by \(\text{tr }(A)\):
                        \[
                        \text{tr }(A) =\sum_{k=1}^n a_{kk}.
                        \]
                    </p>
                </div>

                <div class="theorem">
                    <span class="theorem-title">Theorem: Properties of Trace</span>
                    <p>
                        Let \(A, B\) be \(n \times n\) matrices and \(c \in \mathbb{R}\). The trace satisfies:
                    </p>
                    <ol style="padding-left: 40px;">
                        <li>Linearity: \(\text{tr }(cA) = c(\text{tr }A)\)</li>
                        <li>Additivity: \(\text{tr }(A + B) = \text{tr}A + \text{tr }B\)</li>
                        <li>Transpose invariance: \(\text{tr }(A^\top) = \text{tr}A\)</li>
                        <li>Frobenius inner product: \(\text{tr }(A^\top B) = \sum_{i=1}^m\sum_{j=1}^n a_{ij}b_{ij}\)</li>
                        <li>Cyclic permutation: \(\text{tr }(AB) = \text{tr }(BA)\)</li>
                        <li>Eigenvalue sum: \(\text{tr }A = \sum_{i=1}^n \lambda_i\) where \(\lambda_1, \ldots, \lambda_n\) 
                            are the eigenvalues of \(A\)</li>
                    </ol>
                    <p>
                        Note: For property 5, \(A\) can be \(m \times n\) and \(B\) can be \(n \times m\). More generally, 
                        \(\text{tr}(ABC) = \text{tr}(CAB) = \text{tr}(BCA)\).
                    </p>
                </div>

                <p>
                    Consider a <a href="symmetry.html"><strong>quadratic form</strong></a> \(x^\top Ax \in \mathbb{R}\). 
                    Let \(x\) be a vector in \(\mathbb{R}^n\). By Property 1 and 5, 
                    \[
                    \begin{align*}
                    x^\top Ax = \text{tr }(x^\top Ax) &= \text{tr }(xx^\top A) \\\\
                                            &= (x^\top x)\text{tr }(A).
                    \end{align*}
                    \]
                </p>
                
                <p>
                    Property 4 is actually the notion of the <a href="orthogonality.html"><strong>inner product</strong></a> for matrices. 
                    We call it the <strong>Frobenius inner product</strong>, and also like the Euclidean(\(l_2\)) norm \(\|v\|_2 = \sqrt{v \cdot v} = \sqrt{v^\top v}\) 
                    of vectors, we define the norm for matrices.
                </p>

                <div class="theorem">
                    <span class="theorem-title">Definition: Frobenius Inner Product</span>
                    <p>
                        For matrices \(A, B \in \mathbb{R}^{m \times n}\), the <strong>Frobenius inner product</strong> is defined as:
                        \[
                        \left\langle A, B \right\rangle_F = \text{tr }(A^\top B) = \sum_{i=1}^m\sum_{j=1}^n a_{ij}b_{ij}.
                        \]
                    </p>
                    <p>
                        Inaddition, the <strong>Frobenius norm</strong> of a matrix \(A\) is defined as
                        \[
                        \| A \|_F = \sqrt{\left\langle A, A \right\rangle_F} = \sqrt{\text{tr }(A^\top A)}.
                        \] 
                    </p>
                </div>

            </section>

            <section id="norms" class="section-content">
                <h2>Norms</h2>

                <p>
                    While the trace characterizes matrices through a single scalar, norms provide a way to measure 
                    the "size" or "magnitude" of both vectors and matrices. In machine learning, norms are fundamental 
                    for training stability of models. Norms are used to scale data to a "common" range. It ensures that 
                    every single feature contributes "equally" to the training of the model and reduces sensitivity to 
                    the scale of each feature. This <strong>normalization</strong> process is important in the "distance(i.e., norm)" 
                    based models such as <a href="../Machine_learning/svm.html"><strong>support vector machines(SVMs)</strong></a> 
                    and <strong>\(k\)-nearest neighbor(k-NN)</strong>. 
                    Also, <a href="../Machine_learning/regression.html"><strong>regularization process</strong></a> uses norms to 
                    "penalize" the model complexity, which avoids overfitting and then helps the model <strong>generalize</strong> to new data.
                </p>

                <p>
                    Another popular norm of a matrix is the <strong>nuclear norm(trace norm)</strong>. 
                </p>
                <div class="theorem">
                    <span class="theorem-title">Definition: Nuclear Norm(Trace Norm)</span>
                    <p>
                        Given a matrix \(A \in \mathbb{R}^{m \times n}\), then the nuclear norm of \(A\) is defined as the sum of its 
                        <strong>singular values</strong>:
                        \[
                        \| A\|_{\ast} = \sum_{i} \sigma_i \geq 0.
                        \]
                    </p>
                </div>
                
                <div class="theorem">
                    <span class="theorem-title">Theorem 1:</span>  
                    <p>
                        Let a matrix \(A = U\Sigma V^\top\) by SVD. Then
                        \[
                        \| A\|_{\ast} = \sum_{i} \sigma_i = \text{tr }(\Sigma) = \text{tr }(\sqrt{A^\top A})
                        \].
                    </p>
                </div>

                <div class="proof">
                    <span class="proof-title">Proof:</span>
                    <p>
                        Suppose \(A = U\Sigma V^\top \) by the singular value decomposition. 
                        Since \(A^\top A = (V\Sigma U^\top)(U\Sigma V^\top) = V\Sigma^2 V^\top \), 
                        \[
                        \sqrt{A^\top A} = V\Sigma V^\top
                        \]
                        Then by the cyclic permutation property of trace, 
                        \[
                        \begin{align*}
                        \text{tr }(\sqrt{A^\top A}) = \text{tr }(V\Sigma V^\top) &= \text{tr }(\Sigma V^\top V) \\\\
                                                                        &= \text{tr }(\Sigma ).
                        \end{align*}
                        \]
                    </p>
                </div>
                
                <div class="theorem">
                    <span class="theorem-title">Definition: Induced Norm & Spectral Norm</span>
                    <p>
                        The <strong>induced norm</strong> of \(A\) is defined as
                        \[
                        \| A \|_p = \max_{x \neq 0} \frac{\| Ax \|_p}{\| x \|_p} = \max_{\| x \| =1} \| Ax \|_p. \tag{1}
                        \]
                    </p>
                    <p>
                        Typically, when \(p =2\), we call it the <strong>spectral norm</strong> of \(A\), which is just
                        the largest singular value of \(A\):
                        \[
                        \| A \|_2 = \max_{i} \sigma_i  = \sqrt{\lambda_{\max} (A^\top A)}
                        \] 
                        where \(\lambda_{\max}\) is the largest eigenvalue of \(A^\top A\).
                    </p>
                </div>
            
                <p>
                    The spectral norm is commonly used to control the <strong>Lipschiz continuity</strong> of
                    functions in the <a href="../Machine_learning/neural_networks.html"><strong>neural network</strong></a> in order to prevent 
                    <strong>vanishing or exploding gradients</strong>.
                </p>

                <p>
                    \(\| x \|_p \) in (1) is called the <strong>\(p\)-norm</strong> of the vector \(x\). The Euclidean norm is 
                    a specific case of \(p\)-norm where \(p =2\). In general, we define the \(p\)-norm as follows.
                </p>

                <div class="theorem">
                    <span class="theorem-title">Definition: \(p\)-Norm</span>
                    <p>
                        The <strong>\(p\)-norm</strong> of the vector \(x\) is given by:
                        \[
                        \| x \|_p = (\sum_{i =1} ^n |x_i|^p)^{\frac{1}{p}} \quad , p \geq 1.
                        \]
                    </p>
                </div>

                <p>              
                    When \(p =1\), we call it <strong>Manhattan norm</strong>, which is commonly used in the grid-based
                    environments. Also, if we need <strong>sparsity</strong> in models such as Lasso regression, often, 
                    the Manhattan norm is used as a regularizer. 
                </p>

                <p>
                    when \(p\) approaches \(\infty\), we define the <strong>maximum norm</strong>: 
                    \[
                    \| x \|_\infty = \lim_{p \to \infty} \| x \|_p = \max_{i} |x_i|.
                    \]
                    In numerical analysis, the maximum norm is used for "worst-case" error analysis. Similarly,
                    in machine learning, it is used when we want to minimize the worst error rather than the total
                    error across sample data.
                </p>

            </section>

            <section id="ms" class="section-content">
                <h2>Introduction to Metric Spaces</h2>
                <p>
                    We have learned about <a href="vectorspaces.html"><strong>vector spaces</strong></a> and 
                    norms, which measure "distance" between points(vectors) in \(\mathbb{R}^n\). Although this concept may seem
                    "general" as it extends beyond the familiar 3D space, the vector space with norms is actually a specific
                    case of a more general mathematical structure called a <strong>metric space</strong>. 
                </p>
                <p>
                    Understanding this broader framework helps us see how the norms we use in linear algebra fit into the larger 
                    picture of <strong>mathematical analysis</strong>. As an introduction to this perspective, we define the metric 
                    space \((M, d)\) as follows.
                </p>

                <div class="theorem">
                    <span class="theorem-title">Definition: Metric Space</span> 
                    <p>
                        A <strong>metric space</strong> is an ordered pair \((M, d)\)consisting of a nonempty set 
                        \(M\) and a <strong>metric</strong> \(d\) on \(M\). The metric \(d\) is a function 
                        \(d: M \times M \to \mathbb{R}\) that defines the distance between any two elements of \(M\). 
                        For all \(a, b, c \in M\), the following axioms must hold:
                    </p>
                    <ol style="padding-left: 40px;">
                        <li>Positivity: \(d(a,b) \geq 0\) with equality if and only if \(a = b\).</li>
                        <li>Symmetry: \(d(a,b) = d(b,a)\).</li>
                        <li>The triangle inequality \(d(a,b) \leq d(a,c) + d(c, b)\).</li>
                    </ol>
                </div>

                <p>
                    Often we just say "the metric space \(M\)" if the metric \(d\) is obvious. 
                    <br>
                    Note: Suppose \(X\) and \(Y\) are sets. Then <strong>Cartesian product </strong>of \(X\) and \(Y\)
                    is a set of "ordered" pair:  
                    \[
                    X \times Y = \{(x, y) | x \in X, \, y\in Y\}.
                    \] 
                </p>

                <p>
                    Now, you can see that any vector space equipped with a norm is a metric space. We call it 
                    a <strong>normed vector space</strong>. More specifically, in linear algebra, we have worked within
                    the framework of <strong>inner product spaces</strong> whose norm is defined by the inner product
                    \[
                    d(u, v) = \|u-v\|= \sqrt{\langle u-v, u-v \rangle}.
                    \]
                    The most familiar example of inner product space is the <strong>Euclidean space</strong> \(\mathbb{R}^n\).
                </p>
                
                <p>
                    In <strong>machine learning</strong>, understanding how inner product spaces naturally extend to normed spaces and then to metric 
                    spaces provides the mathematical foundation for defining <strong>similarity</strong> measures, regularization techniques, 
                    and convergence criteria in optimization algorithms.
                </p>
            </section>
        </div>
        <script src="/js/main.js"></script>
    </body>
</html>