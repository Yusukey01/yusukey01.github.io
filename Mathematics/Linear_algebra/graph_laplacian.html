---
layout: default
title: Graph Laplacians and Spectral Methods
level: detail
description: Learn about graph Laplacians, spectral properties, graph signal processing, and their applications in Graph Neural Networks and modern machine learning.
uses_math: true
uses_python: true
---
<!DOCTYPE html>
<html>
    <body>
    <script type="application/ld+json">
    {
    "@context": "https://schema.org",
    "@type": "LearningResource",
    "name": "Graph Laplacians and Spectral Methods",
    "description": "Learn about graph Laplacians, spectral properties, graph signal processing, and their applications in Graph Neural Networks and modern machine learning",
    "learningResourceType": "lesson",
    "educationalUse": "instruction",
    "educationalLevel": "university",
    "interactivityType": "active",
    "author": {
        "@type": "Person",
        "name": "Yusuke Yokota",
        "jobTitle": "Mathematics & Computer Science Educator"
    },
    "publisher": {
        "@type": "Organization",
        "name": "MATH-CS COMPASS",
        "url": "https://yusukey01.github.io"
    },
    "about": [
        { "@type": "Thing", "name": "Graph Laplacian" },
        { "@type": "Thing", "name": "Spectral Graph Theory" },
        { "@type": "Thing", "name": "Normalized Laplacian" },
        { "@type": "Thing", "name": "Random Walk Laplacian" },
        { "@type": "Thing", "name": "Fiedler Vector" },
        { "@type": "Thing", "name": "Spectral Gap" },
        { "@type": "Thing", "name": "Cheeger Inequality" },
        { "@type": "Thing", "name": "Graph Signal Processing" },
        { "@type": "Thing", "name": "Graph Fourier Transform" },
        { "@type": "Thing", "name": "Graph Neural Networks" },
        { "@type": "Thing", "name": "Spectral Clustering" }
    ],
    "teaches": [
        "Graph Laplacian fundamentals and variants",
        "Spectral properties and their interpretations",
        "Graph signal processing techniques",
        "Efficient computation methods",
        "Applications to Graph Neural Networks",
        "Connections to clustering and embeddings"
    ],
    "isPartOf": {
        "@type": "Course",
        "name": "Linear Algebra",
        "description": "Explore the foundations of Linear Algebra, covering key concepts such as linear equations, vector spaces, eigenvalues, orthogonality, least squares, and stochastic matrices",
        "provider": {
        "@type": "Organization",
        "name": "MATH-CS COMPASS",
        "url": "https://yusukey01.github.io"
        },
        "instructor": {
        "@type": "Person",
        "name": "Yusuke Yokota",
        "jobTitle": "Mathematics & Computer Science Educator"
        },
        "courseCode": "I",
        "hasCourseInstance": {
        "@type": "CourseInstance",
        "courseMode": "online",
        "courseWorkload": "PT2H30M",
        "instructor": {
            "@type": "Person",
            "name": "Yusuke Yokota"
        }
        },
        "offers": {
        "@type": "Offer",
        "price": "0",
        "priceCurrency": "USD",
        "availability": "https://schema.org/InStock",
        "category": "free"
        }
    }
    }
    </script>
    
    <div class="hero-section">
        <h1 class="webpage-name">Graph Laplacians and Spectral Methods</h1>
    </div>

    {% include section_navigation.html %}

    <div class="topic-nav">
        <a href="#fundamentals">Graph Laplacian Fundamentals</a>
        <a href="#spectral">Spectral Properties</a>
        <a href="#signal">Graph Signal Processing</a>
        <a href="#computation">Practical Computations</a>
        <a href="#applications">Modern Applications</a>
    </div> 

    <div class="container">
        
        <section id="fundamentals" class="section-content">
            <h2>Graph Laplacian Fundamentals</h2>
            <p>
                The <strong>graph Laplacian</strong> is a matrix operator that plays a central role in spectral graph theory. 
                It's a fundamental tool that captures the connectivity and structure of a graph, enabling powerful 
                analytical techniques. Building on your knowledge of 
                <a href="../Discrete/intro_graph.html">graph theory</a> and <a href="eigenvectors.html">eigenvalues</a>, 
                we'll explore its forms and properties.
            </p>

            <h3>The Unnormalized Laplacian</h3>
            <p>
                For a graph \(G\) with \(n\) vertices, its structure can be represented by an <strong>adjacency matrix</strong> 
                \(\boldsymbol{A}\), where \(A_{ij} = 1\) if vertices \(i\) and \(j\) are connected (and 0 otherwise). 
                The <strong>degree matrix</strong> \(\boldsymbol{D}\) is a diagonal matrix where \(D_{ii}\) is the 
                degree of vertex \(i\).
            </p>
            <p>
                The <strong>unnormalized graph Laplacian</strong> \(\boldsymbol{L}\) is defined as:
                \[
                \boldsymbol{L} = \boldsymbol{D} - \boldsymbol{A}
                \]
            </p>
            <p>
                Here is a concrete example:
                <div style="text-align: center;">
                <img src="images/laplacian.jpg" alt="A simple graph and its corresponding Laplacian matrix calculation."  class="responsive-image">
                </div>
                \[
                \underbrace{
                \begin{bmatrix} 2 & 0 & 0 & 0 & 0 \\ 
                                0 & 2 & 0 & 0 & 0 \\
                                0 & 0 & 4 & 0 & 0 \\
                                0 & 0 & 0 & 1 & 0 \\
                                0 & 0 & 0 & 0 & 1 \\
                \end{bmatrix}
                }_{\boldsymbol{D}}
                - 
                \underbrace{
                \begin{bmatrix} 0 & 1 & 1 & 0 & 0 \\ 
                                1 & 0 & 1 & 0 & 0 \\
                                1 & 1 & 0 & 1 & 1 \\
                                0 & 0 & 1 & 0 & 0 \\
                                0 & 0 & 1 & 0 & 0 \\
                \end{bmatrix}
                }_{\boldsymbol{A}}
                = 
                \underbrace{
                \begin{bmatrix} 2 & -1 & -1 & 0 & 0 \\ 
                                -1 & 2 & -1 & 0 & 0 \\
                                -1 & -1 & 4 & -1 & -1 \\
                                0 & 0 & -1 & 1 & 0 \\
                                0 & 0 & -1 & 0 & 1 \\
                \end{bmatrix}
                }_{\boldsymbol{L}}
                \]
            </p>

            <h3>The Laplacian Quadratic Form (Measuring Smoothness)</h3>
            <p>
                A key insight into the Laplacian comes from its <strong>quadratic form</strong>. 
                If we define a function (or "signal") \(\boldsymbol{f}\) on the graph's vertices, where 
                \(\boldsymbol{f} \in \mathbb{R}^n\) and \(f_i\) is the value at vertex \(i\), the 
                quadratic form measures the "smoothness" of the signal:
                \[
                \boldsymbol{f}^\top \boldsymbol{L} \boldsymbol{f} = \sum_{(i,j) \in E} w_{ij}(f_i - f_j)^2
                \]
                (Assuming edge weights \(w_{ij}\), which are 1 for an unweighted graph).
            </p>
            <p>
                This expression, known as the <strong>Dirichlet energy</strong>, shows that a signal \(\boldsymbol{f}\) 
                is "smooth" (i.e., \(\boldsymbol{f}^\top \boldsymbol{L} \boldsymbol{f}\) is small) if adjacent 
                vertices have similar values. The Laplacian is thus an operator that measures 
                the total variance of a signal across the graph's edges.
            </p>

            <h3>Normalized Laplacians</h3>
            <p>
                The unnormalized Laplacian \(\boldsymbol{L}\) can be biased by vertices with
                large degrees. To address this, two <strong>normalized Laplacians</strong> are commonly used,
                especially in spectral clustering and GNNs:
            </p>
            <ol>
                <li>
                    <strong>Symmetric Normalized Laplacian (\(\mathcal{L}_{sym}\)):</strong>
                    \[
                    \mathcal{L}_{sym} = \boldsymbol{D}^{-1/2} \boldsymbol{L} \boldsymbol{D}^{-1/2} = \boldsymbol{I} - \boldsymbol{D}^{-1/2} \boldsymbol{A} \boldsymbol{D}^{-1/2}
                    \]
                    Its eigenvalues are normalized to the range \([0, 2]\), making it useful for spectral analysis and GNNs.
                </li>
                <li>
                    <strong>Random Walk Normalized Laplacian (\(\mathcal{L}_{rw}\)):</strong>
                    \[
                    \mathcal{L}_{rw} = \boldsymbol{D}^{-1} \boldsymbol{L} = \boldsymbol{I} - \boldsymbol{D}^{-1} \boldsymbol{A}
                    \]
                    This form is closely related to the transition matrix of a random walk on the graph (\(\boldsymbol{P} = \boldsymbol{D}^{-1}\boldsymbol{A}\)).
                </li>
            </ol>
        </section>

        <hr>

        <section id="spectral" class="section-content">
            <h2>Spectral Properties</h2>
            <p>
                The "spectral" in spectral graph theory refers to the <strong>eigenvalues</strong> 
                and <strong>eigenvectors</strong> of the graph Laplacian. Since \(\boldsymbol{L}\) 
                (and \(\mathcal{L}_{sym}\)) is real and symmetric, it has a complete set of 
                real eigenvalues and orthonormal eigenvectors.
            </p>
            <p>
                The eigendecomposition of the unnormalized Laplacian \(\boldsymbol{L}\) is:
                \[
                \boldsymbol{L} = \boldsymbol{V}\,\boldsymbol{\Lambda}\,\boldsymbol{V}^\top, 
                \quad
                \boldsymbol{\Lambda} = \mathrm{diag}(\lambda_0, \lambda_1, \dots, \lambda_{n-1}),
                \]
                with ordered eigenvalues
                \[
                0 = \lambda_0 \le \lambda_1 \le \cdots \le \lambda_{n-1},
                \]
                and orthonormal eigenvectors \(\boldsymbol{v}_0, \dots, \boldsymbol{v}_{n-1}\) 
                (which form a basis for \(\mathbb{R}^n\)).
            </p>

            <h3>The Smallest Eigenvalue (\(\lambda_0\))</h3>
            <p>
                The smallest eigenvalue \(\lambda_0\) is always 0. Its corresponding eigenvector 
                is the constant vector \(\boldsymbol{v}_0 = \boldsymbol{1}\) (a vector of all ones).
                \[
                \boldsymbol{L}\boldsymbol{1} = (\boldsymbol{D} - \boldsymbol{A})\boldsymbol{1} = \boldsymbol{D}\boldsymbol{1} - \boldsymbol{A}\boldsymbol{1} = \boldsymbol{d} - \boldsymbol{d} = \boldsymbol{0}
                \]
                (where \(\boldsymbol{d}\) is the vector of degrees).
            </p>
            <div class="theorem">
                <span class="theorem-title">Eigenvalue Multiplicity and Connectivity</span>
                The <strong>multiplicity of the zero eigenvalue</strong> (how many times \(\lambda_k = 0\)) 
                is equal to the number of <strong>connected components</strong> in the graph. For any 
                connected graph, \(\lambda_0 = 0\) is a simple eigenvalue, meaning \(\lambda_1 > 0\).
            </div>

            <h3>The Fiedler Vector and Algebraic Connectivity</h3>
            <p>
                The second smallest eigenvalue, \(\lambda_1\), is arguably the most important.
            </p>
            <div class="theorem">
                <span class="theorem-title">The Fiedler Vector and Algebraic Connectivity</span>
                The eigenvalue \(\lambda_1\) is called the <strong>algebraic connectivity</strong> 
                (or <strong>Fiedler value</strong>) of the graph.
                <ul style="padding-left: 40px;">
                    <li>\(\lambda_1 > 0\) if and only if the graph is connected.</li>
                    <li>A larger \(\lambda_1\) indicates a more "well-connected" graph (less of a "bottleneck").</li>
                </ul>
                The corresponding eigenvector \(\boldsymbol{v}_1\), known as the <strong>Fiedler vector</strong>,
                is central to spectral clustering. The sign of its entries (+/-) provides a natural
                partition of the graph's vertices into two sets, often revealing the graph's weakest link.
            </div>
            
            <h3>The Spectral Gap and Cheeger's Inequality</h3>
            <p>
                The "spectral gap" (the magnitude of \(\lambda_1\)) is formally related to the
                graph's connectivity by Cheeger's inequality.
            </p>
            <div class="theorem">
                <span class="theorem-title">Cheeger's Inequality</span>
                Let \(h(G)\) be the <strong>Cheeger constant</strong>, which measures the "best" partition 
                of the graph by finding the minimum "cut" normalized by the size of the smaller set:
                \[
                h(G) = \min_{S \subset V, 0 < |S| \leq n/2} \frac{|\partial S|}{\min(|S|, |V \setminus S|)}
                \]
                where \(|\partial S|\) is the number of edges between \(S\) and its complement \(V \setminus S\).
                <br><br>
                Cheeger's inequality bounds \(\lambda_1\) using this constant:
                \[
                \frac{h(G)^2}{2d_{max}} \leq \lambda_1 \leq 2h(G)
                \]
                This shows that a small \(\lambda_1\) (small spectral gap) implies the
                existence of a "bottleneck" cut (small \(h(G)\)), and vice-versa.
            </div>
        </section>

        <hr>

        <section id="signal" class="section-content">
            <h2>Graph Signal Processing (GSP)</h2>
            <p>
                The GSP framework re-interprets the Laplacian's eigenvectors as a 
                <strong>Fourier basis</strong> for signals on the graph, analogous to 
                sines and cosines in classical signal processing.
            </p>
            <div class="theorem">
                <span class="theorem-title">Graph Fourier Transform (GFT)</span>
                Let \(\boldsymbol{V} = [\boldsymbol{v}_0, \boldsymbol{v}_1, \ldots, \boldsymbol{v}_{n-1}]\) 
                be the matrix of orthonormal eigenvectors of \(\boldsymbol{L}\). 
                For any signal \(\boldsymbol{f}\) on the graph, its <strong>Graph Fourier Transform</strong> 
                (\(\hat{\boldsymbol{f}}\)) is its projection onto this basis:
                \[
                \hat{\boldsymbol{f}} = \boldsymbol{V}^T \boldsymbol{f} 
                \quad \text{(Analysis)}
                \]
                And the inverse GFT reconstructs the signal:
                \[
                \boldsymbol{f} = \boldsymbol{V} \hat{\boldsymbol{f}} = \sum_{k=0}^{n-1} \hat{f}_k \boldsymbol{v}_k
                \quad \text{(Synthesis)}
                \]
            </div>

            <p>
                The eigenvalues \(\lambda_k\) represent <strong>frequencies</strong>:
                <ul>
                    <li>
                        <strong>Small \(\lambda_k\) (e.g., \(\lambda_0, \lambda_1\)) ↔ Low Frequencies:</strong> 
                        The corresponding eigenvectors \(\boldsymbol{v}_k\) vary slowly across edges 
                        (they are "smooth").
                    </li>
                    <li>
                        <strong>Large \(\lambda_k\) ↔ High Frequencies:</strong> 
                        The eigenvectors \(\boldsymbol{v}_k\) oscillate rapidly, with adjacent 
                        nodes having very different values.
                    </li>
                </ul>
            </p>
            <p>
                This connects back to the Dirichlet energy. By expanding \(\boldsymbol{f}\) in the 
                eigenbasis, we can see how "rough" it is:
                \[
                \boldsymbol{f}^\top \boldsymbol{L} \boldsymbol{f} = 
                \boldsymbol{f}^\top (\boldsymbol{V} \boldsymbol{\Lambda} \boldsymbol{V}^\top) \boldsymbol{f} =
                (\boldsymbol{V}^T \boldsymbol{f})^\top \boldsymbol{\Lambda} (\boldsymbol{V}^T \boldsymbol{f}) =
                \hat{\boldsymbol{f}}^\top \boldsymbol{\Lambda} \hat{\boldsymbol{f}} =
                \sum_{k=0}^{n-1} \lambda_k \hat{f}_k^2
                \]
                This is the graph equivalent of Parseval's theorem. It shows that the 
                signal's energy is the sum of its spectral components, weighted by 
                their corresponding frequencies (\(\lambda_k\)).
            </p>
        </section>

        <hr>

        <section id="computation" class="section-content">
            <h2>Practical Computations</h2>
            <p>
                For large graphs (e.g., social networks with millions of nodes) seen in
                modern applications, computing the full eigendecomposition (\(O(n^3)\)) is impossible.
            </p>
            <p>
                Instead, iterative methods are used to find only the few
                eigenvectors and eigenvalues that are needed (typically the smallest ones).
            </p>
            <ul style="padding-left: 40px;">
                <li>
                    <strong>Power Iteration:</strong> A simple method to find the dominant
                    eigenvector (corresponding to \(\lambda_{max}\)). Can be adapted (as 
                    "inverse iteration") to find the smallest eigenvalues.
                </li>
                <li>
                    <strong>Lanczos Algorithm:</strong> A powerful iterative method for
                    finding the \(k\) smallest (or largest) eigenvalues and eigenvectors
                    of a symmetric matrix. It is far more efficient than full decomposition
                    when \(k \ll n\).
                </li>
            </ul>
            <p>
                Many modern GNN applications avoid eigendecomposition entirely by
                using polynomial approximations of graph filters, as discussed below.
            </p>
        </section>

        <hr>

        <section id="applications" class="section-content">
            <h2>Modern Applications</h2>
            <p>
                The properties of the graph Laplacian are fundamental to many
                algorithms in machine learning and data science.
            </p>

            <h3>Spectral Clustering</h3>
            <p>
                This is the most direct application of the Fiedler vector (\(\boldsymbol{v}_1\)). 
                To partition a graph into two communities:
                <ol>
                    <li>Compute the Fiedler vector \(\boldsymbol{v}_1\) of \(\boldsymbol{L}\) (or \(\mathcal{L}_{sym}\)).</li>
                    <li>Iterate through the nodes \(i = 1, \dots, n\).</li>
                    <li>If the \(i\)-th entry of \(\boldsymbol{v}_1\) is positive (\(v_{1,i} > 0\)), assign node \(i\) to Community A.</li>
                    <li>If it's negative or zero (\(v_{1,i} \le 0\)), assign node \(i\) to Community B.</li>
                </ol>
                This "sign-based" cut is proven to be a good approximation of the
                optimal "bottleneck" cut (Cheeger's inequality). This can be extended to 
                \(k\) communities using the first \(k\) eigenvectors (a method called
                <strong>spectral embedding</strong>).
            </p>
            
            <h3>Graph Neural Networks (GNNs)</h3>
            <p>
                Many GNNs, like Graph Convolutional Networks (GCNs), are based on
                spectral filtering. The idea is to apply a <strong>filter</strong>
                \(g(\boldsymbol{\Lambda})\) to the graph signal's "frequencies."
                \[
                \boldsymbol{f}_{out} = g(\boldsymbol{L}) \boldsymbol{f}_{in} = 
                \boldsymbol{V} g(\boldsymbol{\Lambda}) \boldsymbol{V}^T \boldsymbol{f}_{in}
                \]
                This is a "convolution" in the graph spectral domain.
            </p>
            <p>
                Directly computing this is too expensive. Instead, methods like <strong>ChebNet</strong>
                approximate the filter \(g(\cdot)\) using a \(K\)-th order polynomial:
                \[
                g_\theta(\boldsymbol{L}) \approx \sum_{k=0}^K \theta_k T_k(\tilde{\boldsymbol{L}})
                \]
                where \(T_k\) are Chebyshev polynomials and \(\tilde{\boldsymbol{L}}\) is
                the scaled Laplacian (\(\mathcal{L}_{sym}\)). This approximation
                is crucial because it allows for <strong>localized</strong> and
                <strong>efficient</strong> computations (a \(K\)-hop neighborhood) without
                ever computing the eigenvectors. The standard GCN is a
                first-order approximation (\(K=1\)) of this process.
            </p>

            <h3>Diffusion and Random Walks</h3>
            <p>
                The Laplacian governs diffusion processes on graphs. The
                <strong>heat equation</strong> on a graph, which describes how
                a "heat" signal \(u(t)\) diffuses over time, is:
            </p>
            <div class="theorem">
                <span class="theorem-title">Heat Diffusion on Graphs</span>
                \[
                \frac{\partial \boldsymbol{u}}{\partial t} = -\boldsymbol{L} \boldsymbol{u}
                \]
                The solution to this differential equation is:
                \[
                \boldsymbol{u}(t) = e^{-t\boldsymbol{L}} \boldsymbol{u}(0) = \sum_{k=0}^{n-1} e^{-t\lambda_k} \langle \boldsymbol{u}(0), \boldsymbol{v}_k \rangle \boldsymbol{v}_k
                \]
                This shows that high-frequency components (large \(\lambda_k\))
                decay very quickly, while the signal converges to its steady
                state (the average value, \(\boldsymbol{v}_0\)) at a rate
                determined by the Fiedler value \(\lambda_1\).
            </div>

            <h3>Other Applications</h3>
            <ul style="padding-left: 40px;">
                <li><strong>Recommender Systems:</strong> Used by companies like Netflix and YouTube to model user-item interactions as a graph and use diffusion or embeddings.</li>
                <li><strong>Protein Structure:</strong> AlphaFold uses GNNs and Laplacian-based features to model the 3D structure of proteins.</li>
                <li><strong>Social Network Analysis:</strong> Community detection (spectral clustering) at scale.</li>
                <li><strong>Computer Vision:</strong> Image segmentation and denoising by treating pixels as graph nodes.</li>
            </ul>

        </section>
        
    </div>
      
    <script src="/js/main.js"></script>

</body>
</html>