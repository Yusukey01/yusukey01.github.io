<!DOCTYPE html>
<html>
    <head> 
        <title>Symmetry</title>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <link rel="stylesheet" href="../styles.css"> 
        <meta name="viewport" content="width=device-width, initial-scale=1">
    </head>
    <body> 
        <div class="toc-container">
            <h2>Contents</h2>
            <ul>
                <li><a href="#sym">Symmetric Matrices</a></li>
                <li><a href="#qf">Quadratic Forms</a></li>
                <li><a href="#svd">Singular Value Decomposition(SVD)</a></li>
                <li><a href="#f_sub">SVD with Fundamental Subspaces</a></li>
            </ul>
        </div>
        <h1 id="sym">Symmetric Matrices</h1>
        <blockquote>
            A <strong>symmetric matrix</strong> is a square matrix \(A\) such that
            \[A^T= A\]
            For example, in the context of least-squares solutions, \(\hat{\beta}\) satisfies \(X^TX\beta = X^TY\).
            In this expression, \(X^TX\) is always symmetric. To confirm, let \(X\) be an \(m \times n\) matrix. Then
            \[(X^TX)^T = X^T(X^T)^T = X^TX.\]
            <br>
            An \(n \times n\) matrix \(A\) is said to be <strong>orthogonally diagonalizable</strong> if there exists 
            an orthogonal matrix \(P\) and a diagonal matrix \(D\) such that:
            \[
            A = PDP^T = PDP^{-1}
            \]
            For this diagonalization, \(A\) must \(n\) linearly independent and orthonormal eigenvectors.
            Importantly, in such cases, \(A\) is symmetric because
            \[
            A^T = (PDP^T)^T = (P^T)^T D^T P^T = PDP^T = A.
            \]
            The converse is also always true. If \(A\) is symmetric, it is guaranteed to be orthogonally diagonalizable.
            <div class="theorem">
                <span class="theorem-title">Theorem 1:</span>
                An \(n \times n\) matrix \(A\) is orthogonally diagonalizable if and only if \(A\) is
                a symmetric matrix. 
            </div>
            On <a href="eigenvectors.html"><strong>Eigenvalues & Eigenvectors</strong></a>, we considered the example:
            \[
            A = \begin{bmatrix} 4 & 1 & 1\\ 1 & 4 & 1 \\ 1 & 1 & 4 \\ \end{bmatrix}
             = PDP^{-1} 
             = \begin{bmatrix} -1 & -1 & 1 \\ 0 & 1 & 1 \\ 1 & 0 & 1 \\ \end{bmatrix}
               \begin{bmatrix} 3 & 0 & 0 \\ 0 & 3 & 0 \\ 0 & 0 & 6 \\ \end{bmatrix} 
               \begin{bmatrix} \frac{-1}{3} & \frac{-1}{3} & \frac{2}{3} \\
                               \frac{-1}{3} & \frac{2}{3} & \frac{-1}{3} \\
                               \frac{1}{3} & \frac{1}{3} & \frac{1}{3} \end{bmatrix}.
            \] 
            Since, \(A\) is symmetric, it must be orthogonally diagonalizable. To ensure \(P\) is orthonormal, we need 
            orthonormal eigenvectors. This can be achieved using the Gram-Schmidt algorithm on the columns of \(P\).
            Since only \(v_1 \cdot v_2 \neq = 0\) needs adjustment, 
            \[
            v'_2 = v_2 - \frac{v_2 \cdot v_1}{v_1 \cdot v_1}v_1 
                   = \begin{bmatrix} -1\\ 1\\ 0\\ \end{bmatrix} - \frac{1}{2}\begin{bmatrix} -1\\ 0\\ 1\\ \end{bmatrix}
                   = \begin{bmatrix} \frac{-1}{2}\\ 1 \\ \frac{-1}{2}  \end{bmatrix}.
            \]
            After normalization, the orthonormal eigenvectors are:
            \[ 
            u_1 = \begin{bmatrix} \frac{-1}{\sqrt{2}} \\ 0\\ \frac{1}{\sqrt{2}} \\ \end{bmatrix}
            ,\quad
            u_2 = \begin{bmatrix} \frac{-1}{\sqrt{6}}\\ \frac{2}{\sqrt{6}}\\ \frac{-1}{\sqrt{6}}\\ \end{bmatrix}
            \quad \text{, and }
            u_3 = \begin{bmatrix} \frac{1}{\sqrt{3}} \\  \frac{1}{\sqrt{3}} \\ \frac{1}{\sqrt{3}} \\ \end{bmatrix}
            \]
            Therefore, 
            \[A = \begin{bmatrix} 4 & 1 & 1\\ 1 & 4 & 1 \\ 1 & 1 & 4 \\ \end{bmatrix}
             = PDP^T
             =   \begin{bmatrix} \frac{-1}{\sqrt{2}} & \frac{-1}{\sqrt{6}} & \frac{1}{\sqrt{3}} \\
                                 0                   & \frac{2}{\sqrt{6}}  & \frac{1}{\sqrt{3}}\\
                                 \frac{1}{\sqrt{2}}  & \frac{-1}{\sqrt{6}} & \frac{1}{\sqrt{3}} \end{bmatrix}
               \begin{bmatrix} 3 & 0 & 0 \\ 0 & 3 & 0 \\ 0 & 0 & 6 \\ \end{bmatrix} 
               \begin{bmatrix} \frac{-1}{\sqrt{2}}   & 0                   & \frac{1}{\sqrt{2}} \\
                               \frac{-1}{\sqrt{6}}   & \frac{2}{\sqrt{6}}  & \frac{-1}{\sqrt{6}}  \\
                               \frac{1}{\sqrt{3}}    & \frac{1}{\sqrt{3}}  & \frac{1}{\sqrt{3}} \end{bmatrix}
            = PDP^{-1}
            \] 
        </blockquote>

        <h1 id="qf">Quadratic Forms</h1>
        <blockquote>
            A <strong>quadratic form</strong> on \(\mathbb{R}^n\) is a function
            \[
            Q(x) = x^TAx \, \in \mathbb{R}^n
            \]
            where \(A\) is an \(n\times n\) <strong>symmetric</strong> matrix. 
            <br><br>
            Let's use our symmetric matrix again:
            \[
            A = \begin{bmatrix} 4 & 1 & 1\\ 1 & 4 & 1 \\ 1 & 1 & 4 \\ \end{bmatrix}
            \]
            Then:
            \[
            \begin{align*}
            Q(x) &=  x^TAx \\\\
                 &= 4x_1^2 +4 x_2^2 + 4x_3^2 + 2x_1x_2 + 2x_2x_3 + 2x_3x_1
            \end{align*}
            \]
            <br>
            We can transform this quadratic form into one with no cross-product terms.
            <br>
            Let \(x = Py\) be a change of variable where \(P\) is an invertible matrix and \(y\) is a
            new variable in \(\mathbb{R}^n\). Since \(A\) is symmetric, by Theorem 1, \(A\) is
            orthogonally diagonalizable. Thus
            \[
            x^TAx = (Py)^TA(Py) = y^TP^TAPy = y^T(P^TAP)y = y^T(P^TPDP^TP)y = y^TDy
            \]
            where \(y = P^{-1}x = P^Tx\).
            <br>
            Since \(P^T A P = D\), where \(D\) is a diagonal matrix, we get:
            \[\begin{align*}
            x^T A x &= y^T D y \\\\
                    &= \lambda_1 y_1^2 + \lambda_2 y_2^2 + \cdots + \lambda_n y_n^2
            \end{align*}
            \]
            where \(\lambda_1, \lambda_2, \ldots, \lambda_n\) are the eigenvalues of \(A\).
            <br>
            For our example, \(Q(x)\) becomes:
            \[
            3y_1^2 + 3y_2^2 + 6 y_3^2
            \]
            <br>
            Thus, the eigenvalues of \(A\) are directly connected to its quadratic form. First, 
            we define the classification of quadratic forms.
            <br><br>
            A quadratic form \(Q(x)\) is classified as:
            <ol class="centered-list">
                <li><strong>Positive definite</strong>:<br>
                    \(\qquad \qquad A \succ 0\) if \(\, \forall x \neq 0, \, Q(x) > 0\). </li>
                <li><strong>Positive semdefinite</strong>:<br>
                    \(\qquad \qquad A \succeq 0\) if \(\, \forall x, \, Q(x) \geq 0\). </li>
                <li><strong>Negative definite</strong>:<br>
                    \(\qquad \qquad A \prec 0\) if \(\, \forall x \neq 0, \, Q(x) < 0\).</li>
                <li><strong>Negative semdefinite</strong>:<br>
                    \(\qquad \qquad A \preceq 0\) if \(\, \forall x, \, Q(x) \leq 0\).</li>
                <li><strong>Indefinite</strong>: <br>
                    \(\qquad \qquad\) otherwise. </li>
            </ol>
            <div class="theorem">
                <span class="theorem-title">Theorem 2:</span>
                Let \(A\) be an \(n \times n\) symmetric matrix. Then a quadratic form \(x^TAx\) is
                <ol>
                    <li>positive definite iff the eigenvalues of \(A\) are all positive. </li>
                    <li>negative definite iff  the eigenvalues of \(A\) are all negative.</li>
                </ol>
            </div>
            As mentined above, an orthogonal change of variable \(x= Py\) transforms \(x^TAx\) into 
            \[
            Q(x) = y^T D y = \lambda_1 y_1^2 + \cdots + \lambda_n y_n^2.
            \]
            where \(\lambda_1, \ldots, \lambda_n\) are the eigenvalues of \(A\).
            <br>
            There is a one-to-one correspondence between all  \(x \neq 0\) and \(y \neq 0\) because \(P\) is an 
            invertible matrix. So, \(Q(x)\) is determined by the signs of eigenvalues(or <strong>spectrum</strong>) of \(A\). 
            The columns of \(P\) are called the <strong>principal axes</strong> of the quadratic form \(x^TAx\).
            <br><br>
            Note: To check whether a matrix \(A\) is positive definite, it is effective to verify the existance of a 
            <strong>Cholesky factorization</strong> \(A = R^TR\) where \(R\) is an upper triangular matrix with positve
            diagonal entries. This factorization is a modified version of the <strong>LU factorization</strong>. 
        </blockquote>

        <h1 id="svd">Singular Value Decomposition(SVD)</h1>
        <blockquote>
            Let \(A\) be an \(m \times n\) matrix. The symmetric matrix \(A^TA\) is orthogonally diagonalizable. 
            Let \(\{v_1, \cdots, v_n\}\) be an orthonormal basis for \(\mathbb{R}^n\) consisting of eigenvectors 
            of \(A^TA\) with corresponding eigenvalues \(\lambda_1, \cdots, \lambda_n\). 
            <br>
            For \(1 \leq i \leq n\), the following holds:
            \[
            \| Av_i \|^2 = (Av_i)^TAv_i = v_i^T(A^TAv_i) = v_i^T(\lambda_iv_i) = \lambda_i \geq 0  \tag{1}
            \]
            We assume the eigenvalues are arranged in descending order:
            \[
            \lambda_1 \geq \lambda_2 \geq \cdots \geq \lambda_n
            \]
            <br>
            For \(1 \leq i \leq n\), the <strong>singular values</strong> of \(A\) are defined as 
            \[
            \sigma_i = \sqrt{\lambda_i} = \| Av_i \| \geq 0.
            \]
            If \(A\) has \(r\) nonzero singular values, then \(\{Av_1, \cdots, Av_r\}\) forms an orthogonal basis 
            for \(\text{Col }A\) and 
            \[\text{rank } A = \dim \text{ Col }A = r.
            \]
            <div class="theorem">
                <span class="theorem-title">Theorem 3: Singular Value Decomposition</span>
                Let \(A\) be an \(m \times n\) matrix with rank \(r\). Then there exsit an \(m \times n\)
                matrix \(\Sigma = \begin{bmatrix} D & 0 \\ 0 & 0 \\ \end{bmatrix}\) where \(D\) is a \(r \times r\)
                diagonal matrix with the first \(r\) <strong>nonzero</strong> singular values of \(A\), \(\sigma_1 \geq \sigma_2 \geq \cdots \geq \sigma_r > 0\)
                and there exist an \(m \times m\) orthogonal matrix \(U\), and an \(n \times n\) orthogonal matrix \(V\) such that
                \[A = U\Sigma V^T\]    
            </div>
            <div class="proof">
                <span class="proof-title">Proof:</span>
                Normalize orthogonal basis for \(\text{Col }A\) \(\{Av_1, \cdots, Av_r\}\), then we obtain an orthonormal basis
                \(\{u_i, \cdots, u_r\}\), where
                \[u_i = \frac{1}{\| Av_i \|}Av_i = \frac{1}{\sigma_i}Av_i \quad 1 \leq i \leq r\]
                or, 
                \[Av_i = \sigma_i u_i \tag{2}\]
                We extend \(\{u_i, \cdots, u_r\}\) to an orthonormal basis for \(\mathbb{R}^m\), \(\{u_i, \cdots, u_r, u_{r+1}, \cdots, u_m\}\).
                <br>Let \(U = \begin{bmatrix}u_1 & \cdots & u_m \end{bmatrix}\) and \(V = \begin{bmatrix}v_1 & \cdots & v_n \end{bmatrix}\).
                <br>Clearly, by construction, \(U\) and \(V\) are orthogonal matrices. Also, by (2),
                \[AV = \begin{bmatrix} \sigma_1u_1 & \cdots & \sigma_ru_r & 0 & \cdots & 0 \end{bmatrix}.\]
                Let \(D\) be a diagonal matrix with diagonal entries \(\sigma_1, \cdots, \sigma_r\) and 
                let \(\Sigma =\begin{bmatrix} D & 0 \\ 0 & 0 \\ \end{bmatrix}\). Then
                \[
                \begin{align*}
                U\Sigma &= \begin{bmatrix}\sigma_1u_1 & \cdots & \sigma_ru_r & 0 & \cdots & 0\end{bmatrix} \\\\
                        &= AV
                \end{align*}
                \]
                Finally, since \(V\) is an orthogonal matrix, 
                \[(U\Sigma ) V^T = (AV)V^T = AI = A.\]
            </div>
            Steps for SVD:
            <ol>
                <li>Compute Orthogonal decomposition of \(A^TA\): </li>
                    Compute the eigenvalues and eigenvectors of  \(A^TA\). <br>             
                    Note: In practice, avoid directly computing \(A^TA\) to prevent numerical errors.
                <li>Construct \(V\) and \(\Sigma\):</li>
                    Arrange eigenvalues in descending order.<br>
                    Use the corresponding unit eigenvectors as the columns of \(V\).
                <li>Construct \(U\):</li>
                    For nonzero singular values, compute \(u_k = \frac{1}{\sigma_k}Av_k\).<br>
                    If there are not enough nonzero singular values, extend the orthonormal basis. 
            </ol>
        </blockquote>

        <h1 id="f_sub">SVD with Fundamental Subspaces</h1>
        <blockquote>
            Consider an \(m \times n\) matrix \(A = U\Sigma V^T\). 
            <br>
            Let \(u_1, \cdots, u_m\) be the <strong>left singular vectors</strong>, \(v_1, \cdots, v_n\) be
            the <strong>right singular vectors</strong>, \(\sigma_1, \cdots, \sigma_n\) be the singular values, and \(r\) be the rank of \(A\).
            <br><br>
            \(\{Av_1, \cdots, Av_r\} = \{\sigma_1u_1, \cdots, \sigma_ru_r\} \) is an orthogonal basis for \(\text{Col }A\) and then
            \(\{u_1, \cdots, u_r\}\) is an orthonormal basis for \(\text{Col }A\). Also, \((\text{Col }A)^{\perp} =  \text{Nul }A^T \).
            Thus, \(\{u_{r+1}, \cdots, u_m\}\) is an orthonormal basis for \(\text{Nul }A^T\).
            <br><br> 
            Since \(\| Av_i \| = \sigma_i = 0\) iff \(i > r \,\), \(v_{r+1}, \cdots, v_n\) span a subspace of \(\text{Nul }A\)
            of dimension \(n-r\), but we know that \(\dim \text{Nul }A = n - \text{rank } A = n -r\).
            Therefore, \(\{v_{r+1}, \cdots, v_n\}\) is an orthonormal basis for \(\text{Nul }A\). 
            <br><br>
            Finally, since \((\text{Nul }A)^{\perp} = \text{Col }A^T  = \text{Row }A  \), \(\{v_1, \cdots, v_r\}\) is an orthonormal basis for \(\text{Row }A\). 
            <br><br> 
            Now, we assume \(A\) is an \(n \times n\) matrix. From above facts, we derive additional invertible matrix theorems (See<a href="determinants.html"><strong>invertible matrix</strong></a>): 
            <ol>
                <li>\((\text{Col }A)^{\perp} = \{0\}\).</li>
                <li>\((\text{Nul }A)^{\perp} = \mathbb{R}^n\).</li>
                <li>\(\text{Row }A = \mathbb{R}^n\). </li>
                <li> \(A\) has \(n\) nonzero singular values. </li>
            </ol>
        </blockquote>
       
        <a href="../../index.html">Back to Home </a>
        <br> <a href="linear_algebra.html">Back to Linear Algebra </a>
    </body>
</html>