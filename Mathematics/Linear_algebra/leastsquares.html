---
layout: default
title: Least-Squares
level: detail
description: Learn about least-squares problems and linear regression.
uses_math: true
uses_python: false
---
<!DOCTYPE html>
<html>
    <body>
        <!-- LearningResource Schema for Content Pages -->
        <script type="application/ld+json">
        {
        "@context": "https://schema.org",
        "@type": "LearningResource",
        "name": "{{ page.title }}",
        "description": "{{ page.description }}",
        "learningResourceType": "lesson",
        "educationalUse": "instruction",
        "educationalLevel": "university",
        "interactivityType": "{% if page.content contains 'Interactive Demo' or page.content contains 'demo' %}active{% else %}expositive{% endif %}",
        "author": {
            "@type": "Person",
            "name": "Yusuke Yokota",
            "jobTitle": "Mathematics & Computer Science Educator"
        },
        "publisher": {
            "@type": "Organization",
            "name": "MATH-CS COMPASS",
            "url": "https://yusukey01.github.io"
        },
        "about": [
            {% if page.url contains 'leastsquares' %}
            { "@type": "Thing", "name": "Least-Squares" },
            { "@type": "Thing", "name": "Linear Regression" },
            { "@type": "Thing", "name": "Normal Equations" },
            { "@type": "Thing", "name": "Orthogonal Projection" },
            { "@type": "Thing", "name": "QR Factorization" },
            { "@type": "Thing", "name": "Design Matrix" },
            { "@type": "Thing", "name": "Residuals" },
            { "@type": "Thing", "name": "Best-Fit Line" }
            {% elsif page.url contains 'eigenvectors' %}
            { "@type": "Thing", "name": "Eigenvalues" },
            { "@type": "Thing", "name": "Eigenvectors" }
            {% elsif page.url contains 'symmetry' %}
            { "@type": "Thing", "name": "Symmetric Matrices" },
            { "@type": "Thing", "name": "Quadratic Forms" }
            {% else %}
            { "@type": "Thing", "name": "Linear Algebra" },
            { "@type": "Thing", "name": "Mathematics" }
            {% endif %}
        ],
        "teaches": [
            "Mathematical concepts",
            "Practical applications",
            "Problem-solving techniques"
        ],
        "isPartOf": {
            "@type": "Course",
            "name": "{% if page.url contains 'Machine_learning' %}Machine Learning{% elsif page.url contains 'Linear_algebra' %}Linear Algebra{% elsif page.url contains 'Calculus' %}Calculus to Optimization & Analysis{% elsif page.url contains 'Probability' %}Probability & Statistics{% elsif page.url contains 'Discrete' %}Discrete Mathematics & Algorithms{% endif %}",
            "description": "{% if page.url contains 'Machine_learning' %}Explore machine learning ideas and applications with mathematical foundations{% elsif page.url contains 'Linear_algebra' %}Explore the foundations of Linear Algebra, covering key concepts such as linear equations, vector spaces, eigenvalues, orthogonality, least squares, and stochastic matrices{% elsif page.url contains 'Calculus' %}Explore key calculus concepts essential for optimization, analysis, and machine learning{% elsif page.url contains 'Probability' %}Explore fundamental concepts of probability and statistics essential for machine learning{% elsif page.url contains 'Discrete' %}Explore the foundations of discrete mathematics and algorithms, covering graph theory, combinatorics, and the theory of computation{% endif %}",
            "provider": {
            "@type": "Organization",
            "name": "MATH-CS COMPASS",
            "url": "https://yusukey01.github.io"
            },
            "instructor": {
            "@type": "Person",
            "name": "Yusuke Yokota",
            "jobTitle": "Mathematics & Computer Science Educator"
            },
            "courseCode": "{% if page.url contains 'Machine_learning' %}V{% elsif page.url contains 'Linear_algebra' %}I{% elsif page.url contains 'Calculus' %}II{% elsif page.url contains 'Probability' %}III{% elsif page.url contains 'Discrete' %}IV{% endif %}",
            "hasCourseInstance": {
            "@type": "CourseInstance",
            "courseMode": "online",
            "courseWorkload": "{% if page.url contains 'Machine_learning' %}PT5H{% elsif page.url contains 'Linear_algebra' %}PT2H{% elsif page.url contains 'Calculus' %}PT3H{% elsif page.url contains 'Probability' %}PT2H30M{% elsif page.url contains 'Discrete' %}PT4H{% endif %}",
            "instructor": {
                "@type": "Person",
                "name": "Yusuke Yokota"
            }
            },
            "offers": {
            "@type": "Offer",
            "price": "0",
            "priceCurrency": "USD",
            "availability": "https://schema.org/InStock",
            "category": "free"
            }
        }
        }
        </script>
        <!-- WebApplication Schema for Interactive Demos -->
        <script type="application/ld+json">
        {
        "@context": "https://schema.org",
        "@type": "WebApplication",
        "name": "{{ page.title }} Interactive Demo",
        "description": "Interactive demonstration of {{ page.title | downcase }} concepts",
        "applicationCategory": "EducationalApplication",
        "operatingSystem": "Web Browser",
        "url": "https://yusukey01.github.io{{ page.url }}",
        "author": {
            "@type": "Person",
            "name": "Yusuke Yokota"
        },
        "applicationSubCategory": "Mathematical Visualization",
        "featureList": [
            "Interactive mathematical visualization",
            "Real-time parameter adjustment",
            "Educational demonstrations"
        ],
        "isAccessibleForFree": true,
        "educationalUse": "instruction",
        "educationalLevel": "university"
        }
        </script>
        <div class="hero-section">
            <h1 class="webpage-name">Least-Squares
            </h1>
        </div>

        {% include section_navigation.html %}

        <div class="topic-nav">
            <a href="#lsp">Least-Squares Problems</a>
            <a href="#lr">Linear Regression</a>
            <a href="#demo">Interactive Linear Regression Demo</a>
        </div> 

        <div class="container">  
           
            <section id="lsp" class="section-content">
            <h2>Least-Squares Problems</h2>
            <p>
            If \(A \in \mathbb{R}^{m \times n}\) and \(b \in \mathbb{R}^m\), a <strong>least-squares solution</strong> of
            \(Ax = b\) is an \(\hat{x} \in \mathbb{R}^n\) s.t.
            \[
            \forall x \in \mathbb{R}^n, \quad \| b - A\hat{x} \| \leq \| b - Ax \|. 
            \]
            We call the norm \(\| b - A\hat{x} \| \) the <strong>least-squares error</strong> of the approximation. 
            <br><br>
            In practice, it is possible that a solution to the equation \(Ax = b\) is required but does not exist
            (i.e., the system is inconsistent). In that case, we try to find an \(x\) that makes \(Ax\) as close to \(b\) 
            as possible. Let
            \[
            \hat{b} = \text{proj}_{\text{Col }A} \, b 
            \]
            Since \(\hat{b} \in \text{Col }A \,\), \(Ax = \hat{b}\) is consistent. 
            This \(\hat{b}\) is the closest point to \(b\) in \(\text{Col }A\).
            Then, there exists a least-squares solution \(\hat{x} \in \mathbb{R}^n\) s.t. \(A\hat{x} = \hat{b}\).
            <br><br>
            Since \(\hat{b}\) is orthogonal to \(b\), we can say that \(b - \hat{b} = b - A\hat{x}\) is orthogonal 
            to \(\text{Col }A\). (Note:\(b = A\hat{x} + (b - A\hat{x})\))
            <br>Let \(a_j\) be a column of \(A\), then 
             \[
             \begin{align*}
             a_j \cdot (b-A\hat{x}) = 0 
                    &\Longrightarrow a_j^T(b-A\hat{x}) = 0 \\\\
                    &\Longrightarrow A^T (b-A\hat{x}) = 0 
             \end{align*}
             \]
            and we obtain 
            \[A^TA\hat{x} = A^Tb\]
            So, the set of least-squares solutions of \(Ax =b\) must satisfy the <strong>normal equations</strong>
            \[A^TAx = A^Tb\]
            <br>
            If \(A^TA\) is invertible(= The columns of \(A\) are <strong>linearly independent</strong>.),
            \[\hat{x} = (A^TA)^{-1}A^Tb\]
            Finally, we also get the orthogonal projection of \(b\) onto the \(\text{Col } A\)
            \[
            \begin{align*}
            \hat{b} &= \text{proj}_{\text{Col }A} \, b  \\\\
                    &= A\hat{x} = A(A^TA)^{-1}A^Tb 
            \end{align*}
            \]
            In practice, computing \(A^TA\) makes relatively large errors in \(\hat{x}\). Instead, the <strong>QR factorization</strong>
            gives us more reliable result. Let \(A = QR\). Then 
            \[
            \begin{align*}
            \hat{x} &= (R^TQ^TQR)^{-1}(R^TQ^T)b  \\\\
                    &= (R^TR)^{-1}(R^TQ^T)b.  \tag{1}
            \end{align*}
            \]
            On the other hand, 
            \[
            \begin{align*}
            A\hat{x} = b &\Longrightarrow Q^TQR\hat{x} = Q^Tb  \\\\
                         &\Longrightarrow  R\hat{x} = Q^Tb  \\\\
                         &\Longrightarrow \hat{x} = R^{-1}Q^Tb \\\\
            \end{align*}
            \]
            Comparing with (1), \((R^TR)^{-1}R^T = R^{-1}\). 
            Thus, the unique least-squares solution can be generated by \(\hat{x} = R^{-1}Q^Tb\) but in practice, solving 
            \[R\hat{x} = Q^Tb\]
            by back-substitution is much faster than computing \(R^{-1}\).
            </p>
            </section>

             <section id="lr" class="section-content">
            <h2>Linear Regression</h2>
            <p>
            Given a set of observed data points \(\{(x_i, y_i)\}_{i = 1}^{n}\) where \(x_i \in \mathbb{R}^d, \quad y_i \in \mathbb{R}\),
            we assume that the given data can be explained by the <strong>linear model</strong>:
            \[Y = X\beta + \epsilon \]
            where 
            <br>
            <ul style="padding-left: 40px;">
                <li><strong>design matrix</strong>: \(X \in \mathbb{R}^{n \times d}\)</li>
                <li><strong>parameter(weight) vector</strong>: \(\beta \in \mathbb{R}^d\)</li>
                <li><strong>observation vector</strong>: \(Y \in \mathbb{R}^n\)</li>
                <li><strong>residual vector</strong>:  \(\epsilon = Y - X\beta\) </li>
            </ul>
            <br>
            The dimension \(d\) is the number of features, and \(n\) is the number of data points. 
            The residual \(\epsilon\) is the "difference" between each observed value \(y_i\) and its corresponding predicted \(y\) value.
            The <strong>least-squares hyperplane</strong> represents the set of predicted \(y\) values based on "estimated" parameters(weights) \(\hat{\beta}\), 
            and it must satisfy the normal equations:
            \[X^TX\hat{\beta} = X^TY\]
            <br><br>

            Note: the linear model is linear in terms of <strong>parameters \(\beta \, \)</strong>, not \(X\). We can choose any non-linear transformation
            for each \(x_i\). For example, \(y = \beta_0 + \beta_1 x^2 + \beta_2 \sin(2\pi x)\) is still a linear model. Thus, \(Y\) is modeled as a 
            <strong>linear combination</strong> of features(predictors) \(X\) with respect to the coefficients(weights) \(\beta\).

            <br><br>
            There are lots of details we should cover in this topic. I leave it for 
            <a href="../Probability/linear_regression.html">Section III-Probability & Statistics - Linear Regression</a>.
            <br>
            Finally, \(X^TX\) is called a <strong>symmetric matrix</strong>. We will learn about the special matrices 
            in the next part. 
            </p>
            </section>


            <section id="demo" class="section-content">
                <h2>Interactive Linear Regression Demo</h2>
                <p>
                    Linear regression finds the best-fitting line or curve for a set of data points by minimizing the sum of squared differences between observed and predicted values. This interactive demo lets you explore how polynomial regression works with different datasets.
                </p>
                <p>
                    Try adding your own data points by clicking on the plot, or use the example datasets to see how different polynomial degrees fit various patterns. The demo visually demonstrates key concepts from least-squares theory and the normal equations.
                </p>
                
                <!-- Here, the visualizer will be rendered -->
                <div id="linear-regression-visualizer"></div>
            </section>
           
        </div>    
        
        <script src="/js/main.js"></script>
        <!-- Linear Regression Visualizer -->
        <script src="/js/linear_regression_visualizer2.js"></script>
          
    </body>
</html>