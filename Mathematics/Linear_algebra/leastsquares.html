<!DOCTYPE html>
<html>
    <head> 
        <title>Least-Squares</title>
        <link rel="stylesheet" href="../styles.css">
        <script id="MathJax-script" defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <meta name="viewport" content="width=device-width, initial-scale=1">
    </head>
    <body>
        <nav class="navbar">
            <div class="logo">
                <h1>Section I - Linear Algebra</h1>
            </div>
            <ul class="nav-links">
                <li><a href="../../index.html">Home</a></li>
                <li><a href="linear_algebra.html">Linear Algebra</a></li>
                <li><a href="../Calculus/calculus.html">Calculus & Optimization</a></li>
                <li><a href="../Probability/probability.html">Probability & Statistics</a></li>
                <li><a href="../Discrete/discrete_math.html">Discrete Mathematics</a></li>
            </ul>
            <div class="menu-toggle">&#9776;</div>
        </nav>

        <div class="hero-section">
            <h1 class="webpage-name">Least-Squares
            </h1>
        </div>

        <div class="topic-nav">
            <a href="#lsp">Least-Squares Problems</a>
            <a href="#lr">Linear Regression</a>
        </div> 

        <div class="container">  
           
            <section id="lsp" class="section-content">
            <h2>Least-Squares Problems</h2>
            <p>
            If \(A \in \mathbb{R}^{m \times n}\) and \(b \in \mathbb{R}^m\), a <strong>least-squares solution</strong> of
            \(Ax = b\) is an \(\hat{x} \in \mathbb{R}^n\) s.t.
            \[\forall x \in \mathbb{R}^n, \quad \| b - A\hat{x} \| \leq \| b - Ax \|. \]
            We call the norm \(\| b - A\hat{x} \| \) the <strong>least-squares error</strong> of the approximation. 
            <br><br>
            In practice, it is possible that a solution of some problem \(Ax = b\) is required but it does not exist
            (= the system is inconsistent). In that case, we try to find an \(x\) that makes \(Ax\) close to \(b\) 
            as much as possible. Let 
            \[\hat{b} = \text{proj}_{\text{Col }A} \, b \]
            Since \(\hat{b} \in \text{Col }A \,\), \(Ax = \hat{b}\) is consistent. 
            This \(\hat{b}\) is the closest point to \(b\) in \(\text{Col }A\).
            Then, there exists a least-squares solution \(\hat{x} \in \mathbb{R}^n\) s.t. \(A\hat{x} = \hat{b}\).
            <br><br>
            Since \(\hat{b}\) is orthogonal to \(b\), we can say that \(b - \hat{b} = b - A\hat{x}\) is orthognal 
            to \(\text{Col }A\). (Note:\(b = A\hat{x} + (b - A\hat{x})\))
            <br>Let \(a_j\) be a column of \(A\), then 
             \[a_j \cdot (b-A\hat{x}) = 0 \Longrightarrow a_j^T(b-A\hat{x}) = 0 \Longrightarrow A^T (b-A\hat{x}) = 0 \]
            and we obtain 
            \[A^TA\hat{x} = A^Tb\]
            So, the set of least-squares solutions of \(Ax =b\) must satisfy the <strong>normal equations</strong>
            \[A^TAx = A^Tb\]
            <br>
            If \(A^TA\) is invertible(= The columns of \(A\) are <strong>linearly independent</strong>.),
            \[\hat{x} = (A^TA)^{-1}A^Tb\]
            Finally, we also get the orthogonal projection of \(b\) onto the \(\text{Col } A\)
            \[
            \hat{b} = \text{proj}_{\text{Col }A} \, b = A\hat{x} = A(A^TA)^{-1}A^Tb 
            \]
            In practice, computing \(A^TA\) makes relatively large errors in \(\hat{x}\). Instead, the <strong>QR factorization</strong>
            gives us more reliable result. Let \(A = QR\), 
            \[\hat{x} = (R^TQ^TQR)^{-1}(R^TQ^T)b = (R^TR)^{-1}(R^TQ^T)b  \tag{1}\]
            On the other hand, 
            \[A\hat{x} = b \Longrightarrow Q^TQR\hat{x} = Q^Tb \Longrightarrow  R\hat{x} = Q^Tb \Longrightarrow \hat{x} = R^{-1}Q^Tb \]
            Comparing with (1), \((R^TR)^{-1}R^T = R^{-1}\). 
            Thus, the unique least-squares solution can be generated by \(\hat{x} = R^{-1}Q^Tb\) but in practice, solving 
            \[R\hat{x} = Q^Tb\]
            by back-substitution is much faster than computing \(R^{-1}\).
            </p>
            </section>

             <section id="lr" class="section-content">
            <h2>Linear Regression</h2>
            <p>
            Given a set of observed data points \(\{(x_i, y_i)\}_{i = 1}^{n}\) where \(x_i \in \mathbb{R}^d, \quad y_i \in \mathbb{R}\),
            we assume that the given data can be explained by the <strong>linear model</strong>:
            \[Y = X\beta + \epsilon \]
            where \(X \in \mathbb{R}^{n \times d}\) is the <strong>design matrix</strong>, \(\beta \in \mathbb{R}^d\) is the <strong>parameter(weight) vector</strong>, \(Y \in \mathbb{R}^n\) is the
            <strong>observation vector</strong>, and \(\epsilon = Y - X\beta\) is a <strong>residual vector</strong>.
            <br><br>
            The dimension \(d\) is the number of features, and \(n\) is the number of data points. 
            The residual \(\epsilon\) is the "difference" between each observed value \(y_i\) and its corresponding predicted \(y\) value.
            The <strong>least-squares hyperplane</strong> represents the set of predicted \(y\) values based on "estimated" parameters(weights) \(\hat{\beta}\), 
            and it must satisfy the normal equations:
            \[X^TX\hat{\beta} = X^TY\]
            <br><br>

            Note: the linear model is linear in terms of <strong>parameters \(\beta \, \)</strong> , not \(X\). We can choose any non-linear transformation
            for each \(x_i\). For example, \(y = \beta_0 + \beta_1 x^2 + \beta_2 \sin(2\pi x)\) is linear. So, \(Y\) is modeled as a 
            <strong>linear combination</strong> of features(predictors) \(X\) with respect to the coefficients(weights) \(\beta\).

            <br><br>
            There are lots of details we should cover in this topic. I leave it for 
            <a href="../Probability/linear_regression.html">Section III-Probability & Statistics - Linear Regression</a>.
            <br>
            Finally, \(X^TX\) is called a <strong>symmetric matrix</strong>. We will learn about the special matrices 
            in the next part. 
            </p>
            </section>
        </div>
        
        <div class="contact-section">
            <a href="../../index.html">Back to Home</a>
            <a href="linear_algebra.html">Back to Linear Algebra </a>
        </div>
        
        <!-- Footer -->
        <footer>
            <div class="footer-content">
                <div class="footer-links">
                    <h3>Quick Links</h3>
                    <ul>
                        <li><a href="../../index.html">Home</a></li>
                        <li><a href="linear_equations.html">Part 1: Linear Equations</a></li>
                        <li><a href="linear_transformation.html">Part 2: Linear Transformation</a></li>
                        <li><a href="matrix_algebra.html">Part 3: Matrix Algebra</a></li>
                        <li><a href="determinants.html">Part 4: Determinants</a></li>
                        <li><a href="vectorspaces.html">Part 5: Vector Spaces</a></li>
                        <li><a href="eigenvectors.html">Part 6: Eigenvalues & Eigenvectors</a></li>
                        <li><a href="orthogonality.html">Part 7: Orthogonality</a></li>
                        <li><a href="leastsquares.html">Part 8: Least-Squares Problems</a></li>
                        <li><a href="symmetry.html">Part 9: Symmetry</a></li>
                        <li><a href="trace.html">Part 10: Trace and Norms</a></li>
                        <li><a href="kronecker.html">Part 11: Kronecker Product & Tensor</a></li>
                        <li><a href="woodbury.html">Part 12: Woodbury Matrix Identity</a></li>
                        <li><a href="stochastic.html">Part 13: Stochastic Matrix</a></li>                     
                    </ul>
                </div>
            </div>
            
            <div class="footer-bottom">
                <p>&copy; 2025 Math-CS Compass. All rights reserved.</p>
            </div>
        </footer>  
    </body>
</html>