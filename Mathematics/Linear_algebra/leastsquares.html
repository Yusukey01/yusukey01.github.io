<!DOCTYPE html>
<html>
    <head> 
        <title>Least-Squares</title>
        <link rel="stylesheet" href="../styles.css">
        <script id="MathJax-script" defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <meta name="viewport" content="width=device-width, initial-scale=1">
    </head>
    <body> 
        <div class="toc-container">
            <h2>Contents</h2>
            <ul>
                <li><a href="#lsp">Least-Squares Problems</a></li>
                <li><a href="#lr">Linear Regression</a></li>
            </ul>
        </div>
        <h1 id="lsp">Least-Squares Problems</h1>
        <blockquote>
            If \(A \in \mathbb{R}^{m \times n}\) and \(b \in \mathbb{R}^m\), a <strong>least-squares solution</strong> of
            \(Ax = b\) is an \(\hat{x} \in \mathbb{R}^n\) s.t.
            \[\forall x \in \mathbb{R}^n, \quad \| b - A\hat{x} \| \leq \| b - Ax \|. \]
            We call the norm \(\| b - A\hat{x} \| \) the <strong>least-squares error</strong> of the approximation. 
            <br><br>
            In practice, it is possible that a solution of some problem \(Ax = b\) is required but it does not exist
            (= the system is inconsistent). In that case, we try to find an \(x\) that makes \(Ax\) close to \(b\) 
            as much as possible. Let 
            \[\hat{b} = \text{proj}_{\text{Col }A} \, b \]
            Since \(\hat{b} \in \text{Col }A \,\), \(Ax = \hat{b}\) is consistent. 
            This \(\hat{b}\) is the closest point to \(b\) in \(\text{Col }A\).
            Then, there exists a least-squares solution \(\hat{x} \in \mathbb{R}^n\) s.t. \(A\hat{x} = \hat{b}\).
            <br><br>
            Since \(\hat{b}\) is orthogonal to \(b\), we can say that \(b - \hat{b} = b - A\hat{x}\) is orthognal 
            to \(\text{Col }A\). (Note:\(b = A\hat{x} + (b - A\hat{x})\))
            <br>Let \(a_j\) be a column of \(A\), then 
             \[a_j \cdot (b-A\hat{x}) = 0 \Longrightarrow a_j^T(b-A\hat{x}) = 0 \Longrightarrow A^T (b-A\hat{x}) = 0 \]
            and we obtain 
            \[A^TA\hat{x} = A^Tb\]
            So, the set of least-squares solutions of \(Ax =b\) must satisfy the <strong>normal equations</strong>
            \[A^TAx = A^Tb\]
            <br>
            If \(A^TA\) is invertible(= The columns of \(A\) are <strong>linearly independent</strong>.),
            \[\hat{x} = (A^TA)^{-1}A^Tb\]
            Finally, we also get the orthogonal projection of \(b\) onto the \(\text{Col } A\)
            \[
            \hat{b} = \text{proj}_{\text{Col }A} \, b = A\hat{x} = A(A^TA)^{-1}A^Tb 
            \]
            In practice, computing \(A^TA\) makes relatively large errors in \(\hat{x}\). Instead, the <strong>QR factorization</strong>
            gives us more reliable result. Let \(A = QR\), 
            \[\hat{x} = (R^TQ^TQR)^{-1}(R^TQ^T)b = (R^TR)^{-1}(R^TQ^T)b  \tag{1}\]
            On the other hand, 
            \[A\hat{x} = b \Longrightarrow Q^TQR\hat{x} = Q^Tb \Longrightarrow  R\hat{x} = Q^Tb \Longrightarrow \hat{x} = R^{-1}Q^Tb \]
            Comparing with (1), \((R^TR)^{-1}R^T = R^{-1}\). 
            Thus, the unique least-squares solution can be generated by \(\hat{x} = R^{-1}Q^Tb\) but in practice, solving 
            \[R\hat{x} = Q^Tb\]
            by back-substitution is much faster than computing \(R^{-1}\).
        </blockquote>

        <h1 id="lr">Linear Regression</h1>
        <blockquote>
            Given a set of observed data points \(\{(x_i, y_i)\}_{i = 1}^{n}\) where \(x_i \in \mathbb{R}^d, \quad y_i \in \mathbb{R}\),
            we assume that the given data can be explained by the <strong>linear model</strong>:
            \[Y = X\beta + \epsilon \]
            where \(X \in \mathbb{R}^{n \times d}\) is the <strong>design matrix</strong>, \(\beta \in \mathbb{R}^d\) is the <strong>parameter(weight) vector</strong>, \(Y \in \mathbb{R}^n\) is the
            <strong>observation vector</strong>, and \(\epsilon = Y - X\beta\) is a <strong>residual vector</strong>.
            <br><br>
            The dimension \(d\) is the number of features, and \(n\) is the number of data points. 
            The residual \(\epsilon\) is the "difference" between each observed value \(y_i\) and its corresponding predicted \(y\) value.
            The <strong>least-squares hyperplane</strong> represents the set of predicted \(y\) values based on "estimated" parameters(weights) \(\hat{\beta}\), 
            and it must satisfy the normal equations:
            \[X^TX\hat{\beta} = X^TY\]
            <br><br>

            Note: the linear model is linear in terms of <strong>parameters \(\beta \, \)</strong> , not \(X\). We can choose any non-linear transformation
            for each \(x_i\). For example, \(y = \beta_0 + \beta_1 x^2 + \beta_2 \sin(2\pi x)\) is linear. So, \(Y\) is modeled as a 
            <strong>linear combination</strong> of features(predictors) \(X\) with respect to the coefficients(weights) \(\beta\).

            <br><br>
            There are lots of details we should cover in this topic. I leave it for 
            <a href="../Probability/linear_regression.html">Section III-Probability & Statistics - Linear Regression</a>.
            <br>
            Finally, \(X^TX\) is called a <strong>symmetric matrix</strong>. We will learn about the special matrices 
            in the next part. 
        </blockquote>

        <a href="../../index.html">Back to Home </a>
        <br> <a href="linear_algebra.html">Back to Linear Algebra </a>
    </body>
</html>