<!DOCTYPE html>
<html>
    <head> 
        <title>The Mean Value Theorem</title>
        <link rel="stylesheet" href="../../css/styles.css">
        <script id="MathJax-script" defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <meta name="viewport" content="width=device-width, initial-scale=1">
    </head>
    <body>
        <nav class="navbar">
            <div class="logo">
                <h1>Section II - Calculus to Optimization & Analysis</h1>
            </div>
            <ul class="nav-links">
                <li><a href="../../index.html">Home</a></li>
                <li><a href="../Linear_algebra/linear_algebra.html">Linear Algebra</a></li>
                <li><a href="calculus.html">Calculus to  Optimization & Analysis</a></li>
                <li><a href="../Probability/probability.html">Probability & Statistics</a></li>
                <li><a href="../Discrete/discrete_math.html">Discrete Mathematics & Algorithms</a></li>
            </ul>
            <div class="menu-toggle">&#9776;</div>
        </nav>

        <div class="hero-section">
            <h1 class="webpage-name">The Mean Value Theorem
            </h1>
        </div>

        <div class="topic-nav">
            <a href="#l_mvt">Lagrange's Mean Value Theorem</a>
            <a href="#taylor">Taylor's Theorem</a>
            <a href="#c_mvt">Cauchy's Mean Value Theorem</a>
            <a href="#h_dim">Higher-dimensional MVT</a>
        </div> 

        <div class="container">  
           
            <section id="l_mvt" class="section-content">
            <h2>Lagrange's Mean Value Theorem</h2>
            <p>
            <div class="theorem">
                <span class="theorem-title">Theorem 1: Lagrange's Mean Value Theorem</span> 
                Suppose a function \(f\) is a continuous on \([a, b]\) and differentiable on \((a, b)\). Then 
                there exists a number \(c \in (a, b)\) such that
                \[
                f'(c) = \frac{f(b) - f(a)}{b - a} \tag{1}
                \]   
            </div>
            Note: The special case of this theorem: if \(f(a) = f(b)\), then f'(c) = 0. is called <strong>Rolle's Theorem</strong>. 
            We ommit the proof but the outile of the proof can be: 
            <br>
            <ul style="padding-left: 40px;">
                <li>case 1: \(f(x)\) is constant, \(f'(x)\) = 0 for all \(c \in (a,b)\).</li>
                <li>case 2: \(f(x) > f(a)\), show that at \(c\), \(f\) has local maximum, then \(f'(c) = 0\) by the Extrem Value Theorem.</li>
                <li>case 3: \(f(x) < f(a)\), similarly, show that at \(c\), \(f\) has local minimum, then \(f'(c) = 0\).</li>
            </ul>
            <br>
            <div class="proof">
                <span class="proof-title">Proof:</span>
                Suppose a function \(f\) is a continuous on \([a, b]\) and differentiable on \((a, b)\).
                <br>
                Consider the line connecting \(a, f(a)\) and \(b, f(b)\). The equation of the line can be written as 
                \[
                y - f(a) = \frac{f(b)-f(a)}{b -a} (x - a)
                \]
                Let g(x) be the vertical difference between \(x, f(x)\) and (x, y). Then 
                \[
                g(x) = f(x) - f(a) - \frac{f(b)-f(a)}{b -a} (x -a) \tag{2}
                \]
                \(g\) is continuous on \([a, b]\) and differentiable on \((a, b)\) because \(g\) is the sum of \(f\) 
                and the first-degree polynomial, both of which are continuous and differentialble on the same interval. 
                <br>
                From Equation (2), 
                \[
                g'(x) = f'(x) = \frac{f(b)-f(a)}{b -a} 
                \]
                Also, clearly, \(g(a) = g(b) = 0\). Thus, by Rolle's Theorem 
                \[
                \exists c \in (a,b) \text{  such that } g'(c) = 0
                \]
                This implies 
                \[
                g'(c) = f'(c) = \frac{f(b)-f(a)}{b -a}  = 0
                \]
                and so 
                \[
                f'(c) = \frac{f(b)-f(a)}{b -a}
                \]
            </div>
            </p>
            </section>

            <section id="taylor" class="section-content">
            <h2>Taylor's Theorem</h2>
            <p>
            Equivalently, Equation (1) can be written as
            \[
            f(b) - f(a) = f'(c)(b - a)
            \]
            This is the Taylor's Theorem on \([a, b]\) with \(n = 1\).
            <div class="theorem">
                <span class="theorem-title">Theorem 2: Taylor's Theorem</span> 
                Let \(n \geq 1\) be an integer and let \(f: \mathbb{R} \to \mathbb{R}\) be \(n\) times 
                differentiable at \(a \in \mathbb{R}\). Then there exists \(g: \mathbb{R} \to \mathbb{R}\) 
                such that 
                \[
                f(x) = \sum_{k = 0}^{n-1} \frac{f^{(k)}(a)}{k !}(x - a)^k + g_n (x) (x - a)^n
                \]
                and 
                \[
                \lim_{x \to a} g_n(x) = 0.
                \]
            </div>
            Note: \(g_n (x) (x - a)^n \) represents the reminder. An alternative and commonly used form of the 
            remainder is the <strong>Lagrange's form of reminder</strong>:
            \[
            R_n (x) = \frac{f^{(n)}(c)}{n!}(x-a)^n \text{ where } c \in (a, x)
            \]
            Another common representation of the remainder, particularly in <strong>asymptotic analysis</strong>, is:
            \[
            R_n (x) = \frac{f^{(n)}(a)}{n!}(x-a)^n + o(| x -a |^n)
            \]
            where the <strong>little-o</strong> notation \(o(| x -a |^n)\) indicates that the term converges to 0 faster than 
            \(| x -a |^n\) as \(x \to a\).
            <br>
            The polynomial appearing in Taylor's Theorem is called <strong>n-th order Taylor polynomial</strong>:
            \[
            \begin{align*}
            T_n (x) &= f(a) + \frac{f'(a)}{1!}(x -a) + \frac{f''(a)}{2!}(x -a)^2 + \cdots + \frac{f^{(n)}(a)}{n!}(x - a)^n \\\\
                    &= \sum_{k = 0}^{n} \frac{f^{(n)}(a)}{n !}(x - a)^n 
            \end{align*}
            \]
            Also, as \(n \to \infty\), if the function \(f\) is infinitely differentiable and the remainder term \(R_n (x)\) 
            tends to zero, Taylor series expansion becomes:
            \[
            f(x) = \sum_{n = 0}^{\infty} \frac{f^{(n)}(a)}{n !}(x - a)^n 
            \]
            This is <strong>Taylor series</strong> of the function \(f\) centered at \(a\). The series converges to \(f\) within 
            it radius of convergence. So, Taylor polynomials are used for approximation of \(f(x)\). For example, the 1st order Taylor 
            polynomial 
            \[
            T_1 (x) = f(a) + f'(a)(x-a)
            \] 
            is equivalent ot the <strong>linearlization </strong> of \(f\) at \(a\), providing a first-order approximation.
            </p>
            </section>

            <section id="c_mvt" class="section-content">
            <h2>Cauchy's Mean Value Theorem</h2>
            <p>
            <div class="theorem">
                <span class="theorem-title">Theorem 3: Cauchy's Mean Value Theorem</span> 
                Let both \(f\) and \(g\) are continuous on \([a,b]\) and differentiable \((a,b)\). If 
                \(\forall x \in (a, b), \, g'(x) \neq 0\) then \(\exists c \in (a, b)\) such that 
                \[
                \frac{f(b)-f(a)}{g(b)-g(a)} = \frac{f'(c)}{g'(c)}.
                \]
            </div>
            Note: Cauchy's Mean Value Theorem generalizes Lagrange's MVT(\(g(x) = x\)) and can be useful in advanced mathematical contexts, 
            but is not commonly applied directly in machine learning.
            </p>
            </section>

            <section id="h_dim" class="section-content">
            <h2>Higher-dimensional MVT</h2>
            <p>
            Before moving to optimization topics, it is important to discuss the higher-dimensional version of the MVT. This theorem 
            is foundational in <strong>gradient descent</strong> algorithms, which rely on gradients to minimize loss functions in multidimensional spaces. 
            Like its one-dimensional counterpart, the higher-dimensional leads to the higher-dimensional Taylor expansions.
            <div class="theorem">
                <span class="theorem-title">Higher-Dimensional MVT</span> 
                Let \(f: X \to \mathbb{R}\) be a differentiable function where X is an open subset of \(\mathbb{R}^n\).
                <br> 
                For any points \(a, b \in X\) such that \( [a, b] \subset X\), \(\, \exists c \in (a,b) \text{ such that }\)
                \[
                f(b) - f(a) = \nabla f(c) \cdot (b-a).
                \]
                <br>
                Note: \([a, b] = \{(1-t)a + tb \, | \, t \in [0, 1] \}\) and \((a, b) = \{(1-t)a + tb \, | \, t \in (0, 1) \}\)
            </div>
            <div class="proof">
                <span class="proof-title">Proof:</span>
                Define \(h(t) = f((1-t)a + tb) = f(a + t(b-a))\) where \(t \in \mathbb{R}\).
                <br>
                By Theorem 1: Lagrange's Mean Value Theorem, 
                <br>
                \(\exists \theta \in (0,1)\)  such that 
                \[
                h'(\theta ) = h(1) - h(0) = f(b) - f(a).
                \]
                Also, 
                \[
                h'(\theta ) = \nabla f(a + \theta (b -a)) \cdot (b - a).
                \]
                Here, let \(c = a + \theta (b-a)\), then we obtain
                \[
                \nabla f(c) \cdot (b - a) = f(b) - f(a).
                \]
            </div>
            Note: This theorem is only for vector-in & "scalar"-out functions.
            </p>
            </section>
        </div>
        <!-- Footer -->
        <footer>
            <div class="footer-content">
                <div class="footer-links">
                    <h3>Quick Links</h3>
                    <ul>
                        <li><a href="../../index.html">Home</a></li>
                        <li><a href="calculus.html">Section II - Calculus to Optimization & Analysis</a></li>
                        <li><a href="linear_approximation.html">The Derivative</a></li>
                        <li><a href="jacobian.html">The Jacobian</a></li>
                        <li><a href="matrix_cal.html">Matrix Derivatives</a></li>
                        <li><a href="numerical_example1.html">Numerical Examples</a></li>
                        <li><a href="det.html">Scalar Functions of Matrices</a></li>
                        <li><a href="mvt.html">Mean Value Theorem</a></li>
                        <li><a href="gradient.html">Gradient Descent</a></li>
                        <li><a href="constrained_opt.html">Constrained Optimization</a></li>
                        <li><a href="newton.html">Newton's Method</a></li>
                        <li><a href="riemann.html">Riemann Integration</a></li>
                        <li><a href="measure.html">Measure Theory</a></li>
                        <li><a href="lebesgue.html">Lebesgue Integration</a></li>
                        <li><a href="duality.html">Duality in Optimization</a></li>
                    </ul>
                </div>
            </div>
            
            <div class="footer-bottom">
                <p>&copy; 2025 Math-CS Compass. All rights reserved.</p>
            </div>
        </footer> 
        <script src="/main.js"></script>  
    </body>
</html>