---
layout: default
title: Foundations of Analysis: Metric Spaces
level: detail
description: Formalize the structure of metric spaces, covering distance, boundary, open and closed sets, topology, and completeness, which are the foundations ensuring optimization algorithms converge to valid solutions.
uses_math: true
uses_python: false
---
<!DOCTYPE html>
<html>
    <body>
        <!-- LearningResource Schema for Fourier Transform -->
        <script type="application/ld+json">
        {
        "@context": "https://schema.org",
        "@type": "LearningResource",
        "name": "Foundations of Analysis: Metric Spaces",
        "description": "Formalize the structure of metric spaces, covering distance, boundary, open and closed sets, topology, and completeness—the foundations ensuring optimization algorithms converge to valid solutions.",
        "learningResourceType": "lesson",
        "educationalUse": "instruction",
        "educationalLevel": "university",
        "interactivityType": "active",
        "author": {
            "@type": "Person",
            "name": "Yusuke Yokota",
            "jobTitle": "Mathematics & Computer Science Educator",
            "knowsAbout": [
            "Fourier Transform",
            "Signal Processing",
            "Frequency Analysis",
            "Fast Fourier Transform",
            "Convolution Theorem",
            "Machine Learning",
            "Harmonic Analysis"
            ]
        },
        "publisher": {
            "@type": "Organization",
            "name": "MATH-CS COMPASS",
            "url": "https://math-cs-compass.com"
        },
        "about": [
            { "@type": "Thing", "name": "Metric Spaces" },
            { "@type": "Thing", "name": "Distance Functions" },
            { "@type": "Thing", "name": "Open and Closed Sets" },
            { "@type": "Thing", "name": "Topology" },
            { "@type": "Thing", "name": "Completeness" },
            { "@type": "Thing", "name": "Convexity" }
        ],
        "teaches": [
            "Formal definition of metric spaces and the three metric axioms",
            "Distance from points to sets and between sets",
            "Isolated points, accumulation points, and nearest points",
            "Boundary, interior, closure, and exterior of sets",
            "Open and closed subsets of metric spaces",
            "Topology determined by a metric",
            "Complete metric spaces and their role in optimization",
            "Open and closed balls in metric spaces",
            "Convex sets in normed linear spaces"
        ],
        "isPartOf": {
            "@type": "Course",
            "name": "Calculus to Optimization & Analysis",
            "description": "Explore optimization techniques and mathematical analysis with applications",
            "provider": {
            "@type": "Organization",
            "name": "MATH-CS COMPASS",
            "url": "https://math-cs-compass.com"
            },
            "instructor": {
            "@type": "Person",
            "name": "Yusuke Yokota",
            "jobTitle": "Mathematics & Computer Science Educator"
            },
            "courseCode": "II",
            "hasCourseInstance": {
            "@type": "CourseInstance",
            "courseMode": "online",
            "courseWorkload": "PT4H",
            "instructor": {
                "@type": "Person",
                "name": "Yusuke Yokota"
            }
            },
            "offers": {
            "@type": "Offer",
            "price": "0",
            "priceCurrency": "USD",
            "availability": "https://schema.org/InStock",
            "category": "free"
            }
        }
        }
        </script>

        <div class="hero-section">
            <h1 class="webpage-name">Metric Spaces & Completeness</h1>
        </div>

        {% include section_navigation.html %}

        <div class="topic-nav">
            <a href="#intro">Introduction</a>
            <a href="#metric">Metric Space</a>
            <a href="#dist">Distance</a>
            <a href="#bound">Boundary</a>
            <a href="#opcl">Open & Closed</a>
            <a href="#ball">Balls</a>
        </div>  

        <div class="container">     
            <section id="intro" class="section-content">
                <h2>Introduction</h2>

                <p>
                    Up until this chapter, we relied on calculus and linear algebra to solve optimization problems.
                    For many practical machine learning applications, these tools are sufficient. However, as we move toward
                    advanced topics like manifold learning or information geometry, intuition alone is no longer enough.
                </p>

                <p>
                    Consider a fundamental question: when we run gradient descent, we assume each step brings us closer to a solution, 
                    and that iterating long enough will reach (or approach) a minimum. But what guarantees this? The sequence of 
                    iterates \(\mathbf{x}_0, \mathbf{x}_1, \mathbf{x}_2, \ldots\) may get arbitrarily close together without ever 
                    settling on an actual point unless the underlying space has the right structure.
                </p>

                <p>
                    Similarly, when we work with function spaces in kernel methods or neural networks, we treat entire functions 
                    as single "points." But what does "distance" mean between two functions? What does it mean for a sequence of 
                    functions to converge? The familiar Euclidean intuition breaks down, and we need a more general framework.
                </p>

                <p>
                    This chapter introduces that framework: <strong>metric spaces</strong> and the notion of <strong>completeness</strong>. 
                    These concepts formalize exactly when iterative algorithms converge to valid solutions. Rather than learning new 
                    computational techniques, we are gaining the language to state precisely what we have been assuming all along and 
                    to understand when those assumptions hold.
                </p>

            </section> 
            
            <section id="metric" class="section-content">
                <h2>Metric Space</h2>
                <p>
                    Remember, In <a href="../Linear_algebra/trace.html#ms"><strong>Section I</strong></a> we defined the measure space and metric
                    as follows. 
                </p>
                <div class="theorem">
                    <span class="theorem-title">Metric Space \((M, d)\):</span> 
                    <p>
                        A <strong>metric space</strong> is an ordered pair \((M, d)\)consisting of a nonempty set 
                        \(M\) and a <strong>metric</strong> \(d\) on \(M\). The metric \(d\) is a function 
                        \(d: M \times M \to \mathbb{R}\) that defines the distance between any two elements of \(M\). 
                        For all \(a, b, c \in M\), the following axioms must hold:
                    </p>
                    <ol style="padding-left: 40px;">
                        <li>Positivity: \(d(a,b) \geq 0\) with equality if and only if \(a = b\).</li>
                        <li>Symmetry: \(d(a,b) = d(b,a)\).</li>
                        <li>The triangle inequality \(d(a,b) \leq d(a,c) + d(c, b)\).</li>
                    </ol>
                </div>
                <p>
                    Like the set theory, for two metric spaces \((X, d)\) and \((Y, e)\), we can define that \(X\) is a 
                    <strong>metric subspace</strong> of \(Y\) and \(Y\) is a <strong>metric superspace</strong> of \(X\) if and 
                    only if \(X\) is a subset of \(Y\) and \(d\) is a restriction of \(e\).
                </p>   
                
                <div class="insight-box">
                    <h3>Insight: Why the Axioms Matter in Practice</h3>
                    <p>
                        You have already used non-Euclidean distances throughout this curriculum. In k-nearest neighbors, 
                        the choice between Euclidean distance, Manhattan distance, or cosine similarity fundamentally changes 
                        which points are considered "close." In kernel methods, the kernel function implicitly defines a 
                        distance in a high-dimensional feature space. These are all metrics on different spaces.
                    </p>
                    <p>
                        The axioms are not arbitrary mathematical restrictions. They encode the minimal properties required for 
                        algorithms to behave sensibly. For instance, if the triangle inequality fails, then a nearest-neighbor 
                        search might return a point \(y\) as "closest" to \(x\), even when a third point \(z\) satisfies 
                        \(d(x, z) < d(x, y)\) via an indirect path. Efficient search structures like KD-trees and ball trees 
                        rely on the triangle inequality for their pruning guarantees.
                    </p>
                </div>
             </section>  
                   
            <section id="dist" class="section-content">
                <h2>Distance</h2>
                <p>
                    With a metric defined, we can extend the notion of distance from pairs of points to more general 
                    situations. In optimization, we rarely care about the distance between two specific points; instead, 
                    we ask questions like: "How far is this point from the feasible region?" or "How close is our current 
                    iterate to the set of optimal solutions?" These questions require measuring distance from a point to 
                    a <em>set</em>.
                </p>
                <div class="theorem">
                    <span class="theorem-title">Definition: Distances</span>
                    Suppose \((X, d)\) is a metric space and \(A\) and \(B\) are subsets of \(X\). 
                    <br>
                    The <strong>distance</strong> from \(x \in X\) to \(A\) to be 
                    \[
                    \text{dist }(x, A) = \inf \{d(x, a) \mid a \in A\}  \quad (\text{Points to Sets})
                    \]
                    and also the <strong>distance</strong> from \(A\) to \(B\) to be 
                    \[
                    \text{dist }(A, B) = \inf \{d(a, b) \mid a \in A, b \in B\}.  \quad (\text{Sets to Sets})
                    \]
                    The both distances depend on what the metric \(d\) is.
                </div>

                <div class="theorem">
                    <span class="theorem-title">Definition: Isolated Points</span>
                    Suppose \(X\) is a metric space, \(S\) is a subset of \(X\), and \(z \in S\). Then \(z\) is said to be 
                    an <strong>isolated point</strong> of \(S\) if and only if 
                    \[
                    \text{dist }(z, S \ \{z\}) \neq 0.
                    \]
                    The collection of isolated points of \(S\) is denoted by \(\text{iso } (S)\).
                </div>

                <p>
                    The complement of isolated points leads to a more important concept for our purposes: <strong>accumulation points</strong>.
                    While isolated points stand alone with "room around them," accumulation points are surrounded by infinitely 
                    many other points from the set no matter how small a neighborhood we consider.
                </p>

                <div class="theorem">
                    <span class="theorem-title">Definition: Accumulation Points</span>
                    Suppose \(X\) is a metric space, \(S\) is a subset of \(X\), and \(z \in X\). Then \(z\) is said to be 
                    an <strong>accumulation point</strong> or a <strong>limit point</strong> of \(S\) in \(X\) if and only if 
                    \[
                    \text{dist }(z, S \ \{z\}) = 0.
                    \]
                    The collection of accumulation points of \(S\) in \(X\) is denoted by \(\text{acc } (S)\).
                </div>

                
                <div class="theorem">
                    <span class="theorem-title">Definition: Nearest Points</span>
                    Suppose \((X, d)\) is a metric space, \(S\) is a subset of \(X\) and \(z \in X\). A member \(s\) of \(S\) is 
                    called a <strong>nearest point</strong> of \(S\) to \(z\) in \(X\) if and only if 
                    \[
                    d(z, s) = \text{dist }(z, S).
                    \]
                </div>
                <p>
                    The concept of distance to a set is fundamental to <strong>margin-based classifiers</strong> (like SVMs), 
                    where the goal is to maximize the distance between a decision boundary and the nearest data points. 
                    Furthermore, the nearest point definition is the mathematical foundation of <strong>projection</strong>. 
                    In constrained optimization, projecting a gradient update back onto the feasible set \(S\) is exactly the 
                    act of finding a nearest point \(s \in S\) to the updated vector \(z\).
                </p>
            </section>

            <section id="bound" class="section-content">
                <h2>Boundary</h2>
                <p>
                    Having established how to measure distance to sets, we can now classify points by their relationship to 
                    a set and its complement. This classification is fundamental to constrained optimization. 
                    When we solve problems like "minimize \(f(x)\) subject to \(g(x) \leq 0\)," 
                    the feasible region is a set \(S\), and the nature of optimal solutions depends heavily on whether they 
                    lie in the <strong>interior</strong> or on the <strong>boundary</strong> of \(S\).
                </p>
                <div class="theorem">
                    <span class="theorem-title">Definition: Boundary Points</span>
                    Suppose \(X\) is a metric space, \(S\) is a subset of \(X\) and \(a \in X\). Then \(a\) is called 
                    a <strong>boundary point</strong> of \(S\) in \(X\) if and only if 
                    \[
                    \text{dist }(a, S) = \text{dist }(a, S^c) = 0.
                    \]
                    The collection of boundary points of \(S\) in \(X\) is called the <strong>boundary</strong> of \(S\) in \(X\) 
                    and is denoted by \(\partial S\). 
                </div>

                <div class="theorem">
                    <span class="theorem-title">Definition: Closure, Interior, and Exterior</span>
                    Suppose \(X\) is a metric space, and \(S\) is a subset of \(X\).
                    <ul style="padding-left: 40px;">
                        <li>The <strong>closure</strong> of \(S\) in \(X\) to be the union:
                            \[
                            \overline{S} = \text{cl}(S) = S \cup \partial S.
                            \]
                        </li>
                        <li>The <strong>interior</strong> of \(S\) in \(X\) to be the difference:
                            \[
                            S^{\circ} = \text{int}(S) = S \ \partial S.
                            \]
                        </li>
                        <li>The <strong>exterior</strong> of \(S\) in \(X\) to be the complement of the closure of \(S\) in \(X\):
                            \[
                            \text{ext}(S) = \left(\overline{S}\right)^c.
                            \]
                            Note that the exterior of \(S\) in \(X\) is the interior of its complement in \(X\):
                            \[
                            \left(\overline{S}\right)^c = (S^c)^{\circ}.
                            \]
                        </li>
                    </ul>            
                </div>

            </section>
            
            <section id="opcl" class="section-content">
                <h2>Open & Closed</h2>

                <p>
                    The concepts of interior and boundary lead naturally to a classification of sets themselves. A set is 
                    <em>open</em> if it consists entirely of interior points—intuitively, every point has "breathing room" 
                    and you can move slightly in any direction without leaving the set. A set is <em>closed</em> if it 
                    contains all its boundary points—the set includes its own "edge."
                </p>

                <div class="theorem">
                    <span class="theorem-title">Definition: Open and Closed Subsets</span>
                    Suppose \(X\) is a metric space and \(S\) is a subset of \(X\). Then \(S\) is said to be 
                    <ol style="padding-left: 40px;">
                        <li>an <strong>open</strong> subset of \(X\), or <strong>open in</strong> \(X\) iff \[S \cap \partial S = \emptyset\]</li>
                        <li>a <strong>closed</strong> subset of \(X\), or <strong>closed in</strong> \(X\) iff \[\partial S \subseteq S\]</li>
                    </ol>
                </div>

                <p>
                    Note that most subsets of metric spaces are neither open nor closed — a set can contain some but not all 
                    of its boundary points. However, the sets that <em>are</em> open form a structure worth naming.
                </p>

                <div class="theorem">
                    <span class="theorem-title">Definition: Topology</span>
                    Suppose \((X, d)\) is a metric space. The collection of <strong>open subsets</strong> of \(X\) is called the 
                    <strong>topology</strong> determined by the metric \(d\).
                </div>

                <p>
                    While we will not develop general topology here, this connection is valuable: many properties 
                    of metric spaces (continuity, convergence, compactness) depend only on which sets are open, not on the 
                    specific numerical values of the metric.
                </p>

                <p>
                    Why do these classifications matter for optimization? If our search space is strictly <strong>open</strong> 
                    (e.g., \(0 < x < 1\)), an algorithm might iterate forever, approaching a boundary value without ever 
                    reaching it—the limit point is excluded from the feasible set. A <strong>closed</strong> set contains 
                    its boundary, ensuring that edge solutions are valid. More fundamentally, the <strong>Extreme Value 
                    Theorem</strong> guarantees that a continuous function attains its maximum and minimum only when the 
                    domain is <strong>compact</strong> — which, in \(\mathbb{R}^n\), means both closed and bounded. We will 
                    formalize compactness in later chapters.
                </p>

                <p>
                    Now we arrive at one of the most important concepts for algorithmic convergence: <strong>completeness</strong>.
                </p>

                <div class="theorem">
                    <span class="theorem-title">Definition: Complete Metric Space</span>
                    <p>
                        A metric space \(X\) is said to be <strong>complete</strong> if and only if 
                        \(X\) is closed in every metric superspace of \(X\).
                    </p>
                </div>

                <p>
                    Intuitively, completeness means the space has "no holes." The real numbers \(\mathbb{R}\) with the 
                    standard metric are complete—this is why <strong>calculus</strong> works. The rational numbers 
                    \(\mathbb{Q}\) are not complete: consider the sequence defined by \(x_1 = 1\) and 
                    \(x_{n+1} = \frac{1}{2}\left(x_n + \frac{2}{x_n}\right)\). Each term is rational, but the sequence 
                    converges to \(\sqrt{2}\), which is irrational. The limit exists in \(\mathbb{R}\) but not in 
                    \(\mathbb{Q}\)—there is a "hole" where \(\sqrt{2}\) should be.
                </p>

                <p>
                    For optimization, completeness ensures that limit points of iterative algorithms actually belong to 
                    the space we're working in. In future chapters, we will develop the theory of convergence and see an 
                    equivalent characterization: a metric space is complete if and only if every Cauchy sequence converges 
                    to a limit within the space.
                </p>

            </section>

            <section id="ball" class="section-content">
                <h2>Balls</h2>
                <p>
                    Open and closed sets can be complicated objects, but we can build them from simpler pieces. The 
                    <strong>open ball</strong> centered at a point \(x\) with radius \(r\) is the set of all points 
                    within distance \(r\) of \(x\). These balls are the "building blocks" of the topology: every open 
                    set can be expressed as a union of open balls.
                </p>
                <p>
                    More importantly for algorithms, balls give us a <strong>coordinate-free</strong> way to talk about 
                    neighborhoods. When we say "there exists a step size \(\alpha\) such that gradient descent stays in the 
                    feasible region," we are implicitly saying the current point lies in an open ball contained within 
                    that region.
                </p>

                <div class="theorem">
                    <span class="theorem-title">Definition: Open Balls & Closed Balls</span>
                    Suppose \((X, d)\) is a metric space and \(x \in X\). For each \(r \in \mathbb{R}^+\), we define:
                    <ol style="padding-left: 40px;">
                        <li>The <strong>open ball</strong> in \(X\) centered at the point \(x\) and with radius \(r\) to be the set
                            \[
                            \mathcal{B}[x ; r) = \{y \in X \mid d(x, y) < r\}.
                            \] 
                        </li>
                        <li>The <strong>closed ball</strong> in \(X\) centered at the point \(x\) and with radius \(r\) to be the set 
                             \[
                            \mathcal{B}[x ; r] = \{y \in X \mid d(x, y) \leq r\}.
                            \]
                        </li>
                    </ol>
                </div>

                <p>
                    Intuitively, we want to say that the open ball in \(X\) is the open subset of \(X\), and similary, the closed ball in \(X\) 
                    is the closed subset of \(X\). Indeed, these are true. We shall show that only the open ball case here. 
                </p>

                <div class="proof">
                    <span class="proof-title">Proof: The Open Ball is Open</span>
                    Suppose \(X, d\) is a metric space, \(x \in X\), and \(r \in \mathbb{R}^+\). Consider the open ball 
                    \[
                    \mathcal{B}[x ; r) = \{y \in X \mid d(x, y) < r\} \subseteq X.
                    \]
                    If \(z \in \mathcal{B}\), then 
                    \[
                    \exists s \in (0, r) \text{ such that } d(x, z) < r - s.
                    \]
                    Now, consider any point \(w\) outside the ball, i.e., \(w \in X \setminus \mathcal{B}\). 
                    By definition, \(d(w, x) \geq r\). Using the triangle inequality, we have:
                    \[
                    d(z, w) \geq d(w, x) - d(x, z) > s. 
                    \]
                    Since \(w\) is an arbitrary point in the complement \(X \setminus \mathcal{B}\), 
                    this inequality holds for all points outside the ball. This implies that the 
                    "shortest distance" (infimum) from \(z\) to the outside the bal is at least \(s\):
                    \[
                    \text{dist }(z, X \setminus \mathcal{B}) = \inf \{d(z, w) \mid w \notin \mathcal{B}\} \geq s.
                    \]
                    Recall the definition of a <strong>boundary point</strong> requires the distance to the complement to be zero. 
                    Since the distance here is strictly positive (\(\geq s\)), \(z\) is <strong>not</strong> a boundary point. 
                    Therefore, being in \(\mathcal{B}\) but not on its boundary, \(z\) must be in the interior:
                    \[
                    z \in \mathcal{B}^{\circ}.
                    \]
                    Since \(z\) was chosen arbitrarily, every point in \(\mathcal{B}\) is an interior point: 
                    \[
                    \mathcal{B} = \mathcal{B}^{\circ}.
                    \]
                    Therefore, \(\mathcal{B}\) is open. 
                </div>

                <p>
                    The open ball also provides the foundation for discussing <strong>convexity</strong> in a coordinate-free manner. 
                    In a normed linear space, all open balls of the same radius have identical shape — each is simply a translation 
                    of the others. Moreover, these balls are themselves convex sets. We can now state the definition of convexity 
                    that applies to general normed spaces, not just \(\mathbb{R}^n\):
                </p>

                <div class="theorem">
                    <span class="theorem-title">Definition: Convex</span>
                    Suppose \(V\) is a normed linear space and \(C \subseteq V\). Then \(C\) is said to be <strong>convex</strong> if and only if 
                    for each \(a, b \in C\), the line segment \(\{ (1 - t)a + tb \mid t \in [0, 1] \}\) jointing \(a\) and \(b\) is included in 
                    \(C\).
                </div>

            </section>  
        </div>
        <script src="/js/main.js"></script>
    </body>
</html>