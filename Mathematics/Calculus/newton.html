<!DOCTYPE html>
<html>
    <head> 
        <title>Newton's method</title>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <script src="https://cdn.jsdelivr.net/pyodide/v0.23.3/full/pyodide.js"></script>
        <script defer src="https://pyscript.net/alpha/pyscript.js"></script>
        <link rel="stylesheet" href="../styles.css">
        <meta name="viewport" content="width=device-width, initial-scale=1">
    </head>
    <body> 
        <h1>Line Search</h1>
        <blockquote>
            In the previous chapter, we used a "fixed" <strong>learning rate</strong>(or <strong>step size</strong>), which was chosen 
            based on experimental results. However, the "optimal" step size can be determined by <strong>line search</strong>:
            \[
            \eta_t = \arg \min _{\eta > 0} \mathcal{L} (\theta_t + \eta d_t) \in \mathbb{R}.
            \]
            Consider a quadratic loss:
            \[
            \mathcal{L}(\theta + \eta d) = \frac{1}{2}(\theta+ \eta d)^T A (\theta+ \eta d) + b^T (\theta + \eta d) + c.
            \]
            where \(\theta, d \in \mathbb{R}^n\), and \(\eta, c \in \mathbb{R}\).
            <br>
            Taking derivative of this function:
            \[
            \begin{align*}
            \frac{d \mathcal{L}(\theta + \eta d)}{d\eta} &= \frac{1}{2} d^T 2A(\theta + \eta d) + d^T b  \\\\
                                                         &= d^T(A\theta + b) + \eta d^T A d
            \end{align*}
            \]
            Setting the derivative equal to zero, we have:
            \[
            \eta = - \frac{d^T(A\theta + b)}{d^T A d}.
            \]
            In practice, we don't need the "exact" line serach because it can be expensive to solve this sub-optimization 
            problem "at each step." 
            <br>
            For example, we can start with some initial step size, and then reduce it by a factor \(c \in (0, 1)\)  at each 
            iteration until we satisfy the following condition: 
            \[
            \mathcal{L}(\theta_t + \eta d_t) \leq \mathcal{L}(\theta_t) + c \eta \nabla \mathcal{L}(\theta_t)^T d_t \tag{1}
            \]
            This is called <strong>Armijo condition(Sufficient Decrease condition)</strong> that ensures sufficient 
            decreasing of our objective function.
            <br>
            Note: Usually, we set \(c = 10^{-4}\).
        </blockquote>

        <h1>Newton's method</h1>
        <blockquote>
            Even though the first-order methods such as the gradient descent is computationally cheap, they do not consider the 
            <strong>curvature</strong>, which often leads to slower convergence. A classic second-order method, <strong>Newton's method</strong> 
            is as follows:
            <div class="theorem">
                <span class="theorem-title">Algorithm 1: NEWTONS_METHOD</span>
                <strong>Input:</strong> objective function\(\mathcal{L}\), tolerance \(\epsilon\), initial step size \(\eta_o\);
                <br> 
                <strong>Output:</strong> stationary point \(\theta^*\);
                <br>
                <strong>begin</strong>
                <br>
                &emsp;\(t \leftarrow 0\);
                <br>
                &emsp;Choose an initial point \(\theta_o\);
                <br>
                &emsp;<strong>repeat: </strong>
                <br>
                &emsp;&emsp;&emsp;Compute gradient: \(g_t = \nabla \mathcal{L}(\theta_t);\)
                <br>
                &emsp;&emsp;&emsp;Compute Hessian: \(H_t = \nabla^2 \mathcal{L}(\theta_t);\)
                <br>
                &emsp;&emsp;&emsp;Solve \(H_t d_t = -g_t\) for \(d_t\);
                <br>
                &emsp;&emsp;&emsp;\(d_t \leftarrow -H_t^{-1}g_t\);
                <br>
                &emsp;&emsp;&emsp;Do LINE_SEARCH to get step size \(\eta_t\) along \(d_t\);
                <br>
                &emsp;&emsp;&emsp;Update parameters: \(\theta_{t+1} = \theta_t + \eta_t d_t;\)
                <br>
                &emsp;&emsp;&emsp;\( t \leftarrow t + 1 ;\)
                <br>
                &emsp;<strong>until</strong> \(\| \nabla \mathcal{L}(\theta_t) \| < \epsilon\);
                <br>
                &emsp;Output \(\theta_t\);
                <br>
                <strong>end</strong>
                <br>
            </div>
            A critical issue of Newton's method is computing the inverse Hessian \(H_t^{-1}\) at 
            each \(t\) step, which is obviously expensive. 

        </blockquote>

        <h1>BFGS method</h1>
        <blockquote>
            In <strong>Quasi-Newton methods</strong>, we approximate the Hessian using the curvature information from the gradient vector at each step. 
            The most common method is called <strong>BFGS(Broyden, Fletcher, Goldfarb and Shanno) method</strong>.
            <div class="theorem">
                <span class="theorem-title">Approximation of Hessian in BFGS</span>
                \[
                H_{t+1} \approx B_{t+1} = B_{t} + \frac{y_t y_t^T}{y_t^Ts_t} - \frac{(B_t s_t)(B_t s_t)^T}{s_t^T B_t s_t}
                \]
                where \(s_t = \theta_t - \theta_{t-1}\) is the step in the parameter space and \(y_t = g_t - g_{t-1}\) is the change in the gradient 
                of \(\mathcal{L}\).
                <br>
                Alternatively, we can iteratively update an approximation to the inverse Hessian: 
                \[
                H_{t+1}^{-1} \approx C_{t+1} = \Big(I - \frac{s_t y_t^T}{y_t^T s_t} \Big) C_t \Big( I - \frac{y_t s_t^T}{y_t^T s_t}\Big) + \frac{s_t s_t^T}{y_t^Ts_t}.
                \]
            </div>
            Note: Check <a href="woodbury.html"><strong>Sherman-Morrison formula</strong></a>. 
            <br><br>
            If the initial \(B_o\) is positive definite(Usually, \(B_o = I\)), and the step size \(\eta\) is found via Condition (1) and the following 
            <strong>curvature condition</strong>: 
            \[
            \nabla \mathcal{L}(\theta_t + \eta  d_t) \geq c_2 \eta \nabla \mathcal{L}(\theta_t)d_t \tag{2}
            \]
            where \(c_2 \in (c, 1)\).
            <br>
            Then \(B_{t+1}\) remains positive definite. Condition (1) and (2) are together called <strong>Wolfe conditions</strong>. 
            <div class="theorem">
                <span class="theorem-title">Theorem 1: Wolfe conditions</span>
                <ol>
                    <li>Sufficient decrease condition</li>
                    \(\mathcal{L}(\theta_t + \eta d_t) \leq \mathcal{L}(\theta_t) + c_1 \eta \nabla \mathcal{L}(\theta_t)^T d_t\)
                    <li>Curvature condition</li>
                    \(\nabla \mathcal{L}(\theta_t + \eta  d_t) \geq c_2 \eta \nabla \mathcal{L}(\theta_t)d_t\)
                    <br>
                    where \(0 < c_1 << c_2 < 1\). For example, c_1 = 10^{-4}, c_2 = 0.9 for Quasi-Newton methods.
                </ol>
            </div>
            In practice, we cannot store the whole Hessian approximation for large-scale problems. It costs \(O(n^2)\) memory space. 
            In <strong>Limited memory BFGS(L-BFGS)</strong>, we only use the \(m\) most recent \(s_t, y_t\) pairs, and do not store the Hessian approximation 
            explicitly. So, we can approximate \(H_t^{-1 }g_t\) by computing a sequence of inner products of these vectors. The memory requirement 
            can be \(O(mn)\), where \(m\) is typically 5 to 20.
            <br>
            <div class="theorem">
                <span class="theorem-title">Algorithm 2: L_BFGS</span>
                <strong>Input:</strong> objective function\(\mathcal{L}\), tolerance \(\epsilon\), initial step size \(\eta_o\);
                <br> 
                <strong>Output:</strong> stationary point \(\theta^*\);
                <br>
                <strong>begin</strong>
                <br>
                &emsp;Choose an initial point \(\theta_o\);
                <br>
                &emsp;Initialize \( s_t, y_t \) storage for \( m \) past updates.
                <br>
                &emsp;Set initial inverse Hessian approximation \( H_o = I \) (identity matrix).
                <br>
                &emsp;\(t \leftarrow 0\);
                <br>
                &emsp;<strong>repeat: </strong>
                <br>
                &emsp;&emsp;Compute \( g_t = \nabla \mathcal{L}(\theta_t) \).
                <br>
                &emsp;&emsp;Set \( q = g_t \)
                <br>
                &emsp;&emsp;Loop backward (from latest stored \( s, y \) pairs):
                <br>
                &emsp;&emsp;&emsp;Compute \( \alpha_i = \frac{s_i^\top q}{y_i^\top s_i} \).
                <br>
                &emsp;&emsp;&emsp;Update \( q = q - \alpha_i y_i \).
                <br>
                &emsp;&emsp;Multiply \( q \) by \( H_o \): \( r = H_o q \).
                <br>
                &emsp;&emsp;Loop forward (from oldest stored \( s, y \) pairs):
                <br>
                &emsp;&emsp;&emsp;Compute \( \beta_i = \frac{y_i^\top r}{y_i^\top s_i} \).
                <br>
                &emsp;&emsp;&emsp;Update \( r = r + s_i (\alpha_i - \beta_i) \).
                <br>
                &emsp;&emsp;Set serch direction  \( p_t = -r \).
                <br>
                &emsp;&emsp;Do LINE_SEARCH: find step size \( \eta_t \) s.t. \( \mathcal{L}(\theta_t + \eta_t p_t) < \mathcal{L}(\theta_t) \)
                <br>
                &emsp;&emsp;Update parameters: \( \theta_{t+1} = \theta_t + \eta_t p_t \).
                <br>
                &emsp;&emsp;Update gradient \( g_{t+1} = \nabla \mathcal{L}(\theta_{t+1}) \).
                <br>
                &emsp;&emsp;Store \( s_t \) and \( y_t \):\( s_t = \theta_{t+1} - \theta_t \) and \( y_t = g_{t+1} - g_t \).
                <br>
                &emsp;&emsp;Maintain memory size:If the number of stored \( s, y \) pairs exceeds \( m \), discard the oldest pair.
                <br>
                &emsp;<strong>until</strong> \( \|g_t\| < \epsilon \);
                <br>
                &emsp;Output \(\theta_t\);
                <br>
                <strong>end</strong>
                
                <br>
            </div>

        </blockquote>

        <a href="../../index.html">Back to Home </a>
        <br> <a href="calculus.html">Back to Calculus </a>

        <script> 
            let pyodide; 
            async function loadPyodideAndPackages() { 
                pyodide = await loadPyodide(); 
                await pyodide.loadPackage("numpy");
                console.log("Pyodide and numpy loaded successfully."); 
            } 
            async function runPythonCode() { 
                const pythonCode = document.querySelector('.python-code').textContent; 
                const outputContainer = document.getElementById('output'); 
                outputContainer.innerHTML = ""; 
                try { 
                    let capturedOutput = []; 
                    pyodide.globals.set("print", function(...args) { 
                        capturedOutput.push(args.join(" ")); 
                    }); 
                    await pyodide.runPythonAsync(pythonCode); 
                    outputContainer.textContent = capturedOutput.join("\n"); 
                } catch (error) { 
                    outputContainer.textContent = `Error: ${error.message}`; 
                    console.error("Execution error:", error); } 
                } 
                loadPyodideAndPackages(); 
        </script>

        <script>
            // Collapsible Section Functionality
            const collapsibleBtns = document.querySelectorAll('.collapsible-btn');

            collapsibleBtns.forEach(btn => {
                btn.addEventListener('click', function() {
                    const content = this.nextElementSibling; // The <div> with the code
                    if (content.style.display === 'block') {
                        content.style.display = 'none'; // Hide the content
                } else {
                        content.style.display = 'block'; // Show the content
                        }
                });
            });
        </script>
        
    </body>
</html>