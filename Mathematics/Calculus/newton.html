<!DOCTYPE html>
<html>
    <head> 
        <title>Newton's method</title>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <script src="https://cdn.jsdelivr.net/pyodide/v0.23.3/full/pyodide.js"></script>
        <script defer src="https://pyscript.net/alpha/pyscript.js"></script>
        <link rel="stylesheet" href="../styles.css">
        <meta name="viewport" content="width=device-width, initial-scale=1">
    </head>
    <body> 
        <h1>Line Search</h1>
        <blockquote>
            In the previous chapter, we used a "fixed" <strong>learning rate</strong>(or <strong>step size</strong>), which was chosen 
            based on experimental results. However, the "optimal" step size can be determined by <strong>line search</strong>:
            \[
            \eta_t = \arg \min _{\eta > 0} \mathcal{L} (\theta_t + \eta d_t) \in \mathbb{R}.
            \]
            Consider a quadratic loss:
            \[
            \mathcal{L}(\theta + \eta d) = \frac{1}{2}(\theta+ \eta d)^T A (\theta+ \eta d) + b^T (\theta + \eta d) + c.
            \]
            where \(\theta, d \in \mathbb{R}^d\), and \(\eta, c \in \mathbb{R}\).
            <br>
            Taking derivative of this function:
            \[
            \begin{align*}
            \frac{d \mathcal{L}(\theta + \eta d)}{d\eta} &= \frac{1}{2} d^T 2A(\theta + \eta d) + d^T b  \\\\
                                                         &= d^T(A\theta + b) + \eta d^T A d
            \end{align*}
            \]
            Setting the derivative equal to zero, we have:
            \[
            \eta = - \frac{d^T(A\theta + b)}{d^T A d}.
            \]
            In practice, we don't need the "exact" line serach because it can be expensive to solve this sub-optimization 
            problem "on each iteration." 
            <br>
            For example, we can start with some initial step size, and then reduce it by a factor \(c \in [0, 1]\)  at each 
            iteration until we satisfy the following condition: 
            \[
            \mathcal{L}(\theta_t + \eta d_t) \leq \mathcal{L}(\theta_t) + c \eta \nabla \mathcal{L}(\theta_t)^T d_t
            \]
            This is called <strong>Armijo condition</strong> that ensures sufficient decreasing of our objective function.
            Note: Usually, we set \(c = 10^{-4}\).
        </blockquote>

        <h1>Newton's method</h1>
        <blockquote>
            Even though the first-order methods such as the gradient descent is computationally cheap, they do not consider the 
            <strong>curvature</strong>, which often leads to slower convergence. A classic second-order method, <strong>Newton's method</strong> 
            is as follows:
            <div class="theorem">
                <span class="theorem-title">Algorithm 1: NEWTONS_METHOD</span>
                <strong>Input:</strong> objective function\(\mathcal{L}\), tolerance \(\epsilon\), initial ste size \(\eta_o\);
                <br> 
                <strong>Output:</strong> stationary point \(\theta^*\);
                <br>
                <strong>begin</strong>
                <br>
                &emsp;\(t \leftarrow 0\);
                <br>
                &emsp;Choose an initial point \(\theta_o\);
                <br>
                &emsp;<strong>repeat: </strong>
                <br>
                &emsp;&emsp;&emsp;Compute gradient: \(g_t = \nabla \mathcal{L}(\theta_t);\)
                <br>
                &emsp;&emsp;&emsp;Compute Hessian: \(H_t = - \nabla^2 \mathcal{L}(\theta_t);\)
                <br>
                &emsp;&emsp;&emsp;Solve \(H_t d_t = -g_t\) for \(d_t\);
                <br>
                &emsp;&emsp;&emsp;\(d_t \leftarrow -H_t^{-1}g_t\);
                <br>
                &emsp;&emsp;&emsp;Do LINE_SEARCH to get step size \(\eta_t\) along \(d_t\);
                <br>
                &emsp;&emsp;&emsp;Update parameters: \(\theta_{t+1} = \theta_t + \eta_t d_t;\)
                <br>
                &emsp;&emsp;&emsp;\( t \leftarrow t + 1 ;\)
                <br>
                &emsp;<strong>until</strong> \(\| \nabla \mathcal{L}(\theta_t) \| < \epsilon\);
                <br>
                &emsp;Output \(\theta_t\);
                <br>
                <strong>end</strong>
                <br>
            </div>
            A critical issue of Newton's method is computing the inverse Hessian \(H_t^{-1}\) on 
            each \(t\) step, which is obviously expensive. 
            
        </blockquote>

        <h1>Quasi-Newton method</h1>
        <blockquote>
        </blockquote>

        <h1>BFGS method</h1>
        <blockquote>
        </blockquote>

        <a href="../../index.html">Back to Home </a>
        <br> <a href="calculus.html">Back to Calculus </a>

        <script> 
            let pyodide; 
            async function loadPyodideAndPackages() { 
                pyodide = await loadPyodide(); 
                await pyodide.loadPackage("numpy");
                console.log("Pyodide and numpy loaded successfully."); 
            } 
            async function runPythonCode() { 
                const pythonCode = document.querySelector('.python-code').textContent; 
                const outputContainer = document.getElementById('output'); 
                outputContainer.innerHTML = ""; 
                try { 
                    let capturedOutput = []; 
                    pyodide.globals.set("print", function(...args) { 
                        capturedOutput.push(args.join(" ")); 
                    }); 
                    await pyodide.runPythonAsync(pythonCode); 
                    outputContainer.textContent = capturedOutput.join("\n"); 
                } catch (error) { 
                    outputContainer.textContent = `Error: ${error.message}`; 
                    console.error("Execution error:", error); } 
                } 
                loadPyodideAndPackages(); 
        </script>

        <script>
            // Collapsible Section Functionality
            const collapsibleBtns = document.querySelectorAll('.collapsible-btn');

            collapsibleBtns.forEach(btn => {
                btn.addEventListener('click', function() {
                    const content = this.nextElementSibling; // The <div> with the code
                    if (content.style.display === 'block') {
                        content.style.display = 'none'; // Hide the content
                } else {
                        content.style.display = 'block'; // Show the content
                        }
                });
            });
        </script>
        
    </body>
</html>