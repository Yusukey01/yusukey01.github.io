<!DOCTYPE html>
<html>
    <head> 
        <title>Newton's method</title>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <script src="https://cdn.jsdelivr.net/pyodide/v0.23.3/full/pyodide.js"></script>
        <script defer src="https://pyscript.net/alpha/pyscript.js"></script>
        <link rel="stylesheet" href="../styles.css">
        <meta name="viewport" content="width=device-width, initial-scale=1">
    </head>
    <body> 
        <h1>Line Search</h1>
        <blockquote>
            In the previous chapter, we used a "fixed" <strong>learning rate</strong>(or <strong>step size</strong>), which was chosen 
            based on experimental results. However, the "optimal" step size can be determined by <strong>line search</strong>:
            \[
            \eta_t = \arg \min _{\eta > 0} \mathcal{L} (\theta_t + \eta d_t) \in \mathbb{R}.
            \]
            Consider a quadratic loss:
            \[
            \mathcal{L}(\theta + \eta d) = \frac{1}{2}(\theta+ \eta d)^\top A (\theta+ \eta d) + b^\top (\theta + \eta d) + c.
            \]
            where \(\theta, d \in \mathbb{R}^n\), and \(\eta, c \in \mathbb{R}\).
            <br>
            Taking derivative of this function:
            \[
            \begin{align*}
            \frac{d \mathcal{L}(\theta + \eta d)}{d\eta} &= \frac{1}{2} d^\top 2A(\theta + \eta d) + d^\top b  \\\\
                                                         &= d^\top (A\theta + b) + \eta d^\top A d
            \end{align*}
            \]
            Setting the derivative equal to zero, we have:
            \[
            \eta = - \frac{d^\top (A\theta + b)}{d^\top A d}.
            \]
            In practice, we don't need the "exact" line serach because it can be expensive to solve this sub-optimization 
            problem "at each step." 
            <br>
            For example, we can start with some initial step size, and then reduce it by a factor \(c \in (0, 1)\)  at each 
            iteration until we satisfy the following condition: 
            \[
            \mathcal{L}(\theta_t + \eta d_t) \leq \mathcal{L}(\theta_t) + c \eta \nabla \mathcal{L}(\theta_t)^\top d_t \tag{1}
            \]
            This is called <strong>Armijo condition(Sufficient Decrease condition)</strong> that ensures sufficient 
            decreasing of our objective function.
            <br>
            Note: Usually, we set \(c = 10^{-4}\).
        </blockquote>

        <h1>Newton's method</h1>
        <blockquote>
            Even though the first-order methods such as the gradient descent is computationally cheap, they do not consider the 
            <strong>curvature</strong>, which often leads to slower convergence. A classic second-order method, <strong>Newton's method</strong> 
            is as follows:
            <div class="theorem">
                <span class="theorem-title">Algorithm 1: NEWTONS_METHOD</span>
                <strong>Input:</strong> objective function \(\mathcal{L}\), tolerance \(\epsilon\), initial step size \(\eta_o\);
                <br> 
                <strong>Output:</strong> stationary point \(\theta^*\);
                <br>
                <strong>begin</strong>
                <br>
                &emsp;\(t \leftarrow 0\);
                <br>
                &emsp;Choose an initial point \(\theta_o\);
                <br>
                &emsp;<strong>repeat: </strong>
                <br>
                &emsp;&emsp;&emsp;Compute gradient: \(g_t = \nabla \mathcal{L}(\theta_t);\)
                <br>
                &emsp;&emsp;&emsp;Compute Hessian: \(H_t = \nabla^2 \mathcal{L}(\theta_t);\)
                <br>
                &emsp;&emsp;&emsp;Solve \(H_t d_t = -g_t\) for \(d_t\);
                <br>
                &emsp;&emsp;&emsp;\(d_t \leftarrow -H_t^{-1}g_t\);
                <br>
                &emsp;&emsp;&emsp;Do LINE_SEARCH to get step size \(\eta_t\) along \(d_t\);
                <br>
                &emsp;&emsp;&emsp;Update parameters: \(\theta_{t+1} = \theta_t + \eta_t d_t;\)
                <br>
                &emsp;&emsp;&emsp;\( t \leftarrow t + 1 ;\)
                <br>
                &emsp;<strong>until</strong> \(\| g_t \| < \epsilon\);
                <br>
                &emsp;Output \(\theta_t\);
                <br>
                <strong>end</strong>
                <br>
            </div>
            A critical issue of Newton's method is computing the inverse Hessian \(H_t^{-1}\) at 
            each \(t\) step, which is obviously expensive. 

        </blockquote>

        <h1>BFGS method</h1>
        <blockquote>
            In <strong>Quasi-Newton methods</strong>, we approximate the Hessian using the curvature information from the gradient vector at each step. 
            The most common method is called <strong>BFGS(Broyden, Fletcher, Goldfarb and Shanno) method</strong>.
            <div class="theorem">
                <span class="theorem-title">Approximation of Hessian in BFGS</span>
                \[
                H_{t+1} \approx B_{t+1} = B_{t} + \frac{y_t y_t^\top}{y_t^\top s_t} - \frac{(B_t s_t)(B_t s_t)^\top }{s_t^\top B_t s_t}
                \]
                where \(s_t = \theta_t - \theta_{t-1}\) is the step in the parameter space and \(y_t = g_t - g_{t-1}\) is the change in the gradient 
                of \(\mathcal{L}\).
                <br>
                Alternatively, we can iteratively update an approximation to the inverse Hessian: 
                \[
                H_{t+1}^{-1} \approx C_{t+1} = \Big(I - \frac{s_t y_t^\top}{y_t^\top s_t} \Big) C_t \Big( I - \frac{y_t s_t^\top}{y_t^\top s_t}\Big) + \frac{s_t s_t^\top}{y_t^\top s_t}.
                \]
            </div>
            Note: Check <a href="../Linear_algebra/woodbury.html"><strong>Sherman-Morrison formula</strong></a>. 
            <br><br>
            If the initial \(B_o\) is positive definite(Usually, \(B_o = I\)), and the step size \(\eta\) is found via Condition (1) and the following 
            <strong>curvature condition</strong>: 
            \[
            \nabla \mathcal{L}(\theta_t + \eta  d_t) \geq c_2 \eta \nabla \mathcal{L}(\theta_t)^\top d_t \tag{2}
            \]
            where \(c_2 \in (c, 1)\).
            <br>
            Then \(B_{t+1}\) remains positive definite. Condition (1) and (2) are together called <strong>Wolfe conditions</strong>. 
            <div class="theorem">
                <span class="theorem-title">Theorem 1: Wolfe conditions</span>
                <ol>
                    <li>Sufficient decrease condition</li>
                    \(\mathcal{L}(\theta_t + \eta d_t) \leq \mathcal{L}(\theta_t) + c_1 \eta \nabla \mathcal{L}(\theta_t)^\top d_t\)
                    <li>Curvature condition</li>
                    \(\nabla \mathcal{L}(\theta_t + \eta  d_t) \geq c_2 \eta \nabla \mathcal{L}(\theta_t)^\top d_t\)
                    <br>
                    where \(0 < c_1 << c_2 < 1\). For example, \(c_1 = 10^{-4}, c_2 = 0.9\) for Quasi-Newton methods.
                </ol>
            </div>
            In practice, we cannot store the whole Hessian approximation for large-scale problems. It costs \(O(n^2)\) memory space. 
            In <strong>Limited memory BFGS(L-BFGS)</strong>, we only use the \(m\) most recent \((s_t, y_t)\) pairs, and do not store the Hessian approximation 
            explicitly. So, we can approximate \(H_t^{-1 }g_t\) by computing a sequence of inner products of these vectors. The memory requirement 
            can be \(O(mn)\), where \(m\) is typically 5 to 20.
            <br>
            <div class="theorem">
                <span class="theorem-title">Algorithm 2: L_BFGS</span>
                <strong>Input:</strong> objective function \(\mathcal{L}\), tolerance \(\epsilon\), initial step size \(\eta_o\);
                <br> 
                <strong>Output:</strong> stationary point \(\theta^*\);
                <br>
                <strong>begin</strong>
                <br>
                &emsp;Choose an initial point \(\theta_o\);
                <br>
                &emsp;Initialize \( (s, y) \) storage for \( m \) past updates;
                <br>
                &emsp;\(t \leftarrow 0\);
                <br>
                &emsp;<strong>repeat: </strong>
                <br>
                &emsp;&emsp;Compute gradient \( g_t = \nabla \mathcal{L}(\theta_t) \);
                <br>
                &emsp;&emsp;Set \( q_t \leftarrow g_t \);
                <br>
                &emsp;&emsp;Loop backward (from latest stored \( (s, y) \) pairs):
                <br>
                &emsp;&emsp;&emsp;Compute \( \alpha_i = \frac{s_i^\top q_t}{y_i^\top s_i} \);
                <br>
                &emsp;&emsp;&emsp;Update \( q_t \leftarrow q_t - \alpha_i y_i \);
                <br>
                &emsp;&emsp;Set \( r_t \leftarrow H_t^0 q_t \);
                <br>
                &emsp;&emsp;Loop forward (from oldest stored \( (s, y) \) pairs):
                <br>
                &emsp;&emsp;&emsp;Compute \( \beta_i = \frac{y_i^\top r_t}{y_i^\top s_i} \);
                <br>
                &emsp;&emsp;&emsp;Update \( r_t \leftarrow r_t + s_i (\alpha_i - \beta_i) \);
                <br>
                &emsp;&emsp;Set serch direction  \( p_t \leftarrow -r_t \);
                <br>
                &emsp;&emsp;Do LINE_SEARCH: find step size \( \eta_t \) along \(p_t\) s.t. \( \mathcal{L}(\theta_t + \eta_t p_t) < \mathcal{L}(\theta_t) \);
                <br>
                &emsp;&emsp;Update parameters: \( \theta_{t+1} \leftarrow \theta_t + \eta_t p_t \);
                <br>
                &emsp;&emsp;Update gradient \( g_{t+1} \leftarrow \nabla \mathcal{L}(\theta_{t+1}) \);
                <br>
                &emsp;&emsp;Store \( (s_t\leftarrow \theta_{t+1} - \theta_t,\, y_t \leftarrow g_{t+1} - g_t) \);
                <br>
                &emsp;&emsp;Maintain memory size:If the number of stored \( s, y \) pairs exceeds \( m \), discard the oldest pair;
                <br>
                &emsp;&emsp;\( t \leftarrow t + 1\);
                <br>
                &emsp;<strong>until</strong> \( \|g_t\| < \epsilon \);
                <br>
                &emsp;Output \(\theta_t\);
                <br>
                <strong>end</strong>
            </div>
            Note: For computational efficiency and numerical stability, instead of calculating \(\frac{1}{s^\top y}\) multiple times, usually 
            a new variable \(\rho = \frac{1}{s^\top y}\) will be introduced. 
            <br>
            A sample code for L-BFGS is as follows: 
            <div class="collapsible-section">
            <button class="collapsible-btn">Show/Hide Code</button>
            <div class="collapsible-content">
            <pre class="python-code">
                import numpy as np

                # Line search with Wolfe conditions 
                def line_search(f, grad_f, theta, p, c1 = 1e-4, c2 = 0.9, max_iter = 100):
                    eta = 1.0
                    eta_low = 0.0
                    eta_high = None

                    phi_0 = f(theta)
                    grad_phi_0 = np.dot(grad_f(theta), p)

                    for _ in range(max_iter):
                        phi_eta = f(theta + eta * p)

                        # Check the first Wolfe condition (sufficient decrease)
                        if phi_eta > phi_0 + c1 * eta * grad_phi_0:
                            eta_high = eta
                        else:
                            # Check the second Wolfe condition (curvature condition)
                            grad_phi_eta = np.dot(grad_f(theta + eta * p), p)
                            if grad_phi_eta < c2 * grad_phi_0:
                                eta_low = eta
                            else:
                                return eta

                        # Update alpha using bisection method
                        if eta_high is not None:
                            eta = (eta_low + eta_high) / 2.0
                        else:
                            eta *= 2.0

                    return eta

                # Limited memory BFGS 
                def limited_bfgs(f, grad_f, theta0, m = 10, tol = 1e-6, max_iter = 2000):
                    
                    theta = theta0.copy()
                    g = grad_f(theta)
                    s_list = []
                    y_list = []
                    rho_list = [] # We introduce rho =  1/s^T y instead of directly using 1/s^T y for efficiency & stability. 

                    for _ in range(max_iter):
                        
                        if np.linalg.norm(g) < tol:
                            break

                        q = g.copy()
                        alpha_list = []  # Need this for the second loop to avoid computing alpha again 

                        # Loop backward through stored (s, y) pairs
                        for s, y, rho in reversed(list(zip(s_list, y_list, rho_list))):
                            alpha = rho * np.dot(s, q)
                            alpha_list.append(alpha)
                            q -= alpha * y

                        # Initial Hessian approximation is identity: H0 = I, so H0 * q = q
                        r = q

                        # Loop forward through stored (s, y) pairs
                        for (s, y, rho), alpha in zip(zip(s_list, y_list, rho_list), reversed(alpha_list)):
                            beta = rho * np.dot(y, r)
                            r += s * (alpha - beta)

                        # Search direction
                        p = -r

                        # Compute the step size by Line search satisfying Wolfe conditions
                        eta = line_search(f, grad_f, theta, p)

                        # Update parameters
                        theta += eta * p
                        grad_next = grad_f(theta)

                        # Update memory for (s, y) pairs
                        s = eta * p
                        y = grad_next - g
                        if np.dot(s, y) > 1e-6:  
                            if len(s_list) == m:
                                s_list.pop(0)
                                y_list.pop(0)
                                rho_list.pop(0)
                            s_list.append(s)
                            y_list.append(y)
                            rho_list.append(1.0 / np.dot(y, s))
                            
                        # Update gradient
                        g = grad_next

                    return theta

                # Objective function and its gradient: 
                # The Rosenbrock function is commonly used for testing optimization algorithms.
                def rosenbrock(x):
                    return np.sum(100 * (x[1:] - x[:-1]**2)**2 + (1 - x[:-1])**2)

                def grad_rosenbrock(x):
                    grad = np.zeros_like(x)
                    grad[:-1] = -400 * x[:-1] * (x[1:] - x[:-1]**2) - 2 * (1 - x[:-1]) # # x_1 to x_n-1
                    grad[1:] += 200 * (x[1:] - x[:-1]**2) # x_2 to x_n
                    return grad

                # Finite difference gradient to check grad_rosenbrock().
                def finite_difference_gradient(f, x, epsilon=1e-6):
                    grad = np.zeros_like(x)
                    for i in range(len(x)):
                        x_forward = x.copy()
                        x_backward = x.copy()
                        x_forward[i] += epsilon
                        x_backward[i] -= epsilon
                        grad[i] = (f(x_forward) - f(x_backward)) / (2 * epsilon)
                    return grad

                if __name__ == "__main__":
                    
                    n = 50 # Dimensionality
                    
                    # Randomly generate an initial point x0:
                    # You could try : x0 = np.ones(n) + 0.3 * np.random.randn(n), which represents adding small 
                    # perturbation around the global minimum x* = [1, ... , 1]^T 
                    x0 =  np.random.randn(n)  
                    numeric_opt = limited_bfgs(rosenbrock, grad_rosenbrock, x0)
                    print("Initial point: \n", x0.tolist())
                    print("\n Numerical optimum: \n", numeric_opt.tolist())

                    '''
                    # If you are not sure about the gradient of the objective, always you can compare it with finite difference.
                    grad_analytic = grad_rosenbrock(x0)
                    grad_numeric = finite_difference_gradient(rosenbrock, x0)
                    relative_error_grad = np.linalg.norm(grad_analytic - grad_numeric) / np.linalg.norm(grad_analytic)
                    print("Relative error:", relative_error_grad) 
                    ''' 
                    # Relative error between the numerical optimum and the global optimum
                    global_opt = np.ones(n)  # The actual global optimum of Rosenbrock function is x* = [1, ... , 1]^T
                    relative_error_optimal = np.linalg.norm(numeric_opt - global_opt) / np.linalg.norm(global_opt)
                    print(f"\n Relative error to the global minimum: {relative_error_optimal*100:.8f}%")
            </pre>
            </div>
            <button class="run-button" onclick="runPythonCode()">Run Code</button>
            <div class="python-output" id="output"></div>
            <br>
            Note: On the above code, we used the <strong>Rosenbrock function</strong>, which is defined as 
            \[
            f(x) = \sum_{i = 1}^{n-1} a (x_{i+1} - x_i^2)^2 + (b - x_i)^2
            \]
            where \(x \in \mathbb{R}^n\), and usually we set the coefficients \(a = 100, \, b = 1\).
            <br>
            The global minimum can be obtained simply at \(x = [1, 1, \cdots, 1]\), but in numerical optimization, converging to 
            the global minimum is difficult because of several reasons:
            <ul>
                <li>Broad "flat" regions & several local minima:</li>
                Optimization algorithms can be trapped there.
                <li>Ill-conditioned:</li> 
                Small changes in one variable can result in large changes in another.
                <li>A narrow, curved valley containing the global minimum:</li> 
                It makes hard for optimization algorithms to navigate. The 
                steep sides of the valley can cause algorithms to overshoot or converge very slowly.
            </ul>
            Rosenbrock function is a commom benchmark for testing the robustness and efficiency of optimization algorithms.
        </blockquote>

        <a href="../../index.html">Back to Home </a>
        <br> <a href="calculus.html">Back to Calculus </a>

        <script> 
            let pyodide; 
            async function loadPyodideAndPackages() { 
                pyodide = await loadPyodide(); 
                await pyodide.loadPackage("numpy");
                console.log("Pyodide and numpy loaded successfully."); 
            } 
            async function runPythonCode() { 
                const pythonCode = document.querySelector('.python-code').textContent; 
                const outputContainer = document.getElementById('output'); 
                outputContainer.innerHTML = ""; 
                try { 
                    let capturedOutput = []; 
                    pyodide.globals.set("print", function(...args) { 
                        capturedOutput.push(args.join(" ")); 
                    }); 
                    await pyodide.runPythonAsync(pythonCode); 
                    outputContainer.textContent = capturedOutput.join("\n"); 
                } catch (error) { 
                    outputContainer.textContent = `Error: ${error.message}`; 
                    console.error("Execution error:", error); } 
                } 
                loadPyodideAndPackages(); 
        </script>

        <script>
            // Collapsible Section Functionality
            const collapsibleBtns = document.querySelectorAll('.collapsible-btn');

            collapsibleBtns.forEach(btn => {
                btn.addEventListener('click', function() {
                    const content = this.nextElementSibling; // The <div> with the code
                    if (content.style.display === 'block') {
                        content.style.display = 'none'; // Hide the content
                } else {
                        content.style.display = 'block'; // Show the content
                        }
                });
            });
        </script>
        
    </body>
</html>