---
layout: default
title: Convergence & Limits in Metric Spaces
level: detail
description: Formalize convergence of sequences in metric spaces, establish the relationship between Cauchy sequences and completeness, and explore boundedness—the theoretical foundation for analyzing iterative algorithms.
uses_math: true
uses_python: false
noindex: true
---
<!DOCTYPE html>
<html>
    <body>
        <!-- LearningResource Schema for Convergence & Limits -->
        <script type="application/ld+json">
        {
        "@context": "https://schema.org",
        "@type": "LearningResource",
        "name": "Convergence & Limits in Metric Spaces",
        "description": "Formalize convergence of sequences in metric spaces, establish the relationship between Cauchy sequences and completeness, and explore boundedness—the theoretical foundation for analyzing iterative algorithms.",
        "learningResourceType": "lesson",
        "educationalUse": "instruction",
        "educationalLevel": "university",
        "interactivityType": "expository",
        "author": {
            "@type": "Person",
            "name": "Yusuke Yokota",
            "jobTitle": "Mathematics & Computer Science Educator"
        },
        "publisher": {
            "@type": "Organization",
            "name": "MATH-CS COMPASS",
            "url": "https://math-cs-compass.com"
        },
        "about": [
            { "@type": "Thing", "name": "Convergence" },
            { "@type": "Thing", "name": "Sequences" },
            { "@type": "Thing", "name": "Cauchy Sequences" },
            { "@type": "Thing", "name": "Limits" },
            { "@type": "Thing", "name": "Boundedness" }
        ],
        "teaches": [
            "Formal definition of sequence convergence in metric spaces",
            "Uniqueness of limits",
            "Cauchy sequences and their properties",
            "Equivalence of completeness and Cauchy convergence",
            "Bounded sets and bounded sequences",
            "Diameter of a set",
            "Relationship between convergence and boundedness"
        ],
        "isPartOf": {
            "@type": "Course",
            "name": "Calculus to Optimization & Analysis",
            "description": "Explore optimization techniques and mathematical analysis with applications",
            "provider": {
            "@type": "Organization",
            "name": "MATH-CS COMPASS",
            "url": "https://math-cs-compass.com"
            },
            "instructor": {
            "@type": "Person",
            "name": "Yusuke Yokota",
            "jobTitle": "Mathematics & Computer Science Educator"
            },
            "courseCode": "II",
            "hasCourseInstance": {
            "@type": "CourseInstance",
            "courseMode": "online",
            "courseWorkload": "PT2H",
            "instructor": {
                "@type": "Person",
                "name": "Yusuke Yokota"
            }
            },
            "offers": {
            "@type": "Offer",
            "price": "0",
            "priceCurrency": "USD",
            "availability": "https://schema.org/InStock",
            "category": "free"
            }
        }
        }
        </script>
        <div class="hero-section">
            <h1 class="webpage-name">Convergence & Limits in Metric Spaces</h1>
        </div>

        {% include section_navigation.html %}

        <div class="topic-nav">
            <a href="#intro">Introduction</a>
            <a href="#convergence">Convergence</a>
            <a href="#cauchy">Cauchy Sequences</a>
            <a href="#bounds">Boundedness</a>
            <a href="#complete-revisit">Completeness Revisited</a>
        </div>  

        <div class="container"> 
            <section id="intro" class="section-content">
                <h2>Introduction</h2>
                <p>
                    In the previous chapter, we introduced completeness as a property ensuring that a metric space 
                    has "no holes." We stated that complete spaces are those where every Cauchy sequence converges. 
                    But what exactly is convergence in a metric space? And what is a Cauchy sequence?
                </p>

                <p>
                    These questions are not merely pure mathematical  — they are fundamental to understanding why optimization 
                    algorithms work. When gradient descent produces a sequence of iterates 
                    \(\mathbf{x}_0, \mathbf{x}_1, \mathbf{x}_2, \ldots\), we need precise language to say what it 
                    means for this sequence to "approach" a solution, and under what conditions we can guarantee 
                    that a limit actually exists.
                </p>

            </section>    
            
            <section id="convergence" class="section-content">
                <h2>Convergence of Sequences</h2>
               
                <p>
                    Here we want to generalize the familiar \(\epsilon\)-\(\delta\) notion from elementary calculus to arbitrary 
                    metric spaces. Why does this matter for modern ML? Because ML operates in spaces 
                    far richer than \(\mathbb{R}^n\): probability distributions (Wasserstein space), functions (reproducing 
                    kernel Hilbert spaces), and even neural network weights (high-dimensional parameter spaces with 
                    non-Euclidean geometry).
                </p>

                <div class="theorem">
                    <span class="theorem-title">Definiton: Tails</span>
                    Suppose \(X\) is a non-empty set and \(x =  \{x_n\}\) is a sequence in \(X\). For each \(m \in \mathbb{N}\), the 
                    set 
                    \[
                    \text{tail}_m (x) = \{x_n \mid n \in \mathbb{N}, n \geq m\}
                    \]
                    is called the \(m\)th <strong>tail</strong> of the sequence  \(\{x_n\}\) .
                </div>

                  <p>
                    The tail captures what happens after we discard finitely many initial terms. 
                    In algorithm analysis, we care about <strong>asymptotic behavior</strong>: whether training eventually stabilizes, 
                    not whether the first few epochs are noisy.
                </p>

                <div class="theorem">
                    <span class="theorem-title">Definiton: Convergence and Limit</span> 
                    Suppose \(X\) is a metric space, \(z \in X\) and  \(\{x_n\}\) is a sequence in \(X\). We say that  \(\{x_n\}\) 
                    <strong>converges</strong> to \(z\) in \(X\), written \(x_n \to z\) if and only if every open subset of \(X\) that contains \(z\) 
                    includes a <strong>tail</strong> of  \(\{x_n\}\) . Furthermore, \(z\) is called the <strong>limit</strong> of  \(\{x_n\}\)  denoted by \(\lim x_n\).
                </div>

                <div class="insight-box"> 
                    <h3>Insight: Convergence in ML Contexts</h3> 
                    <p> 
                        The abstract definition of convergence unifies diverse phenomena in machine learning, shifting the focus from individual 
                        points to the behavior of systems in different spaces: 
                    </p> 
                    <ul> 
                        <li>
                            <strong>Parameter Convergence:</strong><br>
                            In optimization, we expect iterates \(\theta_t \to \theta^*\) under proper conditions 
                            (e.g., Robbins-Monro conditions for SGD). This means that for any \(\epsilon\)-ball around the optimum \(\theta^*\), 
                            the sequence eventually enters and remains within it. <br>
                            Note: Without these conditions, the stochastic noise might prevent the sequence from settling.
                        </li>
                        <li><strong>Distributional Convergence:</strong><br>
                            In Generative Models (like GANs or Diffusion Models), we aim for the generated distribution \(p_G\) to approach 
                            the data distribution \(p_{\text{data}}\). This convergence is measured in probability spaces using 
                            <strong>statistical distances (e.g., KL divergence)</strong> or <strong>metrics(e.g., Wasserstein distance) </strong>
                        </li>
                        <li><strong>Function Space Convergence:</strong><br>
                            The Universal Approximation Theorem states that neural networks are dense in the space of continuous functions. 
                            This implies that as we increase the model capacity, the approximation can converge to any target function \(f\) 
                            under norms like the \(L^\infty\) (uniform convergence) or \(L^2\) norm. 
                        </li> 
                    </ul> 
                </div>

                <p>
                    Convergence in a metric space can be characterized through multiple equivalent lenses:
                </p>

                <div class="theorem">
                    <span class="theorem-title">Theorem: Criteria for Convergence</span> 
                    Suppose \(X\) is a metric space, \(z \in X\) and \(\{x_n\}\) is a sequence in \(X\). The following statements 
                    are logically equivalent:
                    <ol style="padding-left: 40px;">
                        <li>\(\big\cap \left\{\overline{x_n \mid n \in S\} \mid S \subseteq \mathbb{N}, S \text{infinite}\right\} = \{z\}\).</li>
                        <li>\(z \in \big\cap \left\{\overline{x_n \mid n \in S\} \mid S \subseteq \mathbb{N}, S \text{infinite}\right\}\).</li>
                        <li>\(\text{dist }(z, \{x_n \mid n \in S\}) = 0\) for every infinte subset \(S\) of \(\mathbb{N}\).</li>
                        <li>Every open ball centred at \(z\) includes a tail of  \(\{x_n\}\) .</li>
                        <li>Every open subset of \(X\) that contains \(z\) includes a tail of  \(\{x_n\}\) .</li>
                    </ol>
                </div>

                <p>
                    Criterion 4 is the familiar \(\epsilon\)-ball formulation: for every \(\epsilon > 0\), there exists \(N\) 
                    such that \(d(x_n, z) < \epsilon\) for all \(n \geq N\). This is precisely what we check when we set 
                    a convergence tolerance in numerical algorithms.
                </p>

                <p>
                    Criterion 5 reveals the topological perspective. Beyond numerical distances, 
                    \(z\) is the limit because the sequence eventually enters and never leaves any <strong>open</strong> 
                    "neighborhood" of \(z\). This formulation generalizes to spaces where we may not have an explicit 
                    metric but do have a notion of "open sets" (general topological spaces).
                </p>

                <p>
                    Remember, convergent sequences of real numbers converge to "at most" one point. This is because \(\mathbb{R}\) is a 
                    metric space, and uniqueness of limits holds in any metric space. This uniqueness of limits guarantees that optimization 
                    algorithms don't exhibit "mode-switching" behavior where iterates oscillate between multiple candidates indefinitely.
                </p>

                <div class="theorem">
                    <span class="theorem-title">Theorem: The Uniqueness of Limits</span>
                    Suppose \(X\) is a metric space, and \(\{x_n\}\) is a sequence in \(X\) that converges in \(X\). 
                    Then  \(\{x_n\}\) converges to exactly one point in \(X\).
                    \(X\).
                </div>

                <div class="proof">
                    <span class="proof-title">Proof:</span>
                    Suppose \(w, z \in X\) and  \(\{x_n\}\) converges to both \(w\) and \(z\).  Then 
                    \[
                    \big\cap \left\{\overline{x_n \mid n \in S\} \mid S \subseteq \mathbb{N}, S \text{infinite}\right\} = \{w\}.
                    \]
                    and 
                    \[
                    \big\cap \left\{\overline{x_n \mid n \in S\} \mid S \subseteq \mathbb{N}, S \text{infinite}\right\} = \{z\}.
                    \]
                    Thus, \(\{w\} = \{z\}\), and \(w = z\).

                </div>

                 <p>
                    Note that any subsequence of a convergent sequence converges to the same limit as the parent sequence. 
                    Convergence is robust against any infinite sub-sampling and always identifies a single, well-defined limit.
                </p>

            </section>  

            <section id="cauchy" class="section-content">
                <h2>Cauchy Sequences</h2>

                <p>
                    How do we know a sequence "should" converge without knowing the limit? In practice, this is exactly 
                    the situation we face: gradient descent produces iterates, and we need a stopping criterion that 
                    doesn't require oracle knowledge of the optimal solution.
                </p>

                <p>
                    The key insight is to measure whether the sequence elements are <strong>getting closer to each other</strong>, 
                    rather than closer to some unknown target.
                </p>

                <div class="theorem">
                    <span class="theorem-title">Definiton: Cauchy Sequence</span>
                    Suppose \(X\) is a metric space and \(\{x_n\}\) is a sequence in \(X\). We say that \(\{x_n\}\) is a 
                    <strong>Cauchy sequence</strong> in \(X\) if and only if for every \(r \in \mathbb{R}^+\), there is 
                    a ball of \(X\) of radius\(r\) that includes a tail of \(\{x_n\}\) .
                 </div>

                 
                <div class="theorem">
                    <span class="theorem-title">Theorem:</span>
                    <p>
                        Let \((X, d)\) be a metric space. If a sequence \(\{x_n\}\) converges in \(X\), then 
                        \(\{x_n\}\) is a Cauchy sequence.
                    </p>
                    <p>
                        Note that the converse of this statement is not always true.
                    </p>
                   
                </div>

                <div class="proof">
                    <span class="proof-title">Proof Sketch:</span>
                    If \(x_n \to z\), then by Criterion 4 of convergence, every open ball centered at \(z\) includes a tail 
                    of \(\{x_n\}\). A ball of radius \(r\) centered at \(z\) that includes tail\(_N(x)\) is also a ball of 
                    radius \(r\) (though not necessarily centered at any sequence element) that includes that tail. 
                    Hence the Cauchy condition is satisfied.
                </div>


                <div class="insight-box"> 
                    <h3>Insight: The Practical Dilemma</h3> 
                    <p> 
                        In optimization, the target point \(x^*\) is unknown — detecting convergence by checking \(d(x_n, x^*) < \epsilon\) is 
                        therefore a <strong>circular problem</strong>. The Cauchy criterion provides a rigorous theoretical escape: 
                        it ensures convergence by examining whether all future terms in a sequence remain arbitrarily close to one another, 
                        independent of the limit itself. 
                    </p> 
                    <p> However, a gap exists between theory and implementation. Verifying the formal Cauchy condition requires monitoring an 
                        infinite tail of the sequence, which is computationally impossible. In practice, we use <strong>stagnation-based stopping criteria</strong> 
                        (such as \(|\theta_{t+1} - \theta_t| < \epsilon\) over a fixed window). While these are inspired by Cauchy's logic, they are 
                        <strong>heuristic approximations</strong>: they detect when an algorithm has slowed down, but do not mathematically guarantee 
                        that it has reached a global optimum. 
                    </p> 
                </div>

            </section>

            <section id="bounds" class="section-content">
                <h2>Boundedness</h2>

                <p>
                    In pure mathematics, <strong>boundedness</strong> is a property that a sequence <strong>possesses</strong> as a necessary 
                    consequence of convergence. In machine learning, we invert this logic: we <strong>enforce</strong> boundedness as a 
                    mechanism to maintain numerical stability and prevent divergence.
                </p>

                <div class="theorem">
                    <span class="theorem-title">Definiton: Bounded</span>
                    A subset \(S\) of a metric space \(X\) is called a <strong>bounded subset</strong> of \(X\) if and only if 
                    \(S = X = \emptyset \) or \(S\) is included in some ball of \(X\). A metric space \(X\) is said to be 
                    <strong>bounded</strong> if and only if it is a bounded subset of itself. 
                    <br><br>
                    Note: The empty set is vacuously bounded.
                </div>

                <div class="theorem">
                    <span class="theorem-title">Theorem: Criteria for Boundedness</span>
                    Suppose \((X, d)\) is a nonempty metric space, \(z \in X\), and \(S \subseteq X\). The following statements 
                    are logically equivalent: 
                    <ol style="padding-left: 40px;">
                        <li>\(\text{diam }(S) < \infty\).</li>
                        <li>There is a ball of \(X\) centered at \(z\) that includes \(S\).</li>
                        <li>There is a ball of \(X\) that includes \(S\).</li>
                    </ol><br>
                    Note that the <strong>diameter</strong> of \(S\) is defined as 
                    \[
                    \text{diam }(S) = \sup \{d(r, s) \mid r, s \in S\}.
                    \]
                </div>

                <div class="theorem">
                    <span class="theorem-title">Theorem:</span>
                    Suppose \(X\) is a metric space and \(\{x_n\}\) is a equence in \(X\). If \(\{x_n\}\) is Cauchy, then 
                    \(\{x_n\}\) is bounded in \(X\). In other words, if \(\{x_n\}\) converges in \(X\), then \(\{x_n\}\) is 
                    bounded in \(X\). 
                </div>

                <div class="proof">
                    <span class="proof-title">Proof Sketch:</span>
                    If \(\{x_n\}\) is Cauchy, then for \(\epsilon = 1\), there exists \(N\) such that all terms 
                    \(x_n\) with \(n \geq N\) lie within distance 1 of \(x_N\). The finitely many terms 
                    \(x_1, \ldots, x_{N-1}\) are at finite distances from \(x_N\). Taking \(R = \max\{d(x_i, x_N) : i < N\} + 1\), 
                    all terms lie in the ball of radius \(R\) centered at \(x_N\).
                </div>

                <p>
                    <strong>Important:</strong> The converse is false. <strong>Boundedness alone does not guarantee convergence.</strong> 
                    Consider \(x_n = (-1)^n\) in \(\mathbb{R}\). The sequence is bounded (contained in \([-1, 1]\)) but 
                    oscillates forever without converging.
                </p>

                <div class="insight-box">
                        <p>
                            While enforcing a bound does not mathematically guarantee convergence (as the sequence could still oscillate), 
                            it ensures the iterates remain within a <strong>compact region</strong>. This is a prerequisite for the 
                            Cauchy property to even be possible in practice.
                        </p>
                        
                        <ul style="padding-left: 40px;">
                            <li>
                                <strong>Gradient Clipping:</strong><br> 
                                By enforcing \(\|\nabla L\| \leq M\), we ensure the update steps remain in a bounded subset of the parameter 
                                space. This is a heuristic countermeasure against the <strong>exploding gradient problem</strong>, preventing 
                                iterates from escaping to infinity.
                            </li>
                            <li>
                                <strong>Weight Clipping (e.g., WGAN):</strong><br> 
                                Forcing weights into a compact interval \([-c, c]\) ensures <strong>Lipschitz continuity</strong>. 
                                This mathematical constraint is required to bound the slope of the function, stabilizing the dual form 
                                of the Wasserstein distance.
                            </li>
                            <li>
                                <strong>Regularization (\(L^2\)):</strong><br> 
                                Adding a penalty \(\|\theta\|^2\) makes the objective function <strong>coercive</strong>. It ensures 
                                that the set of parameters with low loss remains bounded, effectively "trapping" the optimization process 
                                in a region where a minimum is guaranteed to exist.
                            </li>
                            <li>
                                <strong>Trust Regions:</strong><br> 
                                By restricting each step to a ball of radius \(\Delta\), we ensure that the <strong>Taylor approximation</strong> 
                                (which is only locally valid) remains an accurate model of the loss surface.
                            </li>
                        </ul>
                    </div>

            </section>

            <section id="complete-revisit" class="section-content">
                <h2>Completeness Revisited</h2>
                <p>
                    In the previous chapter, we defined completeness as follows:
                </p>
                <div class="theorem">
                    <span class="theorem-title">Definition: Completeness</span>
                    A metric space \(X\) is said to be <strong>complete</strong> if and only if 
                    \(X\) is closed in every metric superspace of \(X\). 
                </div>

                <p>
                    This formulation prioritizes the structural integrity of the metric space itself—the space has 
                    "no holes" that could be exposed by embedding it in a larger space. Now that we have developed 
                    the Cauchy criterion, we can state an equivalent characterization that is often more useful in practice.
                </p>

                <div class="theorem">
                    <span class="theorem-title">Definition: Completeness</span>
                    A metric space \(X\) is said to be <strong>complete</strong> if and only if 
                    every Cauchy sequence in \(X\) converges in \(X\).
                </div>

                <p>
                    The equivalence of these definitions is a fundamental theorem we will prove in later chapters. 
                    For now, the sequential characterization gives us an operational test: to verify completeness, 
                    we must show that any sequence whose terms get arbitrarily close together eventually reaches 
                    a limit within the space.
                </p>

                <div class="proof">
                    <span class="proof-title">Example:</span>
                    \(\mathbb{R}^n\) (with the Euclidean metric) is complete. This is the Cauchy completeness axiom 
                    of the real numbers, extended to finite-dimensional spaces. However, \(\mathbb{Q}^n\) (rationals) 
                    is not complete — Cauchy sequences can converge to irrational limits.
                </div>

                <div class="insight-box">
                    <p>
                        Completeness is the "floor" of our mathematical world; it ensures that if an algorithm suggests a limit exists (via the Cauchy property), that limit is a valid point within our space. In machine learning, we rely on the following complete spaces:
                    </p>

                    <ul style="padding-left: 40px;">
                        <li>
                            <strong>\(\mathbb{R}^d\) (Parameter Space):</strong><br> 
                            As a finite-dimensional Euclidean space, \(\mathbb{R}^d\) is complete. While this does not guarantee 
                            that SGD <em>will</em> converge (which depends on learning rates and loss geometry), it ensures that any 
                            Cauchy sequence of weights \(\{\theta_t\}\) has a unique, reachable limit \(\theta^* \in \mathbb{R}^d\).
                        </li>
                        <li>
                            <strong>Hilbert Spaces (e.g., RKHS):</strong><br>
                            By definition, a Hilbert space is a complete inner product space. The convergence of kernel-based algorithms 
                            depend on the fact that the optimization occurs in a space where "holes" do not exist.
                        </li>
                        <li>
                            <strong>\(L^p\) Spaces (Function Spaces):</strong><br>
                            The Riesz-Fischer Theorem proves that \(L^p\) spaces are complete. This is fundamental for 
                            <strong>universal approximation</strong>; it allows us to define the "target function" as the 
                            limit of a sequence of neural networks under an integral norm.
                            
                        </li>
                        <li>
                            <strong>Probability Distributions (Wasserstein Space):</strong> 
                            When we train generative models like GANs or Diffusion Models, we are essentially moving one probability 
                            distribution toward another. The completeness of this space ensures that if our "sequence of distributions" 
                            behaves like a Cauchy sequence (getting closer and closer), it will actually converge to a valid target 
                            distribution, rather than disappearing into a void.
                        </li>
                    </ul>
                </div>

            </section>

        </div>
        <script src="/js/main.js"></script>
    </body>
</html>