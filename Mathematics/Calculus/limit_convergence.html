---
layout: default
title: Convergence & Boundedness
topic_id: calc-17
level: detail
uses_math: true
uses_python: false
---
<!DOCTYPE html>
<html>
    <body>
        {% include learning_resource_schema.html topic_id=page.topic_id %}
        
        <div class="hero-section">
            <h1 class="webpage-name">Convergence & Boundedness</h1>
        </div>

        {% include section_navigation.html %}

        <div class="topic-nav">
            <a href="#intro">Introduction</a>
            <a href="#convergence">Convergence</a>
            <a href="#cauchy">Cauchy Sequences</a>
            <a href="#bounds">Boundedness</a>
            <a href="#complete-revisit">Completeness Revisited</a>
        </div>  

        <div class="container"> 
            <section id="intro" class="section-content">
                <h2>Introduction</h2>
                <p>
                    In the previous chapter, we introduced completeness as a property ensuring that a metric space 
                    has "no holes." We stated that complete spaces are those where every Cauchy sequence converges. 
                    But what exactly is convergence in a metric space? And what is a Cauchy sequence?
                </p>

                <p>
                    Traditional calculus often presents these concepts through the lens of pointwise \(\epsilon-N\) inequalities 
                    - a "micro-arithmetic" approach that can sometimes obscure the underlying geometric truth. In this chapter, 
                    we shift our perspective from individual point-to-point distances to structural topology. We define convergence 
                    and Cauchy properties by looking at the behavior of the tail of a sequence: whether the infinite remainder of 
                    our process eventually "settles" into an arbitrarily small neighborhood.
                </p>

                <p>
                    This shift is fundamental to understanding why optimization algorithms work. When gradient descent produces a 
                    sequence of iterate \(\mathbf{x}_0, \mathbf{x}_1, \mathbf{x}_2, \ldots\), we need precise language to describe 
                    how this sequence "concentrates" its energy toward a solution. By treating convergence as a structural event 
                    rather than a mere numerical coincidence, we gain a clearer blueprint for analyzing stability and consistency in 
                    high-dimensional AI-driven landscapes.
                </p>

            </section>    
            
            <section id="convergence" class="section-content">
                <h2>Convergence of Sequences</h2>
               
                <p>
                    Here we want to generalize the familiar \(\epsilon\)-\(\delta\) notion from elementary calculus to arbitrary 
                    metric spaces. Why does this matter for modern ML? Because ML operates in spaces 
                    far richer than \(\mathbb{R}^n\): probability distributions (Wasserstein space), functions (reproducing 
                    kernel Hilbert spaces), and even <a href="../Machine_learning/neural_networks.html"><strong>neural network</strong></a> weights (high-dimensional parameter spaces with 
                    non-Euclidean geometry).
                </p>

                <div class="theorem">
                    <span class="theorem-title">Definition: Tails</span>
                    Suppose \(X\) is a non-empty set and \(\{x_n\}\) is a sequence in \(X\). For each \(m \in \mathbb{N}\), the 
                    set 
                    \[
                    \text{tail}_m \{x_n\} = \{x_n \mid n \in \mathbb{N}, n \geq m\}
                    \]
                    is called the \(m\)th <strong>tail</strong> of the sequence  \(\{x_n\}\) .
                </div>

                  <p> 
                    The tail captures what happens after we discard finitely many initial terms. 
                    In algorithm analysis, we care about <strong>asymptotic behavior</strong>: whether training eventually stabilizes, 
                    not whether the first few epochs are noisy.
                </p>

                <div class="theorem">
                    <span class="theorem-title">Definition: Convergence and Limit</span> 
                    Suppose \(X\) is a metric space, \(z \in X\) and \(\{x_n\}\) is a sequence in \(X\). We say that \(\{x_n\}\) 
                    <strong>converges</strong> to \(z\) in \(X\), written \(x_n \to z\) if and only if every open subset of \(X\) that contains \(z\) 
                    includes a <strong>tail</strong> of \(\{x_n\}\). Furthermore, \(z\) is called the <strong>limit</strong> of \(\{x_n\}\) denoted by \(\lim x_n\).
                </div>

                <div class="insight-box"> 
                    <h3>Insight: Convergence in ML Contexts</h3> 
                    <p> 
                        The abstract definition of convergence unifies diverse phenomena in machine learning, shifting the focus from individual 
                        points to the behavior of systems in different spaces: 
                    </p> 
                    <ul style="padding-left: 40px;"> 
                        <li>
                            <strong>Parameter Convergence:</strong><br>
                            In optimization, we expect iterates \(\theta_t \to \theta^*\) under proper conditions 
                            (e.g., Robbins-Monro conditions for SGD). This means that for any \(\epsilon\)-ball around the optimum \(\theta^*\), 
                            the sequence eventually enters and remains within it. <br>
                            Note: Without these conditions, the stochastic noise might prevent the sequence from settling.
                        </li>
                        <li><strong>Distributional Convergence:</strong><br>
                            In Generative Models (like GANs or Diffusion Models), we aim for the generated distribution \(p_G\) to approach 
                            the data distribution \(p_{\text{data}}\). This convergence is measured in probability spaces using 
                            "statistical" distances (e.g., <a href="../Probability/entropy.html#kl"><strong>KL divergence</strong></a>) 
                            or metrics (e.g., <strong>Wasserstein distance</strong>). (Note that KL divergence is not metric.)
                        </li>
                        <li><strong>Function Space Convergence:</strong><br>
                            The Universal Approximation Theorem states that neural networks are dense in the space of continuous functions. 
                            This implies that as we increase the model capacity, the approximation can converge to any target function \(f\) 
                            under norms like the \(L_{\infty}\) (uniform convergence) or \(L_2\) norm. 
                        </li> 
                    </ul> 
                </div>

                <p>
                    Convergence in a metric space can be characterized through multiple equivalent lenses:
                </p>

                <div class="theorem">
                    <span class="theorem-title">Theorem: Criteria for Convergence</span> 
                    Suppose \(X\) is a metric space, \(z \in X\) and \(\{x_n\}\) is a sequence in \(X\). The following statements 
                    are logically equivalent:
                    <ol style="padding-left: 40px;">
                        <li>
                            \[
                            \bigcap \left\{\overline{x_n \mid n \in S} \mid S \subseteq \mathbb{N}, S \text{infinite}\right\} = \{z\}.
                            \]
                        </li>
                        <li>
                            \[
                            z \in \bigcap \left\{\overline{x_n \mid n \in S} \mid S \subseteq \mathbb{N}, S \text{infinite}\right\}.
                            \]
                        </li>
                        <li>
                            \[
                            \text{dist }(z, \{x_n \mid n \in S\}) = 0
                            \]
                            for every infinite subset \(S\) of \(\mathbb{N}\).</li>
                        <li>Every open ball centred at \(z\) includes a tail of \(\{x_n\}\).</li>
                        <li>Every open subset of \(X\) that contains \(z\) includes a tail of \(\{x_n\}\).</li>
                    </ol>
                </div>

                <p>
                    Criterion 4 is the familiar \(\epsilon\)-ball formulation: for every \(\epsilon > 0\), there exists \(N\) 
                    such that \(d(x_n, z) < \epsilon\) for all \(n \geq N\). This is precisely what we check when we set 
                    a convergence tolerance in numerical algorithms.
                </p>

                <p>
                    Criterion 5 reveals the topological perspective. Beyond numerical distances, 
                    \(z\) is the limit because the sequence eventually enters and never leaves any <strong>open</strong> 
                    "neighborhood" of \(z\). This formulation generalizes to spaces where we may not have an explicit 
                    metric but do have a notion of "open sets" (general topological spaces).
                </p>

                <p>
                    Remember, convergent sequences of real numbers converge to "at most" one point. This is because \(\mathbb{R}\) is a 
                    metric space, and uniqueness of limits holds in any metric space. This uniqueness of limits guarantees that optimization 
                    algorithms don't exhibit "mode-switching" behavior where iterates oscillate between multiple candidates indefinitely.
                </p>

                <div class="theorem">
                    <span class="theorem-title">Theorem: The Uniqueness of Limits</span>
                    Suppose \(X\) is a metric space, and \(\{x_n\}\) is a sequence in \(X\) that converges in \(X\). 
                    Then \(\{x_n\}\) converges to exactly one point in \(X\).
                </div>

                <div class="proof">
                    <span class="proof-title">Proof:</span>
                    Suppose \(w, z \in X\) and \(\{x_n\}\) converges to both \(w\) and \(z\).  Then 
                    \[
                    \bigcap \left \{ \overline{x_n \mid n \in S} \mid S \subseteq \mathbb{N}, S \text{infinite}\right\} = \{w\}.
                    \]
                    and 
                    \[
                    \bigcap \left\{\overline{x_n \mid n \in S} \mid S \subseteq \mathbb{N}, S \text{infinite}\right\} = \{z\}.
                    \]
                    Thus, \(\{w\} = \{z\}\), and \(w = z\).

                </div>

                <p>
                    In the real world, it is usually impossible to observe "everything."  So, we are more interested in 
                    "portions" of the whole sequence. Fortunately, every subsequence of a convergent sequence converges 
                    to the same limit as the parent sequence, which is because every tail of the parent sequence includes 
                    a tail of any its subsequence. 
                </p>

                <div class="theorem">
                    <span class="theorem-title">Theorem: Convergence of Subsequences</span>
                    <p>
                        Suppose \(\{x_n\}\) is a sequence in a metric space \(X\). If \(\{x_n\}\) converges to \(z \in X\), 
                        then every subsequence \(\{x_{m_n}\}\) of \(\{x_n\}\) also converges to \(z\).
                    </p>
                    <p>
                        <strong>Side Note:</strong> In this case, \(z\) is in the closure of every tail of \(\{x_n\}\). Or, equivalently, 
                        every ball centered at \(z\) contains an infinite number of terms of \(\{x_n\}\). 
                    </p>
                </div>

                <p>     
                    This theorem states that convergence is <strong>invariant</strong> under any infinite sub-sampling. 
                    Whether we monitor every single iterate or only record the state every 100 epochs, the identified limit 
                    remains the same.
                </p>


                 <div class="insight-box">
                    <h3>Insight: Consistency Across Observations</h3>
                    <p>
                        In practical machine learning, we rarely inspect every single update of a high-frequency optimizer. 
                        Instead, we "sample" the model's performance at specific intervals.
                    </p>
                    <ul>
                        <li>
                            <strong>Robustness to Checkpointing:</strong><br>
                            This property guarantees that the "limit" we observe through periodic checkpoints is mathematically 
                            identical to the true limit of the continuous training process.
                        </li>
                        <li>
                            <strong>Avoiding Inconsistency:</strong><br>
                            If a sequence were to have subsequences converging to different limits, our evaluation results would 
                            depend entirely on <em>when</em> we measured the performance - a nightmare for reproducibility. 
                            Convergence prevents this "mode-switching" instability.
                        </li>
                    </ul>
                </div>

                <p>
                    Convergence in metric subspace is not exactly the same notion as convergence of subsequences in a metric space. 
                </p>

                <div class="theorem">
                    <span class="theorem-title">Theorem: Convergence in Subspaces</span>
                    <p>
                        Let \(X\) be a metric space, and \(Y\) be a metric subspace of \(X\).
                        Suppose a sequence \(\{x_n\} \subseteq Y\) that converges to \(w \in X\). Then 
                        \(\{x_n\}\) converges in \(Y\) if and only if \(w \in Y\), and in that case, its 
                        limit in \(Y\) is \(w\).
                    </p>
                </div>

                <div class="insight-box">
                    <h3>Insight: Why Subspaces Matter in ML</h3>
                    <p>
                        We often constrain parameters to a subset \(Y\) (e.g., via weight clipping). 
                        The critical question is whether the limit \(L\) stays within our allowed region \(Y\).
                    </p>
                    <ul>
                        <li>
                            <strong>Closed Subsets:</strong> If our constraint set \(Y\) is <strong>closed</strong> in \(\mathbb{R}^n\), 
                            any sequence that converges in the larger space \(\mathbb{R}^n\) is guaranteed 
                            to have its limit in \(Y\).
                        </li>
                        <li>
                            <strong>Numerical Instability:</strong> The set of floating-point numbers \(\mathbb{F} \subseteq \mathbb{R}\) 
                            is a discrete subspace. A sequence might "converge" in \(\mathbb{R}\), but its limit 
                            may not exist in \(\mathbb{F}\), leading to rounding errors or numerical instability.
                        </li>
                    </ul>
                </div>

            </section>  

            <section id="cauchy" class="section-content">
                <h2>Cauchy Sequences</h2>

                <p>
                    How do we know a sequence "should" converge without knowing the limit? In practice, this is exactly 
                    the situation we face: gradient descent produces iterates, and we need a stopping criterion that 
                    doesn't require oracle knowledge of the optimal solution.
                </p>

                <p>
                    The key insight is to measure whether the sequence elements are <strong>getting closer to each other</strong>, 
                    rather than closer to some unknown target.
                </p>

                <div class="theorem">
                    <span class="theorem-title">Definition: Cauchy Sequence</span>
                    Suppose \(X\) is a metric space and \(\{x_n\}\) is a sequence in \(X\). We say that \(\{x_n\}\) is a 
                    <strong>Cauchy sequence</strong> in \(X\) if and only if for every \(r \in \mathbb{R}^+\), there is 
                    a ball of \(X\) of radius \(r\) that includes a tail of \(\{x_n\}\).
                 </div>

                 
                <div class="theorem">
                    <span class="theorem-title">Theorem:</span>
                    <p>
                        Let \((X, d)\) be a metric space. If a sequence \(\{x_n\}\) converges in \(X\), then 
                        \(\{x_n\}\) is a Cauchy sequence.
                    </p>
                    <p>
                        Note that the converse of this statement is not always true.
                    </p>       
                </div>

                <div class="proof">
                    <span class="proof-title">Proof Sketch:</span>
                    Remember, from Criteria for Convergence, every open ball centred at \(z\) includes a tail of \(\{x_n\}\). This implies 
                    definition of the Cauchy sequence.
                 </div>


                <div class="insight-box"> 
                    <h3>Insight: The Practical Dilemma</h3> 
                    <p> 
                        In optimization, the target point \(x^*\) is unknown - detecting convergence by checking \(d(x_n, x^*) < \epsilon\) is 
                        therefore a <strong>circular problem</strong>. The Cauchy criterion provides a rigorous theoretical escape: 
                        it ensures convergence by examining whether all future terms in a sequence remain arbitrarily close to one another, 
                        independent of the limit itself. 
                    </p> 
                    <p> However, a gap exists between theory and implementation. Verifying the formal Cauchy condition requires monitoring an 
                        infinite tail of the sequence, which is computationally impossible. In practice, we use <strong>stagnation-based stopping criteria</strong> 
                        (such as \(|\theta_{t+1} - \theta_t| < \epsilon\) over a fixed window). While these are inspired by Cauchy's logic, they are 
                        <strong>heuristic approximations</strong>: they detect when an algorithm has slowed down, but do not mathematically guarantee 
                        that it has reached a global optimum. 
                    </p> 
                </div>

            </section>

            <section id="bounds" class="section-content">
                <h2>Boundedness</h2>

                <p>
                    In pure mathematics, <strong>boundedness</strong> is a property that a sequence <strong>possesses</strong> as a necessary 
                    consequence of convergence. In machine learning, we invert this logic: we <strong>enforce</strong> boundedness as a 
                    mechanism to maintain numerical stability and prevent divergence.
                </p>

                <div class="theorem">
                    <span class="theorem-title">Definition: Bounded</span>
                    A subset \(S\) of a metric space \(X\) is called a <strong>bounded subset</strong> of \(X\) if and only if 
                    \(S = \emptyset \) or \(S\) is included in some ball of \(X\). A metric space \(X\) is said to be 
                    <strong>bounded</strong> if and only if it is a bounded subset of itself. 
                </div>

                <div class="theorem">
                    <span class="theorem-title">Definition: Diameter</span>
                    Suppose \((X, d)\) is a metric space and \(S\) is a subset of \(X\). 
                    The <strong>diameter</strong> of \(S\) is defined as 
                    \[
                    \text{diam }(S) = \sup \{d(r, s) \mid r, s \in S\}.
                    \]
                </div>

                <div class="theorem">
                    <span class="theorem-title">Theorem: Criteria for Boundedness</span>
                    Suppose \(X\) is a nonempty metric space, \(z \in X\), and \(S \subseteq X\). The following statements 
                    are logically equivalent: 
                    <ol style="padding-left: 40px;">
                        <li>\(\text{diam }(S) < \infty\).</li>
                        <li>There is a ball of \(X\) centered at \(z\) that includes \(S\).</li>
                        <li>There is a ball of \(X\) that includes \(S\).</li>
                    </ol>
                </div>

                <div class="theorem">
                    <span class="theorem-title">Theorem:</span>
                    Suppose \(X\) is a metric space and \(\{x_n\}\) is a sequence in \(X\). If \(\{x_n\}\) is Cauchy, then 
                    \(\{x_n\}\) is bounded in \(X\). In other words, if \(\{x_n\}\) converges in \(X\), then \(\{x_n\}\) is 
                    bounded in \(X\). 
                </div>

                <div class="proof">
                    <span class="proof-title">Proof Sketch:</span>
                    If \(\{x_n\}\) is Cauchy, then for \(\epsilon = 1\), there exists \(N\) such that all terms 
                    \(x_n\) with \(n \geq N\) lie within distance 1 of \(x_N\). The finitely many terms 
                    \(x_1, \ldots, x_{N-1}\) are at finite distances from \(x_N\). Taking \(R = \max\{d(x_i, x_N) : i < N\} + 1\), 
                    all terms lie in the ball of radius \(R\) centered at \(x_N\).
                </div>

                <p>
                    <strong>Important:</strong> The converse is false. <strong>Boundedness alone does not guarantee convergence.</strong> 
                    Consider \(x_n = (-1)^n\) in \(\mathbb{R}\). The sequence is bounded (contained in \([-1, 1]\)) but 
                    oscillates forever without converging.
                </p>

                <div class="insight-box">
                        <h3>Insight:</h3>
                        <p>
                            While enforcing a bound does not mathematically guarantee convergence (as the sequence could still oscillate), 
                            it ensures the iterates remain within a <strong>bounded region</strong>. In the finite-dimensional parameter 
                            spaces typical of machine learning, such a region - when combined with appropriate closure - forms a 
                            <strong>compact set</strong>. This "structural containment" is a prerequisite for the Cauchy property 
                            to even be possible in practice.
                        </p>
                        
                        <ul style="padding-left: 40px;">
                            <li>
                                <strong>Gradient Clipping:</strong><br> 
                                By enforcing \(\|\nabla L\| \leq M\), we ensure the update steps remain in a bounded subset of the parameter 
                                space. This is a heuristic countermeasure against the <strong>exploding gradient problem</strong>, preventing 
                                iterates from escaping to infinity.
                            </li>
                            <li>
                                <strong>Weight Clipping (e.g., WGAN):</strong><br> 
                                Forcing weights into a compact interval \([-c, c]\) ensures <strong>Lipschitz continuity</strong>. 
                                This mathematical constraint is required to bound the slope of the function, stabilizing the dual form 
                                of the Wasserstein distance.
                            </li>
                            <li>
                                <strong>Regularization (\(L^2\)):</strong><br> 
                                Adding a penalty \(\|\theta\|^2\) makes the objective function <strong>coercive</strong>. It ensures 
                                that the set of parameters with low loss remains bounded, effectively "trapping" the optimization process 
                                in a region where a minimum is guaranteed to exist.
                            </li>
                            <li>
                                <strong>Trust Regions:</strong><br> 
                                By restricting each step to a ball of radius \(\Delta\), we ensure that the <strong>Taylor approximation</strong> 
                                (which is only locally valid) remains an accurate model of the loss surface.
                            </li>
                        </ul>
                    </div>

                    <p>
                        While boundedness ensures that our sequence doesn't escape to infinity, it doesn't 
                        guarantee that the set is structurally "solid" enough to contain its own limit points. 
                        To bridge this gap, we consider a property that connects distance to existence 
                        in the most fundamental way.
                    </p>

                    <div class="theorem">
                        <span class="theorem-title">Definition: Nearest-Point Property</span>
                        <p>
                            Suppose \(X\) is a metric space. We say that \(X\) has the <strong>nearest-point property</strong> 
                            if and only if \(X = \emptyset\) or \(X\) admits a nearest point to each point in every metric superspace of 
                            \(X\).
                        </p>
                    </div>

                    <p>
                        This property is logically equivalent to the assertion that 
                        <strong>every bounded sequence in \(X\) has a subsequence that converges in \(X\)</strong>. 
                        It ensures that the space has no "holes" for bounded sequences to fall into.
                    </p>

                    <div class="insight-box">
                        <h3>Insight: Structural Guarantee in Optimization</h3>
                        <p>
                            In optimization, simply knowing your search space is limited (bounded) is only half the battle. 
                            The <strong>nearest-point property</strong> ensures that even if you can't reach an "ideal" point 
                            outside your set, you can always find a best approximation (a projection) within it.
                        </p>
                        <p>
                            For instance, in <strong>Constrained SGD</strong> or <strong>SVM</strong>, we rely on this structural 
                            "solidness" to ensure that our algorithms always have a valid point to converge to. We will 
                            explore how this combines with boundedness to form the broader concept of <em>compactness</em> 
                            in a later chapter.
                        </p>
                    </div>

                    <p>
                        As we move toward more complex spaces, especially when dealing with infinite-dimensional spaces,
                        we need a refined version of boundedness that accounts for the "density" of the set, leading us to 
                        the concept of <strong>total boundedness</strong>.
                    </p>

                    <div class="theorem">
                        <span class="theorem-title">Definition: Total Boundedness</span>
                        A subset \(S\) of a metric space \(X\) is called a <strong>totally bounded subset</strong> of \(X\) 
                        if and only if for each \(r \in \mathbb{R}^+\), there is a finite collection of balls of \(X\) of 
                        radius \(r\) that covers \(S\). The metric space \(X\) is said to be <strong>totally bounded</strong> 
                        if and only if it is a totally bounded subset of itself.
                    </div>

                    <div class="insight-box">
                        <h3>Insight: Total Boundedness in Function Spaces</h3>
                        <p>
                            In the space of neural network functions, total boundedness directly relates to 
                            <strong>covering numbers</strong> - the minimum number of balls of radius \(\epsilon\) 
                            needed to cover a function class.
                        </p>
                        <ul>
                            <li>
                                <strong>Capacity Control:</strong><br>
                                In statistical learning theory, the logarithm of the covering number (metric entropy) 
                                measures the complexity of a model. 
                            </li>
                            <li>
                                <strong>Generalization:</strong><br>
                                A totally bounded function class ensures that we can approximate any function in the 
                                class with a finite set of "representative" networks. Smaller covering numbers imply a 
                                less complex hypothesis space, which typically leads to tighter <strong>generalization bounds</strong>.
                            </li>
                        </ul>
                    </div>
          </section>

            <section id="complete-revisit" class="section-content">
                <h2>Completeness Revisited</h2>
                <p>
                    In the previous chapter, we defined completeness as follows:
                </p>
                <div class="theorem">
                    <span class="theorem-title">Definition: Completeness</span>
                    A metric space \(X\) is said to be <strong>complete</strong> if and only if 
                    \(X\) is closed in every metric superspace of \(X\). 
                </div>

                <p>
                    This formulation prioritizes the structural integrity of the metric space itselfâ€”the space has 
                    "no holes" that could be exposed by embedding it in a larger space. Now that we have developed 
                    the Cauchy criterion, we can state an equivalent characterization that is often more useful in practice.
                </p>

                <div class="theorem">
                    <span class="theorem-title">Definition: Completeness</span>
                    A metric space \(X\) is said to be <strong>complete</strong> if and only if 
                    every Cauchy sequence in \(X\) converges in \(X\).
                </div>

                <p>
                    The equivalence of these definitions is a fundamental theorem we will prove in later chapters. 
                    For now, the sequential characterization gives us an operational test: to verify completeness, 
                    we must show that any sequence whose terms get arbitrarily close together eventually reaches 
                    a limit within the space.
                </p>

                <div class="proof">
                    <span class="proof-title">Example:</span>
                    \(\mathbb{R}^n\) (with the Euclidean metric) is complete. This is the Cauchy completeness axiom 
                    of the real numbers, extended to finite-dimensional spaces. However, \(\mathbb{Q}^n\) (rationals) 
                    is not complete - Cauchy sequences can converge to irrational limits.
                </div>

                <div class="insight-box">
                    <h3>Insight: </h3>
                    <p>
                        Completeness is the "floor" of our mathematical world; it ensures that if an algorithm suggests a limit exists 
                        (via the Cauchy property), that limit is a valid point within our space. In machine learning, we rely on the following 
                        complete spaces:
                    </p>

                    <ul style="padding-left: 40px;">
                        <li>
                            <strong>\(\mathbb{R}^d\) (Parameter Space):</strong><br> 
                            As a finite-dimensional Euclidean space, \(\mathbb{R}^d\) is complete. While this does not guarantee 
                            that SGD <em>will</em> converge (which depends on learning rates and loss geometry), it ensures that any 
                            Cauchy sequence of weights \(\{\theta_t\}\) has a unique, reachable limit \(\theta^* \in \mathbb{R}^d\).
                        </li>
                        <li>
                            <strong>Hilbert Spaces (e.g., RKHS):</strong><br>
                            By definition, a Hilbert space is a complete inner product space. The convergence of kernel-based algorithms 
                            depend on the fact that the optimization occurs in a space where "holes" do not exist.
                        </li>
                        <li>
                            <strong>\(L^p\) Spaces (Function Spaces):</strong><br>
                            The Riesz-Fischer Theorem proves that \(L^p\) spaces are complete. This is fundamental for 
                            <strong>universal approximation</strong>; it allows us to define the "target function" as the 
                            limit of a sequence of neural networks under an integral norm.
                            
                        </li>
                        <li>
                            <strong>Probability Distributions (Wasserstein Space):</strong><br>
                            When training generative models (e.g., GANs or Diffusion Models), we optimize within a space of probability distributions. 
                            Crucially, <strong>the completeness of the Wasserstein space is inherited from the underlying base space</strong> (typically the complete Euclidean space \(\mathbb{R}^n\)). 
                            This structural guarantee ensures that a Cauchy sequence of distributions will always converge to a valid, well-defined distribution, rather than "escaping" the space of measures.
                        </li>
                    </ul>
                </div>

            </section>

        </div>
        <script src="/js/main.js"></script>
    </body>
</html>