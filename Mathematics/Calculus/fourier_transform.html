---
layout: default
title: Fourier Transform
level: detail
description: Learn about the Fourier transform, its properties, and applications to signal processing and machine learning.
uses_math: true
uses_python: true
---
<!DOCTYPE html>
<html>
    <body>
        <!-- LearningResource Schema for Fourier Transform -->
        <script type="application/ld+json">
        {
        "@context": "https://schema.org",
        "@type": "LearningResource",
        "name": "Fourier Transform",
        "description": "Learn about the Fourier transform, its properties, and applications to signal processing and machine learning",
        "learningResourceType": "lesson",
        "educationalUse": "instruction",
        "educationalLevel": "university",
        "interactivityType": "active",
        "author": {
            "@type": "Person",
            "name": "Yusuke Yokota",
            "jobTitle": "Mathematics & Computer Science Educator",
            "knowsAbout": [
            "Fourier Transform",
            "Signal Processing",
            "Frequency Analysis",
            "Fast Fourier Transform",
            "Convolution Theorem",
            "Machine Learning",
            "Harmonic Analysis"
            ]
        },
        "publisher": {
            "@type": "Organization",
            "name": "MATH-CS COMPASS",
            "url": "https://yusukey01.github.io"
        },
        "about": [
            { "@type": "Thing", "name": "Fourier Transform" },
            { "@type": "Thing", "name": "Discrete Fourier Transform" },
            { "@type": "Thing", "name": "Fast Fourier Transform" },
            { "@type": "Thing", "name": "Convolution Theorem" },
            { "@type": "Thing", "name": "Frequency Domain" },
            { "@type": "Thing", "name": "Plancherel Theorem" },
            { "@type": "Thing", "name": "Random Fourier Features" }
        ],
        "teaches": [
            "Understanding the continuous Fourier transform",
            "Computing discrete Fourier transforms efficiently",
            "Applying the convolution theorem",
            "Working in frequency domain representation",
            "Understanding the Fast Fourier Transform algorithm",
            "Connecting Fourier methods to machine learning",
            "Applying Fourier analysis to signal processing"
        ],
        "isPartOf": {
            "@type": "Course",
            "name": "Calculus to Optimization & Analysis",
            "description": "Explore optimization techniques and mathematical analysis with applications",
            "provider": {
            "@type": "Organization",
            "name": "MATH-CS COMPASS",
            "url": "https://yusukey01.github.io"
            },
            "instructor": {
            "@type": "Person",
            "name": "Yusuke Yokota",
            "jobTitle": "Mathematics & Computer Science Educator"
            },
            "courseCode": "II",
            "hasCourseInstance": {
            "@type": "CourseInstance",
            "courseMode": "online",
            "courseWorkload": "PT4H",
            "instructor": {
                "@type": "Person",
                "name": "Yusuke Yokota"
            }
            },
            "offers": {
            "@type": "Offer",
            "price": "0",
            "priceCurrency": "USD",
            "availability": "https://schema.org/InStock",
            "category": "free"
            }
        }
        }
        </script>
        <div class="hero-section">
            <h1 class="webpage-name">Fourier Transform
            </h1>
        </div>

        {% include section_navigation.html %}

        <div class="topic-nav">
            <a href="#continuous">Continuous Fourier Transform</a>
            <a href="#properties">Properties of Fourier Transform</a>
            <a href="#convolution">Convolution Theorem</a>
            <a href="#discrete">Discrete Fourier Transform</a>
            <a href="#fft">Fast Fourier Transform</a>
            <a href="#ml">Applications in Machine Learning</a>
        </div>  

        <div class="container">     
            <section id="continuous" class="section-content">
            <h2>Continuous Fourier Transform</h2>
            <p>
            The <strong>Fourier transform</strong> extends the ideas of <a href="fourier_series.html"><strong>Fourier series</strong></a> 
            from periodic functions to non-periodic functions defined on \(\mathbb{R}\). While Fourier series decompose 
            periodic signals into discrete frequency components, the Fourier transform decomposes general signals into 
            a continuous spectrum of frequencies.
            <br><br>
            <strong>Motivation from Fourier Series:</strong> Consider a function \(f_L\) that is periodic with period \(2L\). 
            It has the complex Fourier series:
            \[
            f_L(x) = \sum_{n=-\infty}^{\infty} c_n e^{-i\frac{n\pi x}{L}}, \quad c_n = \frac{1}{2L}\int_{-L}^{L} f_L(x)e^{i\frac{n\pi x}{L}} \, dx
            \]
            As \(L \to \infty\), the discrete frequencies \(\omega_n = \frac{n\pi}{L}\) become densely packed with spacing 
            \(\Delta\omega = \frac{\pi}{L} \to 0\), and the sum approaches an integral. This limiting process yields the 
            Fourier transform.
            <br><br>
            For a function \(f: \mathbb{R} \to \mathbb{C}\), we define:
            \[
            \boxed{
            \hat{f}(\xi) = \mathcal{F}\{f\}(\xi) = \int_{-\infty}^{\infty} f(x)e^{i x\xi} \, dx
            }
            \]
            where \(\xi \in \mathbb{R}\) is the <strong>frequency variable</strong>.
            <br><br>
            The <strong>inverse Fourier transform</strong> recovers \(f\) from \(\hat{f}\):
            \[
            \boxed{
            f(x) = \mathcal{F}^{-1}\{\hat{f}\}(x) = \frac{1}{2\pi}\int_{-\infty}^{\infty} \hat{f}(\xi)e^{-i x\xi} \, d\xi
            }
            \]
            <br>
            <strong>Note on Conventions:</strong> As discussed in <a href="fourier_series.html#convention">Part 14</a>, we use 
            the mathematical/PDE convention with positive sign in the forward transform and negative in the inverse. Engineering 
            and physics often use the opposite convention: \(\hat{f}(\omega) = \int_{-\infty}^{\infty} f(t)e^{-i\omega t} \, dt\).
            Both are mathematically equivalent—just be consistent within your work.
            <br><br>
            <strong>Function Spaces:</strong> The Fourier transform is well-defined for different function classes:
            <ul style="padding-left: 40px;">
                <li><strong>For \(f \in L^1(\mathbb{R})\)</strong> (absolutely integrable): The integral defining \(\hat{f}\) 
                converges absolutely, and \(\hat{f}\) is bounded and continuous with \(|\hat{f}(\xi)| \leq \|f\|_{L^1}\)</li>
                <li><strong>For \(f \in L^2(\mathbb{R})\)</strong> (square-integrable): The transform is defined via limiting 
                procedures (e.g., truncating \(f\) to compact support, then taking limits). The result satisfies Plancherel's 
                theorem (see Properties section)</li>
                <li><strong>For tempered distributions:</strong> The theory extends to generalized functions, allowing transforms 
                of \(\delta\)-functions, polynomials, and other non-integrable functions important in applications</li>
            </ul>
            <div class="proof">
                <span class="proof-title">Example: Gaussian Function</span>
                Consider the Gaussian function:
                \[
                f(x) = e^{-\frac{x^2}{2\sigma^2}}
                \]
                Its Fourier transform (using the mathematical convention) is:
                \[
                \begin{align*}
                \hat{f}(\xi) &= \int_{-\infty}^{\infty} e^{-\frac{x^2}{2\sigma^2}}e^{i\xi x} \, dx \\\\
                &= \int_{-\infty}^{\infty} e^{-\frac{x^2}{2\sigma^2} + i\xi x} \, dx
                \end{align*}
                \]
                Complete the square in the exponent:
                \[
                -\frac{x^2}{2\sigma^2} + i\xi x = -\frac{1}{2\sigma^2}(x^2 - 2i\sigma^2\xi x) = -\frac{1}{2\sigma^2}((x - i\sigma^2\xi)^2 + \sigma^4\xi^2)
                \]
                Therefore:
                \[
                \hat{f}(\xi) = e^{-\frac{\sigma^2\xi^2}{2}} \int_{-\infty}^{\infty} e^{-\frac{(x-i\sigma^2\xi)^2}{2\sigma^2}} \, dx
                \]
                By contour integration (the integrand is analytic and decays exponentially), shifting the path doesn't change the value:
                \[
                \int_{-\infty}^{\infty} e^{-\frac{(x-i\sigma^2\xi)^2}{2\sigma^2}} \, dx = \int_{-\infty}^{\infty} e^{-\frac{u^2}{2\sigma^2}} \, du = \sigma\sqrt{2\pi}
                \]
                Thus:
                \[
                \boxed{\hat{f}(\xi) = \sigma\sqrt{2\pi} \cdot e^{-\frac{\sigma^2\xi^2}{2}}}
                \]
                This shows that the Gaussian is essentially an eigenfunction of the Fourier transform: its transform is also 
                Gaussian, with inverse width (narrow in space ↔ wide in frequency). This property makes Gaussians fundamental 
                in uncertainty principles, quantum mechanics, and signal processing.
            </div>
            </p>
            </section>

            <section id="properties" class="section-content">
            <h2>Properties of Fourier Transform</h2>
            <p>
            The Fourier transform has several important properties that make it a powerful tool for analysis. 
            We state these using the mathematical convention \(\hat{f}(\xi) = \int_{-\infty}^{\infty} f(x)e^{ix\xi} \, dx\):
            <br><br>
            <strong>1. Linearity:</strong>
            \[
            \mathcal{F}\{\alpha f + \beta g\} = \alpha \mathcal{F}\{f\} + \beta \mathcal{F}\{g\}
            \]
            for constants \(\alpha, \beta \in \mathbb{C}\).
            <br><br>
            <strong>2. Translation (Shift) Property:</strong>
            \[
            \mathcal{F}\{f(x - a)\}(\xi) = e^{-ia\xi}\hat{f}(\xi)
            \]
            A shift in space corresponds to a phase shift in frequency.
            <br><br>
            <strong>3. Modulation Property:</strong>
            \[
            \mathcal{F}\{e^{i\xi_0 x}f(x)\}(\xi) = \hat{f}(\xi - \xi_0)
            \]
            Multiplication by a complex exponential shifts the frequency spectrum.
            <br><br>
            <strong>4. Scaling Property:</strong>
            \[
            \mathcal{F}\{f(ax)\}(\xi) = \frac{1}{|a|}\hat{f}\left(\frac{\xi}{a}\right), \quad a \neq 0
            \]
            This embodies the <strong>uncertainty principle</strong>: compressing a signal in time stretches its frequency 
            spectrum, and vice versa. One cannot localize a signal arbitrarily well in both time and frequency simultaneously.
            <br><br>
            <strong>5. Differentiation Property:</strong>
            \[
            \mathcal{F}\{f'(x)\}(\xi) = -i\xi \hat{f}(\xi)
            \]
            More generally, for the \(n\)-th derivative:
            \[
            \mathcal{F}\{f^{(n)}(x)\}(\xi) = (-i\xi)^n \hat{f}(\xi)
            \]
            This transforms differentiation into algebraic multiplication, which is why Fourier methods are powerful for 
            solving differential equations. Note the sign difference from the engineering convention.
            <br><br>
            <strong>6. Multiplication by \(x^n\) Property:</strong>
            \[
            \mathcal{F}\{x^n f(x)\}(\xi) = i^n \frac{d^n}{d\xi^n}\hat{f}(\xi)
            \]
            Multiplication by powers of \(x\) becomes differentiation in frequency domain.
            <br><br>
            <strong>7. Plancherel's Theorem (Parseval for Non-periodic Functions):</strong>
            <br>
            For \(f, g \in L^2(\mathbb{R})\):
            \[
            \int_{-\infty}^{\infty} f(x)\overline{g(x)} \, dx = \frac{1}{2\pi}\int_{-\infty}^{\infty} \hat{f}(\xi)\overline{\hat{g}(\xi)} \, d\xi
            \]
            In particular, setting \(f = g\):
            \[
            \boxed{\|f\|_{L^2}^2 = \int_{-\infty}^{\infty} |f(x)|^2 \, dx = \frac{1}{2\pi}\int_{-\infty}^{\infty} |\hat{f}(\xi)|^2 \, d\xi = \frac{1}{2\pi}\|\hat{f}\|_{L^2}^2}
            \]
            This generalizes <a href="fourier_series.html#parseval">Parseval's identity</a> and shows that the Fourier 
            transform preserves energy (up to the factor \(2\pi\)). The map \(f \mapsto \frac{1}{\sqrt{2\pi}}\hat{f}\) 
            is a unitary operator on \(L^2(\mathbb{R})\).
            <br><br>
            <strong>8. Fourier Inversion Theorem:</strong>
            <br>
            For suitable functions (e.g., \(f \in L^1(\mathbb{R})\) with \(\hat{f} \in L^1(\mathbb{R})\)):
            \[
            \mathcal{F}\{\mathcal{F}\{f\}(\xi)\}(x) = 2\pi f(-x)
            \]
            Applying the Fourier transform twice (up to normalization) gives a reflection of the original function, 
            demonstrating deep symmetry between spatial and frequency domains.
            <br><br>
            <strong>9. Riemann-Lebesgue Lemma:</strong>
            <br>
            For \(f \in L^1(\mathbb{R})\):
            \[
            \lim_{|\xi| \to \infty} \hat{f}(\xi) = 0
            \]
            The Fourier transform of an integrable function vanishes at infinity. This has important implications for 
            the smoothness-decay duality: smoother functions have faster-decaying transforms.
            </p>
            </section>

            <section id="convolution" class="section-content">
            <h2>Convolution Theorem</h2>
            <p>
            The <strong>convolution</strong> of two functions \(f, g: \mathbb{R} \to \mathbb{C}\) is defined as:
            \[
            (f * g)(x) = \int_{-\infty}^{\infty} f(y)g(x-y) \, dy = \int_{-\infty}^{\infty} f(x-y)g(y) \, dy
            \]
            (The second equality shows convolution is commutative.)
            <br><br>
            The <strong>convolution theorem</strong> states that the Fourier transform converts convolution into pointwise 
            multiplication:
            \[
            \boxed{\mathcal{F}\{f * g\}(\xi) = \hat{f}(\xi) \cdot \hat{g}(\xi)}
            \]
            and conversely, the Fourier transform of a product is a (scaled) convolution:
            \[
            \boxed{\mathcal{F}\{f \cdot g\}(\xi) = \frac{1}{2\pi}(\hat{f} * \hat{g})(\xi)}
            \]
            <br>
            This is one of the most important results in applied mathematics. It allows us to:
            <ul style="padding-left: 40px;">
                <li>Replace expensive convolution operations (\(O(n^2)\) for discrete signals) with multiplication after 
                FFT (\(O(n \log n)\))</li>
                <li>Understand filtering as multiplication in frequency domain</li>
                <li>Analyze linear time-invariant systems through their frequency response</li>
            </ul>
            <br>
            <strong>Applications in Computer Science and ML:</strong>
            <ul style="padding-left: 40px;">
                <li><strong>Signal filtering:</strong> Low-pass, high-pass, and band-pass filters are convolutions in time 
                domain but simple multiplications in frequency domain</li>
                <li><strong>Image processing:</strong> Gaussian blur, edge detection (Sobel, Canny), and sharpening are 
                convolution operations efficiently computed via FFT for large kernels</li>
                <li><strong>Deep learning:</strong> While CNNs typically use small kernels (3×3, 5×5) where direct convolution 
                is efficient, FFT-based convolution is useful for:
                    <ul style="padding-left: 40px;">
                        <li>Large kernels in certain architectures</li>
                        <li>Global convolutions in some vision transformers</li>
                        <li>Efficient implementation of depthwise separable convolutions</li>
                    </ul>
                </li>
                <li><strong>Probability:</strong> The PDF of \(X + Y\) for independent random variables is 
                \(f_{X+Y} = f_X * f_Y\), computed efficiently via FFT</li>
            </ul>
            <div class="proof">
                <span class="proof-title">Proof of Convolution Theorem:</span>
                Using the mathematical convention with \(\hat{f}(\xi) = \int f(x)e^{ix\xi} \, dx\):
                \[
                \begin{align*}
                \mathcal{F}\{f * g\}(\xi) &= \int_{-\infty}^{\infty} (f * g)(x)e^{ix\xi} \, dx \\\\
                &= \int_{-\infty}^{\infty} \left(\int_{-\infty}^{\infty} f(y)g(x-y) \, dy\right) e^{ix\xi} \, dx \\\\
                &= \int_{-\infty}^{\infty} f(y) \left(\int_{-\infty}^{\infty} g(x-y)e^{ix\xi} \, dx\right) dy
                \end{align*}
                \]
                Substituting \(u = x - y\) (so \(x = u + y\) and \(dx = du\)):
                \[
                \begin{align*}
                &= \int_{-\infty}^{\infty} f(y) \left(\int_{-\infty}^{\infty} g(u)e^{i(u+y)\xi} \, du\right) dy \\\\
                &= \int_{-\infty}^{\infty} f(y)e^{iy\xi} \left(\int_{-\infty}^{\infty} g(u)e^{iu\xi} \, du\right) dy \\\\
                &= \left(\int_{-\infty}^{\infty} f(y)e^{iy\xi} \, dy\right) \cdot \left(\int_{-\infty}^{\infty} g(u)e^{iu\xi} \, du\right) \\\\
                &= \hat{f}(\xi) \cdot \hat{g}(\xi)
                \end{align*}
                \]
                The interchange of integration order is justified by Fubini's theorem when \(f, g \in L^1(\mathbb{R})\).
            </div>
            </p>
            </section>

            <section id="discrete" class="section-content">
            <h2>Discrete Fourier Transform (DFT)</h2>
            <p>
            In computational applications, we work with discrete, finite sequences rather than continuous functions. 
            The <strong>Discrete Fourier Transform (DFT)</strong> is the discrete analog of the continuous Fourier transform.
            <br><br>
            <strong>Definition:</strong> For a sequence \(\{x_0, x_1, \ldots, x_{N-1}\}\) of \(N\) complex numbers, the DFT is:
            \[
            X_k = \sum_{n=0}^{N-1} x_n e^{-\frac{2\pi i}{N}kn}, \quad k = 0, 1, \ldots, N-1
            \]
            The <strong>inverse DFT (IDFT)</strong> is:
            \[
            x_n = \frac{1}{N}\sum_{k=0}^{N-1} X_k e^{\frac{2\pi i}{N}kn}, \quad n = 0, 1, \ldots, N-1
            \]
            <br>
            <strong>Connection to Continuous Transform:</strong> The DFT can be viewed as:
            <ul style="padding-left: 40px;">
                <li>Sampling a periodic function at \(N\) equally spaced points</li>
                <li>Computing Fourier series coefficients for the periodized version</li>
                <li>Approximating the continuous Fourier transform for band-limited signals (via sampling theorem)</li>
            </ul>
            <br>
            <strong>Matrix Formulation:</strong> Define \(\omega = e^{-\frac{2\pi i}{N}}\), a primitive \(N\)-th root of unity 
            (so \(\omega^N = 1\)). The DFT matrix is:
            \[
            W = \begin{bmatrix}
            1 & 1 & 1 & \cdots & 1 \\
            1 & \omega & \omega^2 & \cdots & \omega^{N-1} \\
            1 & \omega^2 & \omega^4 & \cdots & \omega^{2(N-1)} \\
            \vdots & \vdots & \vdots & \ddots & \vdots \\
            1 & \omega^{N-1} & \omega^{2(N-1)} & \cdots & \omega^{(N-1)^2}
            \end{bmatrix}
            \]
            where \(W_{kn} = \omega^{kn}\). Then:
            \[
            \mathbf{X} = W\mathbf{x}, \quad \mathbf{x} = \frac{1}{N}W^*\mathbf{X}
            \]
            where \(W^*\) is the conjugate transpose (note \(W^* = \overline{W}\) with \(\overline{\omega^{kn}} = \omega^{-kn}\)).
            <br><br>
            <strong>Properties of the DFT Matrix:</strong>
            <ul style="padding-left: 40px;">
                <li><strong>Orthogonality:</strong> \(W^*W = NI\), so \(W^{-1} = \frac{1}{N}W^*\)</li>
                <li><strong>Symmetry:</strong> \(W\) is symmetric (not Hermitian): \(W^T = W\)</li>
                <li><strong>Eigenvalues:</strong> The eigenvalues of \(W\) are \(\{1, -1, i, -i\}\) with specific multiplicities 
                depending on \(N\)</li>
                <li><strong>Vandermonde structure:</strong> \(W\) is a Vandermonde matrix with nodes at roots of unity</li>
            </ul>
            <br>
            <strong>Computational Complexity:</strong>
            <ul style="padding-left: 40px;">
                <li><strong>Direct computation:</strong> \(O(N^2)\) complex multiplications and additions</li>
                <li><strong>Fast Fourier Transform:</strong> \(O(N \log N)\) operations (see next section)</li>
            </ul>
            <br>
            <strong>Practical Considerations for CS/ML:</strong>
            <ul style="padding-left: 40px;">
                <li><strong>Zero-padding:</strong> Padding sequences to powers of 2 enables efficient FFT algorithms</li>
                <li><strong>Windowing:</strong> Applying window functions (Hamming, Hanning, Blackman) reduces spectral leakage</li>
                <li><strong>Real-valued signals:</strong> For real inputs, \(X_{N-k} = \overline{X_k}\) (conjugate symmetry), 
                allowing optimization to compute only half the spectrum</li>
                <li><strong>Normalization:</strong> Different software uses different conventions for the \(\frac{1}{N}\) factor; 
                always check documentation</li>
            </ul>
            </p>
            </section>

            <section id="fft" class="section-content">
            <h2>Fast Fourier Transform (FFT)</h2>
            <p>
            The <strong>Fast Fourier Transform (FFT)</strong> is not a different transform but an efficient algorithm for 
            computing the DFT, reducing complexity from \(O(N^2)\) to \(O(N \log N)\).
            <br><br>
            <strong>Cooley-Tukey Algorithm (Radix-2 FFT):</strong>
            <br>
            The key insight is to exploit the structure of the DFT matrix using divide-and-conquer. For \(N = 2^m\), 
            split the input into even and odd indices:
            \[
            \begin{align*}
            X_k &= \sum_{n=0}^{N-1} x_n \omega^{kn} \\\\
            &= \sum_{j=0}^{N/2-1} x_{2j} \omega^{k(2j)} + \sum_{j=0}^{N/2-1} x_{2j+1} \omega^{k(2j+1)} \\\\
            &= \sum_{j=0}^{N/2-1} x_{2j} (\omega^2)^{kj} + \omega^k \sum_{j=0}^{N/2-1} x_{2j+1} (\omega^2)^{kj}
            \end{align*}
            \]
            Note that \(\omega^2 = e^{-\frac{4\pi i}{N}} = e^{-\frac{2\pi i}{N/2}}\) is a primitive \((N/2)\)-th root of unity.
            <br><br>
            Let \(E_k\) be the DFT of the even-indexed subsequence and \(O_k\) the DFT of odd-indexed subsequence. 
            Then for \(k = 0, 1, \ldots, N/2-1\):
            \[
            X_k = E_k + \omega^k O_k
            \]
            \[
            X_{k+N/2} = E_k - \omega^k O_k
            \]
            (using the fact that \(\omega^{k+N/2} = -\omega^k\) and the periodicity of DFT).
            <br><br>
            This gives the recurrence \(T(N) = 2T(N/2) + O(N)\), yielding \(T(N) = O(N \log N)\).
            <br><br>
            <strong>Other FFT Algorithms:</strong>
            <ul style="padding-left: 40px;">
                <li><strong>Radix-4, Radix-8:</strong> Reduce the number of complex multiplications further</li>
                <li><strong>Mixed-radix FFT:</strong> Handle arbitrary sizes \(N = p_1^{a_1} \cdots p_k^{a_k}\)</li>
                <li><strong>Prime-factor algorithm:</strong> For \(N = N_1 N_2\) with \(\gcd(N_1, N_2) = 1\)</li>
                <li><strong>Bluestein's algorithm:</strong> Computes DFT of any size using convolution</li>
            </ul>
            <br>
            <strong>Implementation Considerations:</strong>
            <ul style="padding-left: 40px;">
                <li><strong>Bit-reversal:</strong> Input/output reordering required for in-place computation</li>
                <li><strong>Twiddle factors:</strong> Precomputing \(\omega^k\) values improves performance</li>
                <li><strong>Cache optimization:</strong> Modern implementations (FFTW) auto-tune for cache hierarchies</li>
                <li><strong>Parallelization:</strong> FFT naturally parallelizes (used in GPU implementations)</li>
                <li><strong>Numerical stability:</strong> Careful ordering minimizes roundoff error accumulation</li>
            </ul>
            <br>
            <strong>Python Example:</strong>
            <div class="code-block">
                <pre><code class="language-python">
import numpy as np
import matplotlib.pyplot as plt

# Generate a signal with multiple frequency components
fs = 1000  # Sampling frequency
t = np.linspace(0, 1, fs, endpoint=False)
signal = (np.sin(2*np.pi*50*t) +           # 50 Hz
          0.5*np.sin(2*np.pi*120*t) +      # 120 Hz  
          0.3*np.cos(2*np.pi*200*t))       # 200 Hz

# Add some noise
signal += 0.3 * np.random.randn(len(t))

# Compute FFT
fft_vals = np.fft.fft(signal)
fft_freq = np.fft.fftfreq(len(signal), 1/fs)

# Only plot positive frequencies (due to conjugate symmetry)
pos_mask = fft_freq >= 0
plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot(t[:100], signal[:100])
plt.xlabel('Time (s)')
plt.ylabel('Amplitude')
plt.title('Signal (first 100 samples)')

plt.subplot(1, 2, 2)
plt.plot(fft_freq[pos_mask], np.abs(fft_vals[pos_mask]))
plt.xlabel('Frequency (Hz)')
plt.ylabel('Magnitude')
plt.title('Frequency Spectrum')
plt.xlim(0, 300)
plt.show()

# Verify Parseval's theorem
energy_time = np.sum(np.abs(signal)**2)
energy_freq = np.sum(np.abs(fft_vals)**2) / len(signal)
print(f"Energy in time domain: {energy_time:.2f}")
print(f"Energy in frequency domain: {energy_freq:.2f}")
print(f"Relative error: {abs(energy_time - energy_freq)/energy_time:.2e}")
                </code></pre>
            </div>
            </p>
            </section>

            <section id="ml" class="section-content">
            <h2>Applications in Machine Learning</h2>
            <p>
            Fourier methods are fundamental to modern machine learning, particularly in areas requiring efficient computation, 
            signal processing, and function approximation. Here we focus on the most significant and current applications.
            <br><br>
            <strong>1. Fourier Neural Operators (FNO) for Scientific Computing:</strong>
            <br>
            Introduced by Li et al. (2021), FNOs have revolutionized neural PDE solvers and are actively used in weather 
            prediction (FourCastNet by NVIDIA, 2022), fluid dynamics, and climate modeling. They achieve 1000× speedup 
            over traditional solvers while maintaining accuracy.
            <br><br>
            <strong>Key Innovation:</strong> FNOs learn integral operators directly in Fourier space:
            \[
            (\mathcal{K}\varphi)(x) = \int k(x, y)\varphi(y) \, dy \quad \xrightarrow{\text{Fourier}} \quad 
            \widehat{(\mathcal{K}\varphi)}(\xi) = \hat{k}(\xi) \cdot \hat{\varphi}(\xi)
            \]
            By parameterizing \(\hat{k}(\xi)\) with neural networks in frequency domain, FNOs efficiently learn 
            resolution-invariant operators.
            <div class="code-block">
                <pre><code class="language-python">
import torch
import torch.nn as nn
import torch.fft

class SpectralConv2d(nn.Module):
    """2D Fourier layer used in FNO. Efficient for learning PDEs."""
    def __init__(self, in_channels, out_channels, modes1, modes2):
        super().__init__()
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.modes1 = modes1  # Number of Fourier modes to keep
        self.modes2 = modes2
        
        # Complex weights for Fourier coefficients
        scale = 1 / (in_channels * out_channels)
        self.weights1 = nn.Parameter(scale * torch.rand(
            in_channels, out_channels, modes1, modes2, dtype=torch.cfloat))
        self.weights2 = nn.Parameter(scale * torch.rand(
            in_channels, out_channels, modes1, modes2, dtype=torch.cfloat))
    
    def forward(self, x):
        batch, channels, height, width = x.shape
        
        # Compute FFT
        x_ft = torch.fft.rfft2(x)
        
        # Multiply relevant Fourier modes with learnable weights
        out_ft = torch.zeros(batch, self.out_channels, height, width // 2 + 1,
                           dtype=torch.cfloat, device=x.device)
        
        # Low frequencies (top-left corner)
        out_ft[:, :, :self.modes1, :self.modes2] = torch.einsum(
            "bixy,ioxy->boxy",
            x_ft[:, :, :self.modes1, :self.modes2],
            self.weights1)
        
        # High frequencies (bottom-left corner)  
        out_ft[:, :, -self.modes1:, :self.modes2] = torch.einsum(
            "bixy,ioxy->boxy",
            x_ft[:, :, -self.modes1:, :self.modes2],
            self.weights2)
        
        # Inverse FFT
        x = torch.fft.irfft2(out_ft, s=(height, width))
        return x

# Example: Using FNO layer in a model for solving Navier-Stokes equations
class FNO2d(nn.Module):
    def __init__(self, modes=12, width=64):
        super().__init__()
        self.conv0 = SpectralConv2d(3, width, modes, modes)
        self.conv1 = SpectralConv2d(width, width, modes, modes)
        self.conv2 = SpectralConv2d(width, width, modes, modes)
        self.conv3 = SpectralConv2d(width, width, modes, modes)
        self.fc = nn.Linear(width, 1)
        
    def forward(self, x):
        x = self.conv0(x)
        x = torch.nn.functional.gelu(x)
        x = self.conv1(x)
        x = torch.nn.functional.gelu(x)
        x = self.conv2(x)
        x = torch.nn.functional.gelu(x)
        x = self.conv3(x)
        x = self.fc(x.permute(0, 2, 3, 1))
        return x
                </code></pre>
            </div>
            <br>
            <strong>2. State Space Models and Long-Range Sequence Modeling:</strong>
            <br>
            Recent breakthroughs like <strong>S4 (Structured State Spaces)</strong> by Gu et al. (2022) and 
            <strong>Hyena Hierarchy</strong> (2023) use FFT-based convolutions to model sequences with million-token 
            context lengths efficiently. These models compete with Transformers while using \(O(N \log N)\) complexity 
            instead of \(O(N^2)\).
            <br><br>
            <strong>Key Insight:</strong> Long convolutions via FFT enable efficient computation of state space models:
            \[
            y = K * u \quad \text{where} \quad K \in \mathbb{R}^L \text{ is a learned kernel}
            \]
            For sequence length \(L\), direct convolution costs \(O(L^2)\) but FFT reduces this to \(O(L \log L)\):
            <div class="code-block">
                <pre><code class="language-python">
def fft_conv(u, k):
    """Compute convolution u * k using FFT for efficiency.
    Used in S4, Hyena, and other long-range sequence models."""
    
    # Pad kernel to match sequence length
    L = len(u)
    k_padded = np.pad(k, (0, L - len(k)))
    
    # FFT → multiply → inverse FFT
    u_f = np.fft.rfft(u, n=2*L)
    k_f = np.fft.rfft(k_padded, n=2*L)
    y_f = u_f * k_f
    y = np.fft.irfft(y_f)[:L]
    
    return y

# Example: S4 kernel parameterization (simplified)
class S4Kernel(nn.Module):
    def __init__(self, d_model, l_max=1024):
        super().__init__()
        self.l_max = l_max
        
        # HiPPO matrix initialization for long-range memory
        self.A = nn.Parameter(torch.randn(d_model, d_model))
        self.B = nn.Parameter(torch.randn(d_model, 1))
        self.C = nn.Parameter(torch.randn(1, d_model))
        
    def forward(self, L):
        # Generate convolution kernel from state space parameters
        # This is computed once and cached
        k = self.compute_kernel(L)
        return k
    
    def compute_kernel(self, L):
        # Discretize continuous state space
        # Details omitted for brevity
        pass
                </code></pre>
            </div>
            <br>
            <strong>3. Audio and Speech Processing Foundation Models:</strong>
            <br>
            Modern speech models like <strong>Whisper (OpenAI, 2022)</strong>, <strong>AudioLM (Google, 2022)</strong>, 
            and <strong>MusicLM (2023)</strong> fundamentally rely on Fourier transforms for feature extraction:
            <ul style="padding-left: 40px;">
                <li><strong>Mel-spectrograms:</strong> Log-scaled STFT with mel-frequency bins matching human auditory perception</li>
                <li><strong>Learned Fourier features:</strong> Models like HiFi-GAN and WaveGrad learn to generate audio 
                directly from Fourier features</li>
                <li><strong>Phase reconstruction:</strong> Griffin-Lim algorithm and neural vocoders reconstruct phase from magnitude spectrograms</li>
            </ul>
            <div class="code-block">
                <pre><code class="language-python">
import librosa
import torch
import torchaudio.transforms as T

# Modern audio processing pipeline used in Whisper, etc.
class AudioProcessor:
    def __init__(self, sample_rate=16000, n_fft=400, n_mels=80):
        self.sample_rate = sample_rate
        self.mel_transform = T.MelSpectrogram(
            sample_rate=sample_rate,
            n_fft=n_fft,
            hop_length=160,
            n_mels=n_mels,
            f_min=0,
            f_max=8000
        )
        
    def process(self, waveform):
        # Convert to mel-spectrogram (used by Whisper, Wav2Vec2, etc.)
        mel_spec = self.mel_transform(waveform)
        
        # Log-scale for better neural network training
        log_mel = torch.log10(torch.clamp(mel_spec, min=1e-10))
        
        # Normalize (crucial for training stability)
        log_mel = (log_mel + 4.0) / 4.0
        
        return log_mel

# Example: extracting features for speech recognition
processor = AudioProcessor()
# waveform = torch.randn(1, 16000)  # 1 second of audio
# features = processor.process(waveform)
# Now feed 'features' to transformer model (e.g., Whisper encoder)
                </code></pre>
            </div>
            <br>
            <strong>4. Vision Transformers and Frequency-Aware Architectures:</strong>
            <br>
            Recent vision models leverage frequency domain insights:
            <ul style="padding-left: 40px;">
                <li><strong>FNet (Google, 2021):</strong> Replaces attention with FFT, achieving 92% of BERT's accuracy 
                at 7× training speed</li>
                <li><strong>GFNet (Global Filter Networks, 2021):</strong> Learns filters in frequency domain for vision tasks</li>
                <li><strong>Focal Frequency Loss (2020):</strong> Improves image synthesis by matching frequency spectra</li>
            </ul>
            <div class="code-block">
                <pre><code class="language-python">
# FNet: Replacing attention with FFT (simplified)
class FNetBlock(nn.Module):
    """FNet block: FFT mixing instead of self-attention"""
    def __init__(self, d_model):
        super().__init__()
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.ffn = nn.Sequential(
            nn.Linear(d_model, 4 * d_model),
            nn.GELU(),
            nn.Linear(4 * d_model, d_model)
        )
        
    def forward(self, x):
        # x shape: (batch, sequence_length, d_model)
        
        # FFT mixing (replaces self-attention)
        x_complex = torch.fft.fft2(x)  # 2D FFT over sequence and hidden dims
        x_real = x_complex.real
        x = x + self.norm1(x_real)
        
        # Feed-forward network
        x = x + self.norm2(self.ffn(x))
        return x

# Focal Frequency Loss for better image generation
def focal_frequency_loss(pred, target, alpha=1.0):
    """Compute loss in frequency domain for better texture synthesis"""
    pred_fft = torch.fft.rfft2(pred)
    target_fft = torch.fft.rfft2(target)
    
    # Weight matrix emphasizing different frequencies
    h, w = pred_fft.shape[-2:]
    weight = torch.ones_like(pred_fft)
    # Emphasize high frequencies (details)
    weight[..., h//4:, w//4:] *= alpha
    
    loss = torch.mean(weight * torch.abs(pred_fft - target_fft))
    return loss
                </code></pre>
            </div>
            <br>
            <strong>5. Positional Encodings and Implicit Neural Representations:</strong>
            <br>
            Fourier features are crucial for modern neural representations:
            <ul style="padding-left: 40px;">
                <li><strong>NeRF (Neural Radiance Fields, 2020):</strong> Uses Fourier positional encoding to capture 
                high-frequency details in 3D scene reconstruction</li>
                <li><strong>SIREN (2020):</strong> Sinusoidal activation functions act as Fourier basis</li>
                <li><strong>Transformer positional encodings:</strong> Sinusoidal encodings are Fourier features enabling 
                position awareness</li>
            </ul>

            <div class="code-container">
                <div class="collapsible-section">
                    <button class="collapsible-btn">Show/Hide Code</button>
                    <div class="collapsible-content">
                        <pre class="python-code">
                            # Fourier positional encoding (used in NeRF, Transformers)
                            def fourier_encoding(x, L=10):
                                """
                                Maps coordinates to higher dimensional space using Fourier features.
                                Essential for learning high-frequency functions in neural fields.
                                """
                                freqs = 2.0 ** torch.linspace(0, L-1, L)
                                
                                # Apply sinusoidal functions at multiple frequencies
                                encoded = []
                                for freq in freqs:
                                    encoded.append(torch.sin(2 * np.pi * freq * x))
                                    encoded.append(torch.cos(2 * np.pi * freq * x))
                                
                                return torch.cat(encoded, dim=-1)

                            # Example: NeRF-style positional encoding
                            class NeRFEncoder(nn.Module):
                                def __init__(self, input_dim=3, L_pos=10, L_dir=4):
                                    super().__init__()
                                    self.L_pos = L_pos
                                    self.L_dir = L_dir
                                    
                                    # MLPs for processing encoded features
                                    pos_dim = input_dim * (2 * L_pos + 1)
                                    dir_dim = 3 * (2 * L_dir + 1)
                                    
                                    self.pos_net = nn.Sequential(
                                        nn.Linear(pos_dim, 256),
                                        nn.ReLU(),
                                        nn.Linear(256, 256)
                                    )
                                    
                                def forward(self, pos, direction):
                                    # Encode position with more frequencies (fine details)
                                    pos_encoded = fourier_encoding(pos, self.L_pos)
                                    
                                    # Encode viewing direction with fewer frequencies
                                    dir_encoded = fourier_encoding(direction, self.L_dir)
                                    
                                    features = self.pos_net(pos_encoded)
                                    return features

                            # RoPE (Rotary Positional Encoding) - used in modern LLMs like LLaMA
                            def apply_rotary_pos_emb(q, k, cos, sin):
                                """
                                Apply rotary positional encoding (Fourier-based) to query and key.
                                More efficient than traditional positional encodings.
                                """
                                q_embed = (q * cos) + (rotate_half(q) * sin)
                                k_embed = (k * cos) + (rotate_half(k) * sin)
                                return q_embed, k_embed

                            def rotate_half(x):
                                x1, x2 = x.chunk(2, dim=-1)
                                return torch.cat((-x2, x1), dim=-1)                              
                        </pre>
                    </div>
                </div>
            </div>
           
            <br>
            <strong>6. Time Series Foundation Models:</strong>
            <br>
            Modern time series models like <strong>TimeGPT (2023)</strong>, <strong>Lag-Llama (2024)</strong>, and 
            <strong>PatchTST (2023)</strong> use Fourier features for capturing multi-scale seasonality:
              <div class="code-container">
            <div class="collapsible-section">
                <button class="collapsible-btn">Show/Hide Code</button>
                <div class="collapsible-content">
                    <pre class="python-code">
                        # Modern time series feature extraction
                        class TimeSeriesFoundationEncoder:
                            def __init__(self, periods=[24, 168, 8760]):  # hour, week, year
                                self.periods = periods
                                
                            def extract_fourier_features(self, timestamps, n_features=10):
                                """Extract Fourier features for multiple periodicities"""
                                features = []
                                
                                for period in self.periods:
                                    for k in range(1, n_features // 2 + 1):
                                        features.append(np.sin(2 * np.pi * k * timestamps / period))
                                        features.append(np.cos(2 * np.pi * k * timestamps / period))
                                
                                return np.column_stack(features)
                            
                            def extract_spectral_features(self, series):
                                """Extract frequency domain statistics used in foundation models"""
                                fft = np.fft.rfft(series)
                                magnitude = np.abs(fft)
                                phase = np.angle(fft)
                                
                                # Key spectral features
                                features = {
                                    'spectral_entropy': -np.sum(magnitude * np.log(magnitude + 1e-10)),
                                    'spectral_centroid': np.sum(np.arange(len(magnitude)) * magnitude) / np.sum(magnitude),
                                    'dominant_frequency': np.argmax(magnitude),
                                    'spectral_rolloff': np.percentile(magnitude, 85)
                                }
                                return features                  
                    </pre>
                </div>
            </div>
        </div>

            <br>
            These applications demonstrate that Fourier methods are not just theoretical tools but are actively driving 
            state-of-the-art performance in modern ML systems. From enabling million-token context windows to solving 
            PDEs at unprecedented speeds, Fourier transforms remain essential for pushing the boundaries of what neural 
            networks can achieve.
            </p>
            </section>

        </div>
         <script src="/js/collapsible.js"></script>
        <script src="/js/main.js"></script>
    </body>
</html>


        <div class="code-container">
            <div class="collapsible-section">
                <button class="collapsible-btn">Show/Hide Code</button>
                <div class="collapsible-content">
                    <pre class="python-code">
                
                              
                    </pre>
                </div>
            </div>
        </div>