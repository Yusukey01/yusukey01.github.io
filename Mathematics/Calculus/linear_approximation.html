---
layout: default
title: Linear Approximations
topic_id: calc-1
level: detail 
uses_math: true
uses_python: false
---
<!DOCTYPE html>
<html>
    <body>
        {% include learning_resource_schema.html topic_id=page.topic_id %}
        
        <div class="hero-section">
            <h1 class="webpage-name">Linear Approximations</h1>
        </div>

        {% include section_navigation.html %}

        <div class="topic-nav">
            <a href="#lapp">Linear Approximations</a>
            <a href="#diff">Differentials</a>
            <a href="#ex1">Vector Derivatives: Optimization Foundations</a>
            <a href="#ex2">Quadratic Forms \(f(x) = x^TAx\)</a>
            <a href="#ex3">\(L_2\) Norm \(f(x) = \| x \|_2\)</a>   
        </div> 

        <div class="container">  
           
            <section id="lapp" class="section-content">
                <h2>Linear Approximations</h2>
                <p>
                    In the study of complex systems - from the orbits of planets to the loss landscapes of deep neural networks - most functions are 
                    inherently nonlinear and difficult to solve directly. Linear approximation is the fundamental strategy of calculus: it approximates a 
                    complex function \(f(x)\) near a specific point \(x_o\) using the simplest possible tool - a linear function. 
                </p>

                <div class="theorem">
                    <span class="theorem-title">Definition: Linearization</span>
                    <p>
                        The <strong>linearization</strong> of \(f\) at \(x_o\) is the linear function \(L(x)\) defined by:
                        \[
                        L(x) = f(x_o) + f'(x_o)(x - x_o) \approx f(x) \tag{1}
                        \]
                        where \(f'(x_o)\) represents the instantaneous rate of change at \(x_o\):
                        \[
                        f'(x_o) = \lim_{x \to x_o} \frac{f(x)-f(x_o)}{x-x_o}.
                        \]
                    </p>
                </div>

                <p>
                    By constructing the tangent line at \((x_o, f(x_o))\), we create a local model where the function behaves predictably. 
                    This local predictability is what allows optimization algorithms to take "steps" toward a minimum, even when the global 
                    shape of the function is unknown.
                </p>
 
                <p>
                    Equivalently, from equation (1), 
                    \[
                    f(x) - f(x_o) \approx f'(x_o)(x -x_o) 
                    \]
                    The graph of \(L\) is the tangent line at \((x_o, f(x_o))\). Linear approximations form the foundation of differentiation 
                    and provide a <strong>local</strong> linear model for \(f(x)\). This local predictability is what allows optimization algorithms 
                    to take "steps" toward a minimum, even when the global shape of the function is unknown. Now, we extend this concept to differentials.
                </p>
            </section>
    
            <section id="diff" class="section-content">
                <h2>Differentials</h2>
                <p>
                    While linearization focuses on the function value, <strong>differentials</strong> describe the relationship between infinitesimally 
                    small changes in input quantities (\(dx\)) and the resulting change in output quantities (\(dy\)). 
                </p>

                <p>
                    Consider a differentiable function \(y = f(x)\). We define the differential \(dy\) as:
                    \[
                    dy = f'(x)dx.
                    \]
                    In practice, \(dx\) and \(dy\) are arbitrary small numbers. Also, if \(dx \neq 0\), then we recover the 
                    familiar derivative form:
                    \[
                    \frac{dy}{dx} = f'(x)
                    \]
                    where the left side now represents the ratio of differentials.
                </p>
                <p>    
                    As \(dx \to 0\), the differential \(dy\) becomes an increasingly accurate approximation of the true change 
                    \(\Delta y = f(x + dx) - f(x)\). More rigorously, we express this using the asymptotic notation:
                    \[
                    f(x + dx) - f(x) = f'(x)dx + o(dx).
                    \]
                    where \(o(dx)\) represents higher-order terms that vanish faster than \(dx\) as \(dx \to 0\).
                    <br>
                    This highlights that the differential \(dy = f'(x)dx\) serves as a linear approximation to \(\Delta y\).
                </p>
                
                <p>
                    More generally, the differential of \(f\) can be expressed as:
                    \[
                    df = f(x + dx) - f(x) = f'(x)dx,
                    \] 
                    where \(df\) represents the linearized change in \(f(x)\) due to an infinitesimally small change in \(x\).
                </p>

                <p>
                    Here, \(df\) is the change in the output, \(dx\) is the change in the input, and most importantly, 
                    \(f'(x)\) acts as a <strong>linear operator</strong> that maps \(dx\) to \(df\). 
                </p>
                
                <div class="insight-box">
                        <h3>Derivative as an Operator</h3>
                        <p>
                            In foundational calculus, we often view the derivative \(f'(x)\) as a static value (the slope). However, 
                            in high-dimensional computer science, we must view it as a <strong>linear operator</strong>. 
                        </p>
                        <p>
                            In the equation \(df = f'(x)dx\), the derivative acts as a "transformer" that maps a small displacement 
                            in the input space (\(dx\)) to a displacement in the output space (\(df\)). This perspective is vital 
                            when \(x\) is no longer a scalar, but a vector \(\vec{x} \in \mathbb{R}^n\) or a matrix \(X \in \mathbb{R}^{m \times n}\), 
                            leading directly to the concept of the <strong>Jacobian</strong> and <strong>Gradient</strong>.
                        </p>
                </div>
            </section>
        
            <section id="ex1" class="section-content">
                <h2>Vector Derivatives: Optimization Foundations</h2>

                <p>
                    In machine learning, we rarely optimize single variables. We optimize weight <strong>vectors</strong>. Applying differential 
                    notation to vectors requires careful attention to dimensions and the <strong>Product Rule</strong>.
                </p>

                <div class="theorem">
                        <span class="theorem-title">Theorem: Differential Product Rule</span>  
                        If both \(g\) and \(h\) are differentiable mappings, the differential of their product \(f(x) = g(x)h(x)\) is:
                        \[
                        df = (dg)h + g(dh). \tag{1}
                        \]
                </div>

                <div class="proof">
                        <span class="proof-title">Quick derivation:</span>  
                        <p>
                        \[
                            \begin{align*}
                            df &= g(x+dx)h(x+dx) - g(x)h(x) \\\\
                            &= [g(x) +dg][h(x) + dh]-g(x)h(x) \\\\
                            &= g(x)h(x) + (dg)h + g(dh) + (dg)(dh) -g(x)h(x)
                            \end{align*}
                            \]
                            Then \((dg)(dh)\) is negligible, and we get (1).
                        </p>
                </div>

                <h3>Example 1: Squared \(L_2\) Norm \(f(x) = x^Tx\), \(x \in \mathbb{R}^n\)</h3>

                <p>
                    This function represents the squared distance from the origin — a core component of <strong>Mean Squared Error (MSE)</strong>.
                    In this case, the input is the vector \(x\), and the output is the scalar \(x^Tx\).
                </p>

                <p>
                    To compute the derivative of this function, we start with:
                    \[
                    f(x) = x^Tx = \sum_{i=1}^n x_i^2 ,
                    \]
                    where \(x_i\) is the \(i\)-th entry of the vector \(x\).
                </p>

                <p>
                    Then the <strong>gradient</strong> of \(f\), denoted by  \(\nabla f\) is:
                    \[
                    \nabla f = \begin{bmatrix}
                                \frac{\partial f }{\partial x_1} \\ 
                                \frac{\partial f }{\partial x_2} \\ 
                                \vdots \\
                                \frac{\partial f }{\partial x_n}
                                \end{bmatrix}
                            =  \begin{bmatrix} 2x_1 \\ 2x_2\\ \vdots \\  2x_n \end{bmatrix}
                            = 2x
                    \]      
                </p>

                <p>
                    <strong>Note: Input: vector & Output: scalar \(\Longrightarrow\) First derivative: column vector (gradient).</strong>
                </p>

                <p>
                    Now, let's derive the same result using differential notation. Note: \(dx \in \mathbb{R}^n\).
                </p>

                <div class="proof">
                        <span class="proof-title">Example 1:</span>  
                <p>
                    By the <strong>product rule</strong>, and the commutativity of the vector inner product:
                    \[
                    \begin{align*}
                    d(x^Tx) &= (dx^T)x + x^T(dx) \\\\
                            &= x^Tdx + x^Tdx \\\\
                            &= 2x^Tdx.
                    \end{align*}
                    \]
                    Note: \(dx^T = (dx)^T\) because
                    \[\begin{align*}
                    d(x^T) &= (x + dx)^T - x^T \\\\
                        &= x^T + (dx)^T - x^T \\\\
                        &= (dx)^T
                    \end{align*}.
                    \]
                    Thus the gradient is 
                    \[
                    \nabla f = (2x^T)^T = 2x.
                    \]
                    Note: \(2x^T\) is a "row" vector and to get a column vector \(\nabla f\), we need the transpose of \(2x^T\).
                </p>
            </section>

            <section id="ex2" class="section-content">
                <h2>Example 2: Quadratic Forms \(f(x) = x^TAx\)</h2>

                <p>
                    <strong>Quadratic forms</strong> are essential for modeling local curvature — specifically, they form the 
                    basis of the <strong>Hessian matrix</strong> in optimization. Let \(x \in \mathbb{R}^n\) and 
                    \(A \in \mathbb{R}^{n \times n}\).
                </p>

                <div class="proof">
                    <span class="proof-title">Example 2:</span>  
                    <p>
                        Using the differential representation to analyze the change:
                        \[
                        \begin{align*}
                        df &= f(x + dx) - f(x) \\\\
                        &= (x + dx)^T A (x + dx) - x^T A x \\\\
                        &= x^TAx + dx^T A x + x^TAdx + dx^T A dx - x^T A x 
                        \end{align*}
                        \]
                        Here, it is valid to ignore the higher-order term \(dx^T A dx\), which becomes negligible as \(dx \to 0\). Then
                        \[
                        df = dx^TAx + x^TAdx.
                        \]
                        since \(dx^TAx\) is a scalar, \((dx^TAx)^T = x^TA^Tdx\). Then we get:
                        \[
                        df = x^TA^Tdx + x^TAdx = x^T(A^T + A)dx
                        \]
                        Here, 
                        \[
                        (A^T + A)^T = A + A^T =  (A^T + A)
                        \].
                        So, \((A^T + A)^T\) is symmetric and thus: 
                        \[
                        \nabla f = (A+A^T)x.
                        \]
                    </p>
                </div>

                <div class="insight-box">
                    <h3>Symmetry and Optimization Efficiency</h3>
                    <p>
                        In most machine learning contexts, such as the Second-Order Taylor Expansion of a loss function, the matrix \(A\) is 
                        the Hessian, which is <strong>symmetric</strong> (\(A = A^T\)). In this case, the gradient simplifies to:
                        \[
                        \nabla f = (A+A^T)x = (A+A)x  = 2Ax.
                        \]
                        This symmetry is not just a mathematical curiosity; it is exploited by solvers 
                        to reduce memory overhead and computational complexity by nearly half.
                    </p>
                    <p>
                         Moreover, now we can see Example 1 as a special case: \(A\) is an <strong>identity matrix</strong>. 
                    </p>
                </div>

            </section>

            <section id="ex3" class="section-content">
                <h2>Example 3: The \(L_2\) Norm \(f(x) = \| x \|_2\)</h2>
                <p>
                    The \(L_2\) norm, or Euclidean distance, is the most common regularization term used to prevent overfitting 
                    in AI models. Understanding its derivative is key to understanding how weight decay works.
                </p>

                <div class="proof">
                    <span class="proof-title">Example 3:</span>  
                    <p>
                        Let \(r = \| x \|_2\). We know from Example 1 that \(r^2 = x^Tx\). Taking the differential of both sides:
                        \[
                        \begin{align*}
                        d(r^2) &= d(x^Tx) \\\\
                        2rdr   &= 2x^Tdx \quad \text{(Using the result from Example 1)} \\\\
                        dr     &= \frac{x^T}{r}dx = \frac{x^T}{\| x \|_2}dx
                        \end{align*}
                        \]
                        Therefore, the gradient is:
                        \[
                        \nabla \| x \|_2 = \frac{x}{\| x \|_2}
                        \]
                    </p>
                </div>

                <div class="insight-box">
                    <h3>Geometric Interpretation: The Unit Vector</h3>
                    <p>
                        The derivative of the \(L_2\) norm is simply the <strong>unit vector</strong> pointing in the direction of \(x\). 
                    </p>
                    <p>
                        In <strong>Gradient Descent</strong>, this means that the force of regularization is independent of the magnitude 
                        of the weights in terms of direction; it always pulls the weights directly toward the origin with a constant 
                        pressure proportional to the learning rate. This is the mathematical reason why \(L_2\) regularization effectively 
                        "shrinks" weights but rarely sets them to exactly zero (unlike \(L_1\) regularization).
                    </p>
                </div>

            </section>

        </div>

        <script src="/js/main.js"></script>   
    </body>
</html>