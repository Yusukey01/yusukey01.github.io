<!DOCTYPE html>
<html>
    <head> 
        <title>The Derivative of \(f:\mathbb{R}^n \rightarrow \mathbb{R}\)</title>
        <link rel="stylesheet" href="../styles.css">
        <script id="MathJax-script" defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <meta name="viewport" content="width=device-width, initial-scale=1">
    </head>
    <body> 
        <nav class="navbar">
            <div class="logo">
                <h1>Section II - Calculus to Optimization & Analysis</h1>
            </div>
            <ul class="nav-links">
                <li><a href="../../index.html">Home</a></li>
                <li><a href="../Linear_algebra/linear_algebra.html">Linear Algebra</a></li>
                <li><a href="calculus.html">Calculus to Optimization & Analysis</a></li>
                <li><a href="../Probability/probability.html">Probability & Statistics</a></li>
                <li><a href="../Discrete/discrete_math.html">Discrete Mathematics & Algorithms</a></li>
            </ul>
            <div class="menu-toggle">&#9776;</div>
        </nav>

        <div class="hero-section">
            <h1 class="webpage-name">The Derivative of \(f:\mathbb{R}^n \rightarrow \mathbb{R}\)
            </h1>
        </div>

        <div class="topic-nav">
            <a href="#lapp">Linear Approximations</a>
            <a href="#diff">Differentials</a>
            <a href="#ex1">Example 1: \(f(x) = x^Tx\) where \(x \in \mathbb{R}^n\)</a>
            <a href="#ex2">Example 2: \(f(x) = x^TAx\) where \(x \in \mathbb{R}^n\) and \(A \in \mathbb{R}^{n \times n}\)</a>
            <a href="#ex3">Example 3: \(f(x) = \| x \|_2\) where \(x \in \mathbb{R}^n\)</a>   
        </div> 

        <div class="container">  
           
            <section id="lapp" class="section-content">
            <h2>Linear Approximations</h2>
            <p>
            <strong>Linear approximation</strong> is a process of approximation a function (\f(x)\) near a point 
            \(x_o\) using a linear function. It simplifies complex functions locally.
            \[
            L(x) = f(x_o) + f'(x_o)(x -x_o) \approx f(x) \tag{1}
            \]
            where \(f'(x_o) = \lim_{x \to x_o} \frac{f(x)-f(x_o)}{x-x_o}\) is derivative at \(x_o\). 
            <br>
            Equivalently, from equation (1), 
            \[
            f(x) - f(x_o) \approx f'(x_o)(x -x_o) 
            \]
            <br>
            \(L(x)\) is called the <strong>linearization</strong> of \(f\) at \(x_o\). the graph of \(L\) is 
            the tangent line at \((x_o, f(x_o))\). Linear approximations form the foundation of differentiation 
            and provide a local linear model for \(f(x)\). This concept extends naturally to  differentials.
            </p>
            </section>
    
            <section id="diff" class="section-content">
            <h2>Differentials</h2>
            <p>
            <strong>Differentials</strong> describe infinitesimally small changes in quantities. 
            If \(y = f(x)\), where \(f\) is a differentiable function, then the differential \(dx\) is 
            an independent variable and the differential \(dy\) is defined as 
            \[
            dy = f'(x)dx. 
            \]
            In practice, \(dx\) and \(dy\) are arbitrary small numbers. Also, if \(dx \neq 0\), then 
            we recover the familiar derivative form:
            \[
            \frac{dy}{dx} = f'(x)
            \]
            where the left side now represents the ratio of differentials.
            <br><br>
            The exact change in \(y\) corresponding to change in \(x\),(\(dx\)) is given by:
            \[
            \delta y  = f(x + dx) -f(x).  
            \]
            As \(dx \to 0\), the approximation improves
            \[
            dy = f'(x)dx \approx \delta y = f(x + dx) -f(x).
            \] 
            More precisely, the relationship can be written as:
            \[
            f(x + dx) - f(x) = f'(x)dx + o(dx),
            \]
            where \(o(dx)\) is the asymptotic notation, which represents higher-order terms that become negligible as \(dx \to 0\). 
            This highlights that the differential \(dy = f'(x)dx\) serves as a linear approximation to \(\delta y\).
            <br><br>
            More generally, the differential of \(f\) can be expressed as:
            \[
            df = f(x + dx) - f(x) = f'(x)dx,
            \] 
            where \(df\) represents the linearized change in \(f(x)\) due to an infinitesimally small change in \(x\).
            <br><br>
            Here, \(df\) is the change in the output, \(dx\) is the change in the input, and most importantly, 
            \(f'(x)\) acts as a <strong>linear operator</strong> that maps \(dx\) to \(df\). 
            <br>
            The flexibility of differential notation extends naturally to <strong>linear algebra</strong>, where derivatives 
            apply not only to scalars \(x \in \mathbb{R}\), but also to vectors \(\vec{x} \in \mathbb{R}^n\) and 
            matrices \(X \in \mathbb{R}^{m \times n}\). 
            </p>
            </section>
        
            <section id="ex1" class="section-content">
            <h2>Example 1: \(f(x) = x^Tx\) where \(x \in \mathbb{R}^n\)</h2>
            <p>
            In this case, the input is the vector \(x\), and the output is the scalar \(x^Tx\).
            To compute the derivative of this function, we start with:
            \[
            f(x) = x^Tx = \sum_{i=1}^n x_i^2 ,
            \]
            where \(x_i\) is the \(i\)-th entry of the vector \(x\).
            Then the <strong>gradient</strong> of \(f\) is:
            \[
            \nabla f = \begin{bmatrix}
                        \frac{\partial f }{\partial x_1} \\ 
                        \frac{\partial f }{\partial x_2} \\ 
                        \vdots \\
                        \frac{\partial f }{\partial x_n}
                        \end{bmatrix}
                     =  \begin{bmatrix} 2x_1 \\ 2x_2\\ \vdots \\  2x_n \end{bmatrix}
                     = 2x
            \]
            <strong>Note: Input: vector & Output: scalar \(\Longrightarrow\) First derivative: column vector (gradient).</strong>
            <br>
            Now, let's derive the same result using differential notation. Note: \(dx \in \mathbb{R}^n\).
            <br><br>
            By the <strong>product rule</strong>, and the commutativity of the vector inner product:
            \[
            \begin{align*}
            d(x^Tx) &= (dx^T)x + x^T(dx) \\\\
                    &= x^Tdx + x^Tdx \\\\
                    &= 2x^Tdx.
            \end{align*}
            \]
            Note: \(dx^T = (dx)^T\) becuase 
            \[\begin{align*}
            d(x^T) &= (x + dx)^T - x^T \\\\
                   &= x^T + (dx)^T - x^T \\\\
                   &= (dx)^T
            \end{align*}.
            \]
            Thus the gradient is 
            \[
            \nabla f = (2x^T)^T = 2x.
            \]
            Note: \(2x^T\) is a "row" vector and to get a column vector \(\nabla f\), we need the transpose of \(2x^T\).
            <div class="theorem">
                <span class="theorem-title">Theorem 1: Product rule</span>  
                If both \(g\) and \(h\) are differentiable and let \(f(x) = g(x)h(x)\), then 
                \[
                df = (dg)h + g(dh). \tag{1}
                \]
                Quick derivation: 
                \[
                \begin{align*}
                df &= g(x+dx)h(x+dx) - g(x)h(x) \\\\
                   &= [g(x) +dg][h(x) + dh]-g(x)h(x) \\\\
                   &= g(x)h(x) + (dg)h + g(dh) + (dg)(dh) -g(x)h(x)
                \end{align*}
                \]
                Then \((dg)(dh)\) is negligible, and we get (1).
            </div>
            </p>
            </section>

            <section id="ex2" class="section-content">
            <h2>Example 2: \(f(x) = x^TAx\) where \(x \in \mathbb{R}^n\) and \(A \in \mathbb{R}^{n \times n}\)</h2>
            <p>
            Let's use the differential representation: 
            \[\begin{align*}
            df &= f(x + dx) -f(x) \\\\
               &= (x + dx)^T A (x + dx) - x^T A x
            \end{align*}
            \]
            Expanding this expression, we get:
            \[
            df = x^TAx + dx^T A x + x^TAdx + dx^T A dx - x^T A x 
            \]
            It is valid to ignore the higher-order term \(dx^T A dx\), which becomes negligible as \(dx \to 0\).
            Then
            \[
            df = dx^TAx + x^TAdx.
            \]
            since \(dx^TAx\) is a scalar, \((dx^TAx)^T = x^TA^Tdx\). Then we get:
            \[
            df = x^TA^Tdx + x^TAdx = x^T(A^T + A)dx
            \]
            Here, \((A^T + A)^T = A + A^T =  (A^T + A)\), so  \((A^T + A)^T\) is symmetric and thus: 
            \[
            \nabla f = (A+A^T)x.
            \]
            if \(A\) is symmetric (\(x^TAx\) is a quadratic form), \(\nabla f = (A+A^T)x = (A+A)x  = 2Ax\).
            <br>
            Also, Example 1 is a special case: \(A\) is an identity matrix. 
            </p>
            </section>

            <section id="ex3" class="section-content">
            <h2>Example 3: \(f(x) = \| x \|_2\) where \(x \in \mathbb{R}^n\)</h2>
            <p>
            Now, it is simple to find the derivative of <strong>\(L_2\) norm</strong>.
            <br>
            Let \(r = \| x \|\),  and then:
            \[
            \begin{align*}
            & r^2 = x^Tx \\\\
            &\Longrightarrow 2rdr = 2x^Tdr \\\\
            &\Longrightarrow  dr = \frac{x^T}{r} = \frac{x^T}{\| x \|} \\\\\
            &\Longrightarrow  \nabla f = \frac{x}{\| x \|}.
            \end{align*}
            \]
            </p>
            </section>
        </div>
        <!-- Footer -->
        <footer>
            <div class="footer-content">
                <div class="footer-links">
                    <h3>Quick Links</h3>
                    <ul>
                        <li><a href="../../index.html">Home</a></li>
                        <li><a href="calculus.html">Section II - Calculus to Optimization & Analysis</a></li>
                        <li><a href="linear_approximation.html">The Derivative</a></li>
                        <li><a href="jacobian.html">The Jacobian</a></li>
                        <li><a href="matrix_cal.html">Matrix Derivatives</a></li>
                        <li><a href="numerical_example1.html">Numerical Examples</a></li>
                        <li><a href="det.html">Scalar Functions of Matrices</a></li>
                        <li><a href="mvt.html">Mean Value Theorem</a></li>
                        <li><a href="gradient.html">Gradient Descent</a></li>
                        <li><a href="constrained_opt.html">Constrained Optimization</a></li>
                        <li><a href="newton.html">Newton's Method</a></li>
                        <li><a href="riemann.html">Riemann Integration</a></li>
                        <li><a href="measure.html">Measure Theory</a></li>
                        <li><a href="lebesgue.html">Lebesgue Integration</a></li>
                        <li><a href="duality.html">Duality in Optimization</a></li>
                    </ul>
                </div>
            </div>
            
            <div class="footer-bottom">
                <p>&copy; 2025 Math-CS Compass. All rights reserved.</p>
            </div>
        </footer>
        <script src="/main.js"></script>    
    </body>
</html>