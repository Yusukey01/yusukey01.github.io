<!DOCTYPE html>
<html>
    <head> 
        <title>Derivative of Determinant</title>
        <link rel="stylesheet" href="../styles.css">
        <script id="MathJax-script" defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <meta name="viewport" content="width=device-width, initial-scale=1">
    </head>
    <body> 
        <nav class="navbar">
            <div class="logo">
                <h1>Section II - Calculus to Optimization & Analysis</h1>
            </div>
            <ul class="nav-links">
                <li><a href="../../index.html">Home</a></li>
                <li><a href="../Linear_algebra/linear_algebra.html">Linear Algebra</a></li>
                <li><a href="calculus.html">Calculus & Optimization</a></li>
                <li><a href="../Probability/probability.html">Probability & Statistics</a></li>
                <li><a href="../Discrete/discrete_math.html">Discrete Mathematics</a></li>
            </ul>
            <div class="menu-toggle">&#9776;</div>
        </nav>

        <div class="hero-section">
            <h1 class="webpage-name">The Derivative of Scalar Functions of Matrices
            </h1>
        </div>

        <div class="topic-nav">
            <a href="#frob">Derivative of the Frobenius norm</a>
            <a href="#det">Derivative of the Determinant</a>
        </div>  
        <div class="container">     
            <section id="frob" class="section-content">
            <h2>Derivative of the Frobenius norm</h2>
            <p>
                Derivatives implicitly rely on <strong>norms</strong> to measure the magnitude of changes in both the input \(dx\) 
                and the output \(df\) ensuring a consistent comparison of their scales.
                <br><br>
                Recall, the <strong>Frobenius norm</strong> of a matrix \(X \in \mathbb{R}^{m \times n}\) is defined as:
                \[
                f(X) = \| X \|_F = \sqrt{\text{tr }(X^TX)} 
                \] 
                (See <a href="../Linear_algebra/trace.html">Section I - Part 10</a>)
                <br>
                Now, taking the differential \(df\):
                <br><br>
                First, by the chain rule:
                \[
                df = \frac{1}{2\sqrt{\text{tr }(X^TX)}}d[\text{tr }(X^TX)]
                \]
                Note: for any matrix \(A\),
                \[
                \begin{align*}
                d(\text{tr }(A)) &= \text{tr }(A + dA) - \text{tr }(A) \\\\\
                                &= \text{tr }(A) + \text{tr }(dA) - \text{tr }(A) \\\\
                                &= \text{tr }(dA)
                \end{align*}
                \]
                Thus:
                \[
                df = \frac{1}{2\sqrt{\text{tr }(X^TX)}}\text{tr }[d(X^TX)]
                \]
                By the product rule:
                \[
                \begin{align*}
                df &= \frac{1}{2\sqrt{\text{tr }(X^TX)}}\text{tr }[dX^TX + X^TdX]\\\\
                &= \frac{1}{2\sqrt{\text{tr }(X^TX)}}\text{tr }(dX^TX) + \text{tr }(X^TdX)
                \end{align*}
                \]
                Since \(\text{tr }(dX^TX)  = \text{tr }((dX^TX)^T) = \text{tr }(X^TdX)\),
                \[ 
                \begin{align*}
                df &= \frac{1}{2\sqrt{\text{tr }(X^TX)}}2\text{tr }(X^TdX)\\\\
                    &= \frac{1}{\sqrt{\text{tr }(X^TX)}}\text{tr }(X^TdX)
                \end{align*}
                \]
                Here, \(\text{tr }(X^TdX)\) represents the <strong>Frobenius inner product</strong> of \(X\) and \(dX\). Then:
                \[
                df = \left\langle \frac{X}{\sqrt{\text{tr }(X^TX)}}, dX \right\rangle_F  \tag{1}
                \]
                Therefore, 
                \[
                \nabla f = \frac{X}{ \| X \|_F}.
                \]
                <br>
                Note:The expression in (1) is equivalent to
                \[
                df = \text{tr }((\nabla f)^TdX) \tag{2}
                \]
                <br>
                The trace operator satisfies linearity and the cyclic property, making it a convenient way to express derivatives 
                in terms of gradients.
                <br>
                For example, consider \(f(A) = x^TAy\) where \(A\) is 
                a \(m \times n\) matrix, \(x \in \mathbb{R}^m\), and  \(y \in \mathbb{R}^n\).
                <br>
                By the product rule,
                \[
                df = x^TdAy
                \]
                Since \(df\) is a scalar,  taking the trace does not change its value:
                \[
                df = \text{tr }(x^TdAy)
                \]
                By the cyclic property of the trace:
                \[
                df = \text{tr }(yx^TdA)
                \]
                Therefore, comparing this with \(df = \text{tr }((\nabla f)^TdA)\), 
                \[
                \nabla f = (yx^T)^T = xy^T
                \]
            </p>
            </section>
            <section id="det" class="section-content">
                <h2>Derivative of the Determinant</h2>
                <p>
                The derivative of the determinant of a square matrix \(A \in \mathbb{R}^{n \times n}\) be expressed 
                using several equivalent forms.  
                <br>
                Recall: 
                \[
                \text{adj }(A) = \text{cofactor }(A)^T = (\det A)A^{-1}.
                \]
                (See <a href="../Linear_algebra/determinants.html">Section I - Part 4</a>)
                This implies:
                \[
                \text{cofactor }(A) = \text{adj }(A)^T = (\det A)(A^{-1})^T.
                \]
                <br><br>
                By the cofactor expansion of the determinant based on \(i\)-th row of \(A\):
                \[
                \det (A) = A_{i1}C_{i1} + A_{i2}C_{i2} +  \cdots + A_{in}C_{in}
                \]
                Thus, \(\frac{\partial \det A}{\partial A_{ij}} = C_{ij} \) and then:
                \[
                \nabla (\det A) = \text{cofactor } (A)
                \]
                Equivalently, using the expression (2): 
                \[
                \begin{align*}
                d (\det A) &= \text{tr }(\text{cofactor }(A)^T dA) \\\\
                        &= \text{tr }(\text{adj }(A) dA) \\\\
                        &= \text{tr }((\det A)A^{-1} dA)  \tag{3}
                \end{align*}
                \]
                Therefore,
                \[
                \nabla (\det A) = \text{cofactor } (A) = \text{adj }(A)^T = (\det A)(A^{-1})^T
                \]
                
                <br>
                Since \(det A\) is a scalar, the expression (3) can be written as:
                \[
                d(\det A) = (\det A)\text{tr }(A^{-1} dA) \tag{4}
                \]
                Consider the scalar function \(p(x) = \det(xI - A)\), which is the characteristic polynomial of \(A\). 
                In practice, when approximating eigenvalues using numerical methods, we often need to compute the derivative 
                of \(p(x)\) at different values of \(x\).
                <br>
                Applying the expression (4), we have:
                \[
                d(\det (xI-A)) = (\det (xI-A)) \text{tr } ((xI-A)^{-1} d(xI -A))
                \]
                Since \(A\) is constant, \(d(xI -A) = dxI\), and also \(dx\) is a scalar. Thus, 
                \[
                d(\det (xI-A)) = (\det (xI-A)) \text{tr } ((xI-A)^{-1})dx
                \]
                <br>
                Note: While the <strong>analytical approach</strong> provides a useful formula for the differential of many functions, in practice,
                <strong>automatic differentiation</strong>(auto-diff) offers a more stable and efficient way to compute 
                the gradient of functions. Auto-diff allows us to compute the derivative with respect to matrix parameters directly 
                without explicitly computing like \(A^{-1}\), which can introduce numerical instability in certain cases.
                <br><br>
                Here's an example using auto-diff in PyTorch vs using the analytical formula to compute the derivative of \(p(x)\):
                <pre class="python-code">
                import torch

                # Random square matrix
                def generate_matrix(n):
                    return torch.randn(n, n, dtype=torch.float64, requires_grad=False)

                # Characteristic polynomial p(x) = det(xI - A)
                def p(x, A):
                    return torch.det(x * torch.eye(A.shape[0], dtype=A.dtype, device=A.device) - A)

                # Derivative of p(x) using auto-differentiation
                def dp_torch(x, A):
                    x_tensor = torch.tensor([x], requires_grad=True, dtype=A.dtype, device=A.device)
                    grad = torch.autograd.grad(p(x_tensor, A), x_tensor)[0]
                    return grad.item()

                # Analytical formula d(p(x)) = (det (xI-A))*tr((xI-A)^-1)dx 
                def dp(x, A):
                    return (
                        p(x, A).item() *
                        torch.trace(
                            torch.inverse(x * torch.eye(A.shape[0], dtype=A.dtype, device=A.device) - A)
                        ).item()
                    )
                </pre>
                </p>
            </section>
        </div>

        <div class="contact-section">
            <a href="../../index.html">Back to Home</a>
            <a href="calculus.html">Back to Calculus</a>
        </div>

        <!-- Footer -->
        <footer>
            <div class="footer-content">
                <div class="footer-links">
                    <h3>Quick Links</h3>
                    <ul>
                        <li><a href="../../index.html">Home</a></li>
                        <li><a href="calculus.html">Calculus to Optimization & Analysis</a></li>
                        <li><a href="linear_approximation.html">The Derivative</a></li>
                        <li><a href="jacobian.html">The Jacobian</a></li>
                        <li><a href="matrix_cal.html">Matrix Derivatives</a></li>
                        <li><a href="numerical_example1.html">Numerical Examples</a></li>
                        <li><a href="det.html">Scalar Functions of Matrices</a></li>
                        <li><a href="mvt.html">Mean Value Theorem</a></li>
                        <li><a href="gradient.html">Gradient Descent</a></li>
                        <li><a href="constrained_opt.html">Constrained Optimization</a></li>
                        <li><a href="newton.html">Newton's Method</a></li>
                        <li><a href="riemann.html">Riemann Integration</a></li>
                        <li><a href="measure.html">Measure Theory</a></li>
                        <li><a href="lebesgue.html">Lebesgue Integration</a></li>
                        <li><a href="duality.html">Duality in Optimization</a></li>
                    </ul>
                </div>
            </div>
            <div class="footer-bottom">
                <p>&copy; 2025 Mathematics Resource Site. All rights reserved.</p>
            </div>
        </footer>    
    </body>
</html>