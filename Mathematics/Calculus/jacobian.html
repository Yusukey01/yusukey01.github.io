<!DOCTYPE html>
<html>
    <head> 
        <title>Orthogonality</title>
        <link rel="stylesheet" href="styles.css" media="print" onload="this.media='all'">
        <script id="MathJax-script" defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <meta name="viewport" content="width=device-width, initial-scale=1">
    </head>
    <body> 
        <div class="toc-container">
            <h2>Contents</h2>
            <ul>
                <li><a href="#jacobian">Jacobian</a></li>
                <li><a href="#chain">Chain Rule</a></li>
                <li><a href="#backp">Backpropagation</a></li>
            </ul>
        </div>
        <h1 id="jacobian">Jacobian</h1>
        <blockquote>
            Consider \(f(x) \in \mathbb{R}^m\), where \(x \in \mathbb{R}^n\). By the differential notation,
            \[
            df = f'(x)dx
            \]
            where \(df \in \mathbb{R}^m \, \), \(dx \in \mathbb{R}^n\) and here, the linear operator \(f'(x)\) is 
            the \(m \times n\) <strong>Jacobian matrix</strong> such that:
            \[
             J_{ij} = \frac{\partial f_i}{\partial x_j}.
            \]
            Here, \(J_{ij}\) represents the rate of change of the \(i\)-th output \(f_i\) with respect to the 
            \(j\)-th input \(x_j\).
            <br><br>
            For example, Let \(f(x) = \begin{bmatrix}f_1(x) \\ f_2(x) \end{bmatrix}\), 
            and \(x = \begin{bmatrix}x_1 \\ x_2 \end{bmatrix} \). Then
            \[
            df = \begin{bmatrix} \frac{\partial f_1}{\partial x_1} & \frac{\partial f_1}{\partial x_2} \\
                                 \frac{\partial f_2}{\partial x_1} & \frac{\partial f_2}{\partial x_2} 
                 \end{bmatrix}
                 \begin{bmatrix} dx_1 \\ dx_2 \end{bmatrix}
              =  \begin{bmatrix} \frac{\partial f_1}{\partial x_1}dx_1 + \frac{\partial f_1}{\partial x_2}dx_2 \\
                                 \frac{\partial f_2}{\partial x_1}dx_1 + \frac{\partial f_2}{\partial x_2}dx_2 
                 \end{bmatrix}.
            \]
            As you can see, the Jacobian matrix \(f'(x)\) acts as a linear operator that maps changes in \(x\) 
            encoded in \(dx\) to corresponding changes in \(f(x)\) encoded in \(df\). 
            <br>
            <strong>Note: Input & Output: vector \(\Longrightarrow\) First derivative: Jacobian matrix.</strong>
            <br><br>
            Instead of expressing the Jacobian component by component, it is often more convenient to use a 
            symbolic representation. 
            <br>
            For example, consider a trivial case: the matrix equation \(f(x) = Ax\) where \(A \in \mathbb{R}^{m \times n}\) 
            is a constant matrix.
            Then 
            \[
            df = f(x + dx) -f(x) = A(x + dx) - Ax = Adx = f'(x)dx
            \]
            Thus \(f'(x) = A \) is the Jacobian matrix. 
            
        </blockquote>

        <h1 id="chain">Chain Rule</h1>
        <blockquote>
            <div class="theorem">
                <span class="theorem-title">Theorem 1: Chain Rule</span> 
                Suppose \(f(x) = g(h(x))\) where both \(g\) and \(h\) are differentiable. Then
                \[
                df = f'(x)[dx] = g'(h(x))[h'(x)[dx]] 
                \]
            </div>
            Intuitively, if we think about a higher dimensional case, it is clear that the chain rule is not commutative in general. 
            <br>
            For example, consider 
                \[
                x \in \mathbb{R}^n, \, h(x) \in \mathbb{R}^p, \, \text{and } g(h(x)) \in \mathbb{R}^m
                \] 
            Then the output must be the \(m \times n\) <strong>Jacobian matrix</strong> \(f'(x)\).
            <br>
            To construct this \(m \times n\) matrix, we must need the product of the \(m \times p\) Jacobian matrix \(g'(h(x))\) 
            and the \(p \times n\) Jacobian matrix h'(x) in this order. 
        </blockquote>

        <h1 id="backp">Backpropagation</h1>
        <blockquote>
            The backpropagation or more generally, <strong>reverse mode automatic differentiation</strong> is a widely used 
            algorithm in machine learning to train <strong>neural networks</strong>. It computes gradients of the loss function 
            with respect to the network's parameters(weights and biases) to minimize the loss, improving model performance.
            <br>
            Backpropagation works by explicitly transmitting <strong>gradients</strong> layer by layer, maintaining the order 
            dictated by the <strong>chain rule</strong>. At each layer (or function), the gradient of the loss is computed with 
            respect to the output of the previous layer.
            <br><br>
            Consider a small neural network, which has three layers(functions): \(f_1, f_2, f_3\). This network represents 
            a composition of functions:
            \[
            L(x) = f_3(f_2(f_1(x)))
            \]
            where \(x \in \mathbb{R}^n\) represents the inputs(parameters) and \(L(x) \in \mathbb{R}^m\) is the 
            output often called the <strong>loss function</strong>. 
            <br><br>
            Suppose \(f_1 \in \mathbb{R}^p\), \(f_2 \in \mathbb{R}^q\), and \(f_2 \in \mathbb{R}^m\).
            <br>
            To compute the derivative of this function, we apply the chain rule:
            \[
            dL = f_3'f_2'f_1'
               = (m \times q)(q \times p)(p \times n)
            \]
            This is the multiplication of <strong>Jacobian matrices</strong> for each layer. 
            <br><br>
            In <strong>reverse</strong> mode, the algorithm compute this expression from the left \(f_3'\) backward.
            So, in the context of neural network structure, the backpropagation process appears to follow a "reverse" order, 
            starting from the output layer and moving backward through the network. However, from a mathematical perspective, 
            this corresponds to a standard left-to-right multiplication of Jacobian matrices.
            <br><br>
            This reverse order makes it computationally efficient for large scale neural networks with many inputs (\(n\)) 
            and a single output (\(m = 1\)).  
            \[
            (f_3'f_2')f_1' = [(1 \times q)(q \times p)](p \times n) = (1 \times p)(p \times n) = (1 \times n) = (\nabla L)^T
            \]
            This efficiency is due to backpropagation requiring only the computation of vector-Jacobian products, which are 
            less computationally expensive than full matrix multiplications. At each step, only the product of a row 
            vector(the transpose of a local gradient) with a Jacobian matrix is calculated.
            <br><br>
            In contrast,in <strong>forward mode</strong> differentiation requires full matrix multiplications.
            \[
            f_3'(f_2'f_1') = (1 \times q)[(q \times p)(p \times n)] = (1 \times q)(q \times n) = (1 \times n) = (\nabla L)^T
            \]
        </blockquote>

        <a href="../../index.html">Back to Home </a>
        <br> <a href="calculus.html">Back to Calculus </a>
    </body>
</html>