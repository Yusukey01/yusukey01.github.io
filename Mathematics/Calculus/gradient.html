<!DOCTYPE html>
<html>
    <head> 
        <title>Gradient Descent</title>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <link rel="stylesheet" href="../styles.css">
        <meta name="viewport" content="width=device-width, initial-scale=1">
    </head>
    <body> 
        <h1>Introduction to Optimization</h1>
        <blockquote>
            In machine learning, parameter estimation as known as <strong>model fitting</strong> requires solving 
            <strong>optimization problem</strong>:
            \[
            \theta^* \in \arg \min_{\theta \in \Theta} \mathcal{L}(\theta), \qquad \Theta \subset \mathbb{R}^d
            \]
            where \(\mathcal{L}(\theta)\) is a loss function(objective function),  \(\Theta\) is a parameter space, and 
            \(d\) is the number of variables being optimized over. 
            <br><br>
            Since finding a global optimum is too expensive or impossible in practice, our target will be a local optimum.
            <br>
            A point \(\theta^*\) is a local minimum if: 
            <br>
            \[
            \exists \, \delta > 0, \, \forall \, \theta \in \Theta \text{ such that } \| \theta - \theta^* \| < \delta, \, \mathcal{L}(\theta^*) \leq  \mathcal{L}(\theta).
            \]
            For a continuously twice differentiable function \(\mathcal{L}(\theta)\), to confirm that \(\theta^*\) is a local 
            minimum, following two conditions must be satisfied:
            <ol>
                <li>The <strong>gradient vector</strong> is equal to zero.</li>
                \[
                g(\theta^*) = \nabla \mathcal{L}(\theta^*) = 0
                \]
                <li>The <strong>Hessian matrix</strong> is positive definite.</li>
                \[
                H(\theta^*) = \nabla^2 \mathcal{L}(\theta^*) \succ 0
                \]
            </ol>
            <br>
            Often we need the <strong>feasible set</strong> \(\mathcal{C}\) as the subset of the parameter space \(\Theta\) that 
            satisfies a set of <strong>constraints</strong>:
            \[
            \mathcal{C} = \{\theta : g_j (\theta) \leq 0 : j \in \mathcal{I}, \, h_k (\theta) = 0 : k \in \mathcal{E}\} \subset \mathbb{R}^d
            \]
            where \(\mathcal{I}\) is a set of inequality constraints and \(\mathcal{E}\) is a set of equality constraints.
            <br><br>
            Usually, we convert the <strong>constrained optimization problem </strong> into an unconstrained one by introducing penalty 
            terms that measure how much we violate each constraint and adding them to the objective function. 
        </blockquote>

        <h1>Convexity</h1>
        <blockquote>
        Usually we design models so that their training objectives are <strong>convex</strong> because in the convex optimization problem, 
        if the local minimum exists, it is actually the "global" minimum. 
        <br><br>
        A set \(\mathcal{S}\) is a convex if for any \(x, x' \in \mathcal{S}\), 
        \[
        \lambda x + (1 - \lambda) x' \in \mathcal{S}, \, \forall \lambda \in [0, 1].
        \]
        A function \(f(x)\) is said to be a <strong>convex function</strong> if it is defined on a convex set and if for any \(x, y \in \mathcal{S}\) and for any 
        \(0 \leq \lambda \leq 1\),
        \[
        f(\lambda x + (1 - \lambda)y) \leq \lambda f(x) + (1 - \lambda) f(y).
        \]
        <div class="theorem">
            <span class="theorem-title">Theorem 1:</span>
            Let \(f: \mathbb{R} \to \mathbb{R}\) be a twice differentiable function.
            Then \(f\) is a convex function if and only if \(f'' \geq 0\) for all \(x \in \mathbb{R}\).
        </div>
        <div class="proof">
            <span class="proof-title">Proof:</span>
            Suppose that \(\forall x \in \mathbb{R}\), a function \(f: \mathbb{R} \to \mathbb{R}\) is twice differentiable and 
            \(f'' \geq 0\).
            <br>
            By Taylor's theorem, 
            \(\forall z, w \in \mathbb{R}, \, \exists c \in \mathbb{R} \) such that 
            \[
            f(w) = f(z) + f'(z)(w -z) + f''(c) \frac{(w-z)^2}{2}.
            \]
            This implies 
            \[
            f(w) \geq f(z) + f'(z)(w-z). \tag{1}
            \]
            Here, consider two point \(a, b \in \mathbb{R}\), and let \(z = \lambda a + (1-\lambda)b, \, \lambda \in [0, 1]\). By inequality (1), 
            for each point, we have 
            \[
            f(a) \geq f(z) + f'(z)(a-z) \Longrightarrow  \lambda f(a) \geq \lambda f(z) + \lambda f'(z)(a-z) \tag{2}
            \]
            and
            \[
            f(b) \geq f(z) + f'(z)(b-z) \Longrightarrow  (1-\lambda )f(b) \geq (1- \lambda )f(z) + (1-\lambda )f'(z)(b-z) \tag{3}
            \]
            By (2) + (3), we obtain:
            \[
            \begin{align*}
            \lambda f(a) + (1-\lambda )f(b) &\geq f(z) + \lambda f'(z)(a-z) (1-\lambda )f(z) + (1-\lambda )f'(z)(b-z) \\\\
                              &= f(z) + f'(z)(\lambda a + b -\lambda a - b + \lambda b - \lambda b) \\\\
                              &= f(z)
            \end{align*}
            \]
            Thus,
            \[
            \lambda f(a) + (1-\lambda )f(b) \geq f(\lambda a + (1-\lambda)b).
            \]
             By definition of the convex function, \(f\) is convex. 
             <br><br>
             Next, assume that \(f\) is convex. Let \(a < b\), then since \(f\) is convex, for all \(x\),
             \[
             f(x) \geq f(a) + f'(a)(x -a) 
             \]
             Substituting \(x = b\), 
             \[
             f(b) \geq f(a) + f'(a)(b -a) \Longrightarrow f'(a) \leq \frac{f(b) - f(a)}{b - a} \tag{4}
              \]
             and similarly, 
             \[
             f(x) \geq f(b) + f'(b)(x -b)
             \]
             Substituting \(x = a\), 
             \[
             f(a) \geq f(b) + f'(b)(a -b) \Longrightarrow f'(b) \geq \frac{f(b) - f(a)}{b - a} \tag{5}
              \]
              (Note: Since a < b, (a-b) < 0.)
              <br>
              From (4) and (5), we get 
              \[
              f'(a) \leq \frac{f(b) - f(a)}{b - a} \leq f'(b).
              \]
              Thus, \(f'\) is <strong>monotonically increasing</strong> and therefore, \(f'' \geq 0\). 
        </div>
        <br>
        You can see convex functions everywhere in machine learning. For example, the cross-entropy loss function
        (See <a href="../Probability/entropy.html">Entropy</a>) in classification, and the ReLU (Rectified Linear Unit), which 
        is a popular activation function in neural networks are convex. 
        <br><br>
        For \(n\) dimensional case, following theorem is a fundamental aspect of optimization problems. 
        <div class="theorem">
            <span class="theorem-title">Theorem 2:</span>
            Suppose \(f: \mathbb{R}^n \to \mathbb{R}\) is \(C^2\). Then \(f\) is convex if and only if the Hessian matrix 
            \(H = \nabla^2 f(x)\) is positive semidefinite for all \(x \in dom(f)\). Furthermore, \(f\) is strictly convex if \(H\) 
            is positive definite. 
        </div> 
        <div class="proof">
            <span class="proof-title">Proof:</span>
            Suppose for any \(x, y \in \mathbb{R}^n\) and \(\lambda \in (0, 1)\), we define  \(h: [0, 1] \to \mathbb{R}\) 
            as follows:
            \[
            h(\lambda) = f(\lambda a + (1-\lambda)b).
            \]
            For all \(z, w, p \in [0, 1]\), let \(\lambda = pz + (1-p)w\). Then
            \[
            h(\lambda) = f((pz + (1-p)w) a + (1-(pz + (1-p)w))b) \leq ph(z) + (1-p)h(w).
            \]
            Thus, \(h\) is a convex function. 
            <br>
            Rewriting \(h\), we have
            \[
            h(\lambda) = f(b + \lambda(a-b)).
            \]
            Taking the second derivative with respect to \(\lambda\), we obtain:
            \[
            \begin{align*}
            &\frac{dh}{d\lambda} = (\nabla f(b + \lambda (a - b)))^T (a - b) \\\\
            &\frac{d^2h}{d\lambda^2} = (a - b)^T (\nabla^2 f(b + \lambda(a - b))) (a - b)
            \end{align*}
            \]
            Here, \(\nabla^2 f(x)\) is the Hessian matrix of \(f\) and \(\frac{d^2h}{d\lambda^2}\) is in quadratic form. 
            <br>
            For \(h\) to be a convex function, we must have:
            \[
            \frac{d^2h}{d\lambda^2} \geq 0 \quad \forall \lambda \in [0, 1].
            \]
            This implies the Hessian matrix is positive semidefinite for all \(x\).
            <br><br>
            For the sake of space, we omit the full proof.
        </div>
        The relationship between the Hessian matrix and convexity is fundamental to many optimization algorithms and 
        is widely applied in both theoretical and practical optimization problems.
        </blockquote>

        <h1>Gradient Descent</h1>
        <blockquote>
            To find a local minimum of an objective function, we update the current point by moving in the direction that   
            the <strong>negative gradient</strong>, \(- \nabla f\). This is because the gradient, \(\nabla f\)  indicates 
            the direction of steepest increase of the function \(f\). By contrast, the negative gradient points in the 
            direction of the <strong>steepest descent</strong> of \(f\). Thus, "iteratively" moving in the direction 
            of \(- \nabla f\) reduces the value of \(f\) and the value will converge to a local minimum.
            
            <div class="theorem">
                <span class="theorem-title">Algorithm 1: GRADIENT_DESCENT</span>
                <strong>Input:</strong> objective function\(f\), tolerance \(\epsilon\);
                <br> 
                <strong>Output:</strong> stationary point \(\theta^*\);
                <br>
                <strong>begin</strong>
                <br>
                &emsp;Set \(k \leftarrow 0\);
                <br>
                &emsp;Choose an initial point \(\theta^{(0)}\);
                <br>
                &emsp;<strong>repeat: </strong>
                <br>
                &emsp;&emsp;&emsp;Compute gradient: \(d^{(k)} = - \nabla f(\theta^{(k)});\)
                <br>
                &emsp;&emsp;&emsp;Updata learning rate: \(\eta^{(k)} = \arg \min f(\theta^{(k)} + \eta d^{(k)});\)
                <br>
                &emsp;&emsp;&emsp;Update parameters:\(\theta^{(k+1)} = \theta^{(k)} + \eta^{(k)} d^{(k)};\)
                <br>
                &emsp;&emsp;&emsp;\( k \leftarrow k + 1 ;\)
                <br>
                &emsp;<strong>until</strong> \(\| \nabla f(\theta^{(k)}) \| < \epsilon\);
                <br>
                &emsp;Output \(\theta^{(k)}\);
                <br>
                <strong>end</strong>
                <br>
                where \(d^{(k)}\) is a descent direction and \(\eta^{(k)}\) is a step size (or learning rate).
            </div>
        </blockquote>

        <h1>Stochastic Gradient Descent</h1>
        <blockquote>
            In a <strong>stochastic optimization</strong>, we minimize the average value of an objective function:
            \[
            \mathcal{L}(\theta) = \mathbb{E }_{q(z)}[\mathcal{L}(\theta, z)]
            \]
            where \(z\) is a random input to the objective function. \(z\) can be a training exampple or just a random noise term.
            At each iteration, we update:
            \[
            \theta^{(k+1)} = \theta^{(k)} - \eta^{(k)} \nabla \mathcal{L}(\theta^{(k)}, z^{(k)}).
            \]
            This this method is called the <strong>stochastic gradient descent</strong>
            <br>
            Note: Assuming the distribution \(q(z)\) is independent of the parameters we want to optimize. 
            <br><br>
            On both GD and SGD, a critical issue is that at each iteration, we have to compute the gradietnt with respect to 
            all data points. If the given data set is huge, the algorithm becomes too expensive. 
            Instead, we introduce the <strong>mini-batch</strong>. Typical mini-batch size can be \(B = 32, 64, 128, \cdots\), 
            depending on the data size \(N\) and we compute an "approximate" gradient using only a small mini-batch(a subset of data). Intuitively, 
            we quickly find a "good enough" direction to improve the objective. 
            <div class="theorem">
                <span class="theorem-title">Algorithm 2: MINI_BATCH_SGD</span>
                <strong>Input:</strong> dataset \(X\), objective function \(f\), tolerance \(\epsilon\), batch size \(B\), learning rate \(\eta\);
                <br> 
                <strong>Output:</strong> stationary point \(\theta^*\);
                <br>
                <strong>begin</strong>
                <br>
                &emsp;Set \(k \leftarrow  0\);
                <br>
                &emsp;Choose an initial point \(\theta^{(0)}\);
                <br>
                &emsp;<strong>repeat</strong>:
                <br>
                &emsp;&emsp;Shuffle the dataset \(X\) randomly;
                <br>
                &emsp;&emsp;Divide \(X\) into mini-batches \(\{B_1, B_2, \cdots, B_m\}\) each of size \(B\);
                <br>
                &emsp;&emsp;<strong>for</strong> each mini-batch \(B_i\):
                <br>
                &emsp;&emsp;&emsp;&emsp;Compute gradient: \(d^{(k)} = - \frac{1}{|B|} \sum_{x \in B_i} \nabla f(x; \theta^{(k)});\)
                <br>
                &emsp;&emsp;&emsp;&emsp;Update parameters: \(\theta^{(k+1)} = \theta^{(k)} + \eta d^{(k)};\)
                <br>
                &emsp;&emsp;<strong>end for</strong>
                <br>
                &emsp;&emsp;Set \( k \leftarrow k + 1;\)
                <br>
                &emsp;<strong>until</strong> \(\| \nabla f(\theta^{(k)}) \| < \epsilon\);
                <br>
                &emsp;Output \(\theta^{(k)}\);
                <br>
                <strong>end</strong>
            </div>
        Note: Mini-batch SGD typically uses a fixed or dynamically decayed learning rate \(\eta\) instead of line search.
        </blockquote>

        <h1>Sub-gradient Descent</h1>
        <blockquote>
        </blockquote>

        <a href="../../index.html">Back to Home </a>
        <br> <a href="calculus.html">Back to Calculus </a>

        
    </body>
</html>