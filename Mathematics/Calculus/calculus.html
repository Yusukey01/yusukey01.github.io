<!DOCTYPE html>
<html>
    <head> 
        <title>Calculus to Optimization & Analysis</title>
        <link rel="stylesheet" href="../styles.css">
        <script id="MathJax-script" defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>  
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="description" content="Explore key calculus concepts essential for optimization, analysis, and machine learning. 
        Topics include derivatives, Jacobians, gradient descent, Newton's method, constrained optimization, measure theory, and Lebesgue integration." />

    </head>
    <body> 
        <h1>Calculus to Optimization & Analysis</h1>
        <blockquote>
            <p style="text-indent: 40px;">
                <strong>Calculus</strong> is essential in various branches of mathematics. This section is designed to take you from fundamental 
                calculus concepts to the advanced techniques essential for <strong>optimization</strong>. Building on the foundation laid in Linear 
                Algebra, this section explores how calculus is applied to analyze and optimize complex systems. we begin with the classical notion of 
                derivatives—ranging from scalar functions to those of vectors and matrices—and gradually introduce <strong>numerical methods</strong>. 
                In doing so, we not only deepen your understanding of calculus but also provide the essential background and <strong>analytical foundations</strong> 
                that lie behind machine learning.
            </p>
        </blockquote>
        <section>
            <h2><a href="linear_approximation.html"><strong>Part 1: The Derivative of \(f:\mathbb{R}^n \rightarrow \mathbb{R}\)</strong></a></h2>
                    
                <p><strong>Key words:</strong></p>
                <div class="keywords">
                    <span>Linear approximation</span>
                    <span>Linearization</span>
                    <span>Differentials</span>
                    <span>Product rule</span>
                    <span>Gradient</span>
                    <span>Quadratic form</span>
                    <span>\(L_2\) norm</span>
                </div>
            <h2><a href="jacobian.html"><strong>Part 2: The Derivative of \(f:\mathbb{R}^n \rightarrow \mathbb{R}^n\)</strong></a></h2> 
                <p><strong>Key words:</strong></p>
                <div class="keywords">
                    <span>Jacobian matrix</span>
                    <span>Chain rule</span>
                    <span>Backpropagation</span>
                    <span>reverse(forward) mode automatic differentiation</span>
                </div>
            
            <h2><a href="matrix_cal.html"><strong>Part 3: The Derivative of \(f:\mathbb{R}^{n \times n} \rightarrow \mathbb{R}^{n \times n}\)</strong></a></h2>
            <p><strong>Key words:</strong></p>
            <div class="keywords">
                <span>Powers of a matrix</span>
                <span>Inverse of a matrix</span>
                <span>LU decomposition</span>
            </div> 

            <h2><a href="numerical_example1.html"><strong>Part 4: Numerical Examples</strong></a></h2>
            <p><strong>Key words:</strong></p>
            <div class="keywords">
                <span>Finite-difference approximation</span>
                <span>Relative error</span>
                <span>Roundoff error</span>
            </div>

            <h2><a href="det.html"><strong>Part 5: The Derivative of Scalar Functions of Matrices </strong></a></h2>
            <p><strong>Key words:</strong></p>
            <div class="keywords">
                <span>Frobenius inner product</span>
                <span>Frobenius norm</span>
                <span>Trace</span>
                <span>Determinant</span>
                <span>Cofactor</span>
                <span>Adjugate</span>
            </div> 

            <h2><a href="mvt.html"><strong>Part 6: The Mean Value Theorem </strong></a></h2>
            <p><strong>Key words:</strong></p>
            <div class="keywords">
                <span>Rolle's Theorem</span>
                <span>Lagrange's Mean Value Theorem</span>
                <span>Cauchy's Mean Value Theorem</span>
                <span>Taylor's Theorem</span>
                <span>Taylor polynomial</span> 
                <span>little-o notation</span>
                <span>Higher-dimensional MVT</span>
            </div>

            <h2><a href="gradient.html"><strong>Part 7: Gradient Descent (First-order Method) </strong></a></h2>
            <p><strong>Key words:</strong></p>
            <div class="keywords">
                <span>Optimization problems</span>
                <span>Convexity</span>
                <span>Gradient Descent (SD)</span>
                <span>Steepest Descent</span>
                <span>Stochastic Gradient Descent (SGD)</span>
                <span>Mini-batch SGD</span>
                <span>Sub-gradient</span>
                <span>Sub-differentiable</span>   
            </div>
            
            <h2><a href="newton.html"><strong>Part 8: Newton's method (Second-order Method) </strong></a></h2>
            <p><strong>Key words:</strong></p>
            <div class="keywords">
                <span>Line search</span>
                <span>Armijo condition(Sufficient decrease condition)</span>
                <span>Curvature condition</span>
                <span>Wolfe conditions</span>
                <span>Newton's method</span>
                <span>Quasi-Newton methods</span>
                <span>BFGS</span>
                <span>Secant condition</span>
                <span>Inverse Hessian approximation</span>
                <span>Limited memory BFGS</span> 
                <span>Rosenbrock function</span>
            </div> 
            
            <h2><a href="constrained_opt.html"><strong>Part 9: Constrained Optimization </strong></a></h2>
            <p><strong>Key words:</strong></p>
            <div class="keywords">
                <span>Constrained optimization problems</span>
                <span>Penalty terms</span>
                <span>Lagrange Multipliers</span>
                <span>Lagrangian</span>
                <span>Karush-Kuhn-Tucker (KKT) conditions</span>
                <span>Active set</span>
                <span>Slack variables</span>
             </div> 

            <h2><a href="riemann.html"><strong>Part 10: Riemann Integration</strong></a></h2>
            <p><strong>Key words:</strong></p>
            <div class="keywords">
                <span>Riemann integral</span>
                <span>Riemann integrable</span>
                <span>Improper Riemann integration</span>
                <span>Dirichlet function</span>
             </div> 
            
             
            <h2><a href="measure.html"><strong>Part 11: Measure Theory with Probability</strong></a></h2>
            <p><strong>Key words:</strong></p>
            <div class="keywords">
                <span>Sample space</span>
                <span>\(\sigma\)-algebra</span>
                <span>Measurable set</span>
                <span>Measurable space</span>
                <span>Measure</span>
                <span>Probability measure</span>
                <span>Probability space</span>
                <span>Countable additivity (\(\sigma\)-additivity)</span>
                <span>Borel \(\sigma\)-algebra</span>
                <span>Borel set</span>
                <span>Lebesgue measure</span>
             </div> 

        <h2><a href="lebesgue.html"><strong>Part 12: Intro to Lebesgue Integration</strong></a></h2>
            <p><strong>Key words:</strong></p>
            <div class="keywords">
               <span>Lebesgue integral</span>
               <span>Characteristic function (Indicator function)</span>
               <span>Almost everywhere(a.e.)</span>
               <span>Simple function</span>
             </div> 
        </section>

        <h2><a href="duality.html"><strong>Part 13: Duality in Optimization & Analysis</strong></a></h2>
            <p><strong>Key words:</strong></p>
            <div class="keywords">
               <span>Duality</span>
               <span>Weak duality</span>
               <span>Strong duality</span>
               <span>Duality gap</span>
               <span>Smoothness</span>
               <span>Lipschitz continuity</span>
               <span>Contraction mapping</span>
               <span>Convergence rate</span>
             </div> 
        </section>
        <br>
        <a href="../../index.html">Back to Home </a>
    </body>
</html>