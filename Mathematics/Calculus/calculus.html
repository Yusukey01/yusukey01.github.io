<!DOCTYPE html>
<html>
    <head> 
        <title>Calculus to Optimization & Analysis</title>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <link rel="stylesheet" href="../styles.css">
        <meta name="viewport" content="width=device-width, initial-scale=1">
    </head>
    <body> 
        <h1>Calculus to Optimization & Analysis</h1>
        <blockquote>
            <p style="text-indent: 40px;">
            <strong>Calculus</strong> is essential in various branches of mathematics, but in this section we will focus on 
            the key areas of calculus that are particularly relevant to machine learning and we assume that readers have already 
            completed Section I: Linear Algebra. We aim to equip readers with the tools needed for optimization, model training, 
            and analysis in the context of machine learning.
            </p>
        </blockquote>
        <section>
            <h2><a href="linear_approximation.html"><strong>Part 1: The Derivative of \(f:\mathbb{R}^n \rightarrow \mathbb{R}\)</strong></a></h2>
                    
                <p><strong>Key words:</strong></p>
                <div class="keywords">
                    <span>Linear approximation</span>
                    <span>Linearization</span>
                    <span>Differentials</span>
                    <span>Product rule</span>
                    <span>Gradient</span>
                    <span>Quadratic form</span>
                    <span>\(L_2\) norm</span>
                </div>
            <h2><a href="jacobian.html"><strong>Part 2: The Derivative of \(f:\mathbb{R}^n \rightarrow \mathbb{R}^n\)</strong></a></h2> 
                <p><strong>Key words:</strong></p>
                <div class="keywords">
                    <span>Jacobian matrix</span>
                    <span>Chain rule</span>
                    <span>Backpropagation</span>
                    <span>reverse(forward) mode automatic differentiation</span>
                </div>
            
            <h2><a href="matrix_cal.html"><strong>Part 3: The Derivative of \(f:\mathbb{R}^{n \times n} \rightarrow \mathbb{R}^{n \times n}\)</strong></a></h2>
            <p><strong>Key words:</strong></p>
            <div class="keywords">
                <span>Powers of a matrix</span>
                <span>Inverse of a matrix</span>
                <span>LU decomposition</span>
            </div> 

            <h2><a href="numerical_example1.html"><strong>Part 4: Numerical Examples</strong></a></h2>
            <p><strong>Key words:</strong></p>
            <div class="keywords">
                <span>Finite-difference approximation</span>
                <span>Relative error</span>
                <span>Roundoff error</span>
            </div>

            <h2><a href="det.html"><strong>Part 5: The Derivative of Scalar Functions of Matrices </strong></a></h2>
            <p><strong>Key words:</strong></p>
            <div class="keywords">
                <span>Frobenius inner product</span>
                <span>Frobenius norm</span>
                <span>Trace</span>
                <span>Determinant</span>
                <span>Cofactor</span>
                <span>Adjugate</span>
            </div> 

            <h2><a href="mvt.html"><strong>Part 6: The Mean Value Theorem </strong></a></h2>
            <p><strong>Key words:</strong></p>
            <div class="keywords">
                <span>Rolle's Theorem</span>
                <span>Lagrange's Mean Value Theorem</span>
                <span>Cauchy's Mean Value Theorem</span>
                <span>Taylor's Theorem</span>
                <span>Taylor polynomial</span> 
                <span>little-o notation</span>
                <span>Higher-dimensional MVT</span>
            </div>

            <h2><a href="gradient.html"><strong>Part 7: Gradient Descent (First-order Method) </strong></a></h2>
            <p><strong>Key words:</strong></p>
            <div class="keywords">
                <span>Optimization problems</span>
                <span>Convexity</span>
                <span>Gradient Descent (SD)</span>
                <span>Stochastic Gradient Descent (SGD)</span>
                <span>Mini-batch SGD</span>
                <span>Sub-gradient</span>
                <span>Sub-differentiable</span>   
            </div>
            
            <h2><a href="newton.html"><strong>Part 8: Newton's method (Second-order Method) </strong></a></h2>
            <p><strong>Key words:</strong></p>
            <div class="keywords">
                <span>Line search</span>
                <span>Armijo condition(Sufficient decrease condition)</span>
                <span>Curvature condition</span>
                <span>Wolfe conditions</span>
                <span>Newton's method</span>
                <span>Quasi-Newton methods</span>
                <span>BFGS</span>
                <span>Secant condition</span>
                <span>Inverse Hessian approximation</span>
                <span>Limited memory BFGS</span> 
                <span>Rosenbrock function</span>
            </div> 
            
            <h2><a href="constrained_opt.html"><strong>Part 9: Constrained Optimization </strong></a></h2>
            <p><strong>Key words:</strong></p>
            <div class="keywords">
                <span>Constrained optimization problems</span>
                <span>Penalty terms</span>
                <span>Lagrange Multipliers</span>
                <span>Lagrangian</span>
                <span>Karush-Kuhn-Tucker (KKT) conditions</span>
                <span>Active set</span>
                <span>Slack variables</span>
             </div> 

        </section>
        <br>
        <a href="../../index.html">Back to Home </a>
    </body>
</html>