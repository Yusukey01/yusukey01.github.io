<!DOCTYPE html>
<html>
    <head> 
        <title>Duality in Optimization & Analysis</title>
        <link rel="stylesheet" href="../styles.css">
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <meta name="viewport" content="width=device-width, initial-scale=1">
    </head>
    <body> 
        <div class="toc-container">
            <h2>Contents</h2>
            <ul>
                <li><a href="#duality">Duality</a></li>
                <li><a href="#Lip">Lipschitz Continuity</a></li>
            </ul>
        </div> 
            
        <h1 id="duality">Duality</h1>
        <blockquote>
            <strong>Duality</strong> is a fundamental concept in optimization, arising in many settings, including machine learning, convex analysis, and game theory. 
            It would allow us to transform a complex <strong>primal problem</strong> into an often simpler <strong>dual problem</strong>, providing insights into optimality, 
            feasibility, and algorithmic efficiency. 
            <br><br>
            Consider a <strong>primal problem</strong>
            \[
            \min_x f(x) \, \text{ subject to } g_i (x) \leq 0,  \, h_j (x) = 0
            \]
            where \(f(x)\) is the objective function, \(g_i(x)\) are inequality constraints, and \(h_i(x)\) are equality 
            constraints. We define the Lagrangian:
            \[
            L(x, \lambda, \nu) = f(x) + \sum_i \lambda_i g_i(x) + \sum_j \nu_j h_j(x)
            \]
            where \(\lambda_i \geq 0\) are the Lagrange multipliers (also called <strong>dual variables</strong>).
            <br><br>
            The <strong>dual problem</strong> is then: 
            \[
            \max_{\lambda \geq 0, \nu} \, \inf_x L( x, \lambda, \nu).
            \]
            The optimal value of the dual problem, \(d^*\), provides a <strong>lower bound</strong> on the optimal value of the primal 
            problem, \(p^*\):
            \[
            d^* \leq p^*.
            \]
            This fundamental result is known as <strong>weak duality</strong>, which always holds for any optimization problem where a 
            valid dual problem is formulated. 
            <br><br>
            The <strong>duality gap</strong> is the difference between the primal and dual optimal values:
            \[
            p^* - d^*.
            \]
            If the gap is zero (\( p^* = d^* \)), we have <strong>strong duality</strong>, meaning the dual problem perfectly represents 
            the primal problem. 

            Strong duality does not always hold, but can be guaranteed under regularity conditions such as <strong>Slater's condition</strong> 
            (which applies to convex problems). This condition states that if the primal problem is convex and there exists a strictly feasible 
            point (i.e., some \( x \) satisfying all constraints strictly), then strong duality holds.
            <br><br>
            In constrained optimization, duality is closely related to the <a href='constrained_opt.html'><strong>KKT conditions</strong></a>. 
            The optimal solution to the dual problem provides the values of the KKT multipliers (Lagrange multipliers), which characterize the 
            constraints' influence on the optimal solution.
            <br><br>
            Note: Duality is widely used in machine learning and statistical learning theory:
            <ul>
                <li>Support Vector Machines (SVMs):</li>
                The dual form of SVM optimization is often solved instead of the primal because it allows for <strong>kernel tricks</strong> in high-dimensional spaces.
                <li>Regularization (Lasso, Ridge, Elastic Net):</li>
                Dual formulations help derive <strong>generalization bounds</strong> computationally efficient optimization methods.
                <li> Convex Optimization in Deep Learning:</li>
                Many training objectives, such as batch normalization and adversarial training, can be analyzed through duality principles.
            </ul>     
        </blockquote>

        <h1 id="Lip">Lipschitz Continuity</h1>
        <blockquote>
            In convex optimization and machine learning, strong duality and efficient optimization often require certain regularity 
            conditions, such as <strong>Lipschitz continuity</strong> (smoothness condition) of the objective function or its 
            gradient. These conditions impact the stability and convergence of optimization algorithms.
            <br><br>
            A function \(f(x)\) is said to be <strong>Lipschitz continuous</strong> if there exists a constant \(L > 0\) such that: 
            \[
            | f(x) - f(y) | \leq L \| x - y \|
            \]
            for all \(x, y\) in the domain. The constat \(L\) is called <strong>Lipschitz constant</strong>.
            <br>
            Moreover, a function is <strong>\(L\)-smooth</strong> if its gradient \(\nabla f(x)\) is Lipschitz continuous:
            \[
            \| \nabla f(x) \ \nabla f(y) \| \leq L \|x -y\|.
            \]
            This smoothness condition ensures that gradient-based optimization behaves predictably.
            <br><br>
            If the gradient is Lipschitz continuous, gradient descent converges at a linear rate. That is, 
            there exists \( 0 < \mu < 1 \) such that:
            \[
            |\mathcal{L}(\theta_{t+1}) - \mathcal{L}(\theta_*) | \leq \mu | \mathcal{L}(\theta_t) - \mathcal{L}(\theta_*)|. 
            \]
            Here, \(\mu\) is called the <strong>rate of convergence</strong>.
            <br><br>
            For example, consider a quadratic loss function:
            \[
            \mathcal{L}(\theta) = \frac{1}{2}\theta^T A \theta + b^T \theta  + c 
            \]
            where \(A\) is positive definite. 
            <br><br>
            Suppose we use <a href="gradient.html"><strong>steepest descent</strong></a> with exact line search. The convergence rate is given by 
            \[
            \mu = \left(\frac{\lambda_{\max} - \lambda_{\min}}{\lambda_{\max} + \lambda_{\min}}\right)^2
            \]
            where \(\lambda_{\max}\) is the largest eigenvalue of \(A\) and \(\lambda_{\min}\) is the smallest eigenvalue 
            of \(A\). 
            <br>
            Or, using the <strong>condition number</strong> \(\kappa = \frac{\lambda_{\max}}{\lambda_{\min}}\), 
            \[
            \mu = \left(\frac{\kappa -1}{\kappa + 1}\right)^2.
            \]
            <br>
            In general, Problems with low condition numbers converge(or train) faster.
        </blockquote>
    <a href="../../index.html">Back to Home </a>
    <br> <a href="calculus.html">Back to Calculus </a>
</body>
</html>