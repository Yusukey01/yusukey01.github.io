<!DOCTYPE html>
<html>
    <head> 
        <title>Duality in Optimization & Analysis</title>
        <link rel="stylesheet" href="../styles.css">
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <meta name="viewport" content="width=device-width, initial-scale=1">
    </head>
    <body> 
        <div class="toc-container">
            <h2>Contents</h2>
            <ul>
                <li><a href="#smooth">Smooth Optimization</a></li>
                <li><a href="#duality">Duality</a></li>
                <li><a href="#Lip">Lipschitz Continuity</a></li>
                <li><a href="#"></a></li>
            </ul>
        </div>
        <h1 id="smooth">Smootheness</h1>
        <blockquote> 
            In smooth optimization, the objective and constraints are continuously differentiable functions.
            For smooth functions, we can quantify the degree of smoothness using the <strong>Lipschitz constant</strong>. In
            the 1d case, this is defined as any constant \(L \geq 0\) such that
            \[
            \forall x_1, x_2 \in \mathbb{R}, \quad  |f(x_1) - f(x_2) | \leq L | x_1 - x_2 |. 
            \]
            For certain convex problems, with a gradient with bounded Lipschitz constant, one can show that gradient
            descent converges at a linear rate. This means that there exists a number \(0 < \mu < 1\) such that 
            \[
            |\mathcal{L}(\theta_{t+1}) \mathcal{L}(\theta_*) | \leq \mu | \mathcal{L}(\theta_t) - \mathcal{L}(\theta_*)|. 
            \]
            Here, \(\mu\) is called the <strong>rate of convergence</strong>.
        </blockquote>
        
        <h1 id="duality">Duality</h1>
        <blockquote>
            <strong>Duality</strong> arises in optimization models in a wide variety of settings. For example, in 
            <a href="constrained_opt.html"><strong>KKT conditions</strong></a> the optimal solution to the dual problem 
            is a vector of KKT multipliers. 
            <br><br>
            Given a <strong>primal problem</strong>
            \[
            \min_x f(x) \, \text{subject to } g_i (x) \leq 0,  \, h_j (x) = 0
            \]
            where \(f(x)\) is the objective function, \(g_i(x)\) are inequality constranints, and \(h_i(x)\) are equality 
            constraints, and we define the Lagrangian:
            \[
            L(x, \lambda, \nu) = f(x) + \sum_i \lambda_i g_i(x) + \sum_j \nu_j h_j(x)
            \]
            where \(\lambda_i \geq 0\) are the Lagrange multipliers (dual variables).
            <br><br>
            Then its <strong>dual problem</strong> is 
            \[
            \max_{\lambda \geq 0, \nu} \inf_x L( x, \lambda, \nu).
            \]
            The optimal value of this dual problem \(d^*\) provides a <strong>lower bound</strong> on the optimal value of the primal problem \(p^*\).
            \[
            d^* \leq p^*
            \]
            we call this condition <strong>weak duality</strong>. Weak duality always holds for any optimization problem where a valid dual problem is 
            formulated. The difference between the primal and dual optimal values, \(p^* - d^*\) is called <strong>duality gap</strong>.  When the 
            primal and dual optimal values are equal,\(p^* =  d^*\), in other words, the duality gap is zero, we call it <strong>strong duality</strong>.
        </blockquote>
        <h1 id="Lip">Lipschitz Continuity</h1>
        <blockquote>
            In convex optimization, strong duality often requires certain regularity conditions such as 
            <strong>Lipschitz continuity</strong>(smoothness condition) of the objective function \(f(x)\) or its gradient. 
            <br><br>
            \(f(x)\) is <strong>Lipschitz continuous</strong> if there exists \(L > 0\) such that 
            \[
            | f(x) - f(y) | \leq L \| x - y \|
            \]
            for all \(x, y\) in the domain. 
            <br>
            Moreover, the function is <strong>\(L\)-smooth</strong> if its gradient \(\nabla f(x)\) is Lipschitz continuous:
            \[
            \| \nabla f(x) \ \nabla f(y) \| \leq L \|x -y\|.
            \]
            This ensures smooth optimization behavior.
        </blockquote>
    <a href="../../index.html">Back to Home </a>
    <br> <a href="calculus.html">Back to Calculus </a>
</body>
</html>