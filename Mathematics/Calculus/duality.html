<!DOCTYPE html>
<html>
    <head> 
        <title>Duality in Optimization & Analysis</title>
        <link rel="stylesheet" href="../styles.css">
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <meta name="viewport" content="width=device-width, initial-scale=1">
    </head>
    <body> 
        <div class="toc-container">
            <h2>Contents</h2>
            <ul>
                <li><a href="#duality">Duality</a></li>
                <li><a href="#Lip">Lipschitz Continuity</a></li>
            </ul>
        </div> 
            
        <h1 id="duality">Duality</h1>
        <blockquote>
            <strong>Duality</strong> is a fundamental concept in optimization, arising in many settings, including machine learning, convex analysis, and game theory. 
            It would allow us to transform a complex <strong>primal problem</strong> into an often simpler <strong>dual problem</strong>, providing insights into optimality, 
            feasibility, and algorithmic efficiency. 
            <br><br>
            Consider a <strong>primal problem</strong>
            \[
            \min_x f(x) \, \text{ subject to } g_i (x) \leq 0,  \, h_j (x) = 0
            \]
            where \(f(x)\) is the objective function, \(g_i(x)\) are inequality constraints, and \(h_i(x)\) are equality 
            constraints. We define the Lagrangian:
            \[
            L(x, \lambda, \nu) = f(x) + \sum_i \lambda_i g_i(x) + \sum_j \nu_j h_j(x)
            \]
            where \(\lambda_i \geq 0\) are the Lagrange multipliers (also called <strong>dual variables</strong>).
            <br><br>
            The <strong>dual problem</strong> is then: 
            \[
            \max_{\lambda \geq 0, \nu} \, \inf_x L( x, \lambda, \nu).
            \]
            The optimal value of the dual problem, \(d^*\), provides a <strong>lower bound</strong> on the optimal value of the primal 
            problem, \(p^*\):
            \[
            d^* \leq p^*.
            \]
            This fundamental result is known as <strong>weak duality</strong>, which always holds for any optimization problem where a 
            valid dual problem is formulated. 
            <br><br>
            The <strong>duality gap</strong> is the difference between the primal and dual optimal values:
            \[
            p^* - d^*.
            \]
            If the gap is zero (\( p^* = d^* \)), we have <strong>strong duality</strong>, meaning the dual problem perfectly represents 
            the primal problem. 

            Strong duality does not always hold, but can be guaranteed under regularity conditions such as <strong>Slater's condition</strong> 
            (which applies to convex problems). This condition states that if the primal problem is convex and there exists a strictly feasible 
            point (i.e., some \( x \) satisfying all constraints strictly), then strong duality holds.
            <br><br>
            In constrained optimization, duality is closely related to the <a href='constrained_opt.html'><strong>KKT conditions</strong></a>. 
            The optimal solution to the dual problem provides the values of the KKT multipliers (Lagrange multipliers), which characterize the 
            constraints' influence on the optimal solution.
            <br><br>
            Note: Duality is widely used in machine learning and statistical learning theory:
            <ul>
                <li>Support Vector Machines (SVMs):</li>
                The dual form of SVM optimization is often solved instead of the primal because it allows for <strong>kernel tricks</strong> in high-dimensional spaces.
                <li>Regularization (Lasso, Ridge, Elastic Net):</li>
                Dual formulations help derive <strong>generalization bounds</strong> computationally efficient optimization methods.
                <li> Convex Optimization in Deep Learning:</li>
                Many training objectives, such as batch normalization and adversarial training, can be analyzed through duality principles.
            </ul>     
        </blockquote>

        <h1 id="Lip">Lipschitz Continuity</h1>
        <blockquote>
            In convex optimization and machine learning, strong duality and efficient optimization often require certain regularity 
            conditions, such as <strong>Lipschitz continuity</strong> (smoothness condition) of the objective function or its 
            gradient. These conditions impact the stability and convergence of optimization algorithms.
            <br><br>
            A function \(f(x)\) is said to be <strong>Lipschitz continuous</strong> if there exists a constant \(L > 0\) such that: 
            \[
            | f(x) - f(y) | \leq L \| x - y \|
            \]
            for all \(x, y\) in the domain. The constat \(L\) is called <strong>Lipschitz constant</strong>.
            <br>
            Moreover, a function is <strong>\(L\)-smooth</strong> if its gradient \(\nabla f(x)\) is Lipschitz continuous:
            \[
            \| \nabla f(x) \ \nabla f(y) \| \leq L \|x -y\|.
            \]
            This smoothness condition ensures that gradient-based optimization behaves predictably.
            <br><br>
            If the gradient is Lipschitz continuous, gradient descent converges at a linear rate. That is, 
            there exists \( 0 < \mu < 1 \) such that:
            \[
            |\mathcal{L}(\theta_{t+1}) - \mathcal{L}(\theta_*) | \leq \mu | \mathcal{L}(\theta_t) - \mathcal{L}(\theta_*)|. 
            \]
            Here, \(\mu\) is called the <strong>rate of convergence</strong>.
            <br><br>
            For example, consider a quadratic loss function:
            \[
            \mathcal{L}(\theta) = \frac{1}{2}\theta^T A \theta + b^T \theta  + c  \tag{1}
            \]
            where \(A\) is positive definite symmetric Hessian of \(f\).
            <br><br>
            Suppose we use <a href="gradient.html"><strong>steepest descent</strong></a> with exact line search. 
            If \(\alpha_k\) is a sufficiently small constant \(\alpha > 0\), the corresponding iteration: 
            \[
            x_{k+1} = x_k - \alpha \nabla f(x_k), \tag{2}
            \]
            can be shown to be contractive within a sphere centered at \(x^*\), so it converges linearly.
            We simplify Equation (1), and adding a suitable constant, we have 
            \[
            f(x) = \frac{1}{2}(x - x^*)A(x-x^*), \quad \nabla f(x) = A(x-x^*).
            \]
            Then for a constant stepsize \(\alpha\), the steepest descent iteration (2) can be written as 
            \[
            x_{k+1} - x^* = (I - \alpha A)(x_k - x^*).
            \]
            For \alpha = frac{2}{\lambda_{\max}}, where \(\lambda_{\max}\) is the largest eigenvalue of \(A\), 
            the matrix \(I - \alpha A\) has eigenvalues strictly within the unit circle and is a contraction 
            with respect to the Euclidean norm. 
            <br>
            The optimal stepsize \(\alpha^*\) is chosen to equate the constractions at \(\lambda_{\max}\) and 
            \(\lambda_{\min}\):
            \[
            | 1 - \alpha^* \lambda_{\min} | = | 1 - \alpha^* \lambda_{\max} |
            \]
            where \(\lambda_{\min}\) is the smallest eigenvalue of \(A\).
            <br>
            Assuming \(1 - \alpha^* \lambda_{\min} \geq 0\) and \( 1 - \alpha^* \lambda_{\max} \leq 0\), we set 
            \[
            1 - \alpha^* \lambda_{\min} = \alpha^* \lambda_{\max} - 1
            \]
            \[
            \alpha^* = \frac{2}{\lambda_{\min} + \lambda_{\max}}.
            \]
            With this stepsize, we have the linear convergence rate estimate
            \[
            \|x_{x+1} - x^* \| \leq \left( \frac{\frac{\lambda_{\max}}{\lambda_{\min}}-1}{\frac{\lambda_{\max}}{\lambda_{\min}}+1}\right) \|x_k - x^* \|.
            \]
            Therefore, the convergence rate is given by 
            \[
            \mu = \left(\frac{\lambda_{\max} - \lambda_{\min}}{\lambda_{\max} + \lambda_{\min}}\right)^2.
            \] 
            <br>
            Or, using the <strong>condition number</strong> \(\kappa = \frac{\lambda_{\max}}{\lambda_{\min}}\), 
            \[
            \mu = \left(\frac{\kappa -1}{\kappa + 1}\right)^2.
            \]
            <br>
            In general, Problems with low condition numbers converge(or train) faster.
        </blockquote>
    <a href="../../index.html">Back to Home </a>
    <br> <a href="calculus.html">Back to Calculus </a>
</body>
</html>