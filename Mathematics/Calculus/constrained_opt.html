<!DOCTYPE html>
<html>
    <head> 
        <title>Constrained Optimization</title>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <link rel="stylesheet" href="../styles.css">
        <meta name="viewport" content="width=device-width, initial-scale=1">
    </head>
    <body> 
        <h1>Constrained Optimization Problem</h1>
        <blockquote>
            The <strong>constrained optimization problem</strong> is given by 
            \[
            \theta^* \in \arg \min_{\theta \in \mathcal{C}} \mathcal{L}(\theta): \mathbb{R}^D \to \mathbb{R}.
            \]
            Here, \(\mathcal{C}\) is the <strong>feasible set</strong> as the subset of the parameter space \(\Theta \in \mathbb{R}^D\) that 
            satisfies a set of <strong>constraints</strong>:
            \[
            \mathcal{C} = \{\theta \in \mathbb{R}^D : g_i (\theta) \leq 0,  i \in \mathcal{I}, \, h_j (\theta) = 0, j \in \mathcal{E} \}.
            \]
            where \(\mathcal{I}\) is a set of inequality constraints and \(\mathcal{E}\) is a set of equality constraints. 
            <br><br>
            Note: Often, we convert the constrained optimization problem into an unconstrained problem by introducing <strong>penalty 
            terms</strong> that measure how much we violate each constraint and adding them to the objective function. 
        </blockquote>

        <h1>Lagrange multipliers</h1>
        <blockquote>
            First, we consider a constrained optimization problem without inequality constraints. 
            <br><br>
            Assume we have only one equality constraint \(h(\theta) = 0\). So, the constraint surface is given by 
            \[
            \mathcal{C} = \{\theta \in \mathbb{R}^D : h(\theta) = 0\}
            \] 
            where \(h: \mathbb{R}^m \to \mathbb{R}\) is a differentiable function.
            <br>
            Consider another point, \(\theta + \epsilon \in \mathcal{C}\) with sufficiently small \(\epsilon \in \mathbb{R}^D\). 
            By the firtst-order Taylor expansion around \(\theta\), 
            \[
            h(\theta + \epsilon) \approx h(\theta) + \epsilon^T \nabla h(\theta).
            \]
            Since both points are on the same constraint surface, \(h(\theta) = h(\theta + \epsilon)\) and thus \(\epsilon^T \nabla h(\theta) \approx 0\). 
            Also, since \(\epsilon\) is parallel to the constraint surface, \(\nabla h(\theta)\) must be orthogonal to it. Therefore, for any point 
            \(\theta \in \mathcal{C}\), \(\nabla h(\theta)\) is orthogonal to the constraint surface \(\mathcal{C}\).
            <br>
            Note: More formally, the <strong>tangent space</strong> of \(\mathcal{C}\) at \(\theta\) consists of all vectors \(\epsilon\) that satisify 
            \(h(\theta + \epsilon) = 0\). By the Taylor expansion, this is equivalent to the set: 
            \[
            T_{\theta} \mathcal{C} = \{\epsilon \in \mathbb{R}^D : \epsilon^T \nabla h(\theta) =0\}.
            \]
            Since \(\nabla h(\theta)\) satisfied \(\epsilon^T \nabla h(\theta) =0\) for all \(\epsilon \in  T_{\theta} \mathcal{C}\), 
            it follows that \(\nabla h(\theta)\) must be orthogonal to the tangent space of \(\mathcal{C}\).
            <br><br><br>
            We are looking for a point \(\theta^* \in \mathcal{C}\) such that the objective \(\mathcal{L}(\theta)\) is minimized. As mentioned above, such 
            a point must satisfy the condition that \(\nabla h(\theta^*)\) is orthogonal to the constraint surface \(\mathcal{C}\). In addition, 
            to minimize \(\mathcal{L}(\theta)\) on the constraint surface, \(\nabla \mathcal{L}(\theta^*)\) also must be orthogonal to 
            the surface. Otherwise, any movement along the surface would decrease the objective, contradicting the assumption 
            that \(\theta^*\) is minimum.
            <br>
            Since both \(\nabla h(\theta)\) and \(\nabla \mathcal{L}(\theta)\) are orthogonal to the constraint surface at \(\theta^*\), they must be 
            parallel(or anti-parallel) to each other. Thus, there exists a constant(<strong>Lagrange multiplier</strong>), \(\lambda^* \in \mathbb{R}\) 
            such that 
            \[
            \nabla \mathcal{L}(\theta^*) = \lambda^* \nabla h(\theta^*) \tag{1}
            \]
            and an objective(or, <strong>Lagrangian</strong>) is given by 
            \[
            L(\theta, \lambda) = \mathcal{L}(\theta) + \lambda h(\theta).
            \]
            <br>
            At a stationary point of the Lagrangian, 
            \[
            \nabla_{\theta, \lambda}  L(\theta, \lambda) = 0
            \]
            which means
            \[
            \lambda \nabla_{\theta} h(\theta) = \nabla \mathcal{L}(\theta), \qquad h(\theta) = 0.
            \]
            Such a point is called a <strong>critical point</strong>, which statisfies \(h(\theta) = 0\) and Equation (1).
            <br>
            For \(m\) equality constraints: 
            \[
            L(\theta, \lambda) = \mathcal{L}(\theta) + \sum_{j=1}^m \lambda_j h_j(\theta).
            \]
            and 
            \[
            \nabla \mathcal{L}(\theta^*) = \lambda^T \nabla h(\theta^*), \quad \lambda \in \mathbb{R}^m
            \]
            Note: The vectors \(\nabla h_j (\theta^*)\) are linearly independent. 
            
        </blockquote>

        <h1>The KKT Conditions</h1>
        <blockquote>
            Next, consider the case where we have a single inequality constraint \(g(\theta) \leq 0\). We create the lower bound \(\mu g(\theta)\), where 
            \(\mu \geq 0\). So, the Lagrangian can be written as 
            \[
            L(\theta, \mu) = \mathcal{L}(\theta) + \mu g(\theta).
            \]
            For multiple inequality constraints, 
            \[
            L(\theta, \mu) = \mathcal{L}(\theta) + \sum_{i} \mu_i g_i(\theta).
            \]
            If \(g_i (\theta) = 0\), then the inequality constraint \(g_i (\theta) \leq 0\) is said to be <strong>active</strong>.
            <br><br>
            In general, for multiple inequality constraints and equality constraints, we obtain the Lagrangian:
            \[
            L(\theta, \mu, \lambda) = \mathcal{L}(\theta) + \sum_{i} \mu_i g_i(\theta) + \sum_{j} \lambda_j h_j(\theta)
            \]
            and our optimization problem becomes
            \[
            \min_{\theta} \max_{\mu \geq 0, \lambda} L(\theta, \mu, \lambda).
            \]
            <div class="theorem">
                <span class="theorem-title">Theorem 1: <strong>Karush-Kuhn-Tucker (KKT) conditions</strong> </span>
                If \(\mathcal{L}\) and \(g\) are <strong>convex</strong>, then all critical points satisfy the following conditions:
                <ol>
                    <li><strong>Feasibility</strong>:</li>
                        \[
                        g(\theta) \leq 0, \quad h(\theta) = 0
                        \]
                        All original constraints must be satisfied.
                    <li><strong>Stationarity</strong>:</li>
                        \[
                        \nabla \mathcal{L}(\theta^*) + \sum_i \mu_i \nabla g_i (\theta^*) + \sum_j \lambda_j \nabla h_j (\theta^*) = 0
                        \]
                        The solution is a stationary point.
                    <li><strong>Dual feasibility</strong></li>
                        \[
                        \mu \geq 0 
                        \]
                        The penalty for the inequality constraint points in the right direction.
                    <li><strong>Complementary slackness</strong></li>
                        \[
                        \mu \odot g = 0
                        \]
                        For each inequality constraint, either 
                        <ul>
                            <li>\(g_i(\theta^*) = 0, \, \mu_i \neq 0\) (The local oplimal \(\theta^*\) lies "on" the constraint boundary. The constraint is active.) </li>
                            or
                            <li>\(g_i(\theta^*) < 0, \, \mu_i = 0\)(The constraint is inactive.)</li>
                        </ul>
                        
                        Note: "\(\odot\)" represents the element-wise product(also known as Hadamard product). 
                </ol>
            </div>
        </blockquote>

        <a href="../../index.html">Back to Home </a>
        <br> <a href="calculus.html">Back to Calculus </a>
    </body>
</html>