---
layout: default
title: Introduction to Reinforcement Learning
level: detail
description: Learn about Reinforcement Learning fundamentals from theory to algorithms including MDPs, Bellman equations, value iteration, policy iteration, temporal difference learning, Q-learning and SARSA.
uses_math: true
uses_python: false
---
<!DOCTYPE html>
<html>
    <body>
        <!-- LearningResource Schema for Content Pages -->
        <script type="application/ld+json">
        {
        "@context": "https://schema.org",
        "@type": "LearningResource",
        "name": "{{ page.title }}",
        "description": "{{ page.description }}",
        "learningResourceType": "lesson",
        "educationalUse": "instruction",
        "educationalLevel": "university",
        "interactivityType": "{% if page.content contains 'Interactive Demo' or page.content contains 'demo' %}active{% else %}expositive{% endif %}",
        "author": {
            "@type": "Person",
            "name": "Yusuke Yokota",
            "jobTitle": "Mathematics & Computer Science Educator"
        },
        "publisher": {
            "@type": "Organization",
            "name": "MATH-CS COMPASS",
            "url": "https://yusukey01.github.io"
        },
        "about": [
            { "@type": "Thing", "name": "Reinforcement Learning" },
            { "@type": "Thing", "name": "Markov Decision Process" },
            { "@type": "Thing", "name": "MDP" },
            { "@type": "Thing", "name": "Value Functions" },
            { "@type": "Thing", "name": "Policy Optimization" },
            { "@type": "Thing", "name": "Bellman Equations" },
            { "@type": "Thing", "name": "Value Iteration" },
            { "@type": "Thing", "name": "Policy Iteration" },
            { "@type": "Thing", "name": "Temporal Difference Learning" },
            { "@type": "Thing", "name": "Q-Learning" },
            { "@type": "Thing", "name": "SARSA" },
            { "@type": "Thing", "name": "Dynamic Programming" },
            { "@type": "Thing", "name": "Model-Based Reinforcement Learning" },
            { "@type": "Thing", "name": "Model-Free Reinforcement Learning" },
            { "@type": "Thing", "name": "Exploration vs Exploitation" },
            { "@type": "Thing", "name": "Policy Gradient Methods" },
            { "@type": "Thing", "name": "Actor-Critic Methods" }
        ],
        "teaches": [
            "Markov Decision Process formulation",
            "Bellman equations and value functions",
            "Classical planning algorithms",
            "Reinforcement learning algorithms",
            "Temporal difference learning methods",
            "Policy optimization techniques",
            "RL algorithm classification"
        ],
        "isPartOf": {
            "@type": "Course",
            "name": "Machine Learning",
            "description": "Explore machine learning ideas and applications with mathematical foundations",
            "provider": {
                "@type": "Organization",
                "name": "MATH-CS COMPASS",
                "url": "https://yusukey01.github.io"
            },
            "instructor": {
                "@type": "Person",
                "name": "Yusuke Yokota",
                "jobTitle": "Mathematics & Computer Science Educator"
            },
            "courseCode": "V",
            "hasCourseInstance": {
                "@type": "CourseInstance",
                "courseMode": "online",
                "courseWorkload": "PT5H",
                "instructor": {
                    "@type": "Person",
                    "name": "Yusuke Yokota"
                }
            },
            "offers": {
                "@type": "Offer",
                "price": "0",
                "priceCurrency": "USD",
                "availability": "https://schema.org/InStock",
                "category": "free"
            }
        }
        }
        </script>
        
        <!-- WebApplication Schema for Interactive Content -->
        <script type="application/ld+json">
        {
        "@context": "https://schema.org",
        "@type": "WebApplication",
        "name": "{{ page.title }} Interactive Demo",
        "description": "Interactive demonstration of reinforcement learning concepts including MDPs and algorithms",
        "applicationCategory": "EducationalApplication",
        "operatingSystem": "Web Browser",
        "url": "https://yusukey01.github.io{{ page.url }}",
        "author": {
            "@type": "Person",
            "name": "Yusuke Yokota"
        },
        "applicationSubCategory": "Mathematical Visualization",
        "featureList": [
            "Interactive mathematical visualization",
            "Real-time parameter adjustment",
            "Educational demonstrations"
        ],
        "isAccessibleForFree": true,
        "educationalUse": "instruction",
        "educationalLevel": "university"
        }
        </script>
        
        <div class="hero-section">
            <h1 class="webpage-name">Introduction to Reinforcement Learning(RL)</h1>
        </div>

        {% include section_navigation.html %}

        <div class="topic-nav">
            <a href="#intro">Introduction</a>
            <a href="#taxonomy">Classification of RL Algorithms</a>
            <a href="#mdp">Markov Decision Process</a>
            <a href="#value-functions">Value Functions & Bellman Equations</a>
            <a href="#DP">Dynamic Programming Algorithms for RL</a>
            <a href="#td-learning">Temporal Difference Learning</a>
            <a href="#value-based">Value-Based Methods</a>
            <a href="#policy-based">Policy-Based Methods</a>
            <a href="#actor-critic">Actor-Critic Methods</a>
            <a href="#exploration">Exploration vs. Exploitation</a>
        </div>

        <div class="container">  

            <section id="intro" class="section-content">
                <h2>Introduction</h2>

                <p>
                    <strong>Reinforcement Learning (RL)</strong> is a machine learning framework in which an <strong>agent</strong> interacts 
                    with an environment to learn a behavior that maximizes a scalar <strong>reward</strong> signal over time. 
                    Formally, the environment is typically modeled as a <strong>Markov Decision Process (MDP)</strong>, defined by a tuple 
                    \[
                    (\mathcal{S}, \mathcal{A}, \mathcal{T}, r, \gamma)
                    \]
                    where \(\mathcal{S}\) is the state space, \(\mathcal{A}\) is the action space, \(\mathcal{T}\) is the transition function, 
                    \(r\) is the reward function, and \(\gamma\) is the discount factor.
                </p>

                <p>
                    At each time step, the agent observes a state \(s_t \in \mathcal{S}\), selects an action \(a_t \in \mathcal{A}\), receives a 
                    reward \(r_t\), and transitions to a new state \(s_{t+1}\) according to the transition dynamics \(\mathcal{T}(s_{t+1} \mid s_t, a_t)\). 
                    The goal of the agent is to learn a <strong>policy</strong> \(\pi(a \mid s)\) that maximizes the expected cumulative reward:
                    \[
                    \mathbb{E}_\pi \left[ \sum_{t=0}^\infty \gamma^t r_t \right].
                    \]
                </p>

                <p>
                    Reinforcement learning differs fundamentally from other machine learning paradigms:
                    <ul style="padding-left: 40px;">
                        <li><strong>Supervised learning</strong> trains models on ground-truth input-output pairs with explicit correct labels for each input.</li>
                        <li> <strong>Unsupervised learning</strong> discovers hidden structure or representations in unlabeled data without any feedback signal.</li>
                        <li> <strong>Reinforcement learning</strong> uses only a scalar reward signal that provides evaluative feedback based on the agent's interaction with an environment.</li>
                    </ul>
                </p>
              
                <p>
                    In RL, learning is driven by trial-and-error interaction rather than labeled examples. Unlike supervised learning, there are no correct labels for each input, 
                    and unlike unsupervised learning, the objective is not to model data structure but to learn a policy that maximizes expected cumulative reward over time.
                </p>

                <p>
                    RL has been successfully applied in domains such as robotics, recommendation systems, and game-playing.
                    More recently, RL has also become integral to the training of modern <strong>large language models (LLMs)</strong>. 
                    In particular, a method called <strong>Reinforcement Learning from Human Feedback (RLHF)</strong> is used to fine-tune LLMs 
                    to follow <strong>human preferences</strong>, ensuring outputs are more helpful, safe, and better aligned with user intent.
                </p>

                <p>
                    The diversity of RL applications has led to a rich classification of algorithms, each designed to address different aspects of 
                    the learning problem. Understanding this classification is crucial for selecting appropriate methods and comprehending the 
                    relationships between different approaches.
                </p>     

            </section>

            <section id="taxonomy" class="section-content">
                <h2>Classification of RL Algorithms</h2>

                <h3>Model-Based vs Model-Free Methods</h3>
                <p>
                    The most fundamental distinction in RL concerns whether the agent learns an explicit model of the environment.
                </p>

                <p>
                    <strong>Model-based RL</strong> methods learn an explicit model of the environment from data, including:
                    <ul style="padding-left: 40px;">
                        <li>the transition dynamics \(p_T(s' \mid s, a)\), and</li>
                        <li>the reward model \(p_R(r \mid s, a, s')\).</li>
                    </ul>
                </p>

                <p>
                    Once learned, the agent uses <strong>planning</strong> algorithms to compute an optimal policy \(\pi_*\). 
                    Classical planning is based on dynamic programming such as <strong>value iteration(VI)</strong> and <strong>policy iteration(PI)</strong>. 
                    The main advantage is <strong>sample efficiency</strong> - the agent can simulate many trajectories using the learned model without additional 
                    environment interaction. However, model learning introduces <strong>model bias</strong>: if the learned model is inaccurate, the agent 
                    may perform well in simulation but poorly in the real environment.
                </p>

                <p>
                    <strong>Model-free RL</strong> methods bypass explicit environment modeling and directly learn value functions or policies 
                    from experience. These methods are generally simpler to implement and more robust to model errors, but typically require 
                    more environment interactions compared to model-based approaches.
                </p>

                <h3>Value-Based vs Policy-Based</h3>
                <p>
                    This taxonomy classifies algorithms based on <strong>what they learn</strong> in the model-free RL.
                </p>

                <p>
                    <strong>Value-based methods</strong> learn the optimal Q-function from experience, and then derive 
                    policies implicitly:
                    \[
                    \begin{align*}
                    \pi_*(s) &= \arg\max_a Q_*(s,a) \\\\
                             &= \arg\max_a [R(s, a) + \gamma \mathbb{E}_{p_T(s' \mid s, a)}[V_*(s')]]
                    \end{align*}
                    \]
                    (e.g.,<strong>Q-learning</strong>, <strong>SARSA</strong>, and <strong>Deep Q-Networks(DQN)</strong>.)
                </p>

                <p>
                    <strong>Policy-based methods</strong> directly optimize \(J(\pi_{\mathbf{\theta}})\) with respect to the 
                    policy parameter \(\mathbf{\theta}\) using <strong>policy gradient</strong> and can naturally handle continuous action spaces 
                    and stochastic policies. 
                    <br>
                    (e.g., <strong>REINFORCE</strong>, <strong>trust region policy optimization (TRPO)</strong>, and <strong>actor-critic methods</strong> 
                    such as <strong>advantage actor-critic(A2C)</strong>, <strong>deep deterministic policy gradient(DDPG)</strong>, <strong>soft actor-critic(SAC)</strong>, 
                    and <strong>proximal policy optimization (PPO)</strong>.)
                </p>

                <h3>On-Policy vs Off-Policy Methods</h3>
                <p>
                    This taxonomy concerns the data usage strategy during learning.
                </p>

                <p>
                    <strong>On-policy methods</strong> learn the Q-function from data generated by the current policy that is being improved. 
                    The agent expolores the environment and makes updates to its policy based on the actions it has taken.
                    <br>
                    (e.g., <strong>SARSA</strong>, <strong>REINFORCE</strong>, <strong>A2C</strong>, and <strong>PPO</strong>.)
                </p>

                <p>
                    <strong>Off-policy methods</strong> learn from data generated by a different policy that the one being 
                    improved. This allows the agent to reuse old data or learn from data collected by other agents. The policy used to 
                    collect data is called the behavioral policy, and the policy being learned is called the target policy. 
                    They offer higher <strong>sample efficiency</strong> than on-policy methods, but require handling distribution mismatch.
                    <br>
                    (e.g., <strong>Q-learning</strong>, <strong>DQN</strong>, <strong>DDPG</strong>, and <strong>SAC</strong>.)
                </p>

            </section>

            <section id="mdp" class="section-content">
                <h2>Markov Decision Process (MDP)</h2>

                <p>
                    Before diving into the details of RL algorithms, this section presents a detailed probabilistic formulation 
                    of Markov Decision Processes (MDPs), where both transitions and rewards are modeled as stochastic variables. 
                    This perspective extends the classical deterministic view and is particularly useful for analyzing trajectory 
                    distributions and gradient-based learning methods used in modern reinforcement learning.
                </p>

                <p>
                    An agent sequentially interacts with an initially unknown environment to obtain a <strong>trajectory</strong> 
                    or multiple trajectories. A trajectory of length \(T\) is defined as:
                    \[
                    \boldsymbol{\tau} = (s_0, a_0, r_0, s_1, a_1, r_1, s_2, \cdots, s_T),
                    \]
                    where \(s_t\) is a state, \(a_t\) is an action, and \(r_t\) is a reward.
                </p>

                <p>
                    The objective is to optimize the agent's action-selection policy so that the expected discounted cumulative reward
                    is maximized:
                    \[
                    G_0 = \sum_{t = 0}^{T -1} \gamma^t r_t,
                    \]
                    where \(\gamma \in [0, 1]\) is the <strong>discount factor</strong>. We assume the environment follows a 
                    <strong>Markov Decision Process (MDP)</strong>, where the trajectory distribution can be factored into 
                    single-step transition and reward models. The process of estimating an optimal policy from trajectories 
                    is referred to as <strong>learning</strong>.
                </p>

                <p>
                    We define the MDP as the tuple:
                    \[
                    \left\langle \mathcal{S}, \mathcal{A}, p_T, p_R, p_0 \right\rangle,
                    \]
                    where:
                    <ul style="padding-left: 40px;">
                    <li>\(\mathcal{S}\): set of environment states</li>
                    <li>\(\mathcal{A}\): set of available actions</li>
                    <li>\(p_T(s' \mid s, a)\): transition model (next-state distribution)</li>
                    <li>\(p_R(r \mid s, a, s')\): reward model (stochastic reward distribution)</li>
                    <li>\(p_0(s_0)\): initial state distribution</li>
                    </ul>
                </p>

                <p>
                    At time \(t = 0\), the initial state is sampled as \(s_0 \sim p_0\). At each step \(t \geq 0\), the agent observes 
                    state \(s_t \in \mathcal{S}\), selects action \(a_t \sim \pi(a_t \mid s_t)\), and receives reward 
                    \(r_t \sim p_R(r \mid s_t, a_t, s_{t+1})\), where the next state is drawn from \(s_{t+1} \sim p_T(s_{t+1} \mid s_t, a_t)\). 
                    The agent's decision-making is governed by a stochastic policy \(\pi(a \mid s)\).
                </p>

                <p>
                    This interaction at each step is called a <strong>transition</strong>, represented as the tuple:
                    \[
                    (s_t, a_t, r_t, s_{t+1}),
                    \]
                    where:
                    <ul style="padding-left: 40px;">
                    <li>\(a_t \sim \pi(a \mid s_t)\)</li>
                    <li>\(s_{t+1} \sim p_T(s_{t+1} \mid s_t, a_t)\)</li>
                    <li>\(r_t \sim p_R(r \mid s_t, a_t, s_{t+1})\)</li>
                    </ul>
                </p>

                <p>
                    Under policy \(\pi\), the joint distribution of a trajectory \(\boldsymbol{\tau}\) of length \(T\) is given by:
                    \[
                    p(\boldsymbol{\tau}) = p_0(s_0) \prod_{t = 0}^{T -1} 
                    \pi(a_t \mid s_t) \, p_T(s_{t+1} \mid s_t, a_t) \, p_R(r_t \mid s_t, a_t, s_{t+1}).
                    \]
                </p>

                <p>
                    We define the expected <strong>reward function</strong> from the reward model \(p_R\) as the marginal average immediate reward 
                    for taking action \(a\) in state \(s\), integrating over possible next states:
                    \[
                    R(s, a) = \mathbb{E}_{p_T(s' \mid s, a)} \left[ 
                    \mathbb{E}_{p_R(r \mid s, a, s')}[r] 
                    \right].
                    \]
                </p>

                <p>
                    While classical RL literature often defines the reward function deterministically as \(r(s, a)\), 
                    this probabilistic formulation allows us to account for uncertainty in transitions and rewards. 
                    It is particularly useful for gradient-based RL methods, where trajectory likelihoods must be modeled explicitly.
                </p>
            </section>

            <section id="value-functions" class="section-content">
                <h2>Value Functions and Bellman Equations</h2>

                <p>
                    Let \(\boldsymbol{\tau}\) be a trajectory of length \(T\), where \(T\) may be infinite (\(T = \infty\)) for continuing tasks. 
                    The <strong>return</strong> at time \(t\) is defined as the total accumulated reward from that point forward, discounted by a factor \(\gamma \in [0, 1]\):
                    \[
                    \begin{align*}
                    G_t &= r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \cdots + \gamma^{T - t - 1} r_{T - 1} \\\\
                        &= \sum_{k = 0}^{T - t - 1} \gamma^k r_{t + k} \\\\
                        &= \sum_{j = t}^{T - 1} \gamma^{j - t} r_j.
                    \end{align*}
                    \]
                    The discount factor \(\gamma\) ensures that the return remains finite even for infinite-horizon problems, and it gives higher weight to short-term rewards, thereby encouraging the agent to achieve goals sooner.
                </p>

                <p>
                    Given a stochastic policy \(\pi(a \mid s)\), the <strong>state-value function</strong> (or simply, <strong>value function</strong>) is defined as:
                    \[
                    \begin{align*}
                    V_{\pi}(s) &= \mathbb{E}_{\pi} \left[ G_0 \mid s_0 = s \right] \\\\
                            &= \mathbb{E}_{\pi} \left[ \sum_{t = 0}^{\infty} \gamma^t r_t \mid s_0 = s \right],
                    \end{align*}
                    \]
                    where the expectation is over trajectories induced by the policy \(\pi\).
                </p>

                <p>
                    The <strong>action-value function</strong> (or <strong>Q-function</strong>) is defined as:
                    \[
                    \begin{align*}
                    Q_{\pi}(s, a) &= \mathbb{E}_{\pi} \left[ G_0 \mid s_0 = s, \, a_0 = a \right] \\\\
                                &= \mathbb{E}_{\pi} \left[ \sum_{t = 0}^{\infty} \gamma^t r_t \mid s_0 = s, \, a_0 = a \right].
                    \end{align*}
                    \]
                </p>

                <p>
                    The <strong>advantage function</strong> is defined as:
                    \[
                    A_{\pi}(s, a) = Q_{\pi}(s, a) - V_{\pi}(s),
                    \]
                    which quantifies how much better it is to take action \(a\) in state \(s\) and then follow policy \(\pi\), compared to just following \(\pi\) from the beginning.
                </p>

                <p>
                    A policy \(\pi_*\) is called an <strong>optimal policy</strong> if it yields the highest value for every state:
                    \[
                    \forall s \in \mathcal{S}, \quad V_{\pi_*}(s) \geq V_{\pi}(s), \quad \forall \pi.
                    \]
                    Although multiple optimal policies may exist, their value functions are the same: \(V_*(s) = V_{\pi_*}(s)\) and \(Q_*(s, a) = Q_{\pi_*}(s, a)\). 
                    Moreover, every finite MDP admits at least one deterministic optimal policy.
                </p>

                <div class="theorem">
                    <span class="theorem-title"><strong>Bellman's Optimality Equations</strong>:</span>
                    \[
                    \begin{align*}
                    V_*(s) &= \max_a \left[ R(s, a) + \gamma \, \mathbb{E}_{p_T(s' \mid s, a)} \left[ V_*(s') \right] \right] \\\\
                    Q_*(s, a) &= R(s, a) + \gamma \, \mathbb{E}_{p_T(s' \mid s, a)} \left[ \max_{a'} Q_*(s', a') \right]
                    \end{align*}
                    \]
                    The optimal value functions \(V_*\) and \(Q_*\) are the unique fixed points of these equations.
                </div>

                <p>
                    The optimal policy can then be derived by:
                    \[
                    \begin{align*}
                    \pi_*(s) &= \arg \max_a Q_*(s, a) \\\\
                            &= \arg \max_a \left[ R(s, a) + \gamma \, \mathbb{E}_{p_T(s' \mid s, a)} \left[ V_*(s') \right] \right].
                    \end{align*}
                    \]
                    Solving for \(V_*\), \(Q_*\), or \(\pi_*\) is known as <strong>policy optimization</strong>, while computing 
                    \(V_{\pi}\) or \(Q_{\pi}\) for a given policy \(\pi\) is called <strong>policy evaluation</strong>. 
                </p>
                <p>
                   Bellman's equations for <strong>policy evaluation</strong> can be derived from the <strong>optimality equations</strong> 
                   by replacing each maximization over actions, \(\max_a [\,\cdot\,]\), with an expectation over the policy at the corresponding 
                   state, \(\mathbb{E}_{\pi(a \mid s)}[\,\cdot\,]\), wherever action selection occurs.
                    <div class="theorem">
                        <span class="theorem-title"><strong>Bellman's Expectation Equations:</strong></span>
                        <p>
                            For a fixed policy \(\pi\), the state-value function satisfies:
                            </p>
                            \[
                            V_{\pi}(s) = \mathbb{E}_{\pi(a \mid s)} \left[ R(s, a) + \gamma \, \mathbb{E}_{p_T(s' \mid s, a)} \left[ V_{\pi}(s') \right] \right]
                            \]
                            <p>
                            The action-value function satisfies:
                            </p>
                            \[
                            Q_{\pi}(s, a) = R(s, a) + \gamma \, \mathbb{E}_{p_T(s' \mid s, a)} \left[ \mathbb{E}_{\pi(a' \mid s')} \left[ Q_{\pi}(s', a') \right] \right]
                            \]
                    </div>
                </p>
            </section>

            <section id="DP" class="section-content">
                <h2>Dynamic Programming Algorithms for RL</h2>

                <p>
                    <strong>Dynamic programming (DP)</strong> is a technique for solving optimization problems by breaking them down into 
                    simpler subproblems and storing the results to avoid redundant computations. In the context of reinforcement learning, DP methods 
                    solve MDPs exactly when the complete model (transition probabilities and rewards) is known.
                </p>

                <p>
                    The key insight of DP for MDPs is that optimal policies have the property that whatever the initial state and initial decision are, 
                    the remaining decisions must constitute an optimal policy with regard to the state resulting from the first decision. 
                    This is known as <strong>Bellman's principle of optimality</strong>, which leads to the recursive Bellman equations.
                </p>

                <p>
                    Value Iteration (VI) and Policy Iteration (PI) are the two classical dynamic programming methods for solving MDPs. 
                    These algorithms are fundamental to <strong>model-based reinforcement learning</strong>, where an agent first learns the MDP model 
                    from experience and then applies these DP methods to compute optimal policies.
                </p>

                <h3>Value Iteration (VI)</h3>

                <p>
                Let the initial estimate be \(V_0\). We update it as follows: 
                \[
                V_{k+1}(s) = \max_a \left[ R(s, a) + \gamma \sum_{s'} p_T(s' \mid s, a) V_k (s') \right].
                \]
                This is called <strong>Bellman backup</strong>. Each iteration reduces the maximum value function error by a 
                constant factor: 
                \[
                \max_s \| V_{k+1}(s) - V_*(s) \| \leq \gamma \max_s \| V_k (s)  - V_* (s) \|.
                \]
                So, \(V_k\) converges to \(V_*\) as \(k \to \infty\). 
                </p>
                <p>
                    For all possible states \(s\), value iteration computes \(V_*(s)\) and \(\pi_*(s)\), averaging over all possible 
                    next states \(s'\) at each iteration. (Note: If we need the value and policy for only certain starting states, other methods can be used such as 
                    real-time dynamic programming). 
                </p>
                <div class="pseudocode">
                    <span class="pseudocode-title">VALUE_ITERATION</span>
                    <strong>Input:</strong> \(M = \left\langle \mathcal{S}, \mathcal{A}, p_T(s' | s, a), R(s, a), \gamma \right\rangle\)
                    <strong>Output:</strong> \(V_*\), \(\pi_*\)
                    <strong>begin</strong>
                    &emsp;&emsp;Initialize \(V(s)\) arbitrarily for all \(s \in \mathcal{S}\) (typically \(V(s) = 0\))
                    &emsp;&emsp;<strong>repeat:</strong>
                    &emsp;&emsp;&emsp;&emsp;\(\Delta \leftarrow 0\)
                    &emsp;&emsp;&emsp;&emsp;<strong>for each</strong> \(s \in \mathcal{S}\):
                    &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;\(V^{\text{old}}(s) \leftarrow V(s)\)
                    &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;\(V(s) \leftarrow \max_a \left[R(s,a) + \gamma \sum_{s'} p_T(s'|s,a) V(s')\right]\)
                    &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;\(\Delta \leftarrow \max(\Delta, |V^{\text{old}}(s) - V(s)|)\)
                    &emsp;&emsp;<strong>until</strong> \(\Delta < \theta\) (small threshold)
                    &emsp;&emsp;// Extract optimal policy
                    &emsp;&emsp;<strong>for each</strong> \(s \in \mathcal{S}\):
                    &emsp;&emsp;&emsp;&emsp;\(\pi_*(s) \leftarrow \arg\max_a \left[R(s,a) + \gamma \sum_{s'} p_T(s'|s,a) V(s')\right]\)
                    <strong>end</strong>
                </div>

                <h3>Policy Iteration (PI)</h3>
                <p>
                Let \(\mathbf{v}(s) = V_{\pi}(s)\) be the value function encoded as a vector indexed by states \(s\). Also, 
                the reward vector is represented by 
                \[
                \mathbf{r}(s) = \sum_a \pi(a \mid s) R(s, a)
                \]
                and the state transition matrix is written as 
                \[
                \mathbf{T}(s' \mid s) = \sum_a \pi(a \mid s) p_T(s' \mid s, a).
                \]
                Then Bellman's equation for policy evaluation can be represented as a linear system in \(\| \mathcal{S}\|\) unknowns:
                \[
                \mathbf{v} = \mathbf{r} + \gamma \mathbf{T}\mathbf{v}
                \]
                Theoretically, we can solve this by \(\mathbf{v} = (\mathbf{I} - \gamma \mathbf{T})^{-1} \mathbf{r}\), but 
                we can compute \(\mathbf{v}_{t+1} = \mathbf{r} + \gamma \mathbf{T} \mathbf{v}_t\) iteratively. This process is called 
                <strong>policy evaluation</strong>. Once we have evaluated \(V_{\pi}\) for the policy \(\pi\), we need to find a better 
                policy \(\pi'\). 
                </p>
                <p>
                    Now we move on to the <strong>policy improvement</strong> step:
                    \[
                      \pi'(s) = \arg \max_a \{R(s, a) + \gamma \mathbb{E}[V_{\pi}(s')]\}
                    \]
                    This guarantees \(V_{\pi'} \geq V_{\pi}\) because: 
                    \[
                    \begin{align*}
                    \mathbf{v} = \mathbf{r} + \gamma \mathbf{T}\mathbf{v} &\leq  \mathbf{r'} + \gamma \mathbf{T'}\mathbf{v} \\\\
                                                                          &\leq  \mathbf{r'} + \gamma \mathbf{T'}(\mathbf{r'} + \gamma \mathbf{T'}\mathbf{v})\\\\
                                                                          &\leq \cdots  \\\\
                                                                          &= (\mathbf{I} + \gamma \mathbf{T'} + \gamma^2 \mathbf{T'}^2 + \cdots)\mathbf{r'} \\\\
                                                                          &= (\mathbf{I} - \gamma \mathbf{T'})^{-1} \mathbf{r'} \\\\
                                                                          &= \mathbf{v'}

                    \end{align*}
                    \]
                </p>

                <div class="pseudocode">
                    <span class="pseudocode-title">POLICY_ITERATION</span>
                    <strong>Input:</strong> \(M = \left\langle \mathcal{S}, \mathcal{A}, p_T(s' | s, a), R(s, a), \gamma \right\rangle\)
                    <strong>Output:</strong> \(\pi_*\)
                    <strong>begin</strong>
                    &emsp;&emsp;Initialize \(V_{\pi}(s)\) arbitrarily for all \(s \in \mathcal{S}\) (typically \(V_{\pi}(s) = 0\))
                    &emsp;&emsp;Initialize \(\pi(s)\) arbitrarily for all \(s \in \mathcal{S}\)
                    &emsp;&emsp;<strong>repeat:</strong>
                    &emsp;&emsp;&emsp;&emsp;// Policy Evaluation
                    &emsp;&emsp;&emsp;&emsp;<strong>repeat:</strong>
                    &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;\(\Delta \leftarrow 0\)
                    &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<strong>for each</strong> \(s \in \mathcal{S}\):
                    &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;\(V_{\pi}^{\text{old}}(s) \leftarrow V_{\pi}(s)\)
                    &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;\(V_{\pi}(s) \leftarrow \sum_a \pi(a|s) \left[R(s,a) + \gamma \sum_{s'} p_T(s'|s,a) V_{\pi}(s')\right]\)
                    &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;\(\Delta \leftarrow \max(\Delta, |V_{\pi}^{\text{old}}(s) - V_{\pi}(s)|)\)
                    &emsp;&emsp;&emsp;&emsp;<strong>until</strong> \(\Delta < \theta\) (small threshold)
                    &emsp;&emsp;&emsp;&emsp;// Policy Improvement
                    &emsp;&emsp;&emsp;&emsp;\(\text{policy-stable} \leftarrow \text{true}\)
                    &emsp;&emsp;&emsp;&emsp;<strong>for each</strong> \(s \in \mathcal{S}\):
                    &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;\(\text{old-action} \leftarrow \pi(s)\)
                    &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;\(\pi(s) \leftarrow \arg\max_a \left[R(s,a) + \gamma \sum_{s'} p_T(s'|s,a) V_{\pi}(s')\right]\)
                    &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<strong>if</strong> \(\text{old-action} \neq \pi(s)\) <strong>then</strong> \(\text{policy-stable} \leftarrow \text{false}\)
                    &emsp;&emsp;<strong>until</strong> \(\text{policy-stable} = \text{true}\)
                    <strong>end</strong>
                </div>

                <p>
                    In policy iteration algorithm, starting from an initial policy, we alternate between policy evaluation step and 
                    policy improvement step. Within finite iterations, the algorithm will converge to optimal policy since there are at 
                    most \(\|\mathcal{A}\|^{\|\mathcal{S}\|}\) deterministic policies and every update improves the policy. 
                </p>

            </section>

            <section id="td-learning" class="section-content">
                <h2>Temporal Difference Learning</h2>         

                <p>
                    <strong>Temporal Difference (TD) learning</strong> refers to a class of model-free reinforcement 
                    learning methods which learn by bootstrapping from the current estimate of the value function. 
                    TD methods assign credit by means of the difference between temporally successive 
                    predictions, rather than waiting for final outcomes as in Monte Carlo methods.
                </p>

                <p>
                    These methods sample from the environment, like Monte Carlo methods, and perform 
                    updates based on current estimates, like dynamic programming methods. This unique combination 
                    allows TD methods to learn online from incomplete episodes while maintaining computational efficiency through bootstrapping.
                </p>

                <p>
                    Suppose that the agent learns the value function \(V_{\pi}\) for a fixed policy \(\pi\).
                    Given a state transition \((s, a, r, s')\) where \(a \sim \pi(s)\), we update the estimate \(V(s)\) as following:
                    \[
                    V(s_t) \leftarrow V(s_t) + \alpha \left[ r_t + \gamma V(s_{t+1}) - V(s_t) \right]
                    \]
                    where:
                    <ul style="padding-left: 40px;">
                    <li>\(\alpha \in (0,1]\) is the learning rate</li>
                    <li>\(r_t + \gamma V(s_{t+1})\) is the <strong>TD target</strong></li>
                    <li>\(\delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)\) is the <strong>TD error</strong></li>
                    </ul>
                    <br>
                    More generally, 
                    \[
                    \mathbf{w} \leftarrow \mathbf{w} + \alpha [ r_t + \gamma V_{\mathbf{w}}(s_{t+1}) - V_{\mathbf{w}}(s_t)]\nabla_{\mathbf{w}}V_{\mathbf{w}}(s_t).
                    \]
                    The value estimate at one state is used to update the value estimate of a previous state â€” this is called <strong>bootstrapping</strong>. 
                    Unlike <strong>Monte Carlo methods</strong> that must wait for episode completion, TD methods immediately update estimates using the current 
                    approximation of future value, but nothe that unlike SGD methods this update may diverge. 
                </p>

                <p>
                    The TD target \(r_t + \gamma V(s_{t+1})\) serves as a sample-based approximation of the Bellman 
                    expectation equation. From our earlier derivation, we know that:
                    \[
                    V_{\pi}(s) = \mathbb{E}_{\pi} \left[ r + \gamma V_{\pi}(s') \mid s \right]
                    \]
                    The TD update uses the observed reward \(r_t\) and next state \(s_{t+1}\) to approximate this expectation, 
                    then adjusts the current estimate toward this improved target. This establishes TD learning as a method 
                    for solving the Bellman equations through successive approximation, without requiring knowledge of 
                    transition probabilities.
                </p>

                <p>
                    The tabular TD(0) method is one of the simplest TD methods. It is a special 
                    case of more general stochastic approximation methods. Under appropriate conditions 
                    (bounded rewards, finite state space, and appropriate learning rate schedules), TD(0) converges to 
                    the true value function \(V_{\pi}(s)\) for the policy being followed.
                </p>

                <p>
                    TD methods offer several key advantages that make them central to modern reinforcement learning:
                    <ul style="padding-left: 40px;">
                    <li><strong>Online learning</strong>: Updates occur after each time step, enabling continuous learning without waiting for episode completion</li>
                    <li><strong>Computational efficiency</strong>: No need to store or process entire episode returns</li>
                    <li><strong>Lower variance</strong>: Compared to Monte Carlo methods, as updates depend on single-step transitions rather than entire episode outcomes</li>
                    <li><strong>Applicability to continuing tasks</strong>: Can learn in non-episodic environments where episodes never terminate</li>
                    </ul>
                </p>

                <p>
                    TD learning occupies a central position in the model-free reinforcement learning spectrum, 
                    bridging Monte Carlo methods and the bootstrapping principles borrowed from model-based dynamic programming. 
                    While Monte Carlo methods (another class of model-free methods) only adjust their estimates once the final 
                    outcome is known, TD methods adjust predictions to match later, more accurate, predictions about the future 
                    before the final outcome is known.
                </p>

                <p>
                    This positioning allows TD learning to combine the model-free learning capability with the computational 
                    efficiency of bootstrapping, making it particularly suitable for the online learning scenarios that 
                    characterize modern reinforcement learning applications.
                </p>
            </section>

            <section id="value-based" class="section-content">
                <h2>Value-Based Methods</h2>

                <p>
                    Moving from value functions to action-value functions, we can learn optimal policies without knowing 
                    the environment model. Two fundamental algorithms demonstrate different approaches to this problem.
                </p>

                <h3>Q-Learning (Off-Policy)</h3>
                <p>
                    Q-learning learns the optimal action-value function \(Q_*(s,a)\) directly using the update rule:
                    \[
                    Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left[ r_t + \gamma \max_{a'} Q(s_{t+1}, a') - Q(s_t, a_t) \right]
                    \]
                    The key insight is that Q-learning uses the <strong>maximum</strong> over next-state actions, making it 
                    <strong>off-policy</strong> - it learns about the optimal policy while potentially following a different behavioral policy.
                </p>

                <h3>SARSA (On-Policy)</h3>
                <p>
                    SARSA (State-Action-Reward-State-Action) learns the value function for the policy it's currently following:
                    \[
                    Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left[ r_t + \gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t) \right]
                    \]
                    where \(a_{t+1}\) is the action actually taken by the current policy. This makes SARSA <strong>on-policy</strong>.
                </p>

                <h3>Key Differences</h3>
                <p>
                    <ul style="padding-left: 40px;">
                    <li><strong>Convergence</strong>: Q-learning converges to \(Q_*\) regardless of policy followed; SARSA converges to \(Q_\pi\) for the policy being followed</li>
                    <li><strong>Exploration</strong>: SARSA is more conservative, considering the exploration strategy in its updates</li>
                    <li><strong>Safety</strong>: In environments with dangerous actions, SARSA may be safer as it accounts for exploration</li>
                    </ul>
                </p>

                <p>
                    Both algorithms require exploration to discover good actions, leading us to the fundamental challenge 
                    of balancing exploration and exploitation.
                </p>
            </section>

            <section id="policy-based" class="section-content">
                <h2>Policy-Based Methods</h2>

                <p>
                    Policy-based methods directly parameterize and optimize policies without learning value functions. These methods address some fundamental limitations of value-based approaches, particularly in continuous action spaces and when stochastic policies are desired.
                </p>

                <p>
                    <strong>Policy Gradient Methods</strong> optimize a parameterized policy \(\pi_\theta(a \mid s)\) by following the gradient of the expected return. The policy gradient theorem provides the foundation:
                    \[
                    \nabla_\theta J(\theta) = \mathbb{E}_{\pi_\theta}[\nabla_\theta \log \pi_\theta(a \mid s) Q^{\pi_\theta}(s,a)]
                    \]
                    where \(J(\theta)\) is the expected return under policy \(\pi_\theta\).
                </p>

                <p>
                    <strong>REINFORCE</strong> is the fundamental policy gradient algorithm that uses Monte Carlo estimation:
                    \[
                    \nabla_\theta J(\theta) \approx \frac{1}{m} \sum_{i=1}^m \sum_{t=0}^{T-1} \nabla_\theta \log \pi_\theta(a_t^{(i)} \mid s_t^{(i)}) G_t^{(i)}
                    \]
                    where \(G_t^{(i)}\) is the return from time \(t\) in episode \(i\). While REINFORCE provides unbiased gradient estimates, it suffers from high variance due to the Monte Carlo sampling of returns, which can significantly slow learning and lead to unstable policy updates.
                </p>

                <p>
                    Policy gradient methods offer several advantages:
                    <ul style="padding-left: 40px;">
                        <li>Natural handling of continuous action spaces</li>
                        <li>Ability to learn stochastic policies</li>
                        <li>Direct optimization of the performance objective</li>
                        <li>Better convergence properties with function approximation</li>
                    </ul>
                </p>
            </section>

            <section id="actor-critic" class="section-content">
                <h2>Actor-Critic Methods</h2>

                <p>
                    Actor-critic methods combine the benefits of both value-based and policy-based approaches by maintaining two function approximators:
                    <ul style="padding-left: 40px;">
                        <li><strong>Actor</strong> \(\pi_\theta(a \mid s)\): the policy that selects actions</li>
                        <li><strong>Critic</strong> \(V_\phi(s)\) or \(Q_\phi(s,a)\): the value function that evaluates actions</li>
                    </ul>
                </p>

                <p>
                    The critic provides a baseline to reduce the variance of policy gradient estimates. The advantage function is defined as:
                    \[
                    A^{\pi_\theta}(s,a) = Q^{\pi_\theta}(s,a) - V^{\pi_\theta}(s)
                    \]
                    leading to the actor-critic policy gradient:
                    \[
                    \nabla_\theta J(\theta) = \mathbb{E}_{\pi_\theta}[\nabla_\theta \log \pi_\theta(a \mid s) A^{\pi_\theta}(s,a)]
                    \]
                </p>

                <p>
                    In practice, the temporal difference (TD) error can serve as an unbiased estimate of the advantage function under the current policy:
                    \[
                    \delta_t = r_t + \gamma V_\phi(s_{t+1}) - V_\phi(s_t)
                    \]
                    This allows us to approximate \(A^{\pi_\theta}(s_t, a_t) \approx \delta_t\), providing computational efficiency while maintaining theoretical soundness.
                </p>

                <p>
                    Actor-critic methods offer several benefits:
                    <ul style="padding-left: 40px;">
                        <li>Significantly lower variance than pure policy gradient methods like REINFORCE</li>
                        <li>More sample efficient than Monte Carlo methods through bootstrapping</li>
                        <li>Can learn in both discrete and continuous action spaces</li>
                        <li>Better convergence properties with function approximation compared to value-based methods</li>
                        <li>Natural framework for incorporating domain knowledge</li>
                    </ul>
                </p>

                <p>
                    Key actor-critic algorithms include:
                    <ul style="padding-left: 40px;">
                        <li><strong>A3C (Asynchronous Advantage Actor-Critic)</strong>: Uses multiple parallel agents</li>
                        <li><strong>DDPG (Deep Deterministic Policy Gradient)</strong>: For continuous control</li>
                        <li><strong>SAC (Soft Actor-Critic)</strong>: Incorporates maximum entropy principle</li>
                        <li><strong>PPO (Proximal Policy Optimization)</strong>: Clips policy updates for stability</li>
                    </ul>
                </p>
            </section>

            <section id="exploration" class="section-content">
                <h2>Exploration vs. Exploitation</h2>
                <p>
                    The <strong>exploration-exploitation tradeoff</strong> is fundamental to reinforcement learning. An agent must 
                    balance between:
                    <ul style="padding-left: 40px;">
                    <li><strong>Exploitation</strong>: Choosing actions that yield high rewards based on current knowledge</li>
                    <li><strong>Exploration</strong>: Trying new actions to potentially discover better strategies</li>
                    </ul>
                </p>

                <h3>\(\epsilon\)-Greedy Strategy</h3>
                <p>
                    The simplest exploration strategy chooses:
                    \[
                    a_t = \begin{cases}
                    \arg\max_a Q(s_t, a) & \text{with probability } 1-\varepsilon \\
                    \text{random action} & \text{with probability } \varepsilon
                    \end{cases}
                    \]
                    where \(\varepsilon \in [0,1]\) controls the exploration rate. Often \(\varepsilon\) is decayed over time.
                </p>

                <h3>Softmax Action Selection</h3>
                <p>
                    A more sophisticated approach uses the Boltzmann (softmax) distribution:
                    \[
                    P(a_t = a \mid s_t) = \frac{\exp(Q(s_t, a)/\tau)}{\sum_{a'} \exp(Q(s_t, a')/\tau)}
                    \]
                    where \(\tau > 0\) is the temperature parameter. Higher temperatures lead to more exploration.
                </p>

                <h3>Optimistic Initialization</h3>
                <p>
                    Initializing Q-values optimistically (higher than realistic values) encourages exploration 
                    of all actions early in learning, as the agent will be "disappointed" by actual rewards 
                    and try other actions.
                </p>

                <p>
                    The choice of exploration strategy significantly affects learning performance and is often 
                    problem-dependent. More advanced methods like UCB (Upper Confidence Bound) and Thompson 
                    sampling provide principled approaches to this tradeoff.
                </p>
            </section>

        </div>
        <script src="/js/main.js"></script>    
    </body>
</html>