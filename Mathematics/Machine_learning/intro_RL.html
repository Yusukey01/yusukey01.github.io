---
layout: default
title: Introduction to Reinforcement Learning
level: detail
description: Learn about Reinforcement Learning fundamentals from theory to algorithms including MDPs, Bellman equations, value iteration, policy iteration, temporal difference learning, Q-learning and SARSA.
uses_math: true
uses_python: false
---
<!DOCTYPE html>
<html>
    <body>
        <!-- LearningResource Schema for Content Pages -->
        <script type="application/ld+json">
        {
        "@context": "https://schema.org",
        "@type": "LearningResource",
        "name": "{{ page.title }}",
        "description": "{{ page.description }}",
        "learningResourceType": "lesson",
        "educationalUse": "instruction",
        "educationalLevel": "university",
        "interactivityType": "{% if page.content contains 'Interactive Demo' or page.content contains 'demo' %}active{% else %}expositive{% endif %}",
        "author": {
            "@type": "Person",
            "name": "Yusuke Yokota",
            "jobTitle": "Mathematics & Computer Science Educator"
        },
        "publisher": {
            "@type": "Organization",
            "name": "MATH-CS COMPASS",
            "url": "https://yusukey01.github.io"
        },
        "about": [
            { "@type": "Thing", "name": "Reinforcement Learning" },
            { "@type": "Thing", "name": "Markov Decision Process" },
            { "@type": "Thing", "name": "MDP" },
            { "@type": "Thing", "name": "Value Functions" },
            { "@type": "Thing", "name": "Policy Optimization" },
            { "@type": "Thing", "name": "Bellman Equations" },
            { "@type": "Thing", "name": "Value Iteration" },
            { "@type": "Thing", "name": "Policy Iteration" },
            { "@type": "Thing", "name": "Temporal Difference Learning" },
            { "@type": "Thing", "name": "Q-Learning" },
            { "@type": "Thing", "name": "SARSA" },
            { "@type": "Thing", "name": "Dynamic Programming" },
            { "@type": "Thing", "name": "Model-Based Reinforcement Learning" },
            { "@type": "Thing", "name": "Model-Free Reinforcement Learning" },
            { "@type": "Thing", "name": "Exploration vs Exploitation" },
            { "@type": "Thing", "name": "Policy Gradient Methods" },
            { "@type": "Thing", "name": "Actor-Critic Methods" }
        ],
        "teaches": [
            "Markov Decision Process formulation",
            "Bellman equations and value functions",
            "Classical planning algorithms",
            "Reinforcement learning algorithms",
            "Temporal difference learning methods",
            "Policy optimization techniques",
            "RL algorithm classification"
        ],
        "isPartOf": {
            "@type": "Course",
            "name": "Machine Learning",
            "description": "Explore machine learning ideas and applications with mathematical foundations",
            "provider": {
                "@type": "Organization",
                "name": "MATH-CS COMPASS",
                "url": "https://yusukey01.github.io"
            },
            "instructor": {
                "@type": "Person",
                "name": "Yusuke Yokota",
                "jobTitle": "Mathematics & Computer Science Educator"
            },
            "courseCode": "V",
            "hasCourseInstance": {
                "@type": "CourseInstance",
                "courseMode": "online",
                "courseWorkload": "PT5H",
                "instructor": {
                    "@type": "Person",
                    "name": "Yusuke Yokota"
                }
            },
            "offers": {
                "@type": "Offer",
                "price": "0",
                "priceCurrency": "USD",
                "availability": "https://schema.org/InStock",
                "category": "free"
            }
        }
        }
        </script>
        
        <!-- WebApplication Schema for Interactive Content -->
        <script type="application/ld+json">
        {
        "@context": "https://schema.org",
        "@type": "WebApplication",
        "name": "{{ page.title }} Interactive Demo",
        "description": "Interactive demonstration of reinforcement learning concepts including MDPs and algorithms",
        "applicationCategory": "EducationalApplication",
        "operatingSystem": "Web Browser",
        "url": "https://yusukey01.github.io{{ page.url }}",
        "author": {
            "@type": "Person",
            "name": "Yusuke Yokota"
        },
        "applicationSubCategory": "Mathematical Visualization",
        "featureList": [
            "Interactive mathematical visualization",
            "Real-time parameter adjustment",
            "Educational demonstrations"
        ],
        "isAccessibleForFree": true,
        "educationalUse": "instruction",
        "educationalLevel": "university"
        }
        </script>
        
        <div class="hero-section">
            <h1 class="webpage-name">Introduction to Reinforcement Learning(RL)</h1>
        </div>

        {% include section_navigation.html %}

        <div class="topic-nav">
            <a href="#intro">Introduction</a>
            <a href="#taxonomy">Classification of RL Algorithms</a>
            <a href="#exploration">Exploration vs. Exploitation</a>
            <a href="#mdp">Markov Decision Process</a>
            <a href="#value-functions">Value Functions & Bellman Equations</a>
            <a href="#DP">Dynamic Programming Algorithms for RL</a>
            <a href="#MC">Monte Carlo Control</a>
            <a href="#td-learning">Temporal Difference Learning</a>
            <a href="#TD_Con">TD Control Methods: Q-Learning & SARSA</a>
            <a href="#policy-grad">Policy Gradient Methods</a>
            <a href="#actor-critic">Actor-Critic Methods</a>
            
        </div>

        <div class="container">  

            <section id="intro" class="section-content">
                <h2>Introduction</h2>

                <p>
                    <strong>Reinforcement Learning (RL)</strong> is a machine learning framework in which an <strong>agent</strong> interacts 
                    with an environment to learn a behavior that maximizes a scalar <strong>reward</strong> signal over time. 
                    Formally, the environment is typically modeled as a <strong>Markov Decision Process (MDP)</strong>, defined by a tuple 
                    \[
                    (\mathcal{S}, \mathcal{A}, \mathcal{T}, r, \gamma)
                    \]
                    where \(\mathcal{S}\) is the state space, \(\mathcal{A}\) is the action space, \(\mathcal{T}\) is the transition function, 
                    \(r\) is the reward function, and \(\gamma\) is the discount factor.
                </p>

                <p>
                    At each time step, the agent observes a state \(s_t \in \mathcal{S}\), selects an action \(a_t \in \mathcal{A}\), receives a 
                    reward \(r_t\), and transitions to a new state \(s_{t+1}\) according to the transition dynamics \(\mathcal{T}(s_{t+1} \mid s_t, a_t)\). 
                    The goal of the agent is to learn a <strong>policy</strong> \(\pi(a \mid s)\) that maximizes the expected cumulative reward:
                    \[
                    \mathbb{E}_\pi \left[ \sum_{t=0}^\infty \gamma^t r_t \right].
                    \]
                </p>

                <p>
                    Reinforcement learning differs fundamentally from other machine learning paradigms:
                    <ul style="padding-left: 40px;">
                        <li><strong>Supervised learning</strong> trains models on ground-truth input-output pairs with explicit correct labels for each input.</li>
                        <li> <strong>Unsupervised learning</strong> discovers hidden structure or representations in unlabeled data without any feedback signal.</li>
                        <li> <strong>Reinforcement learning</strong> uses only a scalar reward signal that provides evaluative feedback based on the agent's interaction with an environment.</li>
                    </ul>
                </p>
              
                <p>
                    In RL, learning is driven by trial-and-error interaction rather than labeled examples. Unlike supervised learning, there are no correct labels for each input, 
                    and unlike unsupervised learning, the objective is not to model data structure but to learn a policy that maximizes expected cumulative reward over time.
                </p>

                <p>
                    RL has been successfully applied in domains such as robotics, recommendation systems, and game-playing.
                    More recently, RL has also become integral to the training of modern <strong>large language models (LLMs)</strong>. 
                    In particular, a method called <strong>Reinforcement Learning from Human Feedback (RLHF)</strong> is used to fine-tune LLMs 
                    to follow <strong>human preferences</strong>, ensuring outputs are more helpful, safe, and better aligned with user intent.
                </p>

                <p>
                    The diversity of RL applications has led to a rich classification of algorithms, each designed to address different aspects of 
                    the learning problem. Understanding this classification is crucial for selecting appropriate methods and comprehending the 
                    relationships between different approaches.
                </p>     

            </section>

            <section id="taxonomy" class="section-content">
                <h2>Classification of RL Algorithms</h2>

                <h3>Model-Based vs Model-Free Methods</h3>
                <p>
                    The most fundamental distinction in RL concerns whether the agent learns an explicit model of the environment.
                </p>

                <p>
                    <strong>Model-based RL</strong> methods learn an explicit model of the environment from data, including:
                    <ul style="padding-left: 40px;">
                        <li>the transition dynamics \(p_T(s' \mid s, a)\), and</li>
                        <li>the reward model \(p_R(r \mid s, a, s')\).</li>
                    </ul>
                </p>

                <p>
                    Once learned, the agent uses <strong>planning</strong> algorithms to compute an optimal policy \(\pi_*\). 
                    Classical planning is based on dynamic programming such as <strong>value iteration(VI)</strong> and <strong>policy iteration(PI)</strong>. 
                    The main advantage is <strong>sample efficiency</strong> - the agent can simulate many trajectories using the learned model without additional 
                    environment interaction. However, model learning introduces <strong>model bias</strong>: if the learned model is inaccurate, the agent 
                    may perform well in simulation but poorly in the real environment.
                </p>

                <p>
                    <strong>Model-free RL</strong> methods bypass explicit environment modeling and directly learn value functions or policies 
                    from experience. These methods are generally simpler to implement and more robust to model errors, but typically require 
                    more environment interactions compared to model-based approaches.
                </p>

                <h3>Value-Based vs Policy-Based</h3>
                <p>
                    This taxonomy classifies algorithms based on <strong>what they learn</strong> in the model-free RL.
                </p>

                <p>
                    <strong>Value-based methods</strong> learn the optimal Q-function from experience, and then derive 
                    policies implicitly:
                    \[
                    \begin{align*}
                    \pi_*(s) &= \arg\max_a Q_*(s,a) \\\\
                             &= \arg\max_a [R(s, a) + \gamma \mathbb{E}_{p_T(s' \mid s, a)}[V_*(s')]]
                    \end{align*}
                    \]
                    Given a transition \((s, a, r, s')\), we define the <strong>temporal difference</strong> (or <strong>TD error</strong>) as: 
                    \[
                    r + \gamma \max_{a'} Q_w (s', a') - Q_w (s, a)
                    \]
                    where \(Q_w\) is a function approximator, which is trained iteratively. 
                    <br>
                    Note that the expected TD error equals the Bellman error, and when \(Q_w = Q_*\), the TD error is zero in expectation."
                    <br>
                    (e.g.,<strong>Q-learning</strong>, <strong>SARSA</strong>, and <strong>Deep Q-Networks(DQN)</strong>.)
                </p>

                <p>
                    <strong>Policy-based methods</strong> directly optimize \(J(\pi_{\mathbf{\theta}})\) with respect to the 
                    policy parameter \(\mathbf{\theta}\) using <strong>policy gradient</strong> and can naturally handle continuous action spaces 
                    and stochastic policies. 
                    <br>
                    (e.g., <strong>REINFORCE</strong>, <strong>trust region policy optimization (TRPO)</strong>, and <strong>actor-critic methods</strong> 
                    such as <strong>advantage actor-critic(A2C)</strong>, <strong>deep deterministic policy gradient(DDPG)</strong>, <strong>soft actor-critic(SAC)</strong>, 
                    and <strong>proximal policy optimization (PPO)</strong>.)
                </p>

                <h3>On-Policy vs Off-Policy Methods</h3>
                <p>
                    This taxonomy concerns the data usage strategy during learning.
                </p>

                <p>
                    <strong>On-policy methods</strong> learn the Q-function from data generated by the current policy that is being improved. 
                    The agent expolores the environment and makes updates to its policy based on the actions it has taken.
                    <br>
                    (e.g., <strong>SARSA</strong>, <strong>REINFORCE</strong>, <strong>A2C</strong>, and <strong>PPO</strong>.)
                </p>

                <p>
                    <strong>Off-policy methods</strong> learn from data generated by a different policy that the one being 
                    improved. This allows the agent to reuse old data or learn from data collected by other agents. The policy used to 
                    collect data is called the behavioral policy, and the policy being learned is called the target policy. 
                    They offer higher <strong>sample efficiency</strong> than on-policy methods, but require handling distribution mismatch.
                    <br>
                    (e.g., <strong>Q-learning</strong>, <strong>DQN</strong>, <strong>DDPG</strong>, and <strong>SAC</strong>.)
                </p>

            </section>

            <section id="exploration" class="section-content">
                <h2>Exploration vs. Exploitation</h2>
                <p>
                    The <strong>exploration-exploitation tradeoff</strong> is fundamental to reinforcement learning. An agent must 
                    balance between:
                    <ul style="padding-left: 40px;">
                    <li><strong>Exploitation</strong>: Choosing actions that yield high rewards based on current knowledge</li>
                    <li><strong>Exploration</strong>: Trying new actions to potentially discover better strategies</li>
                    </ul>
                </p>

                <h3>\(\epsilon\)-Greedy Strategy</h3>
                <p>
                    The simplest exploration strategy chooses:
                    \[
                    a_t = \begin{cases}
                    \arg\max_a Q(s_t, a) & \text{with probability } 1-\varepsilon \\
                    \text{random action} & \text{with probability } \varepsilon
                    \end{cases}
                    \]
                    where \(\varepsilon \in [0,1]\) controls the exploration rate. Often \(\varepsilon\) is decayed over time.
                </p>

                <h3>Softmax Action Selection</h3>
                <p>
                    A more sophisticated approach uses the Boltzmann (softmax) distribution:
                    \[
                    P(a_t = a \mid s_t) = \frac{\exp(Q(s_t, a)/\tau)}{\sum_{a'} \exp(Q(s_t, a')/\tau)}
                    \]
                    where \(\tau > 0\) is the temperature parameter. Higher temperatures lead to more exploration.
                </p>

                <h3>Optimistic Initialization</h3>
                <p>
                    Initializing Q-values optimistically (higher than realistic values) encourages exploration 
                    of all actions early in learning, as the agent will be "disappointed" by actual rewards 
                    and try other actions.
                </p>

                <p>
                    The choice of exploration strategy significantly affects learning performance and is often 
                    problem-dependent. More advanced methods like UCB (Upper Confidence Bound) and Thompson 
                    sampling provide principled approaches to this tradeoff.
                </p>
            </section>

            <section id="mdp" class="section-content">
                <h2>Markov Decision Process (MDP)</h2>

                <p>
                    Before diving into the details of RL algorithms, this section presents a detailed probabilistic formulation 
                    of Markov Decision Processes (MDPs), where both transitions and rewards are modeled as stochastic variables. 
                    This perspective extends the classical deterministic view and is particularly useful for analyzing trajectory 
                    distributions and gradient-based learning methods used in modern reinforcement learning.
                </p>

                <p>
                    An agent sequentially interacts with an initially unknown environment to obtain a <strong>trajectory</strong> 
                    or multiple trajectories. A trajectory of length \(T\) is defined as:
                    \[
                    \boldsymbol{\tau} = (s_0, a_0, r_0, s_1, a_1, r_1, s_2, \cdots, s_T),
                    \]
                    where \(s_t\) is a state, \(a_t\) is an action, and \(r_t\) is a reward.
                </p>

                <p>
                    The objective is to optimize the agent's action-selection policy so that the expected discounted cumulative reward
                    is maximized:
                    \[
                    G_0 = \sum_{t = 0}^{T -1} \gamma^t r_t,
                    \]
                    where \(\gamma \in [0, 1]\) is the <strong>discount factor</strong>. We assume the environment follows a 
                    <strong>Markov Decision Process (MDP)</strong>, where the trajectory distribution can be factored into 
                    single-step transition and reward models. The process of estimating an optimal policy from trajectories 
                    is referred to as <strong>learning</strong>.
                </p>

                <p>
                    We define the MDP as the tuple:
                    \[
                    \left\langle \mathcal{S}, \mathcal{A}, p_T, p_R, p_0 \right\rangle,
                    \]
                    where:
                    <ul style="padding-left: 40px;">
                    <li>\(\mathcal{S}\): set of environment states</li>
                    <li>\(\mathcal{A}\): set of available actions</li>
                    <li>\(p_T(s' \mid s, a)\): transition model (next-state distribution)</li>
                    <li>\(p_R(r \mid s, a, s')\): reward model (stochastic reward distribution)</li>
                    <li>\(p_0(s_0)\): initial state distribution</li>
                    </ul>
                </p>

                <p>
                    At time \(t = 0\), the initial state is sampled as \(s_0 \sim p_0\). At each step \(t \geq 0\), the agent observes 
                    state \(s_t \in \mathcal{S}\), selects action \(a_t \sim \pi(a_t \mid s_t)\), and receives reward 
                    \(r_t \sim p_R(r \mid s_t, a_t, s_{t+1})\), where the next state is drawn from \(s_{t+1} \sim p_T(s_{t+1} \mid s_t, a_t)\). 
                    The agent's decision-making is governed by a stochastic policy \(\pi(a \mid s)\).
                </p>

                <p>
                    This interaction at each step is called a <strong>transition</strong>, represented as the tuple:
                    \[
                    (s_t, a_t, r_t, s_{t+1}),
                    \]
                    where:
                    <ul style="padding-left: 40px;">
                    <li>\(a_t \sim \pi(a \mid s_t)\)</li>
                    <li>\(s_{t+1} \sim p_T(s_{t+1} \mid s_t, a_t)\)</li>
                    <li>\(r_t \sim p_R(r \mid s_t, a_t, s_{t+1})\)</li>
                    </ul>
                </p>

                <p>
                    Under policy \(\pi\), the joint distribution of a trajectory \(\boldsymbol{\tau}\) of length \(T\) is given by:
                    \[
                    p(\boldsymbol{\tau}) = p_0(s_0) \prod_{t = 0}^{T -1} 
                    \pi(a_t \mid s_t) \, p_T(s_{t+1} \mid s_t, a_t) \, p_R(r_t \mid s_t, a_t, s_{t+1}).
                    \]
                </p>

                <p>
                    We define the expected <strong>reward function</strong> from the reward model \(p_R\) as the marginal average immediate reward 
                    for taking action \(a\) in state \(s\), integrating over possible next states:
                    \[
                    R(s, a) = \mathbb{E}_{p_T(s' \mid s, a)} \left[ 
                    \mathbb{E}_{p_R(r \mid s, a, s')}[r] 
                    \right].
                    \]
                </p>

                <p>
                    While classical RL literature often defines the reward function deterministically as \(r(s, a)\), 
                    this probabilistic formulation allows us to account for uncertainty in transitions and rewards. 
                    It is particularly useful for gradient-based RL methods, where trajectory likelihoods must be modeled explicitly.
                </p>
            </section>

            <section id="value-functions" class="section-content">
                <h2>Value Functions and Bellman Equations</h2>

                <p>
                    Let \(\boldsymbol{\tau}\) be a trajectory of length \(T\), where \(T\) may be infinite (\(T = \infty\)) for continuing tasks. 
                    The <strong>return</strong> at time \(t\) is defined as the total accumulated reward from that point forward, discounted by a factor \(\gamma \in [0, 1]\):
                    \[
                    \begin{align*}
                    G_t &= r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \cdots + \gamma^{T - t - 1} r_{T - 1} \\\\
                        &= \sum_{k = 0}^{T - t - 1} \gamma^k r_{t + k} \\\\
                        &= \sum_{j = t}^{T - 1} \gamma^{j - t} r_j.
                    \end{align*}
                    \]
                    The discount factor \(\gamma\) ensures that the return remains finite even for infinite-horizon problems, and it gives higher weight to short-term rewards, thereby encouraging the agent to achieve goals sooner.
                </p>

                <p>
                    Given a stochastic policy \(\pi(a \mid s)\), the <strong>state-value function</strong> (or simply, <strong>value function</strong>) is defined as:
                    \[
                    \begin{align*}
                    V_{\pi}(s) &= \mathbb{E}_{\pi} \left[ G_0 \mid s_0 = s \right] \\\\
                            &= \mathbb{E}_{\pi} \left[ \sum_{t = 0}^{\infty} \gamma^t r_t \mid s_0 = s \right],
                    \end{align*}
                    \]
                    where the expectation is over trajectories induced by the policy \(\pi\).
                </p>

                <p>
                    The <strong>action-value function</strong> (or <strong>Q-function</strong>) is defined as:
                    \[
                    \begin{align*}
                    Q_{\pi}(s, a) &= \mathbb{E}_{\pi} \left[ G_0 \mid s_0 = s, \, a_0 = a \right] \\\\
                                &= \mathbb{E}_{\pi} \left[ \sum_{t = 0}^{\infty} \gamma^t r_t \mid s_0 = s, \, a_0 = a \right].
                    \end{align*}
                    \]
                </p>

                <p>
                    The <strong>advantage function</strong> is defined as:
                    \[
                    A_{\pi}(s, a) = Q_{\pi}(s, a) - V_{\pi}(s),
                    \]
                    which quantifies how much better it is to take action \(a\) in state \(s\) and then follow policy \(\pi\), compared to just following \(\pi\) from the beginning.
                </p>

                <p>
                    A policy \(\pi_*\) is called an <strong>optimal policy</strong> if it yields the highest value for every state:
                    \[
                    \forall s \in \mathcal{S}, \quad V_{\pi_*}(s) \geq V_{\pi}(s), \quad \forall \pi.
                    \]
                    Although multiple optimal policies may exist, their value functions are the same: \(V_*(s) = V_{\pi_*}(s)\) and \(Q_*(s, a) = Q_{\pi_*}(s, a)\). 
                    Moreover, every finite MDP admits at least one deterministic optimal policy.
                </p>

                <div class="theorem">
                    <span class="theorem-title"><strong>Bellman's Optimality Equations</strong>:</span>
                    \[
                    \begin{align*}
                    V_*(s) &= \max_a \left[ R(s, a) + \gamma \, \mathbb{E}_{p_T(s' \mid s, a)} \left[ V_*(s') \right] \right] \\\\
                    Q_*(s, a) &= R(s, a) + \gamma \, \mathbb{E}_{p_T(s' \mid s, a)} \left[ \max_{a'} Q_*(s', a') \right]
                    \end{align*}
                    \]
                    The optimal value functions \(V_*\) and \(Q_*\) are the unique fixed points of these equations.
                </div>

                <p>
                    The optimal policy can then be derived by:
                    \[
                    \begin{align*}
                    \pi_*(s) &= \arg \max_a Q_*(s, a) \\\\
                            &= \arg \max_a \left[ R(s, a) + \gamma \, \mathbb{E}_{p_T(s' \mid s, a)} \left[ V_*(s') \right] \right].
                    \end{align*}
                    \]
                    Solving for \(V_*\), \(Q_*\), or \(\pi_*\) is known as <strong>policy optimization</strong>, while computing 
                    \(V_{\pi}\) or \(Q_{\pi}\) for a given policy \(\pi\) is called <strong>policy evaluation</strong>. 
                </p>
                <p>
                   Bellman's equations for <strong>policy evaluation</strong> can be derived from the <strong>optimality equations</strong> 
                   by replacing each maximization over actions, \(\max_a [\,\cdot\,]\), with an expectation over the policy at the corresponding 
                   state, \(\mathbb{E}_{\pi(a \mid s)}[\,\cdot\,]\), wherever action selection occurs.
                    <div class="theorem">
                        <span class="theorem-title"><strong>Bellman's Expectation Equations:</strong></span>
                        <p>
                            For a fixed policy \(\pi\), the state-value function satisfies:
                            </p>
                            \[
                            V_{\pi}(s) = \mathbb{E}_{\pi(a \mid s)} \left[ R(s, a) + \gamma \, \mathbb{E}_{p_T(s' \mid s, a)} \left[ V_{\pi}(s') \right] \right]
                            \]
                            <p>
                            The action-value function satisfies:
                            </p>
                            \[
                            Q_{\pi}(s, a) = R(s, a) + \gamma \, \mathbb{E}_{p_T(s' \mid s, a)} \left[ \mathbb{E}_{\pi(a' \mid s')} \left[ Q_{\pi}(s', a') \right] \right]
                            \]
                    </div>
                </p>
            </section>

            <section id="DP" class="section-content">
                <h2>Dynamic Programming Algorithms for RL</h2>

                <p>
                    <strong>Dynamic programming (DP)</strong> is a technique for solving optimization problems by breaking them down into 
                    simpler subproblems and storing the results to avoid redundant computations. In the context of reinforcement learning, DP methods 
                    solve MDPs exactly when the complete model (transition probabilities and rewards) is known.
                </p>

                <p>
                    The key insight of DP for MDPs is that optimal policies have the property that whatever the initial state and initial decision are, 
                    the remaining decisions must constitute an optimal policy with regard to the state resulting from the first decision. 
                    This is known as <strong>Bellman's principle of optimality</strong>, which leads to the recursive Bellman equations.
                </p>

                <p>
                    Value Iteration (VI) and Policy Iteration (PI) are the two classical dynamic programming methods for solving MDPs. 
                    These algorithms are fundamental to <strong>model-based reinforcement learning</strong>, where an agent first learns the MDP model 
                    from experience and then applies these DP methods to compute optimal policies.
                </p>

                <h3>Value Iteration (VI)</h3>

                <p>
                Let the initial estimate be \(V_0\). We update it as follows: 
                \[
                V_{k+1}(s) = \max_a \left[ R(s, a) + \gamma \sum_{s'} p_T(s' \mid s, a) V_k (s') \right].
                \]
                This is called <strong>Bellman backup</strong>. Each iteration reduces the maximum value function error by a 
                constant factor: 
                \[
                \max_s \| V_{k+1}(s) - V_*(s) \| \leq \gamma \max_s \| V_k (s)  - V_* (s) \|.
                \]
                So, \(V_k\) converges to \(V_*\) as \(k \to \infty\). 
                </p>
                <p>
                    For all possible states \(s\), value iteration computes \(V_*(s)\) and \(\pi_*(s)\), averaging over all possible 
                    next states \(s'\) at each iteration. (Note: If we need the value and policy for only certain starting states, other methods can be used such as 
                    real-time dynamic programming). 
                </p>
                <div class="pseudocode">
                    <span class="pseudocode-title">VALUE_ITERATION</span>
                    <strong>Input:</strong> \(M = \left\langle \mathcal{S}, \mathcal{A}, p_T(s' | s, a), R(s, a), \gamma \right\rangle\)
                    <strong>Output:</strong> \(V_*\), \(\pi_*\)
                    <strong>begin</strong>
                    &emsp;&emsp;Initialize \(V(s)\) arbitrarily for all \(s \in \mathcal{S}\) (typically \(V(s) = 0\))
                    &emsp;&emsp;<strong>repeat:</strong>
                    &emsp;&emsp;&emsp;&emsp;\(\Delta \leftarrow 0\)
                    &emsp;&emsp;&emsp;&emsp;<strong>for each</strong> \(s \in \mathcal{S}\):
                    &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;\(V^{\text{old}}(s) \leftarrow V(s)\)
                    &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;\(V(s) \leftarrow \max_a \left[R(s,a) + \gamma \sum_{s'} p_T(s'|s,a) V(s')\right]\)
                    &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;\(\Delta \leftarrow \max(\Delta, |V^{\text{old}}(s) - V(s)|)\)
                    &emsp;&emsp;<strong>until</strong> \(\Delta < \theta\) (small threshold)
                    &emsp;&emsp;// Extract optimal policy
                    &emsp;&emsp;<strong>for each</strong> \(s \in \mathcal{S}\):
                    &emsp;&emsp;&emsp;&emsp;\(\pi_*(s) \leftarrow \arg\max_a \left[R(s,a) + \gamma \sum_{s'} p_T(s'|s,a) V(s')\right]\)
                    <strong>end</strong>
                </div>

                <h3>Policy Iteration (PI)</h3>
                <p>
                Let \(\mathbf{v}(s) = V_{\pi}(s)\) be the value function encoded as a vector indexed by states \(s\). Also, 
                the reward vector is represented by 
                \[
                \mathbf{r}(s) = \sum_a \pi(a \mid s) R(s, a)
                \]
                and the state transition matrix is written as 
                \[
                \mathbf{T}(s' \mid s) = \sum_a \pi(a \mid s) p_T(s' \mid s, a).
                \]
                Then Bellman's equation for policy evaluation can be represented as a linear system in \(\| \mathcal{S}\|\) unknowns:
                \[
                \mathbf{v} = \mathbf{r} + \gamma \mathbf{T}\mathbf{v}
                \]
                Theoretically, we can solve this by \(\mathbf{v} = (\mathbf{I} - \gamma \mathbf{T})^{-1} \mathbf{r}\), but 
                we can compute \(\mathbf{v}_{t+1} = \mathbf{r} + \gamma \mathbf{T} \mathbf{v}_t\) iteratively. This process is called 
                <strong>policy evaluation</strong>. Once we have evaluated \(V_{\pi}\) for the policy \(\pi\), we need to find a better 
                policy \(\pi'\). 
                </p>
                <p>
                    Now we move on to the <strong>policy improvement</strong> step:
                    \[
                      \pi'(s) = \arg \max_a \{R(s, a) + \gamma \mathbb{E}[V_{\pi}(s')]\}
                    \]
                    This guarantees \(V_{\pi'} \geq V_{\pi}\) because: 
                    \[
                    \begin{align*}
                    \mathbf{v} = \mathbf{r} + \gamma \mathbf{T}\mathbf{v} &\leq  \mathbf{r'} + \gamma \mathbf{T'}\mathbf{v} \\\\
                                                                          &\leq  \mathbf{r'} + \gamma \mathbf{T'}(\mathbf{r'} + \gamma \mathbf{T'}\mathbf{v})\\\\
                                                                          &\leq \cdots  \\\\
                                                                          &= (\mathbf{I} + \gamma \mathbf{T'} + \gamma^2 \mathbf{T'}^2 + \cdots)\mathbf{r'} \\\\
                                                                          &= (\mathbf{I} - \gamma \mathbf{T'})^{-1} \mathbf{r'} \\\\
                                                                          &= \mathbf{v'}

                    \end{align*}
                    \]
                </p>

                <div class="pseudocode">
                    <span class="pseudocode-title">POLICY_ITERATION</span>
                    <strong>Input:</strong> \(M = \left\langle \mathcal{S}, \mathcal{A}, p_T(s' | s, a), R(s, a), \gamma \right\rangle\)
                    <strong>Output:</strong> \(\pi_*\)
                    <strong>begin</strong>
                    &emsp;&emsp;Initialize \(V_{\pi}(s)\) arbitrarily for all \(s \in \mathcal{S}\) (typically \(V_{\pi}(s) = 0\))
                    &emsp;&emsp;Initialize \(\pi(s)\) arbitrarily for all \(s \in \mathcal{S}\)
                    &emsp;&emsp;<strong>repeat:</strong>
                    &emsp;&emsp;&emsp;&emsp;// Policy Evaluation
                    &emsp;&emsp;&emsp;&emsp;<strong>repeat:</strong>
                    &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;\(\Delta \leftarrow 0\)
                    &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<strong>for each</strong> \(s \in \mathcal{S}\):
                    &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;\(V_{\pi}^{\text{old}}(s) \leftarrow V_{\pi}(s)\)
                    &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;\(V_{\pi}(s) \leftarrow \sum_a \pi(a|s) \left[R(s,a) + \gamma \sum_{s'} p_T(s'|s,a) V_{\pi}(s')\right]\)
                    &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;\(\Delta \leftarrow \max(\Delta, |V_{\pi}^{\text{old}}(s) - V_{\pi}(s)|)\)
                    &emsp;&emsp;&emsp;&emsp;<strong>until</strong> \(\Delta < \theta\) (small threshold)
                    &emsp;&emsp;&emsp;&emsp;// Policy Improvement
                    &emsp;&emsp;&emsp;&emsp;\(\text{policy-stable} \leftarrow \text{true}\)
                    &emsp;&emsp;&emsp;&emsp;<strong>for each</strong> \(s \in \mathcal{S}\):
                    &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;\(\text{old-action} \leftarrow \pi(s)\)
                    &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;\(\pi(s) \leftarrow \arg\max_a \left[R(s,a) + \gamma \sum_{s'} p_T(s'|s,a) V_{\pi}(s')\right]\)
                    &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<strong>if</strong> \(\text{old-action} \neq \pi(s)\) <strong>then</strong> \(\text{policy-stable} \leftarrow \text{false}\)
                    &emsp;&emsp;<strong>until</strong> \(\text{policy-stable} = \text{true}\)
                    <strong>end</strong>
                </div>

                <p>
                    In policy iteration algorithm, starting from an initial policy, we alternate between policy evaluation step and 
                    policy improvement step. Within finite iterations, the algorithm will converge to optimal policy since there are at 
                    most \(\|\mathcal{A}\|^{\|\mathcal{S}\|}\) deterministic policies and every update improves the policy. 
                </p>

            </section>

             <section id="MC" class="section-content">
                <h2>Monte Carlo Control</h2>         
                <p>
                    Having established the theoretical foundations of MDPs and dynamic programming, we now turn to <strong>model-free methods</strong> that 
                    learn directly from experience. We begin with <strong>Monte Carlo (MC) control</strong>, a fundamental <strong>value-based method</strong> 
                    that estimates value functions by sampling complete episodes and using actual returns.
                </p>

                <p>The Monte Carlo control method works as follows:</p>
                    <ol style="padding-left: 40px;">
                        <li>The agent takes action \(a\) in state \(s\)</li>
                        <li>Samples the rest of the trajectory according to policy \(\pi\)</li>
                        <li>Computes the actual return \(G_t\) (sum of discounted rewards) from that state-action pair</li>
                        <li>The episode terminates when a terminal state is reached</li>
                    </ol>
                    <br>
                    <p>
                        We use this approach within a <strong>generalized policy iteration</strong> framework to find an optimal policy. 
                        The process alternates between policy evaluation (using MC sampling to estimate \(Q_\pi(s,a)\)) and 
                        policy improvement. At iteration \(k\), the policy improvement step becomes:
                        \[
                        \pi_{k+1}(s) = \arg \max_{a} Q_k (s, a)
                        \] 
                        where \(Q_k\) is estimated using Monte Carlo sampling of complete episodes.
                    </p>

                    <p>
                        Note that to achieve convergence to the optimal policy while maintaining adequate exploration, 
                        we can use an <strong>\(\epsilon\)-greedy policy</strong> that balances exploitation of current 
                        knowledge with exploration of potentially better actions.
                    </p>

                <p>
                    Monte Carlo methods provide <strong>unbiased estimates</strong> since they use actual returns. However, MC 
                    methods are not efficient for value-based RL because they require unrolling complete trajectories whose returns 
                    are sums of random rewards generated by stochastic state transitions, leading to <strong>high variance</strong> 
                    in value estimates. Moreover, MC methods work well only for episodic tasks since they need complete trajectories for each update step to obtain reliable 
                    estimates. This requirement makes them unsuitable for continuing tasks or online learning scenarios.
                </p>

                <p>
                    To address these limitations, we now turn to a more efficient approach that uses <strong>bootstrapping</strong> — updating 
                    estimates based on other estimates rather than waiting for final outcomes. Note that this concept of bootstrapping 
                    in reinforcement learning is distinct from statistical bootstrapping used in other areas of machine learning.
                </p>
             </section>

            <section id="td-learning" class="section-content">
                <h2>Temporal Difference Learning</h2>         

                <p>
                    <strong>Temporal Difference (TD) learning</strong> methods address the limitations of Monte Carlo methods through 
                    <strong>bootstrapping</strong> — updating value estimates using single-step transitions plus current estimates 
                    of successor states rather than waiting for complete returns. In other words, TD methods incrementally reduce the 
                    Bellman error for sampled states by learning from individual transitions rather than complete trajectories. 
                </p>

                <h3>Theoretical Foundation</h3>
                <p>
                    The TD target \(r_t + \gamma V(s_{t+1})\) serves as a sample-based approximation of the Bellman 
                    expectation equation. From our earlier derivation, we know that:
                    \[
                    V_{\pi}(s) = \mathbb{E}_{\pi} \left[ r + \gamma V_{\pi}(s') \mid s \right]
                    \]
                    The TD update uses the observed reward \(r_t\) and next state \(s_{t+1}\) to approximate this expectation, 
                    then adjusts the current estimate toward this improved target. This establishes TD learning as a method 
                    for solving the Bellman equations through successive approximation, without requiring knowledge of 
                    transition probabilities.
                </p>

                <p>
                    Under appropriate conditions (bounded rewards, finite state space, and appropriate learning rate schedules), 
                    tabular TD(0) converges to the true value function \(V_{\pi}(s)\) for the policy being followed.
                </p>

                <h3>TD(0): One-Step Temporal Difference</h3>
                <p>
                    The simplest TD method, <strong>TD(0)</strong>, updates value estimates after each single step. 
                    Suppose that the agent learns the value function \(V_{\pi}\) for a fixed policy \(\pi\).
                    Given a state transition \((s, a, r, s')\) where \(a \sim \pi(s)\), we update the estimate \(V(s)\) as follows:
                    \[
                    V(s_t) \leftarrow V(s_t) + \alpha \left[ r_t + \gamma V(s_{t+1}) - V(s_t) \right]
                    \]
                    where:
                    <ul style="padding-left: 40px;">
                    <li>\(\alpha \in (0,1]\) is the learning rate</li>
                    <li>\(r_t + \gamma V(s_{t+1})\) is the <strong>TD target</strong></li>
                    <li>\(\delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)\) is the <strong>TD error</strong></li>
                    </ul>
                </p>

                <h3>n-Step TD Methods</h3>
                <p>
                    TD methods can be generalized to look ahead multiple steps. The <strong>n-step return</strong> is defined as:
                    \[
                    G_{t:t+n} = R_{t+1} + \gamma R_{t+2} + \cdots + \gamma^{n-1} R_{t+n} + \gamma^n V(S_{t+n})
                    \]
                    The corresponding n-step TD update becomes:
                    \[
                    V(S_t) \leftarrow V(S_t) + \alpha [G_{t:t+n} - V(S_t)]
                    \]
                    This provides a spectrum of methods: \(n=1\) gives TD(0), and as \(n\) increases toward the episode length, 
                    the method approaches Monte Carlo behavior by using longer actual return sequences.
                </p>

                <h3>TD(λ) and λ-returns</h3>
                <p>
                    Rather than using a fixed n-step lookahead, <strong>TD(λ)</strong> methods use a weighted average of all n-step returns. 
                    The <strong>λ-return</strong> is defined as:
                    \[
                    G_t^{\lambda} = (1-\lambda) \sum_{n=1}^{\infty} \lambda^{n-1} G_{t:t+n}
                    \]
                    where \(\lambda \in [0,1]\) controls the weighting between different n-step returns.
                </p>

                <p>
                    The parameter \(\lambda\) creates a different type of interpolation than n-step methods:
                    <ul style="padding-left: 40px;">
                    <li>When \(\lambda = 0\): Only the 1-step return is used, reducing to TD(0)</li>
                    <li>When \(\lambda = 1\): For episodic tasks, this gives Monte Carlo behavior through full credit assignment</li>
                    <li>For \(0 < \lambda < 1\): Creates a weighted blend that emphasizes shorter-term returns but includes longer-term effects</li>
                    </ul>
                </p>

                <p>
                    Note that TD(λ) with \(\lambda = 1\) achieves Monte Carlo-equivalent behavior through a different mechanism than 
                    n-step TD with large n. While n-step methods extend the lookahead horizon, TD(λ) uses eligibility traces to 
                    distribute credit backward through time, allowing for online updates even in continuing tasks.
                </p>

                <p>
                    The λ-return provides a principled way to interpolate between the low variance but biased estimates of TD(0) 
                    and the high variance but unbiased estimates of Monte Carlo methods. <strong>TD(0) is thus a special case</strong> 
                    of the more general TD(λ) framework. Importantly, both n-step TD and TD(λ) can achieve Monte Carlo behavior, 
                    but through different parameterizations: n-step methods by extending the lookahead horizon (large n), 
                    and TD(λ) methods through the weighting parameter (\(\lambda = 1\)).
                </p>

                <h3>Function Approximation</h3>
                <p>
                    More generally, for function approximation where \(V_\mathbf{w}(s)\) is parameterized by weights \(\mathbf{w}\), 
                    the TD(0) update becomes:
                    \[
                    \mathbf{w} \leftarrow \mathbf{w} + \alpha [ r_t + \gamma V_{\mathbf{w}}(s_{t+1}) - V_{\mathbf{w}}(s_t)]\nabla_{\mathbf{w}}V_{\mathbf{w}}(s_t)
                    \]
                    Note that with function approximation, TD methods may not always converge, unlike the tabular case.
                </p>

                <h3>Key Advantages</h3>
                <p>
                    TD methods offer several key advantages that make them central to modern reinforcement learning:
                    <ul style="padding-left: 40px;">
                    <li><strong>Online learning</strong>: Updates occur after each time step, enabling continuous learning without waiting for episode completion.</li>
                    <li><strong>Computational efficiency</strong>: No need to store or process entire episode returns.</li>
                    <li><strong>Lower variance</strong>: TD(0) has lower variance due to bootstrapping, though this comes at the cost of introducing bias.</li>
                    <li><strong>Applicability to continuing tasks</strong>: Can learn in non-episodic environments where episodes never terminate.</li>
                    </ul>
                </p>
                
            </section>

            <section id="TD_Con" class="section-content">
                <h2>TD Control Methods: Q-Learning & SARSA</h2>

                 <p>
                    Having established the theoretical foundations of temporal difference learning, we now turn to specific 
                    <strong>TD control algorithms</strong> that learn optimal policies by estimating action-value functions. 
                    While the previous section focused on TD methods for policy evaluation (learning V_π), these methods 
                    extend TD learning to the control problem by learning Q-functions and deriving policies from them.
                </p>

                <p>
                    Both Q-learning and SARSA are temporal difference methods that bootstrap using single-step transitions, 
                    but they differ fundamentally in their approach to policy learning: one learns the optimal policy 
                    directly (off-policy), while the other learns the policy it follows (on-policy).
                </p>

                <h3>SARSA (On-Policy TD Control)</h3>
                <p>
                    The agent follows a policy \(\pi\) to learn the Q-function in every step to select actions. Under a transition \((s, a, r, s')\), the TD update rule is: 
                    \[
                    Q(s, a) \leftarrow Q(s, a) + \alpha \left[ r + \gamma Q(s', a') - Q(s, a) \right]
                    \]
                    where \(a' \sim \pi(s')\) is the action that the agent will take in state \(s'\). This makes SARSA <strong>on-policy</strong>.
                    <br><br>
                    Note: SARSA is named after the augmented transition \((s, a, r, s', a')\).
                </p>

                <h3>Q-Learning (Off-Policy TD Control)</h3>
                <p>
                    Instead of the sampled next action \(a' \sim \pi(s')\) in SARSA, we use a greed action in \(s'\):
                    \[
                    a' = \arg \max_b Q(s', b),
                    \]
                    making it <strong>off-policy</strong>. Thus the TD update rule for a transition \((s, a, r, s')\) is given by:  
                    \[
                    Q(s, a) \leftarrow Q(s, a) + \alpha \left[ r_t + \gamma \max_{b} Q(s', b) - Q(s, a) \right].
                    \]
                </p>

                <h3>Key Differences</h3>
                <p>
                    <ul style="padding-left: 40px;">
                    <li><strong>Convergence</strong>: Q-learning converges to \(Q_*\) (optimal action-value function) regardless of policy followed; SARSA converges to \(Q_{\pi}\) for the specific policy being followed</li>
                    <li><strong>Policy dependence</strong>: SARSA accounts for the actual exploration policy in its updates, while Q-learning assumes optimal action selection regardless of the behavioral policy</li>
                    <li><strong>Behavioral differences</strong>: In some environments, SARSA may learn different policies than Q-learning because it considers the exploration policy's actual behavior, while Q-learning optimizes assuming greedy action selection</li>
                    </ul>
                </p>

                <p>
                    Both algorithms require exploration to discover good actions, leading us to the fundamental challenge 
                    of balancing exploration and exploitation.
                </p>
            </section>

            <section id="policy-grad" class="section-content">
                <h2>Policy Gradient Methods</h2>

                <p>
                    Unlike value-based methods that learn value functions and derive policies implicitly, 
                    <strong>policy-based methods</strong> directly parameterize and optimize policies without necessarily 
                    learning value functions. These methods address fundamental limitations of value-based approaches, 
                    particularly in continuous action spaces and when stochastic policies are desired.
                </p>

                <p>
                    Our objective is the expected return of a policy: 
                    \[
                    \begin{align*}
                    J(\pi) &= \mathbb{E}_{p_0(s_0)}[V_{\pi}(s_0)] \\\\
                           &= \mathbb{E}_{p_0(s_0)\pi(a_0 \mid s_0)}[Q_{\pi}(s_0, a_0)].
                    \end{align*}
                    \]
                </p>

                <p>
                    Let \(\pi_{\theta}\) be parameterized by \(\mathbf{\theta}\). We compute the gradient of the objective with respect to \(\mathbf{\theta}\). 
                    \[
                    \begin{align*}
                    \nabla_{\mathbf{\theta}} J(\pi_{\theta}) 
                                    &= \mathbb{E}_{p_0(s_0)} \left[\nabla_{\mathbf{\theta}} \left(\sum_{a_0}\pi_{\mathbf{\theta}}(a_0 \mid s_0)Q_{\pi_{\theta}}(s_0, a_0) \right) \right] \\\\
                                    &= \mathbb{E}_{p_0(s_0)} \left[\sum_{a_0} \nabla \pi_{\mathbf{\theta}}(a_0 \mid s_0)Q_{\pi_{\theta}}(s_0, a_0) \right] 
                                        + \mathbb{E}_{p_0(s_0)\pi_{\mathbf{\theta}}(a_0 \mid s_0)} [\nabla_{\mathbf{\theta}}Q_{\pi_{\mathbf{\theta}}}(s_0, a_0)].
                                     
                    \end{align*}
                    \]
                    Here, 
                    \[
                    \begin{align*}
                    \nabla_{\mathbf{\theta}}Q_{\pi_{\mathbf{\theta}}}(s_0, a_0) 
                    &= \nabla_{\mathbf{\theta}}\left[R(s_0, a_0) + \gamma \mathbb{E}_{p_T(s_1 \mid s_0, a_0)} [V_{\pi_{\mathbf{\theta}}}(s_1)]\right] \\\\
                    &= \gamma \nabla_{\mathbf{\theta}} \mathbb{E}_{p_T(s_1 \mid s_0, a_0)} [V_{\pi_{\mathbf{\theta}}}(s_1)].
                    \end{align*}
                    \]
                    Repeating the same steps, we obtain the <strong>policy gradient theorem</strong>: 
                    \[
                    \begin{align*}
                    \nabla_{\theta}J(\pi_{\theta}) 
                    &= \sum_{t = 0}^{\infty} \gamma^t \mathbb{E}_{p_t(s)} \left[\sum_a \nabla_{\theta}\pi_{\theta}(a \mid s) Q_{\pi_{\theta}}(s, a)\right] \\\\
                    &= \frac{1}{1 -\gamma} \mathbb{E}_{p_{\pi_{\theta}}^{\infty}(s)} \left[ \sum_a \nabla_{\theta}\pi_{\theta}(a \mid s) Q_{\pi_{\theta}}(s, a) \right] \\\\
                    &= \frac{1}{1 -\gamma} \mathbb{E}_{p_{\pi_{\theta}}^{\infty}(s) \pi_{\theta}(a \mid s)} \left[ \nabla_{\theta} \log \pi_{\theta}(a \mid s) Q_{\pi_{\theta}}(s, a) \right]
                    \end{align*}
                    \]
                    where \(p_t(s)\) is the probability of visiting s in time \(t\) is the agent starts with \(s_0 \sim p_0\) following \(\pi_{\theta}\), and 
                    \[
                    p_{\pi_{\theta}}^{\infty}(s) = (1 - \gamma) \sum_{t = 0}^{\infty} \gamma^t p_t(s)
                    \]
                    is the normalized discounted state visitation distribution. 
                </p>

                <p>
                    To reduce high variance, we can introduce a baseline:
                    \[
                    b(s) = V_{\pi_{\theta}}(s)
                    \]
                    and then the policy gradient theorem becomes: 
                    \[
                    \nabla_{\theta}J(\pi_{\theta}) 
                    = \frac{1}{1 -\gamma} \mathbb{E}_{p_{\pi_{\theta}}^{\infty}(s) \pi_{\theta}(a \mid s)} \left[ \nabla_{\theta} \log \pi_{\theta}(a \mid s) (Q_{\pi_{\theta}}(s, a) - b(s)) \right].
                    \]
                </p>

                <p>
                    <strong>REINFORCE</strong> is the fundamental policy gradient algorithm that uses Monte Carlo estimation:
                    \[
                    \begin{align*}
                    \nabla_\theta J(\pi_\theta) 
                    &= \frac{1}{1 -\gamma} \mathbb{E}_{p_{\pi_{\theta}}^{\infty}(s) \pi_{\theta}(a \mid s)} \left[ \nabla_{\theta} \log \pi_{\theta}(a \mid s) Q_{\pi_{\theta}}(s, a) \right]\\\\
                    &\approx \sum_{t=0}^{T-1} \gamma^t G_t \nabla_\theta \log \pi_\theta(a_t \mid s_t)
                    \end{align*}
                    \]
                    where \(G_t^{(i)}\) is the return from time \(t\):
                    \[
                    G_t &= r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \cdots \gamma^{T-r-1} r_{t-1}.
                    \]
                    
                    Here, using a baseline in the gradient estimte, we obtain the REINFORCE update rule: 
                    \[
                    \mathbf{\theta} \leftarrow \mathbf{\theta} + \alpha \sum_{t = 0}^{T -1} \gamma^t (G_t - b(s_t)) \nabla_\theta \log \pi_\theta (a_t \mid s_t).
                    \]
                </p>

                <div class="pseudocode">
                    <span class="pseudocode-title">REINFORCE</span>
                    <strong>begin</strong>
                    &emsp;&emsp;Set initial policy parameters \(\mathbf{\theta}\), and baseline parameters \(\mathbf{w}\)
                    &emsp;&emsp;<strong>repeat:</strong>
                    &emsp;&emsp;&emsp;&emsp;Sample an episode \(\tau = (s_0, a_0, r_0, s_1, cdots, s_T)\) using the policy \(\pi_\theta\)
                    &emsp;&emsp;&emsp;&emsp;Compute \(G_t\) for all \(t \in \{0, 1, \cdots, T-1\}\)
                    &emsp;&emsp;&emsp;&emsp;<strong>for</strong> \(t = 0, 1, \cdots, T-1\) <strong>do</strong>
                    &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; \(\delta = G_t - V_\mathbf{w}(s_t)\) 
                    &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; \(\mathbf{w} \leftarrow \mathbf{w} + \alpha_{\mathbf{w}} \delta \nabla_{\mathbf{w}}V_{\mathbf{w}(s_t)}\)
                    &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; \(\mathbf{\theta} \leftarrow \mathbf{\theta} + \alpha_{\mathbf{\theta}} \gamma^t \delta \nabla_{\mathbf{\theta}} \log \pi_{\mathbf{\theta}}(a_t \mid s_t) \)
                    &emsp;&emsp;<strong>until</strong> converged
                    <strong>end</strong>
                </div>

            </section>

            <section id="actor-critic" class="section-content">
                <h2>Actor-Critic Methods</h2>

                <p>
                    <strong>Actor-critic methods</strong> are a class of <strong>policy-based methods</strong> that address 
                    the high variance problem of pure policy gradient approaches like REINFORCE. Rather than using only 
                    policy gradients or only value functions, these hybrid methods combine the benefits of both approaches 
                    by maintaining two function approximators:
                    <ul style="padding-left: 40px;">
                        <li><strong>Actor</strong> \(\pi_\theta(a \mid s)\): the policy that selects actions (policy-based component)</li>
                        <li><strong>Critic</strong> \(V_\phi(s)\) or \(Q_\phi(s,a)\): the value function that evaluates actions (value-based component)</li>
                    </ul>
                </p>

                 <p>
                    The critic provides a baseline to reduce the variance of policy gradient estimates while maintaining 
                    the policy-based nature of the method. The advantage function is defined as:
                    \[
                    A^{\pi_\theta}(s,a) = Q^{\pi_\theta}(s,a) - V^{\pi_\theta}(s)
                    \]
                    leading to the actor-critic policy gradient:
                    \[
                    \nabla_\theta J(\theta) = \mathbb{E}_{\pi_\theta}[\nabla_\theta \log \pi_\theta(a \mid s) A^{\pi_\theta}(s,a)]
                    \]
                </p>

                <p>
                    In practice, the temporal difference (TD) error can serve as an unbiased estimate of the advantage function under the current policy:
                    \[
                    \delta_t = r_t + \gamma V_\phi(s_{t+1}) - V_\phi(s_t)
                    \]
                    This allows us to approximate \(A^{\pi_\theta}(s_t, a_t) \approx \delta_t\), providing computational efficiency while maintaining theoretical soundness.
                </p>

                <p>
                    Actor-critic methods offer several benefits:
                    <ul style="padding-left: 40px;">
                        <li>Significantly lower variance than pure policy gradient methods like REINFORCE</li>
                        <li>More sample efficient than Monte Carlo methods through bootstrapping</li>
                        <li>Can learn in both discrete and continuous action spaces</li>
                        <li>Better convergence properties with function approximation compared to value-based methods</li>
                        <li>Natural framework for incorporating domain knowledge</li>
                    </ul>
                </p>

                <p>
                    Key actor-critic algorithms include:
                    <ul style="padding-left: 40px;">
                        <li><strong>A3C (Asynchronous Advantage Actor-Critic)</strong>: Uses multiple parallel agents</li>
                        <li><strong>DDPG (Deep Deterministic Policy Gradient)</strong>: For continuous control</li>
                        <li><strong>SAC (Soft Actor-Critic)</strong>: Incorporates maximum entropy principle</li>
                        <li><strong>PPO (Proximal Policy Optimization)</strong>: Clips policy updates for stability</li>
                    </ul>
                </p>
            </section>

            

        </div>
        <script src="/js/main.js"></script>    
    </body>
</html>