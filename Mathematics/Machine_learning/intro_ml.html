---
layout: default
title: Intro to Machine Learning 
level: detail
description: Intro to machine learning basics.
uses_math: true
uses_python: false
---
<!DOCTYPE html>
<html>
    <body> 
        <div class="hero-section">
            <h1 class="webpage-name">Intro to Machine Learning 
            </h1>
        </div>

        <div class="topic-nav">
            <a href="#intro">The Law of Intelligence</a></li>
            <a href="#sup">Supervised vs Unsupervised</a>
            <a href="#basic">Basic Categories of ML</a>
            <a href="#process">Standard Process of ML</a>
        </div> 

        <div class="container">  
            <section id="intro" class="section-content">
                <h2>The Law of Intelligence</h2>

                <p>
                <strong>Artificial intelligence (AI)</strong> has become one of the most influential technologies of our time, powering 
                applications from search engines to self-driving cars. Before diving into its technical details, it's worth stepping back 
                and asking: what is <strong>intelligence</strong> itself, and what does it mean to replicate it artificially?
                </p>

                <p>
                For example, the laws of motion and aerodynamics govern both natural and human-made flight. 
                We accept without hesitation that birds can fly through the air, and we trust airplanes to carry us safely across continents. 
                This shared trust comes from our understanding of the same physical principles that explain both. Similarly, if we could uncover 
                the fundamental laws of intelligence, we might someday build machines that "think" with the same confidence we have in machines 
                that fly.
                </p>

                <p>
                Even though creating a truly intelligent system — one that rivals the flexibility and generality of the human mind — remains an 
                open scientific challenge, we do have guiding principles. Modern approaches to AI are built on frameworks like <strong>Bayesian decision theory</strong> 
                and <strong>information processing</strong>. These form the theoretical foundation for <strong>machine learning (ML)</strong>, which is 
                a subfield of artificial intelligence that focuses on developing algorithms that enable computers to learn from data and improve 
                their performance on specific tasks over time.
                </p>


                <p>
                    A widely cited formal definition of machine learning comes from computer scientist Tom M. Mitchell:
                 </p>
                    
                <blockquote>
                    <strong>A computer program is said to learn from experience \(E\), with respect to some class of tasks \(T\) and 
                    performance measure \(P\), if its performance at tasks in \(T\), as measured by \(P\), improves with experience \(E\).</strong>
                    <small>(T. Mitchell, <em>Machine Learning</em>, McGraw Hill, 1997)</small>
                </blockquote>  


                <p>
                    One branch of machine learning, called <strong>deep learning</strong>, utilizes large <strong>neural networks</strong> to perform complex tasks such as:   
                    <ul style="padding-left: 40px;">
                      <li><strong>Autonomous vehicles</strong>: Self-driving cars and drones leverage deep learning to interpret sensor data, navigate environments, and make real-time decisions, enhancing transportation safety and efficiency.</li>
                      <li><strong>Medical diagnostics</strong>: Deep learning models analyze medical images and patient data to detect diseases such as cancer and heart conditions, enabling early diagnosis and personalized treatment plans.</li>
                      <li><strong>Scientific discovery</strong>: AI accelerates research in fields like materials science and genomics by predicting molecular structures and interactions, leading to breakthroughs in drug development and sustainable materials.</li>
                    </ul>
                </p>
                <p>
                    One of the most impactful applications of deep learning today is the development of <strong>Large Language Models (LLMs)</strong>. 
                    These models, such as <strong>GPT-4.5</strong> by OpenAI, <strong>Gemini 2.5 Pro</strong> by Google DeepMind, <strong>Claude 3.7 Sonnet</strong> 
                    by Anthropic, and <strong>Llama 3</strong> by Meta, are built using deep neural networks — specifically the <strong>transformer</strong> architecture — 
                    and are trained on massive text datasets. LLMs have demonstrated remarkable capabilities in language understanding, text generation, translation, and 
                    even <strong>reasoning</strong>. They represent the cutting edge of deep learning research and are a driving force behind the current AI revolution.
                </p>
                
                <p>
                    Despite their impressive performance, current large language models still have fundamental limitations. While they can generate 
                    fluent text and mimic reasoning patterns, they lack true understanding, contextual grounding, and self-awareness. These models rely 
                    on vast amounts of data and computational power, and they still fall short of the efficiency, adaptability, and generalization abilities 
                    seen in human cognition. This invites a deeper question: what makes natural intelligence so effective — and how might we capture some 
                    of that power in artificial systems?
                </p>
                
                <p>
                    In nature, intelligent organisms often rely on <em>heuristics</em> — simple, fast approximations — rather than fully rational or optimal solutions. 
                    The human brain, for example, can recognize faces and objects almost instantly, sometimes even producing optical illusions due to its shortcuts. 
                    This observation suggests that the future of AI may benefit not only from mathematics and computer science, but also from insights in neuroscience, 
                    cognitive science, and psychology. 
                </p>
                
                <p>
                To build truly intelligent machines, we may need to understand not just how to compute optimally, but how to approximate intelligently.
                </p>
                    
            </section>

            <section id="sup" class="section-content">
                <h2>Supervised vs Unsupervised</h2>
                <p>
                    Machine learning encompasses various approaches, primarily distinguished by the presence or absence of labeled data:
                    <ul style="padding-left: 40px;">
                        <li><strong>Supervised Learning:</strong></li>
                        This approach involves training models on datasets that include both input features and corresponding output labels. 
                        The model learns to map inputs to outputs, enabling it to make predictions on new, unseen data. Common applications 
                        include image classification, spam detection, and medical diagnosis.

                        <li><strong>Unsupervised Learning:</strong></li> 
                        This approach deals with unlabeled data. The model attempts to uncover hidden patterns or intrinsic structures within 
                        the data. Techniques like clustering and dimensionality reduction fall under this category, with applications in 
                        customer segmentation, anomaly detection, and exploratory data analysis.
                    </ul>
                </p>
                <p>
                    Additionally, modern machine learning has introduced hybrid approaches:
                    <ul style="padding-left: 40px;">
                        <li><strong>Semi-Supervised Learning:</strong></li>
                        Combines a small amount of labeled data with a large amount of unlabeled data during training. This method is 
                        particularly useful when labeling data is expensive or time-consuming.
                        <li><strong>Self-Supervised Learning:</strong></li>
                        A subset of unsupervised learning where the system generates its own labels from the input data. This approach 
                        has gained prominence in training large language models and computer vision systems.
                    </ul>
                </p>
            </section>

            <section id="basic" class="section-content">
                <h2>Basic Categories of Machine Learning</h2>
                <p>
                    Machine learning tasks are broadly categorized based on the nature of the prediction or pattern recognition involved:
                    <ul style="padding-left: 40px;">
                        <li><strong>Regression</strong>: Predicts continuous numerical values.</li>
                        Examples: Forecasting stock prices, estimating real estate values, predicting temperature changes.
                        <br>
                        Methods: Linear regression

                        <li><strong>Classification</strong>: Assigns inputs into predefined discrete categories.</li>
                        Examples: Email spam detection, handwriting recognition, disease diagnosis.
                        <br>
                        Methods: Logistic regression, support vector machines, decision trees, and neural networks.

                        <li><strong>Clustering</strong>: Groups similar data points without predefined labels.</li>
                        Examples: Customer segmentation, document categorization, image grouping.
                        <br>
                        Methods: K-means clustering, hierarchical clustering.

                        <li><strong>Dimensionality Reduction</strong>: Reduces the number of input variables in a dataset while preserving essential information.</li>
                        Examples: Data visualization, noise reduction, feature extraction.
                        <br>
                        Methods: Principal Component Analysis (PCA), t-Distributed Stochastic Neighbor Embedding (t-SNE), Autoencoders.

                        <li><strong>Reinforcement Learning</strong>: Trains agents to make sequences of decisions by rewarding desired behaviors and punishing undesired ones.</li>
                        Examples: Robotics control, game playing, autonomous driving.
                        <br>
                        Methods: Q-Learning, Deep Q-Networks (DQN), Proximal Policy Optimization (PPO).
                    </ul>      
                </p>
            </section>

            <section id="process" class="section-content">
                <h2>Standard Process of ML</h2>
                <p>
                    The machine learning process involves several key steps to develop effective models:
                    <ol style="padding-left: 40px;">
                        <li><strong>Problem Definition</strong>:</li>
                        Clearly articulate the problem and determine whether machine learning is an appropriate solution.
                        <li><strong>Data Collection</strong>:</li>
                        Gather relevant data from various sources, ensuring quality and representativeness.
                        <li><strong>Data Preprocessing</strong>:</li>
                        Clean and prepare the data by handling missing values, encoding categorical variables, and normalizing features.
                        <li><strong>Data Splitting</strong>:</li>
                        Divide the dataset into training, validation, and test sets to evaluate model performance effectively.
                        <li><strong>Model and Optimization Procedure Selection</strong>:</li>
                        Choose suitable algorithms based on the problem type (e.g., regression, classification) and data characteristics.
                        <li><strong>Training</strong>:</li>
                        Feed the training data into the model, allowing it to learn patterns and relationships.
                        <li><strong>Evaluation</strong>:</li>
                        Assess the model's performance using the validation set and appropriate metrics (e.g., accuracy, precision, recall).
                        <li><strong>Hyperparameter Tuning</strong>:</li>
                        Optimize model parameters to enhance performance, often using techniques like grid search or random search.
                        <li><strong>Testing</strong>:</li>
                        Evaluate the final model on the test set to estimate its performance on unseen data.
                        <li><strong>Deployment</strong>:</li>
                        Integrate the model into a production environment where it can make real-time predictions.
                        <li><strong>Monitoring and Maintenance</strong>:</li>
                        Continuously monitor the model's performance and update it as necessary to accommodate new data or changing conditions.
                    </ol>   
                </p>
            </section>

        </div>
        <script src="/js/main.js"></script> 
    </body>
</html>