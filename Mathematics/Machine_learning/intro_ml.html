---
layout: default
title: Intro to Machine Learning 
level: detail
description: Intro to machine learning basics.
uses_math: true
uses_python: false
---
<!DOCTYPE html>
<html>
    <body> 
        <div class="hero-section">
            <h1 class="webpage-name">Intro to Machine Learning 
            </h1>
        </div>

        <div class="topic-nav">
            <a href="#intro">The Law of Intelligence</a></li>
            <a href="#sup">Supervised vs Unsupervised</a>
            <a href="#basic">Basic Categories of ML</a>
            <a href="#process">Standard Process of ML</a>
        </div> 

        <div class="container">  
            <section id="intro" class="section-content">
                <h2>The Law of Intelligence</h2>

                <p>
                <strong>Artificial intelligence (AI)</strong> has become one of the most influential technologies of our time, powering 
                applications from search engines to self-driving cars. Before diving into its technical details, it's worth stepping back 
                and asking: what is <strong>intelligence</strong> itself, and what does it mean to replicate it artificially?
                </p>

                <p>
                For example, the laws of motion and aerodynamics govern both natural and human-made flight. 
                We accept without hesitation that birds can fly through the air, and we trust airplanes to carry us safely across continents. 
                This shared trust comes from our understanding of the same physical principles that explain both. Similarly, if we could uncover 
                the fundamental laws of intelligence, we might someday build machines that "think" with the same confidence we have in machines 
                that fly.
                </p>

                <p>
                Even though creating a truly intelligent system — one that rivals the flexibility and generality of the human mind — remains an 
                open scientific challenge, we do have guiding principles. Modern approaches to AI are built on frameworks like <strong>Bayesian decision theory</strong> 
                and <strong>information processing</strong>. These form the theoretical foundation for <strong>machine learning (ML)</strong>, which is 
                a subfield of artificial intelligence that focuses on developing algorithms that enable computers to learn from data and improve 
                their performance on specific tasks over time.
                </p>


                <p>
                    A widely cited formal definition of machine learning comes from computer scientist Tom M. Mitchell:
                 </p>
                    
                <blockquote>
                    <strong>A computer program is said to learn from experience \(E\), with respect to some class of tasks \(T\) and 
                    performance measure \(P\), if its performance at tasks in \(T\), as measured by \(P\), improves with experience \(E\).</strong>
                    <small>(T. Mitchell, <em>Machine Learning</em>, McGraw Hill, 1997)</small>
                </blockquote>  


                <p>
                    One branch of machine learning, called <strong>deep learning</strong>, utilizes large <strong>neural networks</strong> to perform complex tasks such as:
                </p>
                    
                    <ul style="padding-left: 40px;">
                      <li><strong>Autonomous vehicles</strong>: Self-driving cars and drones leverage deep learning to interpret sensor data, navigate environments, and make real-time decisions, enhancing transportation safety and efficiency.</li>
                      <li><strong>Medical diagnostics</strong>: Deep learning models analyze medical images and patient data to detect diseases such as cancer and heart conditions, enabling early diagnosis and personalized treatment plans.</li>
                      <li><strong>Scientific discovery</strong>: AI accelerates research in fields like materials science and genomics by predicting molecular structures and interactions, leading to breakthroughs in drug development and sustainable materials.</li>
                    </ul>
                
                <p>
                    One of the most impactful applications of deep learning today is the development of <strong>Large Language Models (LLMs)</strong>. 
                    These models, such as <strong>GPT-4.5</strong> by OpenAI, <strong>Gemini 2.5 Pro</strong> by Google DeepMind, <strong>Claude 3.7 Sonnet</strong> 
                    by Anthropic, and <strong>Llama 3</strong> by Meta, are built using deep neural networks — specifically the <strong>transformer</strong> architecture — 
                    and are trained on massive text datasets. LLMs have demonstrated remarkable capabilities in language understanding, text generation, translation, and 
                    even <strong>reasoning</strong>. They represent the cutting edge of deep learning research and are a driving force behind the current AI revolution.
                </p>
                
                <p>
                    Despite their impressive performance, current large language models still have fundamental limitations. While they can generate 
                    fluent text and mimic reasoning patterns, they lack true understanding, contextual grounding, and self-awareness. These models rely 
                    on vast amounts of data and computational power, and they still fall short of the efficiency, adaptability, and generalization abilities 
                    seen in human cognition. This invites a deeper question: what makes natural intelligence so effective — and how might we capture some 
                    of that power in artificial systems?
                </p>
                
                <p>
                    In nature, intelligent organisms often rely on <em>heuristics</em> — simple, fast approximations — rather than fully rational or optimal solutions. 
                    The human brain, for example, can recognize faces and objects almost instantly, sometimes even producing optical illusions due to its shortcuts. 
                    This observation suggests that the future of AI may benefit not only from mathematics and computer science, but also from insights in neuroscience, 
                    cognitive science, and psychology. 
                </p>
                
                <p>
                To build truly intelligent machines, we may need to understand not just how to compute optimally, but how to approximate intelligently.
                </p>
                    
            </section>

            <section id="sup" class="section-content">
                <h2>Supervised vs Unsupervised</h2>
                <p>
                    
                </p>
            </section>

            <section id="basic" class="section-content">
                <h2>Basic Categories of Machine Learning</h2>
                <p>
                    <ol style="padding-left: 40px;">
                        <li><strong>Regression</strong></li>
                        Predicting a continuous value based on input features.
                        <br>
                        Examples: Forecasting temperature, stock prices. Predicting housing price based on features.
                        <br>
                        Methods: Linear regression, polynomial regression, ridge regression, 
                        <li><strong>Classification</strong></li>
                        Assigning inputs into predefined categories or labels.
                        <br>
                        Examples: Spam detection, Diagnosis of disease, Image recognition
                        <br>
                        Methods: Logistic regression, support vector machines, decision trees, and neural networks.
                        <li><strong>Unsupervised Learning</strong></li>
                        Learning structures from data without labeled outputs.
                        <br>
                        Examples: Finding similar images, Community detection.
                        <br>
                        Methods: K-means clustering, hierarchical clustering, PCA, autoencoders
                    </ol>      
                </p>
            </section>

            <section id="process" class="section-content">
                <h2>Standard Process of ML</h2>
                <p>
                    <ol style="padding-left: 40px;">
                        <li><strong>Collect Data</strong></li>
                        Randomly split the data into Train and Test. 
                        <li><strong>Choose a Hypothesis Class or Model</strong></li>
                        Capture the patterns in the data
                        <li><strong>Choose a Loss Function</strong></li>
                        <li><strong>Choose an Optimization Procedure</strong></li>
                        <li><strong>Justify the Accuracy of the Estimation(Prediction)</strong></li>
                        Apply the model to predict future unknown observations.
                    </ol>   
                </p>
            </section>

        </div>
        <script src="/js/main.js"></script> 
    </body>
</html>