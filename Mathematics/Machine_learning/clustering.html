---
layout: default
title: Clustering
level: detail
description: Learn about clustering basics via K means clustering and spectral clustering. 
uses_math: true
uses_python: false
---
<!DOCTYPE html>
<html>
    <body> 
        <!-- LearningResource Schema for Clustering -->
        <script type="application/ld+json">
        {
        "@context": "https://schema.org",
        "@type": "LearningResource",
        "name": "Clustering",
        "description": "Learn about clustering basics via K means clustering and spectral clustering",
        "learningResourceType": "lesson",
        "educationalUse": "instruction",
        "educationalLevel": "university",
        "interactivityType": "active",
        "author": {
            "@type": "Person",
            "name": "Yusuke Yokota",
            "jobTitle": "Mathematics & Computer Science Educator",
            "knowsAbout": [
            "Clustering Algorithms",
            "Unsupervised Learning",
            "K-means Clustering",
            "Spectral Clustering",
            "Graph Theory",
            "Machine Learning"
            ]
        },
        "publisher": {
            "@type": "Organization",
            "name": "MATH-CS COMPASS",
            "url": "https://yusukey01.github.io"
        },
        "about": [
            { "@type": "Thing", "name": "Clustering" },
            { "@type": "Thing", "name": "K-means Clustering" },
            { "@type": "Thing", "name": "Spectral Clustering" },
            { "@type": "Thing", "name": "Vector Quantization" },
            { "@type": "Thing", "name": "Unsupervised Learning" },
            { "@type": "Thing", "name": "Graph Laplacian" },
            { "@type": "Thing", "name": "Dirichlet Energy" },
            { "@type": "Thing", "name": "K-means++ Initialization" },
            { "@type": "Thing", "name": "One-hot Encoding" }
        ],
        "teaches": [
            "K-means clustering algorithm and implementation",
            "Vector quantization for data compression",
            "Spectral clustering for non-convex data structures",
            "Graph Laplacian theory and applications",
            "Normalized graph Laplacian techniques",
            "K-means++ initialization strategy",
            "Eigenvector analysis for clustering"
        ],
        "isPartOf": {
            "@type": "Course",
            "name": "Machine Learning",
            "description": "Explore machine learning ideas and applications with mathematical foundations",
            "provider": {
            "@type": "Organization",
            "name": "MATH-CS COMPASS",
            "url": "https://yusukey01.github.io"
            },
            "instructor": {
            "@type": "Person",
            "name": "Yusuke Yokota",
            "jobTitle": "Mathematics & Computer Science Educator"
            },
            "courseCode": "V",
            "hasCourseInstance": {
            "@type": "CourseInstance",
            "courseMode": "online",
            "courseWorkload": "PT5H",
            "instructor": {
                "@type": "Person",
                "name": "Yusuke Yokota"
            }
            },
            "offers": {
            "@type": "Offer",
            "price": "0",
            "priceCurrency": "USD",
            "availability": "https://schema.org/InStock",
            "category": "free"
            }
        }
        }
        </script>

        <!-- WebApplication Schema for Interactive Demo -->
        <script type="application/ld+json">
        {
        "@context": "https://schema.org",
        "@type": "WebApplication",
        "name": "Clustering Interactive Demo",
        "description": "Interactive demonstration of K-means and spectral clustering algorithms with real-time visualization",
        "applicationCategory": "EducationalApplication",
        "operatingSystem": "Web Browser",
        "url": "https://yusukey01.github.io/Mathematics/Machine_learning/clustering.html",
        "author": {
            "@type": "Person",
            "name": "Yusuke Yokota"
        },
        "applicationSubCategory": "Clustering Algorithm Visualization",
        "featureList": [
            "Interactive K-means clustering visualization",
            "Spectral clustering demonstration",
            "Real-time cluster center updates",
            "K-means++ initialization comparison",
            "Algorithm convergence visualization",
            "Multiple dataset generation options"
        ],
        "isAccessibleForFree": true,
        "educationalUse": "instruction",
        "educationalLevel": "university",
        "about": [
            { "@type": "Thing", "name": "K-means Clustering Demo" },
            { "@type": "Thing", "name": "Spectral Clustering Visualization" },
            { "@type": "Thing", "name": "Unsupervised Learning Demo" }
        ]
        }
        </script>
        <div class="hero-section">
            <h1 class="webpage-name">Clustering</h1>
        </div>

        {% include section_navigation.html %}

        <div class="topic-nav">
            <a href="#intro">Introduction</a>
            <a href="#K-means">K-means Clustering</a>
            <a href="#vq">Vector Quantization (VQ)</a>
            <a href="#sp">Spectral Clustering</a>
            <a href="#demo">Demo</a>
        </div> 

        <div class="container">  

            <section id="intro" class="section-content">
                <h2>Introduction</h2>
                <p>
                    <strong>Clustering</strong> is one of the core tasks in <strong>unsupervised learning</strong>. Unlike supervised learning, where models learn from labeled data, 
                    unsupervised learning aims to uncover hidden patterns or structures in data without any predefined labels. Among these tasks, clustering 
                    is particularly important — it groups data points into clusters based on their similarity, revealing the underlying structure of the data.
                </p>

                <p>
                    In previous sections, we explored techniques like <strong>Principal Component Analysis (PCA)</strong> and <strong>autoencoders</strong>, which 
                    reduce the dimensionality of high-dimensional data while preserving meaningful information. These dimensionality reduction methods helped us uncover 
                    compact representations of data, often referred to as <strong>latent features</strong>.
                </p>

                <p>
                    Now that we have the tools to represent complex data in a lower-dimensional space, we turn to the next natural question: 
                    <strong>Can we identify meaningful groupings within this reduced space?</strong> This is the goal of clustering, which allows us to segment 
                    data into coherent groups — for example, grouping similar images, organizing unlabeled documents, or detecting communities in social networks.
                </p>
      
            </section>

            <section id="K-means" class="section-content">
                <h2>K-means Clustering</h2>
                <p>
                    We begin with <strong>K-means clustering</strong>, one of the most widely used and intuitive algorithms.
                </p>
                <p>
                    We assume that there are \(K\) <strong>cluster centers</strong> \(\mu_k \in \mathbb{R}^D\), for \(k = 1, \cdots, K\). 
                    Each data point \(x_n \in \mathbb{R}^D\) is assigned to its nearest center:
                    \[
                    z_n^* = \arg \min_k \| x_n - \mu_k \|_2^2.
                    \]
                </p>

                <p>
                    Given the assignments, the cluster centers are updated as the mean of points in each cluster:
                    \[
                    \mu_k = \frac{1}{N_k} \sum_{n : z_n =k} x_n, \,\text{where } N_k = \sum_{n=1}^N \mathbb{I}[z_n = k].
                    \]
                </p>

                <p>
                    These two steps — assignment and update — are repeated until convergence. 
                    This process can be viewed as minimizing a cost function known as the <strong>distortion</strong>, which 
                    is equivalent to the squared Frobenius norm of the reconstruction error: 
                    \[
                    \begin{align*}
                    J(M, Z) &= \sum_{n = 1}^N \| x_n - \mu_{z_n} \|_2^2 \\\\
                            &= \| X - ZM^\top \|_F^2
                    \end{align*}
                    \] 
                    where \(X \in \mathbb{R}^{N \times D}\), \(Z \in \{0, 1\} ^{N \times K}\), and \(M \in \mathbb{R}^{D \times K}\) contains 
                    the cluster centers \(\mu_k\) in its columns. 
                    <br>
                    Note: \(Z\) is the <strong>one-hot encoding</strong>(or <strong>dummy encoding</strong>) matrix whose entries are 
                    \[
                    Z_{nk} =  \begin{cases}
                                1, &\text{ if \(x_n\) is assigned to cluster \(k\) } \\
                                0, &\text{ otherwise}.
                                \end{cases}
                    \]
                </p>    
            </section>

             <section id="vq" class="section-content">
                <h2>Vector Quantization (VQ)</h2>
                <p>
                    A key idea of <strong>vector quantization</strong> is to compress data by replacing each high-dimensional vector 
                    \(x_n \in \mathbb{R}^D\) with a discrete symbol \(z_n \in \{1, \cdots, K\}\), which is an index into a 
                    <strong>codebook</strong> of \(K\) prototype vectors, \(\mu_k \in \mathbb{R}^D\).
                </p>    
                <p>
                    Each data point is encoded by finding the index of the closest prototype using Euclidean distance:
                    \[
                    z_n := f_e (x_n) = \arg \min_k \| x_n - \mu_k \|^2.
                    \]
                </p>
                <p>
                    The corresponding decoder simply maps the index back to the prototype:
                    \[
                    f_d(k) = \mu_k.
                    \]
                    The reconstruction of each data point is therefore \(\hat{x}_n = f_d(z_n) = \mu_{z_n}\).
                </p>
                <p>
                    The quality of a codebook is measured using the <strong>reconstruction error</strong> (also called distortion):
                    \[
                    \begin{align*}
                    J &= \frac{1}{N} \sum_{n =1}^N \| x_n - f_d (f_e (x_n))\|^2  \\\\
                      &= \frac{1}{N} \sum_{n=1}^N \|x_n - \mu_{z_n} \|^2.
                    \end{align*}
                    \]
                    Indeed, this is equivalent to the cost function minimized by the <strong>K-means algorithm</strong>, which can be interpreted 
                    as a special case of vector quantization where the codebook is learned by minimizing distortion via iterative updates. VQ is 
                    more focused on signal compression and encoding, whereas K-means is usually employed for data analysis and clustering tasks.
                </p>
                
            </section>

             <section id="sp" class="section-content">
                <h2>Spectral Clustering</h2>
                <p>
                    Traditional clustering methods like K-means assume clusters are linearly separable and spherical in shape. 
                    However, many real-world datasets exhibit complex, non-convex structures that are not well-captured by distance alone. 
                    <strong>Spectral clustering</strong> addresses this limitation by transforming the data into a new space that reflects 
                    the connectivity structure of the data, often represented as a <strong>graph</strong>.
                </p>

                <p>
                    The idea is to build a <strong>similarity graph</strong> where each data point is a node, and edges encode pairwise similarities. 
                    Let \(\mathbf{W} \in \mathbb{R}^{N \times N}\) be a symmetric <strong>weight matrix</strong> such that \(W_{ij} = W_{ji}  \geq 0\) measures 
                    the similarity between points \(i\) and \(j\). The <strong>degree</strong> of node \(i\) is defined as:
                    \[
                    d_i = \sum_{j=1}^N W_{ij},
                    \]
                    and we define the <strong>degree matrix</strong> \(\mathbf{D} \in \mathbb{R}^{N \times N}\) as a diagonal matrix with entries \(D_{ii} = d_i\).
                </p>

                <p>
                    The fundamental object in spectral clustering is the <strong>graph Laplacian</strong>, defined as:
                    \[
                    \mathbf{L} = \mathbf{D} - \mathbf{W}.
                    \]
                    So, the elements of \(\mathbf{L}\) are given by 
                    \[
                     \begin{cases}
                        d_i &\text{if \(i = j\)} \\
                        -W_{ij} &\text{if \(i \neq j\) & \(W_{ij} \neq 0\)} \\
                        0 &\text{Otherwise}.
                    \end{cases}
                    \]
                    The graph Laplacian captures the structure of the graph in a way that supports clustering via <strong>eigenvector analysis</strong>.
                </p>

                <p>
                    <div class="theorem">
                        <span class="theorem-title">Theorem:</span>
                        Let \(G\) be an undirected (possibly weighted) graph with Laplacian \(\mathbf{L} = \mathbf{D} - \mathbf{W}\). Then the number of connected 
                        components of \(G\) is equal to the multiplicity \(k\) of the eigenvalue 0 of \(\mathbf{L}\). Moreover, the eigenspace corresponding 
                        to eigenvalue 0 is spanned by the indicator vectors \(\mathbf{1}_{S_1}, \ldots, \mathbf{1}_{S_k}\), where each \(S_j\) is a 
                        connected component of \(G\).
                    </div>
                    This makes spectral methods especially powerful for separating well-connected groups in a graph.

                     <div class="proof">
                        <span class="proof-title">Proof:</span>
                        Let \(G = (V, E)\) be an undirected (possibly weighted) graph with Laplacian \(\mathbf{L} = \mathbf{D} - \mathbf{W}\), 
                        where \(\mathbf{W}\) is a symmetric weight matrix and \(\mathbf{D}\) is the diagonal degree matrix \(D_{ii} = \sum_j W_{ij}\).

                        <br><br>
                        For any vector \(\mathbf{f} \in \mathbb{R}^N\), the quadratic form of the Laplacian is:
                        \[
                        \mathbf{f}^\top \mathbf{L} \mathbf{f} = \mathbf{f}^\top (\mathbf{D} - \mathbf{W})\mathbf{f}
                        = \sum_i D_{ii} f_i^2 - \sum_{i,j} W_{ij} f_i f_j.
                        \]

                        Since \(D_{ii} = \sum_j W_{ij}\), we can write:
                        \[
                        \sum_i D_{ii} f_i^2 = \sum_{i,j} W_{ij} f_i^2.
                        \]

                        So the full expression becomes:
                        \[
                        \mathbf{f}^\top \mathbf{L} \mathbf{f} = \sum_{i,j} W_{ij} f_i^2 - \sum_{i,j} W_{ij} f_i f_j \tag{1}
                        \]

                        Now, since \(W\) is symmetric, \(W_{ij} = W_{ji}\), then:
                        \[
                        \begin{align*}
                        \sum_{i,j} W_{ij} f_i^2 &= \frac{1}{2} \sum_{i,j} W_{ij}f_i^2 + \frac{1}{2} \sum_{i,j} W_{ji}f_j^2 \\\\
                                                &= \frac{1}{2} \sum_{i,j} W_{ij}(f_i^2 + f_j^2).
                        \end{align*}
                        \]

                        Plugging this into the expression (1):
                        \[
                        \begin{align*}
                        \mathbf{f}^\top \mathbf{L} \mathbf{f} &= \frac{1}{2} \sum_{i,j} W_{ij}(f_i^2 + f_j^2) - \sum_{i,j} W_{ij} f_i f_j \\\\
                                                              &= \frac{1}{2} \sum_{i,j} W_{ij}(f_i^2 + f_j^2 - 2f_i f_j) \\\\
                                                              &= \boxed{\frac{1}{2} \sum_{i,j} W_{ij} (f_i - f_j)^2.}
                         \end{align*}
                        \]

                        <br>
                        <strong>Case 1: \(k = 1\)</strong><br>
                        Suppose \(\mathbf{f} \in \mathbb{R}^n\) is an eigenvector corresponding to eigenvalue 0, i.e., \(\mathbf{L} \mathbf{f} = 0\). Then:
                        \[
                        \mathbf{f}^\top \mathbf{L} \mathbf{f} = 0 \quad \Rightarrow \quad \sum_{i,j} W_{ij}(f_i - f_j)^2 = 0.
                        \]

                        Since \(W_{ij} \geq 0\), each term \((f_i - f_j)^2\) must be zero wherever \(W_{ij} > 0\). That is, for every edge \((i,j) \in E\), we have:
                        \[
                        f_i = f_j.
                        \]

                        Because the graph is connected, there is a path between any two vertices. Applying the above equality recursively along paths in the graph implies that:
                        \[
                        f_i = f_j \quad \text{for all } i,j.
                        \]

                        Therefore, \(\mathbf{f}\) must be constant on all vertices — i.e., \(\mathbf{f} \in \text{span}\{\mathbf{1}\}\). Hence, the nullspace of \(\mathbf{L}\) 
                        is one-dimensional and spanned by the constant vector \(\mathbf{1} = [1, 1, \dots, 1]^\top\). This confirms that the eigenvalue 0 has multiplicity 1.
                        <br><br>

                        <strong>Case 2: \(k > 1\)</strong><br>
                        Then \(\mathbf{W}\) and \(\mathbf{L}\) can be written in block diagonal form, where each block corresponds to one connected component. 
                        Specifically,
                        \[
                        \mathbf{L} = \begin{bmatrix}
                                        L_1 & 0 & \cdots & 0 \\
                                        0 & L_2 & \cdots & 0 \\
                                        \vdots & \vdots & \ddots & \vdots \\
                                        0 & 0 & \cdots & L_k
                                    \end{bmatrix},
                        \]
                        where each \(L_i\) is the Laplacian of the \(i\)-th connected component. 
                        From Case 1, each \(L_i\) has nullspace spanned by the constant vector \(\mathbf{1}_{S_i}\) 
                        (the indicator vector on component \(S_i\)). Therefore, the full nullspace of \(\mathbf{L}\) is spanned 
                        by \(\mathbf{1}_{S_1}, \ldots, \mathbf{1}_{S_k}\), and the eigenvalue 0 has multiplicity \(k\).
                     </div>
                </p>
                <p>
                    <strong>Note:</strong> Clearly, the graph Laplacian \(\mathbf{L}\) is symmetric and positive semi-definite, since 
                    \(\mathbf{f}^\top \mathbf{L} \mathbf{f} \geq 0\) for all \(\mathbf{f} \in \mathbb{R}^N\). This expression is called the 
                    <strong>Laplacian quadratic form</strong>, or the <strong>Dirichlet energy</strong> of \(\mathbf{f}\):
                    \[
                    \mathbf{f}^\top \mathbf{L} \mathbf{f} = \frac{1}{2} \sum_{i,j} W_{ij}(f_i - f_j)^2.
                    \]
                    It measures the <strong>smoothness</strong> of the function \(\mathbf{f}\) on the graph: it becomes small when adjacent nodes 
                    (i.e., those with \(W_{ij} > 0\)) have similar values. 
                    <br><br>
                    The Dirichlet energy plays a central role in <strong>spectral graph theory</strong>. In spectral clustering, minimizing Dirichlet 
                    energy reveals groupings where nodes are strongly connected internally but weakly connected across clusters — 
                    making it a powerful tool for discovering natural partitions in complex data.
                </p>

                <p>
                    In practice, graphs are often exhibit irregular structure: 
                    nodes may have highly varying degrees, and clusters are not perfectly block-separated. 
                    This means the raw graph Laplacian \(\mathbf{L} = \mathbf{D} - \mathbf{W}\) can be dominated by high-degree nodes, 
                    leading to unbalanced or misleading spectral embeddings.
                </p>

                <p>
                    To address this, we use the <strong>normalized graph Laplacian</strong>, which compensates for 
                    differences in node connectivity and ensures that each node contributes more equally to the 
                    spectral structure: 
                    \[
                    \begin{align*}
                    \mathbf{L}_{\text{sym}} &= \mathbf{D}^{-\frac{1}{2}} \mathbf{L} \mathbf{D}^{-\frac{1}{2}} \\\\
                                   &= \mathbf{I} - \mathbf{D}^{-\frac{1}{2}} \mathbf{W} \mathbf{D}^{-\frac{1}{2}}
                    \end{align*}
                    \]
                    where \(\mathbf{D}\) is the diagonal degree matrix and \(\mathbf{W}\) is the symmetric weight (similarity) matrix. In this case, 
                    for the normalized graph Laplacian \(\mathbf{L}_{\text{sym}}\), the eigenspace of zero eigenvalue is spanned by 
                    \[
                    \mathbf{D}^{\frac{1}{2}} \mathbf{1}_{S_k}
                    \]
                    ,where \(\mathbf{1}_{S_k}\) are the indicator vectors of the connected components.
                </p>

                <p>
                    This matrix \(\mathbf{L}_{\text{sym}}\) is also symmetric and positive semi-definite. 
                    It ensures that the resulting spectral embedding is not biased toward high-degree nodes.
                </p>

                <p>
                    The <strong>spectral clustering algorithm</strong> proceeds as follows:
                    <div class="theorem">
                    <ol style="padding-left: 40px;">
                        <li>Compute the symmetric normalized Laplacian \(\mathbf{L}_{\text{sym}} = \mathbf{I} - \mathbf{D}^{-\frac{1}{2}} \mathbf{W} \mathbf{D}^{-\frac{1}{2}}\).</li>
                        <li>Find the smallest \(K\) eigenvectors of \(\mathbf{L}_{\text{sym}}\) and stack them column-wise into a matrix \(\mathbf{U} \in \mathbb{R}^{N \times K}\).</li>
                        <li>Normalize each row of \(\mathbf{U}\) to have unit norm, forming a matrix \(\mathbf{T} \in \mathbb{R}^{N \times K}\): 
                            \[
                            T_{i \cdot} = \frac{U_{i \cdot}}{\| U_{i \cdot}\|}.
                            \]
                        </li>
                        <li>Apply K-means clustering to the rows of \(\mathbf{T}\).</li>
                        <li>Assign the original data point \(x_i\) to cluster \(k\) if row \(i\) of \(\mathbf{T}\) is assigned to cluster \(k\).</li>
                    </ol>
                    </div>
                </p>                            
            </section>

             <section id="demo" class="section-content">
                <h2>Demo</h2>
                <div id="clustering_visualizer"></div>
                <p>
                    <strong>Note:</strong> In random initialization, we simply pick \(K\) data points uniformly at random to serve as the initial cluster centers. 
                    While this method is fast, it can be highly sensitive to outliers or imbalanced data distributions, often leading to poor cluster quality 
                    or slow convergence. To address this, the <strong>K-means++ initialization</strong> strategy is widely used in practice. 
                    It selects initial centers more carefully by favoring points that are far apart, significantly improving the stability and performance of K-means clustering.
                </p>

                 <div class="theorem">
                        <span class="theorem-title">Algorithm: K-means++ Initialization</span>
                <ol style="padding-left: 40px;">
                    <li>Choose the first center \(\mu_1\) uniformly at random from the dataset \(\{x_1, \ldots, x_N\}\).</li>
                    <li>For each remaining point \(x_i\), compute its squared distance to the nearest selected center:
                        \[
                        D(x_i) = \min_{1 \leq j \leq m} \|x_i - \mu_j\|_2^2
                        \]
                        where \(\mu_j\) is one of the centers already chosen.
                    </li>
                    <li>Choose the next center \(\mu_{m+1}\) from the data points with probability proportional to \(D(x_i)\):
                        \[
                        \Pr(x_i \text{ is chosen}) = \frac{D(x_i)}{\sum_j D(x_j)}
                        \]
                    </li>
                    <li>Repeat steps 2-3 until \(K\) centers have been selected.</li>
                    <li>Proceed with the standard K-means algorithm using these \(K\) initial centers.</li>
                </ol>
                 </div>

            </section>


        </div>
        <script src="/js/main.js"></script>  
        <script src="/js/clustering_visualizer.js"></script>  
    </body>
</html>