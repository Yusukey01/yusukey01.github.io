---
layout: default
title: Clustering
level: detail
description: Learn about clustering basics via K means clustering and spectral clustering. 
uses_math: true
uses_python: false
---
<!DOCTYPE html>
<html>
    <body> 
        <div class="hero-section">
            <h1 class="webpage-name">Clustering</h1>
        </div>

        <div class="topic-nav">
            <a href="#intro">Introduction</a>
            <a href="#K-means">K-means Clustering</a>
            <a href="#vq">Vector Quantization (VQ)</a>
            <a href="#sp">Spectral Clustering</a>
            <a href="#demo">Demo</a>
        </div> 

        <div class="container">  

            <section id="intro" class="section-content">
                <h2>Introduction</h2>
                <p>
                    <strong>Clustering</strong> is one of the core tasks in <strong>unsupervised learning</strong>. Unlike supervised learning, where models learn from labeled data, 
                    unsupervised learning aims to uncover hidden patterns or structures in data without any predefined labels. Among these tasks, clustering 
                    is particularly important — it groups data points into clusters based on their similarity, revealing the underlying structure of the data.
                </p>

                <p>
                    In previous sections, we explored techniques like <strong>Principal Component Analysis (PCA)</strong> and <strong>autoencoders</strong>, which 
                    reduce the dimensionality of high-dimensional data while preserving meaningful information. These dimensionality reduction methods helped us uncover 
                    compact representations of data, often referred to as <strong>latent features</strong>.
                </p>

                <p>
                    Now that we have the tools to represent complex data in a lower-dimensional space, we turn to the next natural question: 
                    <strong>Can we identify meaningful groupings within this reduced space?</strong> This is the goal of clustering, which allows us to segment 
                    data into coherent groups — for example, grouping similar images, organizing unlabeled documents, or detecting communities in social networks.
                </p>
      
            </section>

            <section id="K-means" class="section-content">
                <h2>K-means Clustering</h2>
                <p>
                    We begin with <strong>K-means clustering</strong>, one of the most widely used and intuitive algorithms.
                </p>
                <p>
                    We assume that there are \(K\) <strong>cluster centers</strong> \(\mu_k \in \mathbb{R}^D\), for \(k = 1, \cdots, K\). 
                    Each data point \(x_n \in \mathbb{R}^D\) is assigned to its nearest center:
                    \[
                    z_n^* = \arg \min_k \| x_n - \mu_k \|_2^2.
                    \]
                </p>

                <p>
                    Given the assignments, the cluster centers are updated as the mean of points in each cluster:
                    \[
                    \mu_k = \frac{1}{N_k} \sum_{n : z_n =k} x_n, \,\text{where } N_k = \sum_{n=1}^N \mathbb{I}[z_n = k].
                    \]
                </p>

                <p>
                    These two steps — assignment and update — are repeated until convergence. 
                    This process can be viewed as minimizing a cost function known as the <strong>distortion</strong>, which 
                    is equivalent to the squared Frobenius norm of the reconstruction error: 
                    \[
                    \begin{align*}
                    J(M, Z) &= \sum_{n = 1}^N \| x_n - \mu_{z_n} \|_2^2 \\\\
                            &= \| X - ZM^\top \|_F^2
                    \end{align*}
                    \] 
                    where \(X \in \mathbb{R}^{N \times D}\), \(Z \in \{0, 1\} ^{N \times K}\), and \(M \in \mathbb{R}^{D \times K}\) contains 
                    the cluster centers \(\mu_k\) in its columns. 
                    <br>
                    Note: \(Z\) is the <strong>one-hot eocoding</strong>(or <strong>dummy encoding</strong>) matrix whose entries are 
                    \[
                    Z_{nk} =  \begin{cases}
                                1, &\text{ if \(x_n\) is assigned to cluster \(k\) } \\
                                0, &\text{ otherwise}.
                                \end{cases}
                    \]
                </p>    
            </section>

             <section id="vq" class="section-content">
                <h2>Vector Quantization (VQ)</h2>
                <p>
                    A key idea of <strong>vector quantization</strong> is to compress data by replacing each high-dimensional vector 
                    \(x_n \in \mathbb{R}^D\) with a discrete symbol \(z_n \in \{1, \cdots, K\}\), which is an index into a 
                    <strong>codebook</strong> of \(K\) prototype vectors, \(\mu_k \in \mathbb{R}^D\).
                </p>    
                <p>
                    Each data point is encoded by finding the index of the closest prototype using Euclidean distance:
                    \[
                    z_n := f_e (x_n) = \arg \min_k \| x_n - \mu_k \|^2.
                    \]
                </p>
                <p>
                    The corresponding decoder simply maps the index back to the prototype:
                    \[
                    f_d(k) = \mu_k.
                    \]
                    The reconstruction of each data point is therefore \(\hat{x}_n = f_d(z_n) = \mu_{z_n}\).
                </p>
                <p>
                    The quality of a codebook is measured using the <strong>reconstruction error</strong> (also called distortion):
                    \[
                    \begin{align*}
                    J &= \frac{1}{N} \sum_{n =1}^N \| x_n - f_d (f_e (x_n))\|^2  \\\\
                      &= \frac{1}{N} \sum_{n=1}^N \|x_n - \mu_{z_n} \|^2.
                    \end{align*}
                    \]
                    Indeed, this is equivalent to the cost function minimized by the <strong>K-means algorithm</strong>, which can be interpreted 
                    as a special case of vector quantization where the codebook is learned by minimizing distortion via iterative updates. VQ is 
                    more focused on signal compression and encoding, whereas K-means is usually employed for data analysis and clustering tasks.
                </p>
                
            </section>

             <section id="sp" class="section-content">
                <h2>Spectral Clustering</h2>
                <p>
                    Traditional clustering methods like K-means assume clusters are linearly separable and spherical in shape. 
                    However, many real-world datasets exhibit complex, non-convex structures that are not well-captured by distance alone. 
                    <strong>Spectral clustering</strong> addresses this limitation by transforming the data into a new space that reflects 
                    the connectivity structure of the data, often represented as a <strong>graph</strong>.
                </p>

                <p>
                    The idea is to build a <strong>similarity graph</strong> where each data point is a node, and edges encode pairwise similarities. 
                    Let \(\mathbf{W} \in \mathbb{R}^{N \times N}\) be a symmetric <strong>weight matrix</strong> such that \(W_{ij} = W_{ji}  \geq 0\) measures 
                    the similarity between points \(i\) and \(j\). The <strong>degree</strong> of node \(i\) is defined as:
                    \[
                    d_i = \sum_{j=1}^N W_{ij},
                    \]
                    and we define the <strong>degree matrix</strong> \(\mathbf{D} \in \mathbb{R}^{N \times N}\) as a diagonal matrix with entries \(D_{ii} = d_i\).
                </p>

                <p>
                    The fundamental object in spectral clustering is the <strong>graph Laplacian</strong>, defined as:
                    \[
                    \mathbf{L} = \mathbf{D} - \mathbf{W}.
                    \]
                    So, the elements of \(\mathbf{L}\) are given by 
                    \[
                     \begin{cases}
                        d_i &\text{if \(i = j\)} \\
                        -W_{ij} &\text{if \(i \neq j\) & \(W_{ij} \neq 0\)} \\
                        0 &\text{Otherwise}.
                    \end{cases}
                    \]
                    The graph Laplacian captures the structure of the graph in a way that supports clustering via <strong>eigenvector analysis</strong>.
                </p>

                <p>
                    <div class="theorem">
                        <span class="theorem-title">Theorem:</span>
                        Let \(G\) be an undirected (possibly weighted) graph with Laplacian \(\mathbf{L} = \mathbf{D} - \mathbf{W}\). Then the number of connected 
                        components of \(G\) is equal to the multiplicity \(k\) of the eigenvalue 0 of \(\mathbf{L}\). Moreover, the eigenspace corresponding 
                        to eigenvalue 0 is spanned by the indicator vectors \(\mathbf{1}_{S_1}, \ldots, \mathbf{1}_{S_k}\), where each \(S_j\) is a 
                        connected component of \(G\).
                    </div>
                    This makes spectral methods especially powerful for separating well-connected groups in a graph.

                     <div class="proof">
                        <span class="proof-title">Proof:</span>
                        Let \(G = (V, E)\) be an undirected (possibly weighted) graph with Laplacian \(\mathbf{L} = \mathbf{D} - \mathbf{W}\), 
                        where \(\mathbf{W}\) is a symmetric weight matrix and \(\mathbf{D}\) is the diagonal degree matrix \(D_{ii} = \sum_j W_{ij}\).

                        <br><br>
                        For any vector \(\mathbf{f} \in \mathbb{R}^n\), the quadratic form of the Laplacian is:
                        \[
                        \mathbf{f}^\top L \mathbf{f} = \mathbf{f}^\top (D - W)\mathbf{f}
                        = \sum_i D_{ii} f_i^2 - \sum_{i,j} W_{ij} f_i f_j.
                        \]

                        Since \(D_{ii} = \sum_j W_{ij}\), we can write:
                        \[
                        \sum_i D_{ii} f_i^2 = \sum_{i,j} W_{ij} f_i^2.
                        \]

                        So the full expression becomes:
                        \[
                        \mathbf{f}^\top L \mathbf{f} = \sum_{i,j} W_{ij} f_i^2 - \sum_{i,j} W_{ij} f_i f_j.
                        \]

                        Now, observe that because \(W\) is symmetric (\(W_{ij} = W_{ji}\)), we can symmetrize both terms:
                        \[
                        \begin{aligned}
                        \sum_{i,j} W_{ij} f_i^2 &= \frac{1}{2} \sum_{i,j} W_{ij}(f_i^2 + f_j^2), \\
                        \sum_{i,j} W_{ij} f_i f_j &= \sum_{i,j} W_{ij} f_i f_j \quad \text{(unchanged since symmetric)}.
                        \end{aligned}
                        \]

                        Plugging this into the quadratic form:
                        \[
                        \mathbf{f}^\top L \mathbf{f} = \frac{1}{2} \sum_{i,j} W_{ij}(f_i^2 + f_j^2 - 2f_i f_j).
                        \]

                        Applying the algebraic identity \((f_i - f_j)^2 = f_i^2 + f_j^2 - 2f_i f_j\), we get:
                        \[
                        \boxed{
                            \mathbf{f}^\top L \mathbf{f} = \frac{1}{2} \sum_{i,j} W_{ij} (f_i - f_j)^2.
                        }
                        \]
                        <br>
                        <strong>Case 1: \(k = 1\)</strong><br>
                        Suppose \(\mathbf{f} \in \mathbb{R}^n\) is an eigenvector corresponding to eigenvalue 0, i.e., \(L \mathbf{f} = 0\). Then:
                        \[
                        \mathbf{f}^\top \mathbf{L} \mathbf{f} = 0 \quad \Rightarrow \quad \sum_{i,j} W_{ij}(f_i - f_j)^2 = 0.
                        \]

                        Since \(W_{ij} \geq 0\), each term \((f_i - f_j)^2\) must be zero wherever \(W_{ij} > 0\). That is, for every edge \((i,j) \in E\), we have:
                        \[
                        f_i = f_j.
                        \]

                        Because the graph is connected, there is a path between any two vertices. Applying the above equality recursively along paths in the graph implies that:
                        \[
                        f_i = f_j \quad \text{for all } i,j.
                        \]

                        Therefore, \(\mathbf{f}\) must be constant on all vertices — i.e., \(\mathbf{f} \in \text{span}\{\mathbf{1}\}\). Hence, the nullspace of \(\mathbf{L}\) 
                        is one-dimensional and spanned by the constant vector \(\mathbf{1} = [1, 1, \dots, 1]^\top\). This confirms that the eigenvalue 0 has multiplicity 1.
                        <br><br>

                        <strong>Case 2: \(k > 1\)</strong><br>
                        Then \(\mathbf{W}\) and \(\mathbf{L}\) can be written in block diagonal form, where each block corresponds to one connected component. 
                        Specifically,
                        \[
                        \mathbf{L} = \begin{bmatrix}
                                        L_1 & 0 & \cdots & 0 \\
                                        0 & L_2 & \cdots & 0 \\
                                        \vdots & \vdots & \ddots & \vdots \\
                                        0 & 0 & \cdots & L_k
                                    \end{bmatrix},
                        \]
                        where each \(L_i\) is the Laplacian of the \(i\)-th connected component. 
                        From Case 1, each \(L_i\) has nullspace spanned by the constant vector \(\mathbf{1}_{S_i}\) 
                        (the indicator vector on component \(S_i\)). Therefore, the full nullspace of \(\mathbf{L}\) is spanned 
                        by \(\mathbf{1}_{S_1}, \ldots, \mathbf{1}_{S_k}\), and the eigenvalue 0 has multiplicity \(k\).
                     </div>
                </p>

                <p>
                    In practice, graphs are often exhibit irregular structure: 
                    nodes may have highly varying degrees, and clusters are not perfectly block-separated. 
                    This means the raw graph Laplacian \(\mathbf{L} = \mathbf{D} - \mathbf{W}\) can be dominated by high-degree nodes, 
                    leading to unbalanced or misleading spectral embeddings.
                </p>

                <p>
                    To address this, we use the <strong>normalized graph Laplacian</strong>, which compensates for 
                    differences in node connectivity and ensures that each node contributes more equally to the 
                    spectral structure: 
                    \[
                    \begin{align*}
                    \mathbf{L}_{\text{sym}} &= \mathbf{D}^{-\frac{1}{2}} \mathbf{L} \mathbf{D}^{-\frac{1}{2}} \\\\
                                   &= \mathbf{I} - \mathbf{D}^{-\frac{1}{2}} \mathbf{W} \mathbf{D}^{-\frac{1}{2}}
                    \end{align*}
                    \]
                    where \(\mathbf{D}\) is the diagonal degree matrix and \(\mathbf{W}\) is the symmetric weight (similarity) matrix. In this case, 
                    the eigenspace of 0 is spanned by \(\mathbf{D}^{\frac{1}{2}} \mathbf{1}_{S_k}\)
                </p>

                <p>
                    This matrix \(\mathbf{L}_{\text{sym}}\) is symmetric, positive semi-definite, and has important theoretical properties. 
                    In particular, it ensures that the resulting spectral embedding is not biased toward high-degree nodes.
                </p>


                <h3>Algorithm: Spectral Clustering</h3>
                <p>
                    The spectral clustering algorithm proceeds as follows:
                   
                    <ol style="padding-left: 40px;">
                        <li>Compute the symmetric normalized Laplacian \(\mathbf{L}_{\text{sym}} = \mathbf{I} - \mathbf{D}^{-\frac{1}{2}} \mathbf{W} \mathbf{D}^{-\frac{1}{2}}\).</li>
                        <li>Find the smallest \(K\) eigenvectors of \(\mathbf{L}_{\text{sym}}\) and stack them column-wise into a matrix \(\mathbf{U} \in \mathbb{R}^{N \times K}\).</li>
                        <li>Normalize each row of \(\mathbf{U}\) to have unit norm, forming a matrix \(\mathbf{T} \in \mathbb{R}^{N \times K}\): 
                            \[
                            T_{i \cdot} = \frac{U_{i \cdot}}{\| U_{i \cdot}\|}.
                            \]
                        </li>
                        <li>Apply K-means clustering to the rows of \(\mathbf{T}\).</li>
                        <li>Assign the original data point \(x_i\) to cluster \(k\) if row \(i\) of \(\mathbf{T}\) is assigned to cluster \(k\).</li>
                    </ol>
                </p>                            
            </section>

             <section id="demo" class="section-content">
                <h2>Demo</h2>
                <div id="clustering_visualizer"></div>
            </section>


        </div>
        <script src="/js/main.js"></script>  
        <script src="/js/clustering_visualizer.js"></script>  
    </body>
</html>