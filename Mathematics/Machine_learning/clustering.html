---
layout: default
title: Clustering
level: detail
description: Learn about clustering basics via K means clustering and spectral clustering. 
uses_math: true
uses_python: false
---
<!DOCTYPE html>
<html>
    <body> 
        <div class="hero-section">
            <h1 class="webpage-name">Clustering</h1>
        </div>

        <div class="topic-nav">
            <a href="#intro">Introduction</a>
            <a href="#K-means">K-means Clustering</a>
            <a href="#vq">Vector Quantization (VQ)</a>
            <a href="#sp">Spectral Clustering</a>
            <a href="#demo">Demo</a>
        </div> 

        <div class="container">  

            <section id="intro" class="section-content">
                <h2>Introduction</h2>
                <p>
                    <strong>Clustering</strong> is one of the core tasks in <strong>unsupervised learning</strong>. Unlike supervised learning, where models learn from labeled data, 
                    unsupervised learning aims to uncover hidden patterns or structures in data without any predefined labels. Among these tasks, clustering 
                    is particularly important — it groups data points into clusters based on their similarity, revealing the underlying structure of the data.
                </p>

                <p>
                    In previous sections, we explored techniques like <strong>Principal Component Analysis (PCA)</strong> and <strong>autoencoders</strong>, which 
                    reduce the dimensionality of high-dimensional data while preserving meaningful information. These dimensionality reduction methods helped us uncover 
                    compact representations of data, often referred to as <strong>latent features</strong>.
                </p>

                <p>
                    Now that we have the tools to represent complex data in a lower-dimensional space, we turn to the next natural question: 
                    <strong>Can we identify meaningful groupings within this reduced space?</strong> This is the goal of clustering, which allows us to segment 
                    data into coherent groups — for example, grouping similar images, organizing unlabeled documents, or detecting communities in social networks.
                </p>
      
            </section>

            <section id="K-means" class="section-content">
                <h2>K-means Clustering</h2>
                <p>
                    We begin with <strong>K-means clustering</strong>, one of the most widely used and intuitive algorithms.
                </p>
                <p>
                    We assume that there are \(K\) <strong>cluster centers</strong> \(\mu_k \in \mathbb{R}^D\), for \(k = 1, \cdots, K\). 
                    Each data point \(x_n \in \mathbb{R}^D\) is assigned to its nearest center:
                    \[
                    z_n^* = \arg \min_k \| x_n - \mu_k \|_2^2.
                    \]
                </p>

                <p>
                    Given the assignments, the cluster centers are updated as the mean of points in each cluster:
                    \[
                    \mu_k = \frac{1}{N_k} \sum_{n : z_n =k} x_n, \,\text{where } N_k = \sum_{n=1}^N \mathbb{I}[z_n = k].
                    \]
                </p>

                <p>
                    These two steps — assignment and update — are repeated until convergence. 
                    This process can be viewed as minimizing a cost function known as the <strong>distortion</strong>, which 
                    is equivalent to the squared Frobenius norm of the reconstruction error: 
                    \[
                    \begin{align*}
                    J(M, Z) &= \sum_{n = 1}^N \| x_n - \mu_{z_n} \|_2^2 \\\\
                            &= \| X - ZM^\top \|_F^2
                    \end{align*}
                    \] 
                    where \(X \in \mathbb{R}^{N \times D}\), \(Z \in \{0, 1\} ^{N \times K}\), and \(M \in \mathbb{R}^{D \times K}\) contains 
                    the cluster centers \(\mu_k\) in its columns. 
                    <br>
                    Note: \(Z\) is the <strong>one-hot eocoding</strong>(or <strong>dummy encoding</strong>) matrix whose entries are 
                    \[
                    Z_{nk} =  \begin{cases}
                                1, &\text{ if \(x_n\) is assigned to cluster \(k\) } \\
                                0, &\text{ otherwise}.
                                \end{cases}
                    \]
                </p>    
            </section>

             <section id="vq" class="section-content">
                <h2>Vector Quantization (VQ)</h2>
                <p>
                    A key idea of <strong>vector quantization</strong> is to compress data by replacing each high-dimensional vector 
                    \(x_n \in \mathbb{R}^D\) with a discrete symbol \(z_n \in \{1, \cdots, K\}\), which is an index into a 
                    <strong>codebook</strong> of \(K\) prototype vectors, \(\mu_k \in \mathbb{R}^D\).
                </p>    
                <p>
                    Each data point is encoded by finding the index of the closest prototype using Euclidean distance:
                    \[
                    z_n := f_e (x_n) = \arg \min_k \| x_n - \mu_k \|^2.
                    \]
                </p>
                <p>
                    The corresponding decoder simply maps the index back to the prototype:
                    \[
                    f_d(k) = \mu_k.
                    \]
                    The reconstruction of each data point is therefore \(\hat{x}_n = f_d(z_n) = \mu_{z_n}\).
                </p>
                <p>
                    The quality of a codebook is measured using the <strong>reconstruction error</strong> (also called distortion):
                    \[
                    \begin{align*}
                    J &= \frac{1}{N} \sum_{n =1}^N \| x_n - f_d (f_e (x_n))\|^2  \\\\
                      &= \frac{1}{N} \sum_{n=1}^N \|x_n - \mu_{z_n} \|^2.
                    \end{align*}
                    \]
                    This is exactly equivalent to the cost function minimized by the <strong>K-means algorithm</strong>, which can be interpreted 
                    as a special case of vector quantization where the codebook is learned by minimizing distortion via iterative updates.
                </p>
                
            </section>

             <section id="sp" class="section-content">
                <h2>Spectral Clustering</h2>
                     
            </section>

             <section id="demo" class="section-content">
                <h2>Demo</h2>
                <div id="clustering_visualizer"></div>
            </section>


        </div>
        <script src="/js/main.js"></script>  
        <script src="/js/clustering_visualizer.js"></script>  
    </body>
</html>