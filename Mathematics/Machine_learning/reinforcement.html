---
layout: default
title: Reinforcement Learning
level: detail
description: Learn about Reinforcement Learning basics.
uses_math: true
uses_python: false
---
<!DOCTYPE html>
<html>
    <body> 
        <div class="hero-section">
            <h1 class="webpage-name">Reinforcement Learning</h1>
        </div>

        {% include section_navigation.html %}

        <div class="topic-nav">
            <a href="#intro">Introduction</a>
            <a href="#"></a>
            <a href="#"></a>
            <a href="#"></a>
            <a href="#demo">Demo</a>
        </div>

        <div class="container">  

            <section id="intro" class="section-content">
                <h2>Introduction</h2>

                <p>
                    <strong>Reinforcement Learning (RL)</strong> is a machine learning framework in which an <strong>agent</strong> interacts 
                    with an environment to learn a behavior that maximizes a scalar <strong>reward</strong> signal over time. 
                    Formally, the environment is typically modeled as a <strong>Markov Decision Process (MDP)</strong>, defined by a tuple 
                    \[
                    (\mathcal{S}, \mathcal{A}, \mathcal{T}, r, \gamma)
                    \]
                    where \(\mathcal{S}\) is the state space, \(\mathcal{A}\) is the action space, \(\mathcal{T}\) is the transition function, 
                    \(r\) is the reward function, and \(\gamma\) is the discount factor.
                </p>

                <p>
                    At each time step, the agent observes a state \(s_t \in \mathcal{S}\), selects an action \(a_t \in \mathcal{A}\), receives a 
                    reward \(r_t\), and transitions to a new state \(s_{t+1}\) according to the transition dynamics \(\mathcal{T}(s_{t+1} \mid s_t, a_t)\). 
                    The goal of the agent is to learn a <strong>policy</strong> \(\pi(a \mid s)\) that maximizes the expected cumulative reward:
                    \[
                    \mathbb{E}_\pi \left[ \sum_{t=0}^\infty \gamma^t r_t \right].
                    \]
                </p>

                <p>
                    Reinforcement learning differs from <strong>supervised learning</strong>, where models are trained on ground-truth input-output pairs, 
                    and from <strong>unsupervised learning</strong>, which aims to discover hidden structure or representations in unlabeled data.
                    In RL, learning is driven by a scalar <em>reward signal</em> that provides evaluative feedback based on the agent's interaction with an environment.
                    Unlike supervised learning, there are no correct labels for each input, and unlike unsupervised learning, the objective is not to model data structure 
                    but to learn a policy that maximizes expected cumulative reward over time.
                </p>

                <p>
                    RL has been successfully applied in domains such as robotics, recommendation systems, and game-playing.
                    More recently, RL has also become integral to the training of modern <strong>large language models (LLMs)</strong>. 
                    In particular, a method called <strong>Reinforcement Learning from Human Feedback (RLHF)</strong> is used to fine-tune LLMs 
                    to follow <strong>human preferences</strong>, ensuring outputs are more helpful, safe, and aligned with user intent.
                </p>     
            </section>

            <section id="mdp" class="section-content">
            <h2>Markov Decision Process (MDP)</h2>
            
            <p>
            In this section, we present a probabilistic formulation of Markov Decision Processes (MDPs), where both 
            transitions and rewards are modeled as stochastic variables. This perspective is particularly useful for 
            analyzing trajectory distributions and gradient-based learning methods.
            </p>

            <p>
                An agent sequentially interacts with an initially unknown environment to obtain a <strong>trajectory</strong> or 
                multiple trajectories. 
                The Trajectory of length \(T\) can be defined by: 
                \[
                \mathbf{\tau} = (s_0, a_0, r_0, s_1, a_1, r_1, s_2, \cdots, s_T)
                \]
                where \(s_t\) is a state, \(a_t\) is an action, and \(r_t\) is a reward. 
            </p>
            <p>
                We want to optimize the action-selection policy so that the discounted cumulative reward:
                \[
                G_0 = \sum_{t = 0}^{T -1} \gamma^t r_t 
                \]
                is maximized for some <strong>discount factor</strong> \(\gamma \in [0, 1]\). Here, we consider 
                <strong>Markov decision process (MDP)</strong>, then the generative model for the trajectory \mathbf{\tau} can be 
                factored into single-step models. We call the process to find an optimal policy from trajectories <strong>learning</strong>.
            </p>

            <p>
                We define MDP as a tuple:
                \[
                \left\langle \mathcal{S}, \mathcal{A}, p_T, p_R, p_0 \right\rangle
                \]
                where 
                <ul style="padding-left: 40px;">
                    <li>\(\mathcal{S}\): a set of environment states</li>
                    <li>\(\mathcal{A}\): a set of actions that the agent can take</li>
                    <li>\(p_T\): a transition model</li>
                    <li>\(p_R\): a reward model</li>
                    <li>\(p_0\): an initial state distribution</li>
                </ul>
                At starting time \(t = 0\), the initial state is \(s_0 \sim p_0\). At time \(t \geq 0\), the agent 
                observes the environment state \(s_t \in \mathcal{S}\) following a policy \(\pi\) to take an 
                action \(a_t \in \mathcal{A}\). On the other hand, the environment distributes a reward signal \(r_t \in \mathcal{R}\)
                and enters a new state \(s_{t+1} \in \mathcal{S}\). The policy is stochastic with \(\pi(a | s)\) being the probability 
                of choosing an action \(a\) in state \(s\).
            </p>
            <p>
                The precess at each step is called a <strong>transition</strong>. At tune \(t\), the transition consists of the 
                tuple 
                \[
                (s_t, a_t, r_t, s_{t+1})
                \]
                where 
                <ul style="padding-left: 40px;">
                    <li>\(a_t \sim \pi(s_t)\)</li>
                    <li>\(s_{t+1} \sim p_T(s_t, a_t)\)</li>
                    <li>\(r_t \sim p_R(s_t, a_t, s_{t+1})\).</li>
                </ul>
            </p>
            <p>
                Therefore, under the policy \(\pi\), the probability of generating a trajectory \(\mathbf{\tau}\) of length 
                \(T\) can be expressed by: 
                \[
                p(\mathbf{\tau}) = p_0(s_0)\prod_{t = 0}^{T -1} \pi(a_t | s_t)p_T(s_{t+1}|s_t, a_t)p_R(r_t | s_t, a_t, s_{s+1}).
                \]
                Moreover, we define the <strong>reward function</strong> from the reawrd model \(p_R\) as tje average iommediate reaward of 
                taking action \(a\) in state \(s\) with the next state marginalized:
                \[
                R(s, a) = \mathbb{E}_{p_T(s' | s, a)} \left[\mathbb{E}_{p(r | s, a, s')}[r]\right].
                \]
            </p>

            <p>
            While classical RL literature often defines the reward function deterministically as \( r(s, a) \), 
            we treat it here as a distribution \( p_R(r | s, a, s') \), which allows us to model uncertainty and 
            derive expectations explicitly. The expected reward function is then recovered via:
            \[
            R(s, a) = \mathbb{E}_{p_T(s' | s, a)} \left[\mathbb{E}_{p_R(r | s, a, s')}[r]\right].
            \]
            </p>

            </section>

           <section id="value-functions" class="section-content">
            <h2>Return and Value Functions</h2>
            <p>
                Define return \(G_t = \sum_{k=0}^\infty \gamma^k r_{t+k}\). Introduce value function \(V^\pi(s)\), action-value \(Q^\pi(s, a)\), 
                and optimal versions. Explain their roles in policy evaluation.
            </p>
            </section>

            <section id="td" class="section-content">
            <h2>Temporal Difference Learning</h2>
            <p>
                Explain bootstrapping in TD methods. Introduce TD(0), the update rule, and its connection to 
                Monte Carlo and dynamic programming.
            </p>
            </section>

            <section id="qlearning" class="section-content">
            <h2>Q-Learning and SARSA</h2>
            <p>
                Introduce Q-learning (off-policy) and SARSA (on-policy). Present update rules and compare their exploration behavior. 
                Use gridworld or bandit example.
            </p>
            </section>

            <section id="dqn" class="section-content">
            <h2>Deep Q-Network (DQN)</h2>
            <p>
                Extend Q-learning to function approximation using deep neural networks. Mention key DQN tricks: target networks, 
                experience replay. Briefly note Double DQN and Dueling DQN variants.
            </p>
            </section>

            <section id="policy-gradient" class="section-content">
            <h2>Policy Gradient and Actor-Critic</h2>
            <p>
                Introduce policy-based methods like REINFORCE. Derive the policy gradient objective. 
                Explain limitations and how actor-critic methods address them. Optionally mention PPO or A2C.
            </p>
            </section>

            <section id="onoffpolicy" class="section-content">
            <h2> On-Policy vs. Off-Policy Learning</h2>
            <p>
                Define and compare on-policy and off-policy learning. Explain importance sampling, 
                behavior vs. target policy.
            </p>
            </section>

            <section id="modelbased" class="section-content">
            <h2>Model-Based Reinforcement Learning</h2>
            <p>
                Overview of learning or using environment models. 
                Contrast planning (e.g., value iteration) vs. model-free learning. 
                Mention Model Predictive Control (MPC) briefly.
            </p>
            </section>

            <section id="exploration" class="section-content">
            <h2>Exploration vs. Exploitation</h2>
            <p>
                Discuss the exploration-exploitation tradeoff. Introduce ε-greedy, softmax action selection, 
                and entropy regularization.
            </p>
            </section>

            <section id="rlhf" class="section-content">
            <h2>Reinforcement Learning from Human Feedback (RLHF)</h2>
            <p>
                Introduce RLHF as used in LLM fine-tuning. Explain reward model training from preference data 
                and PPO-based optimization. Mention alternatives like RLAIF.
            </p>
            </section>

            <section id="demo" class="section-content">
                <h2>Demo</h2>
                 <div id=""></div>
            </section>


        </div>
        <script src="/js/main.js"></script>    
    </body>
</html>