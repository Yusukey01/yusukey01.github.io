---
layout: default
title: Reinforcement Learning
level: detail
description: Learn about Reinforcement Learning basics.
uses_math: true
uses_python: false
---
<!DOCTYPE html>
<html>
    <body> 
        <div class="hero-section">
            <h1 class="webpage-name">Reinforcement Learning</h1>
        </div>

        {% include section_navigation.html %}

        <div class="topic-nav">
            <a href="#intro">Introduction</a>
            <a href="#"></a>
            <a href="#"></a>
            <a href="#"></a>
            <a href="#demo">Demo</a>
        </div>

        <div class="container">  

            <section id="intro" class="section-content">
                <h2>Introduction</h2>

                <p>
                    <strong>Reinforcement Learning (RL)</strong> is a machine learning framework in which an <strong>agent</strong> interacts 
                    with an environment to learn a behavior that maximizes a scalar <strong>reward</strong> signal over time. 
                    Formally, the environment is typically modeled as a <strong>Markov Decision Process (MDP)</strong>, defined by a tuple 
                    \[
                    (\mathcal{S}, \mathcal{A}, \mathcal{T}, r, \gamma)
                    \]
                    where \(\mathcal{S}\) is the state space, \(\mathcal{A}\) is the action space, \(\mathcal{T}\) is the transition function, 
                    \(r\) is the reward function, and \(\gamma\) is the discount factor.
                </p>

                <p>
                    At each time step, the agent observes a state \(s_t \in \mathcal{S}\), selects an action \(a_t \in \mathcal{A}\), receives a 
                    reward \(r_t\), and transitions to a new state \(s_{t+1}\) according to the transition dynamics \(\mathcal{T}(s_{t+1} \mid s_t, a_t)\). 
                    The goal of the agent is to learn a <strong>policy</strong> \(\pi(a \mid s)\) that maximizes the expected cumulative reward:
                    \[
                    \mathbb{E}_\pi \left[ \sum_{t=0}^\infty \gamma^t r_t \right].
                    \]
                </p>

                <p>
                    Reinforcement learning differs from <strong>supervised learning</strong>, where models are trained on ground-truth input-output pairs, 
                    and from <strong>unsupervised learning</strong>, which aims to discover hidden structure or representations in unlabeled data.
                    In RL, learning is driven by a scalar <em>reward signal</em> that provides evaluative feedback based on the agent's interaction with an environment.
                    Unlike supervised learning, there are no correct labels for each input, and unlike unsupervised learning, the objective is not to model data structure 
                    but to learn a policy that maximizes expected cumulative reward over time.
                </p>

                <p>
                    RL has been successfully applied in domains such as robotics, recommendation systems, and game-playing.
                    More recently, RL has also become integral to the training of modern <strong>large language models (LLMs)</strong>. 
                    In particular, a method called <strong>Reinforcement Learning from Human Feedback (RLHF)</strong> is used to fine-tune LLMs 
                    to follow <strong>human preferences</strong>, ensuring outputs are more helpful, safe, and aligned with user intent.
                </p>     
            </section>

            <section id="mdp" class="subsection-content">
            <h2>Markov Decision Process (MDP)</h2>
            <p>
                An agent sequentially interacts with an initially unknown environment to obtain a <strong>trajectory</strong>. 
                The Trajectory of length \(T\) can be defined by: 
                \[
                \mathcal{t} = (s_0, a_0, r_0, s_1, a_1, r_1, s_2, \cdots, s_T)
                \]
                where \(s_t\) is a state, \(a_t\) is an action, and \(r_t\) is a reward. 
            </p>
            <p>
                We want to optimize the action-selection policy so that the discounted cumulative reward:
                \[
                G_0 = \sum_{t = 0}^{T -1} \gamma^t r_t 
                \]
                is maximized for some <strong>discount factor</strong> \(\gamma \in [0, 1]\). Here, we consider 
                <strong>Markov decision process (MDP)</strong>, then the generative model for the trajectory can be 
                factored into single-step models. 
            </p>

                Formal definition of RL as an MDP \((\mathcal{S}, \mathcal{A}, \mathcal{T}, r, \gamma)\). 
                Explain states, actions, transition probabilities, reward functions, and discount factor. 
                Show agent–environment interaction loop diagram.
            </p>
            </section>

           <section id="value-functions" class="subsection-content">
            <h2>Return and Value Functions</h2>
            <p>
                Define return \(G_t = \sum_{k=0}^\infty \gamma^k r_{t+k}\). Introduce value function \(V^\pi(s)\), action-value \(Q^\pi(s, a)\), 
                and optimal versions. Explain their roles in policy evaluation.
            </p>
            </section>

            <section id="td" class="subsection-content">
            <h2>Temporal Difference Learning</h2>
            <p>
                Explain bootstrapping in TD methods. Introduce TD(0), the update rule, and its connection to 
                Monte Carlo and dynamic programming.
            </p>
            </section>

            <section id="qlearning" class="subsection-content">
            <h2>Q-Learning and SARSA</h2>
            <p>
                Introduce Q-learning (off-policy) and SARSA (on-policy). Present update rules and compare their exploration behavior. 
                Use gridworld or bandit example.
            </p>
            </section>

            <section id="dqn" class="subsection-content">
            <h2>Deep Q-Network (DQN)</h2>
            <p>
                Extend Q-learning to function approximation using deep neural networks. Mention key DQN tricks: target networks, 
                experience replay. Briefly note Double DQN and Dueling DQN variants.
            </p>
            </section>

            <section id="policy-gradient" class="subsection-content">
            <h2>Policy Gradient and Actor-Critic</h2>
            <p>
                Introduce policy-based methods like REINFORCE. Derive the policy gradient objective. 
                Explain limitations and how actor-critic methods address them. Optionally mention PPO or A2C.
            </p>
            </section>

            <section id="onoffpolicy" class="subsection-content">
            <h2> On-Policy vs. Off-Policy Learning</h2>
            <p>
                Define and compare on-policy and off-policy learning. Explain importance sampling, 
                behavior vs. target policy.
            </p>
            </section>

            <section id="modelbased" class="subsection-content">
            <h2>Model-Based Reinforcement Learning</h2>
            <p>
                Overview of learning or using environment models. 
                Contrast planning (e.g., value iteration) vs. model-free learning. 
                Mention Model Predictive Control (MPC) briefly.
            </p>
            </section>

            <section id="exploration" class="subsection-content">
            <h2>Exploration vs. Exploitation</h2>
            <p>
                Discuss the exploration-exploitation tradeoff. Introduce ε-greedy, softmax action selection, 
                and entropy regularization.
            </p>
            </section>

            <section id="rlhf" class="subsection-content">
            <h2>Reinforcement Learning from Human Feedback (RLHF)</h2>
            <p>
                Introduce RLHF as used in LLM fine-tuning. Explain reward model training from preference data 
                and PPO-based optimization. Mention alternatives like RLAIF.
            </p>
            </section>

            <section id="demo" class="section-content">
                <h2>Demo</h2>
                 <div id=""></div>
            </section>


        </div>
        <script src="/js/main.js"></script>    
    </body>
</html>