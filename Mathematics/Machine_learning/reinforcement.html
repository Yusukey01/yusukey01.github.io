---
layout: default
title: Intro to Reinforcement Learning
level: detail
description: Learn about Reinforcement Learning basics.
uses_math: true
uses_python: false
---
<!DOCTYPE html>
<html>
    <body>
        <!-- LearningResource Schema for Content Pages -->
        <script type="application/ld+json">
        {
        "@context": "https://schema.org",
        "@type": "LearningResource",
        "name": "{{ page.title }}",
        "description": "{{ page.description }}",
        "learningResourceType": "lesson",
        "educationalUse": "instruction",
        "educationalLevel": "university",
        "interactivityType": "{% if page.content contains 'Interactive Demo' or page.content contains 'demo' %}active{% else %}expositive{% endif %}",
        "author": {
            "@type": "Person",
            "name": "Yusuke Yokota",
            "jobTitle": "Mathematics & Computer Science Educator"
        },
        "publisher": {
            "@type": "Organization",
            "name": "MATH-CS COMPASS",
            "url": "https://yusukey01.github.io"
        },
        "about": [
            {% if page.url contains 'reinforcement' %}
            { "@type": "Thing", "name": "Reinforcement Learning" },
            { "@type": "Thing", "name": "Markov Decision Process" },
            { "@type": "Thing", "name": "MDP" },
            { "@type": "Thing", "name": "Value Functions" },
            { "@type": "Thing", "name": "Policy Optimization" },
            { "@type": "Thing", "name": "Bellman Equations" }
            {% elsif page.url contains 'classification' %}
            { "@type": "Thing", "name": "Classification" },
            { "@type": "Thing", "name": "Logistic Regression" },
            { "@type": "Thing", "name": "Machine Learning" }
            {% elsif page.url contains 'neural' %}
            { "@type": "Thing", "name": "Neural Networks" },
            { "@type": "Thing", "name": "Deep Learning" }
            {% else %}
            { "@type": "Thing", "name": "Mathematics" },
            { "@type": "Thing", "name": "Machine Learning" }
            {% endif %}
        ],
        "teaches": [
            "Mathematical concepts",
            "Practical applications",
            "Problem-solving techniques"
        ],
        "isPartOf": {
            "@type": "Course",
            "name": "{% if page.url contains 'Machine_learning' %}Machine Learning{% elsif page.url contains 'Linear_algebra' %}Linear Algebra{% elsif page.url contains 'Calculus' %}Calculus to Optimization & Analysis{% elsif page.url contains 'Probability' %}Probability & Statistics{% elsif page.url contains 'Discrete' %}Discrete Mathematics & Algorithms{% endif %}",
            "description": "{% if page.url contains 'Machine_learning' %}Explore machine learning ideas and applications with mathematical foundations{% elsif page.url contains 'Linear_algebra' %}Explore the foundations of Linear Algebra, covering key concepts such as linear equations, vector spaces, eigenvalues, orthogonality, least squares, and stochastic matrices{% elsif page.url contains 'Calculus' %}Explore key calculus concepts essential for optimization, analysis, and machine learning{% elsif page.url contains 'Probability' %}Explore fundamental concepts of probability and statistics essential for machine learning{% elsif page.url contains 'Discrete' %}Explore the foundations of discrete mathematics and algorithms, covering graph theory, combinatorics, and the theory of computation{% endif %}",
            "provider": {
            "@type": "Organization",
            "name": "MATH-CS COMPASS",
            "url": "https://yusukey01.github.io"
            },
            "instructor": {
            "@type": "Person",
            "name": "Yusuke Yokota",
            "jobTitle": "Mathematics & Computer Science Educator"
            },
            "courseCode": "{% if page.url contains 'Machine_learning' %}V{% elsif page.url contains 'Linear_algebra' %}I{% elsif page.url contains 'Calculus' %}II{% elsif page.url contains 'Probability' %}III{% elsif page.url contains 'Discrete' %}IV{% endif %}",
            "hasCourseInstance": {
            "@type": "CourseInstance",
            "courseMode": "online",
            "courseWorkload": "{% if page.url contains 'Machine_learning' %}PT5H{% elsif page.url contains 'Linear_algebra' %}PT2H{% elsif page.url contains 'Calculus' %}PT3H{% elsif page.url contains 'Probability' %}PT2H30M{% elsif page.url contains 'Discrete' %}PT4H{% endif %}",
            "instructor": {
                "@type": "Person",
                "name": "Yusuke Yokota"
            }
            },
            "offers": {
            "@type": "Offer",
            "price": "0",
            "priceCurrency": "USD",
            "availability": "https://schema.org/InStock",
            "category": "free"
            }
        }
        }
        </script>
        <!-- WebApplication Schema for Interactive Demos -->
        <script type="application/ld+json">
        {
        "@context": "https://schema.org",
        "@type": "WebApplication",
        "name": "{{ page.title }} Interactive Demo",
        "description": "Interactive demonstration of {{ page.title | downcase }} concepts",
        "applicationCategory": "EducationalApplication",
        "operatingSystem": "Web Browser",
        "url": "https://yusukey01.github.io{{ page.url }}",
        "author": {
            "@type": "Person",
            "name": "Yusuke Yokota"
        },
        "applicationSubCategory": "Mathematical Visualization",
        "featureList": [
            "Interactive mathematical visualization",
            "Real-time parameter adjustment",
            "Educational demonstrations"
        ],
        "isAccessibleForFree": true,
        "educationalUse": "instruction",
        "educationalLevel": "university"
        }
        </script> 
        <div class="hero-section">
            <h1 class="webpage-name">Intro to Reinforcement Learning</h1>
        </div>

        {% include section_navigation.html %}

        <div class="topic-nav">
            <a href="#intro">Introduction</a>
            <a href="#rl-types">Model-Based and Model-Free RL</a>
            <a href="#mdp">Markov Decision Process (MDP)</a>
            <a href="#value-functions">Return and Value Functions</a>
        </div>

        <div class="container">  

            <section id="intro" class="section-content">
                <h2>Introduction</h2>

                <p>
                    <strong>Reinforcement Learning (RL)</strong> is a machine learning framework in which an <strong>agent</strong> interacts 
                    with an environment to learn a behavior that maximizes a scalar <strong>reward</strong> signal over time. 
                    Formally, the environment is typically modeled as a <strong>Markov Decision Process (MDP)</strong>, defined by a tuple 
                    \[
                    (\mathcal{S}, \mathcal{A}, \mathcal{T}, r, \gamma)
                    \]
                    where \(\mathcal{S}\) is the state space, \(\mathcal{A}\) is the action space, \(\mathcal{T}\) is the transition function, 
                    \(r\) is the reward function, and \(\gamma\) is the discount factor.
                </p>

                <p>
                    At each time step, the agent observes a state \(s_t \in \mathcal{S}\), selects an action \(a_t \in \mathcal{A}\), receives a 
                    reward \(r_t\), and transitions to a new state \(s_{t+1}\) according to the transition dynamics \(\mathcal{T}(s_{t+1} \mid s_t, a_t)\). 
                    The goal of the agent is to learn a <strong>policy</strong> \(\pi(a \mid s)\) that maximizes the expected cumulative reward:
                    \[
                    \mathbb{E}_\pi \left[ \sum_{t=0}^\infty \gamma^t r_t \right].
                    \]
                </p>

                <p>
                    Reinforcement learning differs from <strong>supervised learning</strong>, where models are trained on ground-truth input-output pairs, 
                    and from <strong>unsupervised learning</strong>, which aims to discover hidden structure or representations in unlabeled data.
                    In RL, learning is driven by a scalar <em>reward signal</em> that provides evaluative feedback based on the agent's interaction with an environment.
                    Unlike supervised learning, there are no correct labels for each input, and unlike unsupervised learning, the objective is not to model data structure 
                    but to learn a policy that maximizes expected cumulative reward over time.
                </p>

                <p>
                    RL has been successfully applied in domains such as robotics, recommendation systems, and game-playing.
                    More recently, RL has also become integral to the training of modern <strong>large language models (LLMs)</strong>. 
                    In particular, a method called <strong>Reinforcement Learning from Human Feedback (RLHF)</strong> is used to fine-tune LLMs 
                    to follow <strong>human preferences</strong>, ensuring outputs are more helpful, safe, and better aligned with user intent.
                </p>     
            </section>

            <section id="rl-types" class="section-content">
                <h2>Model-Based and Model-Free Reinforcement Learning</h2>

                <p>
                    Reinforcement Learning (RL) algorithms can be broadly categorized into two classes based on how they interact with the environment: 
                    <strong>model-based</strong> and <strong>model-free</strong> methods.
                </p>

                <p>
                    <strong>Model-based RL</strong> aims to learn an explicit model of the environment from data. This includes learning:
                    <ul style="padding-left: 40px;">
                    <li>the transition dynamics \(p_T(s' \mid s, a)\), and</li>
                    <li>the reward model \(p_R(r \mid s, a, s')\).</li>
                    </ul>
                    Once the model is learned from trajectories, classical planning algorithms such as 
                    <em>value iteration</em> or <em>policy iteration</em> can be applied to compute an optimal policy.
                </p>

                <p>
                    In contrast, <strong>model-free RL</strong> methods bypass explicit modeling of the environment. 
                    Instead, they directly learn value functions (e.g., Q-learning) or policies (e.g., REINFORCE, PPO) based on sampled experience. 
                    These methods are generally simpler to implement but may require more interaction data.
                </p>

                <p>
                    In both paradigms, the environment is formally assumed to follow a <strong>Markov Decision Process (MDP)</strong>, 
                    which we now define in the following section.
                </p>
            </section>


            <section id="mdp" class="section-content">
                <h2>Markov Decision Process (MDP)</h2>

                <p>
                    In this section, we present a probabilistic formulation of Markov Decision Processes (MDPs), where both 
                    transitions and rewards are modeled as stochastic variables. This perspective is particularly useful for 
                    analyzing trajectory distributions and gradient-based learning methods.
                </p>

                <p>
                    An agent sequentially interacts with an initially unknown environment to obtain a <strong>trajectory</strong> 
                    or multiple trajectories. A trajectory of length \(T\) is defined as:
                    \[
                    \boldsymbol{\tau} = (s_0, a_0, r_0, s_1, a_1, r_1, s_2, \cdots, s_T),
                    \]
                    where \(s_t\) is a state, \(a_t\) is an action, and \(r_t\) is a reward.
                </p>

                <p>
                    The objective is to optimize the agent's action-selection policy so that the expected discounted cumulative reward
                    is maximized:
                    \[
                    G_0 = \sum_{t = 0}^{T -1} \gamma^t r_t,
                    \]
                    where \(\gamma \in [0, 1]\) is the <strong>discount factor</strong>. We assume the environment follows a 
                    <strong>Markov Decision Process (MDP)</strong>, where the trajectory distribution can be factored into 
                    single-step transition and reward models. The process of estimating an optimal policy from trajectories 
                    is referred to as <strong>learning</strong>.
                </p>

                <p>
                    We define the MDP as the tuple:
                    \[
                    \left\langle \mathcal{S}, \mathcal{A}, p_T, p_R, p_0 \right\rangle,
                    \]
                    where:
                    <ul style="padding-left: 40px;">
                    <li>\(\mathcal{S}\): set of environment states</li>
                    <li>\(\mathcal{A}\): set of available actions</li>
                    <li>\(p_T(s' \mid s, a)\): transition model (next-state distribution)</li>
                    <li>\(p_R(r \mid s, a, s')\): reward model (stochastic reward distribution)</li>
                    <li>\(p_0(s_0)\): initial state distribution</li>
                    </ul>
                </p>

                <p>
                    At time \(t = 0\), the initial state is sampled as \(s_0 \sim p_0\). At each step \(t \geq 0\), the agent observes 
                    state \(s_t \in \mathcal{S}\), selects action \(a_t \sim \pi(a_t \mid s_t)\), and receives reward 
                    \(r_t \sim p_R(r \mid s_t, a_t, s_{t+1})\), where the next state is drawn from \(s_{t+1} \sim p_T(s_{t+1} \mid s_t, a_t)\). 
                    The agent's decision-making is governed by a stochastic policy \(\pi(a \mid s)\).
                </p>

                <p>
                    This interaction at each step is called a <strong>transition</strong>, represented as the tuple:
                    \[
                    (s_t, a_t, r_t, s_{t+1}),
                    \]
                    where:
                    <ul style="padding-left: 40px;">
                    <li>\(a_t \sim \pi(a \mid s_t)\)</li>
                    <li>\(s_{t+1} \sim p_T(s_{t+1} \mid s_t, a_t)\)</li>
                    <li>\(r_t \sim p_R(r \mid s_t, a_t, s_{t+1})\)</li>
                    </ul>
                </p>

                <p>
                    Under policy \(\pi\), the joint distribution of a trajectory \(\boldsymbol{\tau}\) of length \(T\) is given by:
                    \[
                    p(\boldsymbol{\tau}) = p_0(s_0) \prod_{t = 0}^{T -1} 
                    \pi(a_t \mid s_t) \, p_T(s_{t+1} \mid s_t, a_t) \, p_R(r_t \mid s_t, a_t, s_{t+1}).
                    \]
                </p>

                <p>
                    We define the expected <strong>reward function</strong> from the reward model \(p_R\) as the marginal average immediate reward 
                    for taking action \(a\) in state \(s\), integrating over possible next states:
                    \[
                    R(s, a) = \mathbb{E}_{p_T(s' \mid s, a)} \left[ 
                    \mathbb{E}_{p_R(r \mid s, a, s')}[r] 
                    \right].
                    \]
                </p>

                <p>
                    While classical RL literature often defines the reward function deterministically as \(r(s, a)\), 
                    this probabilistic formulation allows us to account for uncertainty in transitions and rewards. 
                    It is particularly useful for gradient-based RL methods, where trajectory likelihoods must be modeled explicitly.
                </p>
                </section>

            <section id="value-functions" class="section-content">
                <h2>Return and Value Functions</h2>

                <p>
                    Let \(\boldsymbol{\tau}\) be a trajectory of length \(T\), where \(T\) may be infinite (\(T = \infty\)) for continuing tasks. 
                    The <strong>return</strong> at time \(t\) is defined as the total accumulated reward from that point forward, discounted by a factor \(\gamma \in [0, 1]\):
                    \[
                    \begin{align*}
                    G_t &= r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \cdots + \gamma^{T - t - 1} r_{T - 1} \\\\
                        &= \sum_{k = 0}^{T - t - 1} \gamma^k r_{t + k} \\\\
                        &= \sum_{j = t}^{T - 1} \gamma^{j - t} r_j.
                    \end{align*}
                    \]
                    The discount factor \(\gamma\) ensures that the return remains finite even for infinite-horizon problems, and it gives higher weight to short-term rewards, thereby encouraging the agent to achieve goals sooner.
                </p>

                <p>
                    Given a stochastic policy \(\pi(a \mid s)\), the <strong>state-value function</strong> (or simply, <strong>value function</strong>) is defined as:
                    \[
                    \begin{align*}
                    V_{\pi}(s) &= \mathbb{E}_{\pi} \left[ G_0 \mid s_0 = s \right] \\\\
                            &= \mathbb{E}_{\pi} \left[ \sum_{t = 0}^{\infty} \gamma^t r_t \mid s_0 = s \right],
                    \end{align*}
                    \]
                    where the expectation is over trajectories induced by the policy \(\pi\).
                </p>

                <p>
                    The <strong>action-value function</strong> (or <strong>Q-function</strong>) is defined as:
                    \[
                    \begin{align*}
                    Q_{\pi}(s, a) &= \mathbb{E}_{\pi} \left[ G_0 \mid s_0 = s, \, a_0 = a \right] \\\\
                                &= \mathbb{E}_{\pi} \left[ \sum_{t = 0}^{\infty} \gamma^t r_t \mid s_0 = s, \, a_0 = a \right].
                    \end{align*}
                    \]
                </p>

                <p>
                    The <strong>advantage function</strong> is defined as:
                    \[
                    A_{\pi}(s, a) = Q_{\pi}(s, a) - V_{\pi}(s),
                    \]
                    which quantifies how much better it is to take action \(a\) in state \(s\) and then follow policy \(\pi\), compared to just following \(\pi\) from the beginning.
                </p>

                <p>
                    A policy \(\pi_*\) is called an <strong>optimal policy</strong> if it yields the highest value for every state:
                    \[
                    \forall s \in \mathcal{S}, \quad V_{\pi_*}(s) \geq V_{\pi}(s), \quad \forall \pi.
                    \]
                    Although multiple optimal policies may exist, their value functions are the same: \(V_*(s) = V_{\pi_*}(s)\) and \(Q_*(s, a) = Q_{\pi_*}(s, a)\). 
                    Moreover, every finite MDP admits at least one deterministic optimal policy.
                </p>

                <div class="theorem">
                    <span class="theorem-title"><strong>Bellman's Optimality Equations</strong>:</span>
                    \[
                    \begin{align*}
                    V_*(s) &= \max_a \left[ R(s, a) + \gamma \, \mathbb{E}_{p_T(s' \mid s, a)} \left[ V_*(s') \right] \right] \\\\
                    Q_*(s, a) &= R(s, a) + \gamma \, \mathbb{E}_{p_T(s' \mid s, a)} \left[ \max_{a'} Q_*(s', a') \right]
                    \end{align*}
                    \]
                    The optimal value functions \(V_*\) and \(Q_*\) are the unique fixed points of these equations.
                </div>

                <p>
                    The optimal policy can then be derived by:
                    \[
                    \begin{align*}
                    \pi_*(s) &= \arg \max_a Q_*(s, a) \\\\
                            &= \arg \max_a \left[ R(s, a) + \gamma \, \mathbb{E}_{p_T(s' \mid s, a)} \left[ V_*(s') \right] \right].
                    \end{align*}
                    \]
                    Solving for \(V_*\), \(Q_*\), or \(\pi_*\) is known as <strong>policy optimization</strong>, while computing 
                    \(V_{\pi}\) or \(Q_{\pi}\) for a given policy \(\pi\) is called <strong>policy evaluation</strong>. 
                </p>
                <p>
                   Bellman's equations for <strong>policy evaluation</strong> can be derived from the <strong>optimality equations</strong> 
                   by replacing each maximization over actions, \(\max_a [\,\cdot\,]\), with an expectation over the policy at the corresponding 
                   state, \(\mathbb{E}_{\pi(a \mid s)}[\,\cdot\,]\), wherever action selection occurs.
                    <div class="theorem">
                        <span class="theorem-title"><strong>Bellman's Expectation Equations:</strong></span>
                        <p>
                            For a fixed policy \(\pi\), the state-value function satisfies:
                            </p>
                            \[
                            V_{\pi}(s) = \mathbb{E}_{\pi(a \mid s)} \left[ R(s, a) + \gamma \, \mathbb{E}_{p_T(s' \mid s, a)} \left[ V_{\pi}(s') \right] \right]
                            \]
                            <p>
                            The action-value function satisfies:
                            </p>
                            \[
                            Q_{\pi}(s, a) = R(s, a) + \gamma \, \mathbb{E}_{p_T(s' \mid s, a)} \left[ \mathbb{E}_{\pi(a' \mid s')} \left[ Q_{\pi}(s', a') \right] \right]
                            \]
                    </div>
                </p>
               
                <p>
                    These Bellman equations provide the mathematical foundation for solving Markov Decision Processes. 
                    When the MDP is fully known, <strong>classical planning methods</strong> like Value Iteration and Policy Iteration 
                    can compute optimal policies by directly applying these equations. When the environment is unknown, 
                    <strong>reinforcement learning methods</strong> learn optimal policies through interaction: model-based RL 
                    first learns environment models then applies planning methods, while model-free methods like Q-learning and SARSA 
                    use sampling-based approximations of the Bellman equations. These planning and learning algorithms 
                    will be covered in detail in our foundational algorithms page.
                </p>
            </section>
           
            


        </div>
        <script src="/js/main.js"></script>    
    </body>
</html>