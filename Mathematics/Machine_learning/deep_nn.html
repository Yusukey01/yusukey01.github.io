---
layout: default
title: Intro to Deep Neural Networks
level: detail
description: Learn about deep neural networks basics via Attention mechanisms and Transformer architecture.
uses_math: true
uses_python: false
---
<!DOCTYPE html>
<html>
    <body> 
        <div class="hero-section">
            <h1 class="webpage-name">Intro to Deep Neural Networks</h1>
        </div>

        <div class="topic-nav">
            <a href="#CNN">Convolutional Neural Networks (CNNs)</a>
            <a href="#RNN">Recurrent Neural Networks (RNNs)</a>
            <a href="#attention">Attention Mechanisms</a>
            <a href="#"></a>
            <a href="#demo">Demo</a>
        </div> 

        <div class="container">  

            <section id="CNN" class="section-content">
                <h2>Convolutional Neural Networks (CNNs): The First Breakthrough</h2>
                <p> 
                    Deep neural networks have undergone remarkable evolution over the past few decades. From early image 
                    classifiers to today's powerful language and multimodal models, the field has progressed through a series 
                    of foundational innovations. 
                </p>

                <p> 
                    In the 1990s, <strong>Convolutional Neural Networks (CNNs)</strong> emerged as a biologically inspired architecture for 
                    processing grid-like data such as images. CNNs apply local filters across spatial dimensions, enabling them to learn hierarchical 
                    features while drastically reducing the number of parameters compared to fully connected networks. 
                </p> 

                <p> The breakthrough came in 2012, when <strong>AlexNet</strong> won the ImageNet competition by a wide margin. Designed by Alex Krizhevsky, 
                    it used stacked convolutional and pooling layers, ReLU activation, and dropout regularization, all trained on GPUs. AlexNet launched the 
                    deep learning revolution by proving that large-scale deep networks could outperform traditional computer vision methods. 
                </p> 

                <p> 
                    This success inspired deeper and more sophisticated CNN architectures: 
                    <ul style="padding-left: 40px;"> 
                        <li><strong>ResNet</strong> (2015) introduced <em>residual connections</em>, enabling the training of very deep networks (e.g., 152 layers) by mitigating vanishing gradients.</li> 
                        <li><strong>DenseNet</strong> (2017) took it further by connecting each layer to every other layer in a feed-forward fashion, promoting feature reuse and improving gradient flow.</li>
                    </ul> 
                    These architectures dominated image recognition tasks and are still used in specialized domains like medical imaging and remote sensing. 
                </p>
            </section>

            <section id="RNN" class="section-content">
                <h2> Recurrent Neural Networks (RNNs): Modeling Sequences</h2>
                <p> 
                    While CNNs excel at spatial data, they are not suited for sequential data such as text, audio, or time series. 
                    This led to the development of <strong>Recurrent Neural Networks (RNNs)</strong>, which process inputs sequentially 
                    while maintaining a hidden state to capture temporal dependencies. 
                </p> 

                <p> 
                    However, basic RNNs struggled with long-term dependencies due to vanishing gradients. To address this, 
                    <strong>Long Short-Term Memory (LSTM)</strong> networks (Hochreiter & Schmidhuber, 1997) and 
                    <strong>Gated Recurrent Units (GRUs)</strong> were developed. These architectures use gating mechanisms to 
                    selectively retain or discard information over time, and became foundational for tasks like machine translation, 
                    speech recognition, and music modeling. 
                </p>
            </section>

            <section id="attention" class="section-content">
                <h2>Attention Mechanisms and the Rise of Transformers</h2>
                <p> 
                    Despite their success, RNNs process sequences step by step, which limits parallelism and long-range modeling. This 
                    led to the development of the <strong>attention mechanism</strong>, which allows models to dynamically focus on different 
                    parts of the input when generating an output. 
                </p> 
                <p> 
                    The concept culminated in the introduction of the <strong>Transformer</strong> architecture (Vaswani et al., 2017), which 
                    relies entirely on <strong>self-attention</strong> instead of recurrence or convolution. This breakthrough enabled efficient, 
                    scalable modeling of long sequences with full parallelization, revolutionizing natural language processing. 
                </p> 

                <p> 
                    Transformers now form the foundation of state-of-the-art models like BERT, GPT, and Vision Transformers (ViTs). They also 
                    generalize beyond text to support multimodal learning, including image, video, audio, and code. 
                </p>
            </section>

            <section id="" class="section-content">
                <h2></h2>
            </section>


        </div>
        <script src="/js/main.js"></script>  
        <script src="/js/clustering_visualizer.js"></script>  
    </body>
</html>