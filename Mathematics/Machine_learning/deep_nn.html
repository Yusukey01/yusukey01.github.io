---
layout: default
title: Intro to Deep Neural Networks
level: detail
description: Learn about deep neural networks basics via Attention mechanisms and Transformer architecture.
uses_math: true
uses_python: false
---
<!DOCTYPE html>
<html>
    <body> 
        <div class="hero-section">
            <h1 class="webpage-name">Intro to Deep Neural Networks</h1>
        </div>

        <div class="topic-nav">
            <a href="#CNN">Convolutional Neural Networks (CNNs)</a>
            <a href="#RNN">Recurrent Neural Networks (RNNs)</a>
            <a href="#attention">Attention Mechanisms</a>
            <a href="#transformer">The Rise of Transformers</a>
            <a href="#demo">Demo</a>
        </div> 

        <div class="container">  

            <section id="CNN" class="section-content">
                <h2>Convolutional Neural Networks (CNNs): The First Breakthrough</h2>
                <p> 
                    Deep neural networks have undergone remarkable evolution over the past few decades. From early image 
                    classifiers to today's powerful language and multimodal models, the field has progressed through a series 
                    of foundational innovations. 
                </p>

                <p>
                    In the 1990s, <strong>Convolutional Neural Networks (CNNs)</strong> emerged as a biologically inspired architecture for processing grid-like data such as images. 
                    A CNN uses <em>convolutional layers</em> that apply learnable filters across local spatial regions. Mathematically, a 2D convolution operation is given by:
                    \[
                    (f * x)(i, j) = \sum_m \sum_n f(m, n) \cdot x(i - m, j - n)
                    \]
                    where \(f\) is the filter (or kernel) and \(x\) is the input image. This operation captures local spatial patterns while sharing parameters across the image.
                </p>

                <p>
                    The breakthrough came in 2012, when <strong>AlexNet</strong> won the ImageNet competition. It stacked convolution and pooling layers, used the ReLU nonlinearity \( \text{ReLU}(x) = \max(0, x) \), 
                    and applied dropout to reduce overfitting. AlexNet demonstrated that deep CNNs trained on GPUs could vastly outperform traditional vision pipelines.
                </p>

                <p>
                    This success led to deeper architectures:
                    <ul style="padding-left: 40px;"> 
                        <li><strong>ResNet</strong> introduced <em>residual connections</em> to learn functions of the form \( \mathcal{F}(x) + x \), helping gradients propagate through deep networks.</li> 
                        <li><strong>DenseNet</strong> connected each layer to all previous layers via concatenation, enhancing feature reuse and reducing redundancy.</li>
                    </ul> 
                    These models advanced image recognition and set the stage for modern vision architectures.
                </p>
            </section>

            <section id="RNN" class="section-content">
                <h2> Recurrent Neural Networks (RNNs): Modeling Sequences</h2>
                <p>
                    <strong>Recurrent Neural Networks (RNNs)</strong> process sequential data by maintaining a hidden state across time steps. 
                    At each time step \(t\), the hidden state \(h_t\) is updated using the previous state and the current input:
                    \[
                    h_t = \sigma(W x_t + U h_{t-1} + b)
                    \]
                    where \(\sigma\) is an activation function (e.g., \(\tanh\)), \(W\) and \(U\) are weight matrices, and \(x_t\) is the input.
                </p>

                <p>
                    While RNNs can, in principle, capture long-term dependencies, in practice they suffer from <em>vanishing and exploding gradients</em>. 
                    To mitigate this, <strong>Long Short-Term Memory (LSTM)</strong> units were introduced. LSTMs use gating mechanisms:
                    \[
                    \begin{aligned}
                    f_t &= \sigma(W_f x_t + U_f h_{t-1} + b_f) \quad &\text{(forget gate)} \\
                    i_t &= \sigma(W_i x_t + U_i h_{t-1} + b_i) \quad &\text{(input gate)} \\
                    o_t &= \sigma(W_o x_t + U_o h_{t-1} + b_o) \quad &\text{(output gate)} \\
                    \tilde{c}_t &= \tanh(W_c x_t + U_c h_{t-1} + b_c) \\
                    c_t &= f_t \odot c_{t-1} + i_t \odot \tilde{c}_t \\
                    h_t &= o_t \odot \tanh(c_t)
                    \end{aligned}
                    \]
                    These gates allow the network to learn when to forget, update, and output information, enabling stable training over longer sequences.
                </p>
            </section>

            <section id="attention" class="section-content">
                <h2>Attention Mechanisms</h2>
                <p>
                    Recurrent architectures process sequences step-by-step, which limits parallelism and makes modeling long-range dependencies challenging.  
                    <strong>Attention mechanisms</strong> address this by computing a weighted sum of all input feature vectors, allowing the model to 
                    “attend” to the most relevant parts of the sequence regardless of distance.
                </p>

                <p>
                    A model can be more flexible if its weights depend on the inputs(hidden feature vectors) 
                    \(\mathbf{X} \in \mathbb{R}^{m \times v}\) rather than fixed: 
                    \[
                    \mathbf{Z} = \varphi (\mathbf{V}\mathbf{W}(\mathbf{Q}, \mathbf{K}))
                    \]
                    where 
                    <ul style="padding-left: 40px;">
                        <li>\(\mathbf{Q} \in \mathbb{R}^{m \times q}\) is a set of <strong>queries</strong></li>
                        <li>\(\mathbf{K} \in \mathbb{R}^{m \times q}\) is a set of <strong>keys</strong></li>
                        <li>\(\mathbf{V} \in \mathbb{R}^{m \times v}\) is a set of <strong>values</strong></li>
                    </ul>
                    <br>
                    This multiplicative interaction is called <strong>attention</strong>.
                </p>
                <p>
                    To compute output \(\mathbf{z}_j\), the mode use its corresponding query \(\mathbf{q}_j\) and 
                    compare it to each key \(\mathbf{k}_i\) to obtain the similarity score \(\alpha_{ij}\) where \(0 \leq \alpha_{ij} \leq 1\) and 
                    \(\sum_i \alpha_{ij} = 1\). Thus, 
                    \[
                    \mathbf{z}_j = \sum_{i}\alpha_{ij}\mathbf{v}_i.
                    \]
                </p>
                <p>
                    If \(\mathbf{q}, \, \mathbf{k} \in \mathbb{R}^d\) and these are independent random variables with zero mean and 
                    unit variance, the mean of their inner product \(\mathbf{q}^\top \mathbf{k} = 0\) and the variance is \(d\). Then we obtain 
                    the <strong>scaled dot-product attention</strong>:
                    \[
                    a(\mathbf{q}, \mathbf{k}) = \frac{\mathbf{q}^\top \mathbf{k}}{\sqrt{d}}.
                    \]
                    The scale factor \(\frac{1}{\sqe{d}}\) controls the variance so that it remains 1 regardless of the size of the inputs.
                </p>
                
                <p>
                    In practice, we use minibatches of \(n\) vectors at a time. So, the learned query, key, and value matrices derived from the input are:
                    \[
                    \mathbf{Q} \in \mathbb{R}^{n \times d}, \, \mathbf{K} \in \mathbb{R}^{m \times d}, \, \mathbf{V} \in \mathbb{R}^{m \times v}
                    \]
                    and then we the attention-weighted output is give by:
                    \[
                    \text{Attention }(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{softmax } \left(\frac{\mathbf{Q} \mathbf{K}^\top}{\sqrt{d}}\right) \mathbf{V} \in \mathbb{R}^{n \times v}
                    \]
                    where the softmax function is applied row-wise.
                </p>
            </section>

            <section id="transformer" class="section-content">
                <h2>The Rise of Transformers</h2>
                <p>
                    The <strong>Transformer</strong> architecture (Vaswani et al., 2017) relies entirely on self-attention and forgoes recurrence. 
                    Each layer applies multi-head self-attention and feedforward sublayers:
                    \[
                    \text{MultiHead}(X) = \text{Concat}(\text{head}_1, \dots, \text{head}_h) W^O, \quad \text{head}_i = \text{Attention}(XW_i^Q, XW_i^K, XW_i^V)
                    \]
                    This enables parallel processing of entire sequences, efficient long-range context modeling, and scalability to large datasets.
                </p>

                <p>
                    Transformers now power models like BERT, GPT, and Vision Transformers, and have become the foundation of modern AI across text, images, and multimodal data.
                </p>
            </section>

            <section id="demo" class="section-content">
                <h2>Demo</h2>
                 <div id="transformer_demo"></div>
            </section>


        </div>
        <script src="/js/main.js"></script>  
        <script src="/js/deep_nn.js"></script>  
    </body>
</html>