---
layout: default
title: Intro to Deep Neural Networks
level: detail
description: Learn about deep neural networks basics via Attention mechanisms and Transformer architecture.
uses_math: true
uses_python: false
---
<!DOCTYPE html>
<html>
    <body> 
        <div class="hero-section">
            <h1 class="webpage-name">Intro to Deep Neural Networks</h1>
        </div>

        <div class="topic-nav">
            <a href="#CNN">Convolutional Neural Networks (CNNs)</a>
            <a href="#RNN">Recurrent Neural Networks (RNNs)</a>
            <a href="#attention">Attention Mechanisms</a>
            <a href="#transformer">The Rise of Transformers</a>
            <a href="#demo">Demo</a>
        </div> 

        <div class="container">  

            <section id="CNN" class="section-content">
                <h2>Convolutional Neural Networks (CNNs): The First Breakthrough</h2>
                <p> 
                    Deep neural networks have undergone remarkable evolution over the past few decades. From early image 
                    classifiers to today's powerful language and multimodal models, the field has progressed through a series 
                    of foundational innovations. 
                </p>

                <p>
                    In the 1990s, <strong>Convolutional Neural Networks (CNNs)</strong> emerged as a biologically inspired architecture for processing grid-like data such as images. 
                    A CNN uses <em>convolutional layers</em> that apply learnable filters across local spatial regions. Mathematically, a 2D convolution operation is given by:
                    \[
                    (f * x)(i, j) = \sum_m \sum_n f(m, n) \cdot x(i - m, j - n)
                    \]
                    where \(f\) is the filter (or kernel) and \(x\) is the input image. This operation captures local spatial patterns while sharing parameters across the image.
                </p>

                <p>
                    The breakthrough came in 2012, when <strong>AlexNet</strong> won the ImageNet competition. It stacked convolution and pooling layers, used the ReLU nonlinearity \( \text{ReLU}(x) = \max(0, x) \), 
                    and applied dropout to reduce overfitting. AlexNet demonstrated that deep CNNs trained on GPUs could vastly outperform traditional vision pipelines.
                </p>

                <p>
                    This success led to deeper architectures:
                    <ul style="padding-left: 40px;"> 
                        <li><strong>ResNet</strong> introduced <em>residual connections</em> to learn functions of the form \( \mathcal{F}(x) + x \), helping gradients propagate through deep networks.</li> 
                        <li><strong>DenseNet</strong> connected each layer to all previous layers via concatenation, enhancing feature reuse and reducing redundancy.</li>
                    </ul> 
                    These models advanced image recognition and set the stage for modern vision architectures.
                </p>
            </section>

            <section id="RNN" class="section-content">
                <h2> Recurrent Neural Networks (RNNs): Modeling Sequences</h2>
                <p>
                    <strong>Recurrent Neural Networks (RNNs)</strong> process sequential data by maintaining a hidden state across time steps. 
                    At each time step \(t\), the hidden state \(h_t\) is updated using the previous state and the current input:
                    \[
                    h_t = \sigma(W x_t + U h_{t-1} + b)
                    \]
                    where \(\sigma\) is an activation function (e.g., \(\tanh\)), \(W\) and \(U\) are weight matrices, and \(x_t\) is the input.
                </p>

                <p>
                    While RNNs can, in principle, capture long-term dependencies, in practice they suffer from <em>vanishing and exploding gradients</em>. 
                    To mitigate this, <strong>Long Short-Term Memory (LSTM)</strong> units were introduced. LSTMs use gating mechanisms:
                    \[
                    \begin{aligned}
                    f_t &= \sigma(W_f x_t + U_f h_{t-1} + b_f) \quad &\text{(forget gate)} \\
                    i_t &= \sigma(W_i x_t + U_i h_{t-1} + b_i) \quad &\text{(input gate)} \\
                    o_t &= \sigma(W_o x_t + U_o h_{t-1} + b_o) \quad &\text{(output gate)} \\
                    \tilde{c}_t &= \tanh(W_c x_t + U_c h_{t-1} + b_c) \\
                    c_t &= f_t \odot c_{t-1} + i_t \odot \tilde{c}_t \\
                    h_t &= o_t \odot \tanh(c_t)
                    \end{aligned}
                    \]
                    These gates allow the network to learn when to forget, update, and output information, enabling stable training over longer sequences.
                </p>
            </section>

            <section id="attention" class="section-content">
                <h2>Attention Mechanisms</h2>
                <p>
                    Recurrent architectures process sequences step-by-step, which limits parallelism and makes modeling long-range dependencies challenging.  
                    <strong>Attention mechanisms</strong> address this by computing a weighted sum of all input feature vectors, allowing the model to 
                    “attend” to the most relevant parts of the sequence regardless of distance.
                </p>

                <p>
                    In traditional feedforward neural networks, each layer computes hidden activations as a linear transformation of input activations followed by a nonlinearity:
                    \[
                    \mathbf{Z} = \varphi (\mathbf{X}\mathbf{W}) \in \mathbb{R}^{m \times v'}
                    \]
                    where \(\mathbf{X} \in \mathbb{R}^{m \times v}\) is a matrix of input (or hidden) feature vectors, and 
                    \(\mathbf{W} \in \mathbb{R}^{v \times v'}\) is a fixed weight matrix learned during training.
                </p>

                <p>
                    This formulation uses the same set of weights \(\mathbf{W}\) for every input position. To allow more flexibility, 
                    we can instead make the weights input-dependent. That is, we can allow:
                    \[
                    \mathbf{Z} = \varphi (\mathbf{X}\mathbf{W}(\mathbf{X})),
                    \]
                    where the transformation matrix \(\mathbf{W}(\mathbf{X})\) varies depending on the input itself.
                    This allows the model to dynamically adjust its computations based on the input context.
                    Such input-dependent multiplicative interactions are central to <strong>attention mechanisms</strong>.
                </p>

                <p>
                    In more general settings, the input-dependent transformation matrix can be computed from separate sets of learned representations called 
                    queries, keys, and values. This leads to a broader formulation of attention:
                    \[
                    \mathbf{Z} = \varphi (\mathbf{V}\mathbf{W}(\mathbf{Q}, \mathbf{K}))
                    \]
                    where:
                    <ul style="padding-left: 40px;">
                        <li>\(\mathbf{Q} \in \mathbb{R}^{m \times q}\) is a set of <strong>queries</strong></li>
                        <li>\(\mathbf{K} \in \mathbb{R}^{m \times q}\) is a set of <strong>keys</strong></li>
                        <li>\(\mathbf{V} \in \mathbb{R}^{m \times v}\) is a set of <strong>values</strong></li>
                    </ul>
                    <br>
                    The transformation matrix \(\mathbf{W}(\mathbf{Q}, \mathbf{K})\) represents a learned or computed relationship between queries and keys.
                    Note: In practice, these matrices are often obtained by applying learned linear projections to the input sequence.
                </p>

                
                 <p>
                    In attention, the core idea is to compute each output vector \(\mathbf{z}_j\) as a weighted sum over all value vectors \(\mathbf{v}_i\), 
                    where the weights \(\alpha_{ij}\) are determined by a similarity between the query \(\mathbf{q}_j\) and each key \(\mathbf{k}_i\):
                    \[
                    \mathbf{z}_j = \sum_{i}\alpha_{ij}\mathbf{v}_i
                    \]
                    where \(0 \leq \alpha_{ij} \leq 1\) and \(\sum_i \alpha_{ij} = 1\). This allows the model to selectively attend to relevant parts of the input.
                </p>

                 <p>
                    A common similarity function is the inner product between query and key vectors. If 
                    \(\mathbf{q}, \mathbf{k} \in \mathbb{R}^d\) are independent random variables with zero mean and unit variance, 
                    then their inner product \(\mathbf{q}^\top \mathbf{k}\) has zero mean and variance \(d\). 
                    To prevent large variance from pushing the softmax into extreme regions, we normalize it:
                    \[
                    a(\mathbf{q}, \mathbf{k}) = \frac{\mathbf{q}^\top \mathbf{k}}{\sqrt{d}}.
                    \]
                    This is known as <strong>scaled dot-product attention score</strong>, where the scale factor 
                    \(\frac{1}{\sqrt{d}}\) normalizes the variance so that it remains stable(i.e., \(= 1\)) as the dimension \(d\) increases.  
                    This is used to compute <strong>attention weights</strong> \(\alpha_{ij}\) after applying the softmax function.
                </p>
               
                <p>
                    In practice, we compute attention over minibatches of \(n\) query vectors. Let:
                    \[
                    \mathbf{Q} \in \mathbb{R}^{n \times d}, \quad \mathbf{K} \in \mathbb{R}^{m \times d}, \quad \mathbf{V} \in \mathbb{R}^{m \times v}
                    \]
                    Then the attention-weighted output is computed as:
                    \[
                    \text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{softmax}\left(\frac{\mathbf{Q} \mathbf{K}^\top}{\sqrt{d}}\right) \mathbf{V} \in \mathbb{R}^{n \times v}
                    \]
                    where the softmax is applied row-wise to normalize each row of the \(n \times m\) similarity matrix.
                </p>

                <p>
                    This formulation was introduced in the <strong>Transformer</strong> model (Vaswani et al., 2017) and enables:
                    <ul style="padding-left: 40px;">
                        <li>Full parallelization across positions,</li>
                        <li>Effective modeling of long-range dependencies, and</li>
                        <li>Dynamic, data-dependent weighting of inputs.</li>
                    </ul>
                </p>
            </section>

            <section id="transformer" class="section-content">
                <h2>The Rise of Transformers</h2>
                <p>
                    The <strong>Transformer</strong> architecture (Vaswani et al., 2017) relies entirely on self-attention and forgoes recurrence. 
                    Each layer applies multi-head self-attention and feedforward sublayers:
                    \[
                    \text{MultiHead}(X) = \text{Concat}(\text{head}_1, \dots, \text{head}_h) W^O, \quad \text{head}_i = \text{Attention}(XW_i^Q, XW_i^K, XW_i^V)
                    \]
                    This enables parallel processing of entire sequences, efficient long-range context modeling, and scalability to large datasets.
                </p>

                <p>
                    Transformers now power models like BERT, GPT, and Vision Transformers, and have become the foundation of modern AI across text, images, and multimodal data.
                </p>
            </section>

            <section id="demo" class="section-content">
                <h2>Demo</h2>
                 <div id="transformer_demo"></div>
            </section>


        </div>
        <script src="/js/main.js"></script>  
        <script src="/js/deep_nn.js"></script>  
    </body>
</html>