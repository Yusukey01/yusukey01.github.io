---
layout: default
title: Intro to Deep Neural Networks
level: detail
description: Learn about deep neural networks basics via Attention mechanisms and Transformer architecture.
uses_math: true
uses_python: false
---
<!DOCTYPE html>
<html>
    <body> 
        <div class="hero-section">
            <h1 class="webpage-name">Intro to Deep Neural Networks</h1>
        </div>

        <div class="topic-nav">
            <a href="#CNN">Convolutional Neural Networks (CNNs)</a>
            <a href="#RNN">Recurrent Neural Networks (RNNs)</a>
            <a href="#attention">Attention Mechanisms</a>
            <a href="#"></a>
            <a href="#demo">Demo</a>
        </div> 

        <div class="container">  

            <section id="CNN" class="section-content">
                <h2>Convolutional Neural Networks (CNNs): The First Breakthrough</h2>
                <p> 
                    Deep neural networks have undergone remarkable evolution over the past few decades. From early image 
                    classifiers to today's powerful language and multimodal models, the field has progressed through a series 
                    of foundational innovations. 
                </p>

                <p>
                    In the 1990s, <strong>Convolutional Neural Networks (CNNs)</strong> emerged as a biologically inspired architecture for processing grid-like data such as images. 
                    A CNN uses <em>convolutional layers</em> that apply learnable filters across local spatial regions. Mathematically, a 2D convolution operation is given by:
                    \[
                    (f * x)(i, j) = \sum_m \sum_n f(m, n) \cdot x(i - m, j - n)
                    \]
                    where \(f\) is the filter (or kernel) and \(x\) is the input image. This operation captures local spatial patterns while sharing parameters across the image.
                </p>

                <p>
                    The breakthrough came in 2012, when <strong>AlexNet</strong> won the ImageNet competition. It stacked convolution and pooling layers, used the ReLU nonlinearity \( \text{ReLU}(x) = \max(0, x) \), 
                    and applied dropout to reduce overfitting. AlexNet demonstrated that deep CNNs trained on GPUs could vastly outperform traditional vision pipelines.
                </p>

                <p>
                    This success led to deeper architectures:
                    <ul style="padding-left: 40px;"> 
                        <li><strong>ResNet</strong> introduced <em>residual connections</em> to learn functions of the form \( \mathcal{F}(x) + x \), helping gradients propagate through deep networks.</li> 
                        <li><strong>DenseNet</strong> connected each layer to all previous layers via concatenation, enhancing feature reuse and reducing redundancy.</li>
                    </ul> 
                    These models advanced image recognition and set the stage for modern vision architectures.
                </p>
            </section>

            <section id="RNN" class="section-content">
                <h2> Recurrent Neural Networks (RNNs): Modeling Sequences</h2>
                <p>
                    <strong>Recurrent Neural Networks (RNNs)</strong> process sequential data by maintaining a hidden state across time steps. 
                    At each time step \(t\), the hidden state \(h_t\) is updated using the previous state and the current input:
                    \[
                    h_t = \sigma(W x_t + U h_{t-1} + b)
                    \]
                    where \(\sigma\) is an activation function (e.g., \(\tanh\)), \(W\) and \(U\) are weight matrices, and \(x_t\) is the input.
                </p>

                <p>
                    While RNNs can, in principle, capture long-term dependencies, in practice they suffer from <em>vanishing and exploding gradients</em>. 
                    To mitigate this, <strong>Long Short-Term Memory (LSTM)</strong> units were introduced. LSTMs use gating mechanisms:
                    \[
                    \begin{aligned}
                    f_t &= \sigma(W_f x_t + U_f h_{t-1} + b_f) \quad &\text{(forget gate)} \\
                    i_t &= \sigma(W_i x_t + U_i h_{t-1} + b_i) \quad &\text{(input gate)} \\
                    o_t &= \sigma(W_o x_t + U_o h_{t-1} + b_o) \quad &\text{(output gate)} \\
                    \tilde{c}_t &= \tanh(W_c x_t + U_c h_{t-1} + b_c) \\
                    c_t &= f_t \odot c_{t-1} + i_t \odot \tilde{c}_t \\
                    h_t &= o_t \odot \tanh(c_t)
                    \end{aligned}
                    \]
                    These gates allow the network to learn when to forget, update, and output information, enabling stable training over longer sequences.
                </p>
            </section>

            <section id="attention" class="section-content">
                <h2>Attention Mechanisms and the Rise of Transformers</h2>
                <p>
                    RNNs process sequences step-by-step, limiting parallelism and making it hard to model long-range dependencies. 
                    <strong>Attention mechanisms</strong> solve this by computing a weighted combination of all input positions, allowing the model to focus on relevant parts.
                </p>

                <p>
                    The basic attention computation involves:
                    \[
                    \text{Attention}(Q, K, V) = \text{softmax}\left( \frac{QK^\top}{\sqrt{d_k}} \right) V
                    \]
                    where \(Q\), \(K\), and \(V\) are learned query, key, and value matrices derived from the input. This formulation allows the model to aggregate information across the sequence in a data-dependent way.
                </p>

                <p>
                    The <strong>Transformer</strong> architecture (Vaswani et al., 2017) relies entirely on self-attention and forgoes recurrence. 
                    Each layer applies multi-head self-attention and feedforward sublayers:
                    \[
                    \text{MultiHead}(X) = \text{Concat}(\text{head}_1, \dots, \text{head}_h) W^O, \quad \text{head}_i = \text{Attention}(XW_i^Q, XW_i^K, XW_i^V)
                    \]
                    This enables parallel processing of entire sequences, efficient long-range context modeling, and scalability to large datasets.
                </p>

                <p>
                    Transformers now power models like BERT, GPT, and Vision Transformers, and have become the foundation of modern AI across text, images, and multimodal data.
                </p>
            </section>

            <section id="demo" class="section-content">
                <h2></h2>
                 <div id="transformer_demo"></div>
            </section>


        </div>
        <script src="/js/main.js"></script>  
        <script src="/js/deep_nn.js"></script>  
    </body>
</html>