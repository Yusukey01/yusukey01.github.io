---
layout: default
title: Intro to Deep Neural Networks
level: detail
description: Learn about deep neural networks basics via Attention mechanisms and Transformer architecture.
uses_math: true
uses_python: false
---
<!DOCTYPE html>
<html>
    <body> 
        <div class="hero-section">
            <h1 class="webpage-name">Intro to Deep Neural Networks</h1>
        </div>

        <div class="topic-nav">
            <a href="#CNN">Convolutional Neural Networks (CNNs)</a>
            <a href="#ResNet">ResNet: Residual Connections</a>
            <a href="#normalization">Layer Normalization</a>
            <a href="#attention">Attention Mechanisms</a>
            <a href="#self">Self-Attention</a>
            <a href="#multihead">Multi-Head Attention</a>
            <a href="#position">Positional Encoding</a>
            <a href="#transformer">Transformers</a>
            <a href="#demo">Demo</a>
        </div>

        <div class="container">  

            <section id="CNN" class="section-content">
                <h2>Convolutional Neural Networks (CNNs)</h2>
                <p>
                    <strong>Deep neural networks (DNNs)</strong> have undergone remarkable evolution over the past few decades. From early image 
                    classifiers to today's cutting-edge language and multimodal models, the field has progressed through a series of foundational innovations. 
                    Broadly speaking, modern DNNs are typically <strong>feedforward networks</strong>, where information flows in a single direction — from input 
                    to output — without any loops or recurrence. This design simplifies computations, making them faster and more stable.
                </p>


                <p>
                    In the 1990s, <strong>Convolutional Neural Networks (CNNs)</strong> emerged as an architecture for processing grid-like data such as images. 
                    A CNN uses <em>convolutional layers</em> that apply learnable filters across local spatial regions. Mathematically, a 2D convolution operation is given by:
                    \[
                    (f * x)(i, j) = \sum_m \sum_n f(m, n) \cdot x(i - m, j - n)
                    \]
                    where \(f\) is the filter (or kernel) and \(x\) is the input image. This operation captures local spatial patterns while sharing parameters across the image.
                </p>

                <p>
                    The breakthrough came in 2012, when <strong>AlexNet</strong> won the ImageNet competition. It stacked convolution and pooling layers, used the ReLU nonlinearity \( \text{ReLU}(x) = \max(0, x) \), 
                    and applied dropout to reduce overfitting. AlexNet demonstrated that deep CNNs trained on GPUs could vastly outperform traditional vision pipelines.
                </p>

                <p>
                    This success led to the deeper architecture, <strong>ResNet</strong>(2015), which introduced <strong>residual connections</strong>.
                    Such a model advanced image recognition and set the stage for modern vision architectures.
                </p>
            </section>

            <section id="CNN" class="section-content">
                <h2>ResNet: Residual Connections</h2>
                <p>
                    <strong>ResNet</strong> is a feedforward architecture in which each layer uses a <strong>residual block</strong> of the form:
                    \[
                    \mathcal{F}_{\ell}' (\mathbf{x}) = \mathcal{F}_{\ell} (\mathbf{x}) + \mathbf{x}
                    \]
                    where \(\mathcal{F}_{\ell}\) is a nonlinear transformation at layer \(\ell\), and the input \(\mathbf{x}\) is added back directly.
                </p>

                <p>
                    The activations at the final layer \(L\) can be expressed recursively in terms of an earlier layer \(\ell\):
                    \[
                    \mathbf{z}_L = \mathbf{z}_{\ell} + \sum_{i = \ell}^{L - 1} \mathcal{F}_i (\mathbf{z}_i; \mathbf{\theta}_i),
                    \]
                    where \(\mathbf{\theta}_i\) denotes the parameters of the transformation at layer \(i\).
                </p>

                <p>
                    This formulation mitigates the <strong>vanishing gradient problem</strong> in deep neural networks, enabling the 
                    successful training of very deep models.
                </p>
                <p>
                    Applying the chain rule, the gradient of the loss \(\mathcal{L}\) with respect to the parameters at layer \(\ell\) becomes:
                    \[
                    \begin{align*}
                    \frac{\partial \mathcal{L}}{\partial \mathbf{\theta}_{\ell}} 
                        &= \frac{\partial \mathbf{z}_{\ell}}{\partial \mathbf{\theta}_{\ell}} \frac{\partial \mathcal{L}}{\partial \mathbf{z}_{\ell}} \\\\
                        &= \frac{\partial \mathbf{z}_{\ell}}{\partial \mathbf{\theta}_{\ell}} \frac{\partial \mathcal{L}}{\partial \mathbf{z}_{L}} \frac{\partial \mathbf{z}_L}{\partial \mathbf{z}_{\ell}} \\\\
                        &= \frac{\partial \mathbf{z}_{\ell}}{\partial \mathbf{\theta}_{\ell}} \frac{\partial \mathcal{L}}{\partial \mathbf{z}_{L}} 
                                \left(1 + \sum_{i = \ell}^{L-1} \frac{\partial \mathcal{F}_i (\mathbf{z}_i : \mathbf{\theta}_i)}{\partial \mathbf{z}_{\ell}}\right) \\\\
                        &= \frac{\partial \mathbf{z}_{\ell}}{\partial \mathbf{\theta}_{\ell}} \frac{\partial \mathcal{L}}{\partial \mathbf{z}_{L}} + \text{other terms}
                    \end{align*}
                    \]
                    The <em>“other terms”</em> in the gradient capture indirect paths through deeper residual blocks and reflect how updates propagate backward through the network.
                    These terms may diminish in magnitude in very deep networks, but they are generally not zero and play a critical role in learning refined representations.
                    Residual networks avoid the vanishing gradient problem not by eliminating these terms, but by ensuring they are <strong>not the only path</strong> for gradient flow.
                </p>
            </section>

            <section id="normalization" class="section-content">
                <h2>Layer Normalization</h2>
                <p>
                    <strong>Layer normalization</strong> is a normalization technique introduced by Ba, Kiros, and Hinton (2016) to improve the stability and convergence of deep networks. 
                    Unlike batch normalization, which normalizes across the batch dimension, layer normalization operates independently for each input sample and normalizes across the feature dimensions.
                </p>

                <p>
                    Given an input vector \(\mathbf{x} \in \mathbb{R}^d\), layer normalization computes the normalized output as:
                    \[
                    \text{LayerNorm}(\mathbf{x}) = \frac{\mathbf{x} - \mu}{\sqrt{\sigma^2 + \epsilon}} \cdot \gamma + \beta
                    \]
                    where:
                </p>

                <ul style="padding-left: 40px;">
                    <li>\(\mu = \frac{1}{d} \sum_{i=1}^d x_i\) is the mean of the input features</li>
                    <li>\(\sigma^2 = \frac{1}{d} \sum_{i=1}^d (x_i - \mu)^2\) is the variance</li>
                    <li>\(\gamma, \beta \in \mathbb{R}^d\) are learnable scale and shift parameters</li>
                    <li>\(\epsilon\) is a small constant added for numerical stability</li>
                </ul>

                <p>
                    Since layer normalization does not rely on batch statistics, it is well-suited to models where inputs have variable length or where batch size is small. 
                    These properties make it particularly useful in architectures with recurrent or attention-based computations.
                </p>

                <p>
                    Although it became widely known through its use in Transformer models, layer normalization was introduced prior to both residual connections and self-attention. 
                    It serves as a general-purpose method for stabilizing intermediate activations in deep networks.
                </p>
                </section>


            <section id="attention" class="section-content">
                <h2>Attention Mechanisms</h2>
                <p>
                    Recurrent architectures process sequences step-by-step, which limits parallelism and makes modeling long-range dependencies challenging.  
                    <strong>Attention mechanisms</strong> address this by computing a weighted sum of all input feature vectors, allowing the model to 
                    “attend” to the most relevant parts of the sequence regardless of distance.
                </p>

                <p>
                    In traditional feedforward neural networks, each layer computes hidden activations as a linear transformation of input activations followed by a nonlinearity:
                    \[
                    \mathbf{Z} = \varphi (\mathbf{X}\mathbf{W}) \in \mathbb{R}^{m \times v'}
                    \]
                    where \(\mathbf{X} \in \mathbb{R}^{m \times v}\) is a matrix of input (or hidden) feature vectors, and 
                    \(\mathbf{W} \in \mathbb{R}^{v \times v'}\) is a fixed weight matrix learned during training.
                </p>

                <p>
                    This formulation uses the same set of weights \(\mathbf{W}\) for every input position. To allow more flexibility, 
                    we can instead make the weights input-dependent. That is, we can allow:
                    \[
                    \mathbf{Z} = \varphi (\mathbf{X}\mathbf{W}(\mathbf{X})),
                    \]
                    where the transformation matrix \(\mathbf{W}(\mathbf{X})\) varies depending on the input itself.
                    This allows the model to dynamically adjust its computations based on the input context.
                    Such input-dependent multiplicative interactions are central to <strong>attention mechanisms</strong>.
                </p>

                <p>
                    In more general settings, the input-dependent transformation matrix can be computed from separate sets of learned representations called 
                    queries, keys, and values. This leads to a broader formulation of attention:
                    \[
                    \mathbf{Z} = \varphi (\mathbf{V}\mathbf{W}(\mathbf{Q}, \mathbf{K}))
                    \]
                    where:
                    <ul style="padding-left: 40px;">
                        <li>\(\mathbf{Q} \in \mathbb{R}^{m \times q}\) is a set of <strong>queries</strong></li>
                        <li>\(\mathbf{K} \in \mathbb{R}^{m \times q}\) is a set of <strong>keys</strong></li>
                        <li>\(\mathbf{V} \in \mathbb{R}^{m \times v}\) is a set of <strong>values</strong></li>
                    </ul>
                    <br>
                    The transformation matrix \(\mathbf{W}(\mathbf{Q}, \mathbf{K})\) represents a learned or computed relationship between queries and keys.
                    Note: In practice, these matrices are often obtained by applying learned linear projections to the input sequence.
                </p>

                
                 <p>
                    In attention, the core idea is to compute each output vector \(\mathbf{z}_j\) as a weighted sum over all value vectors \(\mathbf{v}_i\), 
                    where the weights \(\alpha_{ij}\) are determined by a similarity between the query \(\mathbf{q}_j\) and each key \(\mathbf{k}_i\):
                    \[
                    \mathbf{z}_j = \sum_{i}\alpha_{ij}\mathbf{v}_i
                    \]
                    where \(0 \leq \alpha_{ij} \leq 1\) and \(\sum_i \alpha_{ij} = 1\). This allows the model to selectively attend to relevant parts of the input.
                </p>

                 <p>
                    A common similarity function is the inner product between query and key vectors. If 
                    \(\mathbf{q}, \mathbf{k} \in \mathbb{R}^d\) are independent random variables with zero mean and unit variance, 
                    then their inner product \(\mathbf{q}^\top \mathbf{k}\) has zero mean and variance \(d\). 
                    To prevent large variance from pushing the softmax into extreme regions, we normalize it:
                    \[
                    a(\mathbf{q}, \mathbf{k}) = \frac{\mathbf{q}^\top \mathbf{k}}{\sqrt{d}}.
                    \]
                    This is known as <strong>scaled dot-product attention score</strong>, where the scale factor 
                    \(\frac{1}{\sqrt{d}}\) normalizes the variance so that it remains stable(i.e., \(= 1\)) as the dimension \(d\) increases.  
                    This is used to compute <strong>attention weights</strong> \(\alpha_{ij}\) after applying the softmax function.
                </p>
               
                <p>
                    In practice, we compute attention over minibatches of \(n\) query vectors. Let:
                    \[
                    \mathbf{Q} \in \mathbb{R}^{n \times d}, \quad \mathbf{K} \in \mathbb{R}^{m \times d}, \quad \mathbf{V} \in \mathbb{R}^{m \times v}
                    \]
                    Then the attention-weighted output is computed as:
                    \[
                    \text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{softmax}\left(\frac{\mathbf{Q} \mathbf{K}^\top}{\sqrt{d}}\right) \mathbf{V} \in \mathbb{R}^{n \times v}
                    \]
                    where the softmax is applied row-wise to normalize each row of the \(n \times m\) similarity matrix.
                </p>

           </section>

            <section id="self" class="section-content">  
            <h2>Self-Attention</h2>  
            <p>  
                <strong>Self-attention</strong> is a mechanism that allows each position \(i\) in a sequence to attend to all other positions 
                in the same sequence. Given a sequence of input tokens \(\mathbf{x}_1, \cdots, \mathbf{x}_n \in \mathbb{R}^d\),
                a sequence of outputs is given by:
                \[
                \mathbf{y}_i = \text{Attention}(\mathbf{q}_i, (\mathbf{x}_1, \mathbf{x}_1), \cdots, (\mathbf{x}_n, \mathbf{x}_n )) \in \mathbb{R}^d
                \]
                where the quert is \(q_i\) and the keys and valuse are \(\mathbf{x}_1, \cdots \mathbf{x}_n\).
            </p>
            <p>
                This mechanism captures contextual relationships between tokens by allowing information to flow from any position to any other in a single layer.
                Through training, all the outputs have been known. Thus the model can evaluate \(\mathbf{y}_i\) <strong>in parallel</strong>, which 
                overcomes the sequential bottleneck of past DNNs. 
            </p>

            </section>  

            <section id="multihead" class="section-content">
                <h2>Multi-Head Attention</h2>
                <p>
                    While a single self-attention mechanism can capture dependencies between tokens, using only one set of projections 
                    may limit the model's expressiveness. <strong>Multi-head attention (MHA)</strong> addresses this by computing 
                    multiple self-attention operations in parallel using different learned projections, called "heads." 
                </p>

                <p>
                    Formally, given an input matrix \( X \in \mathbb{R}^{n \times d} \), each attention head applies its own learned projections to compute queries, keys, and values:
                    \[
                    \text{head}_i = \text{Attention}\left(\mathbf{W}_i^{(q)} \mathbf{q}, \{\mathbf{W}_i^{(k)} \mathbf{k}_j, \mathbf{W}_i^{(v)}\mathbf{v}_j\}\right) \in \mathbb{R}^{p_v},
                    \]
                    where \( \mathbf{W}_i^{(q)}, \mathbf{W}_i^{(k)}, \mathbf{W}_i^{(v)} \in \mathbb{R}^{p_v \times d} \) are projection matrices specific to head \( i \).
                </p>

                <p>
                    All heads are computed in parallel and their outputs are concatenated and linearly projected to produce the final output:
                    \[
                    \text{MHA}(\mathbf{X}) = \text{MHA}\left(\mathbf{q}, \{\mathbf{k}_j, \mathbf{v}_j\}\right) 
                        = \mathbf{W}_o  \begin{bmatrix} 
                                    \text{head}_1 \\
                                    \vdots \\
                                    \text{head}_h\\
                                \end{bmatrix}.
                    \]
                    where \( \mathbf{W}_o \in \mathbb{R}^{d \times hp_v} \) is a learned output projection matrix.
                </p>

                <p>
                    By using multiple heads, the model can attend to information from different representation subspaces and capture a richer set of relationships in the data. 
                    Each head may focus on different positions or interaction patterns, improving both performance and interpretability.
                </p>
            </section>

            
            <section id="position" class="section-content"> 
                <h2>Positional Encoding</h2> 
                <p> 
                    The standard self-attention is permutation invariant, which means the model ignores the order of tokens in an 
                    input sequence. To address this, <strong>positional encoding</strong> is added to the input embeddings to inject information about 
                    the position of each token. A widely used method is to define fixed, deterministic positional encodings using 
                    sine and cosine functions of varying frequencies. 
                </p> 

                <p> 
                    The positional encoding vector for position \( i \) and dimension \( j \) is defined as follows: 
                    \[ 
                    p_{i, 2j} = \sin \left( \frac{i}{C^{2j/d}} \right) 
                    \]
                    \[ 
                    p_{i, 2j+1} = \cos \left( \frac{i}{C^{2j/d}} \right) 
                    \] 
                    where: 
                    <ul style="padding-left: 40px;"> 
                        <li>\( i \) is the position index in the sequence, starting from 0,</li> 
                        <li>\( j \) is the dimension index in the positional encoding vector,</li> 
                        <li>\( d \) is the total dimension of the model (e.g., 512),</li> 
                        <li>\( C \) is a constant (usually set to 10,000) to control the frequency scale.</li> 
                    </ul> 
                </p>

                 <p> 
                    This formulation ensures that each dimension of the positional encoding corresponds to a sinusoid of a 
                    different frequency. As a result, any two positions will have a unique positional encoding vector, and the 
                    relative position between tokens can be represented as a linear function of the encodings. 
                </p> 

                <p> 
                    <strong>Example:</strong> Suppose the model dimension is \( d = 4 \), and we compute the positional encoding for 
                    position \( i = 2 \). Then we compute each component of the encoding as:
                    \[ 
                    \begin{align*}
                    &p_{2,0} = \sin \left( \frac{2}{10,000^{0/4}} \right) = \sin(2) \\\\      
                    &p_{2,1} = \cos \left( \frac{2}{10,000^{0/4}} \right) = \cos(2) \\\\
                    &p_{2,2} = \sin \left( \frac{2}{10,000^{2/4}} \right) = \sin \left( \frac{2}{100} \right) \\\\
                    &p_{2,3} = \cos \left( \frac{2}{10,000^{2/4}} \right) = \cos \left( \frac{2}{100} \right) 
                    \end{align*}
                    \] 
                    </p> 

                    <p> 
                        This results in the positional encoding vector for position 2 as: 
                        \[ 
                        \mathbf{p}_2 = \left[ \sin(2),\ \cos(2),\ \sin(0.02),\ \cos(0.02) \right].
                        \]
                    </p> 

                     <p> These encodings are added element-wise to the token embeddings at each position, allowing the model to 
                        incorporate position information without relying on sequence-aware architectures like CNNs. 
                        Moreover, the use of periodic functions enables the model to potentially generalize to sequences longer 
                        than those seen during training. 
                    </p> 
                </section>

            <section id="transformer" class="section-content">
                <h2>Transformers</h2>


                <p> 
                    The <strong>Transformer</strong> architecture, introduced by Vaswani et al. (2017), replaces recurrence and convolution with a mechanism based entirely on self-attention. 
                    In both the encoder and decoder, the core components are multi-head self-attention layers, feedforward networks, residual connections, and layer normalization. 
                    The design enables full parallelization across sequence positions during training, and allows the model to capture long-range dependencies through direct pairwise 
                    interactions between tokens. By using learned attention weights that vary depending on the input, the model dynamically determines which parts of the sequence to emphasize 
                    at each layer.
                </p>

                <p>
                    Transformer-based models form the basis of many state-of-the-art systems across domains such as text, vision, and multimodal learning. 
                    In particular, <strong>large language models (LLMs)</strong> such as the <strong>GPT</strong> family, developed by OpenAI, are built on 
                    stacked Transformer decoder blocks trained on massive text corpora. These models leverage the scalability and expressiveness of the 
                    Transformer to generate coherent and context-aware output across a wide range of tasks.
                </p>

                <div class="pseudocode">
                    <span class="pseudocode-title">Transformer Encoder & Decoder</span>
                    <strong>EncoderBlock(X)</strong>
                    &emsp;&emsp;Z = LayerNorm(MHA(Q =X, K = X, V = X) + X)
                    &emsp;&emsp;E = LayerNorm(FeedForward(Z) + Z)
                    &emsp;&emsp;<strong>return</strong> E
                    <br>
                    <strong>Encoder(X, N)</strong>
                    &emsp;&emsp;E = POS(Embed(X))
                    &emsp;&emsp;for \(n\) in range (N)
                    &emsp;&emsp;&emsp;&emsp; E = EncoderBlock(E)
                    &emsp;&emsp;<strong>return</strong> E
                    <br>
                    <strong>DecoderBlock(Y, E)</strong>
                    &emsp;&emsp;Z = LayerNorm(MHA(Q =Y, K = Y, V = Y) + Y)
                    &emsp;&emsp;Z' = LayerNorm(MHA(Q = Z, K = E, V = E) + Z)
                    &emsp;&emsp;D = LayerNorm(FeedForward(Z') + Z')
                    &emsp;&emsp;<strong>return</strong> D
                    <br>
                    <strong>Decoder(Y, E, N)</strong>
                    &emsp;&emsp;D = POS(Embed(Y))
                    &emsp;&emsp;for \(n\) in range (N)
                    &emsp;&emsp;&emsp;&emsp; D = DecoderBlock(D, E)
                    &emsp;&emsp;<strong>return</strong> D
                </div>
                <p>
                    Note:
                    <ul style="padding-left: 40px;">
                        <li><strong>MHA(...)</strong> : the Multi-head self-attention. Each token attends to every other token.</li>
                        <li><strong>+ (...)</strong> : residual connection. The input is added back to the output of attention. </li>
                        <li><strong>LayerNorm(...)</strong> : Layer normalization, which is applied after the residual connection to stabilize training.</li>
                        <li><strong>FeedForward(...)</strong> : A position-wise fully connected feedforward network is applied to each token vector independently.</li>
                        <li><strong>Embed(...)</strong> : Each token is mapped to a high-dimensional embedding vector.</li>
                        <li><strong>POS(...)</strong> : Positional encoding is added to embeddings to inject token order information.</li>
                        <li><strong>N</strong>: the number of copies of the block</li>
                    </ul>
                </p>
            </section>

            <section id="demo" class="section-content">
                <h2>Demo</h2>
                 <div id="transformer_demo"></div>
            </section>


        </div>
        <script src="/js/main.js"></script>  
        <script src="/js/deep_nn.js"></script>  
    </body>
</html>