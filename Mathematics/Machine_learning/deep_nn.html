---
layout: default
title: Intro to Deep Neural Networks
level: detail
description: Learn about deep neural networks basics via Attention mechanisms and Transformer architecture.
uses_math: true
uses_python: false
---
<!DOCTYPE html>
<html>
    <body> 
        <div class="hero-section">
            <h1 class="webpage-name">Intro to Deep Neural Networks</h1>
        </div>

        <div class="topic-nav">
            <a href="#CNN">Convolutional Neural Networks (CNNs)</a>
            <a href="#RNN">Recurrent Neural Networks (RNNs)</a>
            <a href="#attention">Attention Mechanisms</a>
            <a href="#self">Self-Attention</a>
            <a href="#position">Positional Encoding</a>
            <a href="#transformer">The Rise of Transformers</a>
            <a href="#demo">Demo</a>
        </div> 

        <div class="container">  

            <section id="CNN" class="section-content">
                <h2>Convolutional Neural Networks (CNNs): The First Breakthrough</h2>
                <p> 
                    Deep neural networks have undergone remarkable evolution over the past few decades. From early image 
                    classifiers to today's powerful language and multimodal models, the field has progressed through a series 
                    of foundational innovations. 
                </p>

                <p>
                    In the 1990s, <strong>Convolutional Neural Networks (CNNs)</strong> emerged as a biologically inspired architecture for processing grid-like data such as images. 
                    A CNN uses <em>convolutional layers</em> that apply learnable filters across local spatial regions. Mathematically, a 2D convolution operation is given by:
                    \[
                    (f * x)(i, j) = \sum_m \sum_n f(m, n) \cdot x(i - m, j - n)
                    \]
                    where \(f\) is the filter (or kernel) and \(x\) is the input image. This operation captures local spatial patterns while sharing parameters across the image.
                </p>

                <p>
                    The breakthrough came in 2012, when <strong>AlexNet</strong> won the ImageNet competition. It stacked convolution and pooling layers, used the ReLU nonlinearity \( \text{ReLU}(x) = \max(0, x) \), 
                    and applied dropout to reduce overfitting. AlexNet demonstrated that deep CNNs trained on GPUs could vastly outperform traditional vision pipelines.
                </p>

                <p>
                    This success led to deeper architectures:
                    <ul style="padding-left: 40px;"> 
                        <li><strong>ResNet</strong> introduced <em>residual connections</em> to learn functions of the form \( \mathcal{F}(x) + x \), helping gradients propagate through deep networks.</li> 
                        <li><strong>DenseNet</strong> connected each layer to all previous layers via concatenation, enhancing feature reuse and reducing redundancy.</li>
                    </ul> 
                    These models advanced image recognition and set the stage for modern vision architectures.
                </p>
            </section>

            <section id="RNN" class="section-content">
                <h2> Recurrent Neural Networks (RNNs): Modeling Sequences</h2>
                <p>
                    <strong>Recurrent Neural Networks (RNNs)</strong> process sequential data by maintaining a hidden state across time steps. 
                    At each time step \(t\), the hidden state \(h_t\) is updated using the previous state and the current input:
                    \[
                    h_t = \sigma(W x_t + U h_{t-1} + b)
                    \]
                    where \(\sigma\) is an activation function (e.g., \(\tanh\)), \(W\) and \(U\) are weight matrices, and \(x_t\) is the input.
                </p>

                <p>
                    While RNNs can, in principle, capture long-term dependencies, in practice they suffer from <em>vanishing and exploding gradients</em>. 
                    To mitigate this, <strong>Long Short-Term Memory (LSTM)</strong> units were introduced. LSTMs use gating mechanisms:
                    \[
                    \begin{aligned}
                    f_t &= \sigma(W_f x_t + U_f h_{t-1} + b_f) \quad &\text{(forget gate)} \\
                    i_t &= \sigma(W_i x_t + U_i h_{t-1} + b_i) \quad &\text{(input gate)} \\
                    o_t &= \sigma(W_o x_t + U_o h_{t-1} + b_o) \quad &\text{(output gate)} \\
                    \tilde{c}_t &= \tanh(W_c x_t + U_c h_{t-1} + b_c) \\
                    c_t &= f_t \odot c_{t-1} + i_t \odot \tilde{c}_t \\
                    h_t &= o_t \odot \tanh(c_t)
                    \end{aligned}
                    \]
                    These gates allow the network to learn when to forget, update, and output information, enabling stable training over longer sequences.
                </p>
            </section>

            <section id="attention" class="section-content">
                <h2>Attention Mechanisms</h2>
                <p>
                    Recurrent architectures process sequences step-by-step, which limits parallelism and makes modeling long-range dependencies challenging.  
                    <strong>Attention mechanisms</strong> address this by computing a weighted sum of all input feature vectors, allowing the model to 
                    “attend” to the most relevant parts of the sequence regardless of distance.
                </p>

                <p>
                    In traditional feedforward neural networks, each layer computes hidden activations as a linear transformation of input activations followed by a nonlinearity:
                    \[
                    \mathbf{Z} = \varphi (\mathbf{X}\mathbf{W}) \in \mathbb{R}^{m \times v'}
                    \]
                    where \(\mathbf{X} \in \mathbb{R}^{m \times v}\) is a matrix of input (or hidden) feature vectors, and 
                    \(\mathbf{W} \in \mathbb{R}^{v \times v'}\) is a fixed weight matrix learned during training.
                </p>

                <p>
                    This formulation uses the same set of weights \(\mathbf{W}\) for every input position. To allow more flexibility, 
                    we can instead make the weights input-dependent. That is, we can allow:
                    \[
                    \mathbf{Z} = \varphi (\mathbf{X}\mathbf{W}(\mathbf{X})),
                    \]
                    where the transformation matrix \(\mathbf{W}(\mathbf{X})\) varies depending on the input itself.
                    This allows the model to dynamically adjust its computations based on the input context.
                    Such input-dependent multiplicative interactions are central to <strong>attention mechanisms</strong>.
                </p>

                <p>
                    In more general settings, the input-dependent transformation matrix can be computed from separate sets of learned representations called 
                    queries, keys, and values. This leads to a broader formulation of attention:
                    \[
                    \mathbf{Z} = \varphi (\mathbf{V}\mathbf{W}(\mathbf{Q}, \mathbf{K}))
                    \]
                    where:
                    <ul style="padding-left: 40px;">
                        <li>\(\mathbf{Q} \in \mathbb{R}^{m \times q}\) is a set of <strong>queries</strong></li>
                        <li>\(\mathbf{K} \in \mathbb{R}^{m \times q}\) is a set of <strong>keys</strong></li>
                        <li>\(\mathbf{V} \in \mathbb{R}^{m \times v}\) is a set of <strong>values</strong></li>
                    </ul>
                    <br>
                    The transformation matrix \(\mathbf{W}(\mathbf{Q}, \mathbf{K})\) represents a learned or computed relationship between queries and keys.
                    Note: In practice, these matrices are often obtained by applying learned linear projections to the input sequence.
                </p>

                
                 <p>
                    In attention, the core idea is to compute each output vector \(\mathbf{z}_j\) as a weighted sum over all value vectors \(\mathbf{v}_i\), 
                    where the weights \(\alpha_{ij}\) are determined by a similarity between the query \(\mathbf{q}_j\) and each key \(\mathbf{k}_i\):
                    \[
                    \mathbf{z}_j = \sum_{i}\alpha_{ij}\mathbf{v}_i
                    \]
                    where \(0 \leq \alpha_{ij} \leq 1\) and \(\sum_i \alpha_{ij} = 1\). This allows the model to selectively attend to relevant parts of the input.
                </p>

                 <p>
                    A common similarity function is the inner product between query and key vectors. If 
                    \(\mathbf{q}, \mathbf{k} \in \mathbb{R}^d\) are independent random variables with zero mean and unit variance, 
                    then their inner product \(\mathbf{q}^\top \mathbf{k}\) has zero mean and variance \(d\). 
                    To prevent large variance from pushing the softmax into extreme regions, we normalize it:
                    \[
                    a(\mathbf{q}, \mathbf{k}) = \frac{\mathbf{q}^\top \mathbf{k}}{\sqrt{d}}.
                    \]
                    This is known as <strong>scaled dot-product attention score</strong>, where the scale factor 
                    \(\frac{1}{\sqrt{d}}\) normalizes the variance so that it remains stable(i.e., \(= 1\)) as the dimension \(d\) increases.  
                    This is used to compute <strong>attention weights</strong> \(\alpha_{ij}\) after applying the softmax function.
                </p>
               
                <p>
                    In practice, we compute attention over minibatches of \(n\) query vectors. Let:
                    \[
                    \mathbf{Q} \in \mathbb{R}^{n \times d}, \quad \mathbf{K} \in \mathbb{R}^{m \times d}, \quad \mathbf{V} \in \mathbb{R}^{m \times v}
                    \]
                    Then the attention-weighted output is computed as:
                    \[
                    \text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{softmax}\left(\frac{\mathbf{Q} \mathbf{K}^\top}{\sqrt{d}}\right) \mathbf{V} \in \mathbb{R}^{n \times v}
                    \]
                    where the softmax is applied row-wise to normalize each row of the \(n \times m\) similarity matrix.
                </p>

                <p>
                    This formulation was introduced in the <strong>Transformer</strong> model (Vaswani et al., 2017) and enables:
                    <ul style="padding-left: 40px;">
                        <li>Full parallelization across positions,</li>
                        <li>Effective modeling of long-range dependencies, and</li>
                        <li>Dynamic, data-dependent weighting of inputs.</li>
                    </ul>
                </p>
            </section>

            
            <section id="self" class="section-content">  
            <h2>Self-Attention</h2>  
            <p>  
                Self-attention is a mechanism that allows each position in a sequence to attend to all other positions in the same sequence. Given an input sequence  
                \[
                \mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_n \in \mathbb{R}^d,
                \]  
                the model computes a new sequence of the same length where each output vector \(\mathbf{y}_i\) is a weighted combination of transformed input vectors.
            </p>

            <p>
                Each input \(\mathbf{x}_i\) is projected into three vectors:
                \[
                \mathbf{q}_i = W^Q \mathbf{x}_i,\quad \mathbf{k}_i = W^K \mathbf{x}_i,\quad \mathbf{v}_i = W^V \mathbf{x}_i,
                \]
                where \(W^Q, W^K, W^V \in \mathbb{R}^{d' \times d}\) are learnable matrices, and \(d'\) is the internal representation dimension for queries, keys, and values.
            </p>

            <p>
                The output at position \(i\) is calculated as:
                \[
                \mathbf{y}_i = \sum_{j=1}^n \alpha_{ij} \mathbf{v}_j,
                \]
                where \(\alpha_{ij}\) is the attention weight from position \(i\) to position \(j\), computed using scaled dot-product attention:
                \[
                \alpha_{ij} = \frac{\exp\left( \frac{\mathbf{q}_i^\top \mathbf{k}_j}{\sqrt{d'}} \right)}{\sum_{l=1}^{n} \exp\left( \frac{\mathbf{q}_i^\top \mathbf{k}_l}{\sqrt{d'}} \right)}.
                \]
            </p>

            <p>
                This mechanism captures contextual relationships between tokens by allowing information to flow from any position to any other in a single layer. However, self-attention by itself is <strong>permutation-invariant</strong>—it treats the input as a set rather than a sequence. This means it does not encode the order of tokens unless explicit positional information is added separately.
            </p>

            <p>
                Self-attention can be computed efficiently for the entire sequence using matrix operations. Let \(Q, K, V \in \mathbb{R}^{n \times d'}\) be matrices formed by stacking all \(\mathbf{q}_i\), \(\mathbf{k}_i\), and \(\mathbf{v}_i\) row-wise:
                \[
                \text{SelfAttention}(X) = \text{softmax}\left( \frac{QK^\top}{\sqrt{d'}} \right) V,
                \]
                where \(X = [\mathbf{x}_1^\top; \ldots; \mathbf{x}_n^\top]\) is the input matrix. This formulation enables parallel computation across all positions.
            </p>

                <h2>Self-Attention</h2> 
                <p>  
                    Given a sequence of input tokens \(\mathbf{x}_1, \cdots, \mathbf{x}_n \in \mathbb{R}^d\). 
                    <strong>self-attention</strong> generate a sequence og outputs via:
                    \[
                    \mathbf{y}_i = \text{Attention}(\mathbf{q}_i, (\mathbf{x}_1, \mathbf{x}_1), \cdots, (\mathbf{x}_n, \mathbf{x}_n ))
                    \]
                    where the quert is \(q_i\) and the keys and valuse are \(\mathbf{x}_1, \cdots \mathbf{x}_n\).
                </p>
                <p>
                    Via training, all the outputs have been known. Thus the model can evaluate the function <strong>in parallel</strong>, which 
                    overcomes the sequential bottleneck of RNNs. 
                </p>

            </section>  
            
            <section id="position" class="section-content"> 
                <h2>Positional Encoding</h2> 
                <p> 
                    In transformer-based architectures, there is no recurrence or convolution to model the order of tokens in a 
                    sequence. To address this, <strong>positional encoding</strong> is added to the input embeddings to inject information about 
                    the position of each token. A widely used method is to define fixed, deterministic positional encodings using 
                    sine and cosine functions of varying frequencies. 
                </p> 

                <p> 
                    The positional encoding vector for position \( i \) and dimension \( j \) is defined as follows: 
                    \[ 
                    p_{i, 2j} = \sin \left( \frac{i}{C^{2j/d}} \right) 
                    \]
                    \[ 
                    p_{i, 2j+1} = \cos \left( \frac{i}{C^{2j/d}} \right) 
                    \] 
                    where: 
                    <ul> 
                        <li>\( i \) is the position index in the sequence, starting from 0,</li> 
                        <li>\( j \) is the dimension index in the positional encoding vector,</li> 
                        <li>\( d \) is the total dimension of the model (e.g., 512),</li> 
                        <li>\( C \) is a constant (usually set to 10,000) to control the frequency scale.</li> 
                    </ul> 
                </p>

                 <p> 
                    This formulation ensures that each dimension of the positional encoding corresponds to a sinusoid of a 
                    different frequency. As a result, any two positions will have a unique positional encoding vector, and the 
                    relative position between tokens can be represented as a linear function of the encodings. 
                </p> 

                <p> 
                    <strong>Example:</strong> Suppose the model dimension is \( d = 4 \), and we compute the positional encoding for 
                    position \( i = 2 \). Then we compute each component of the encoding as:
                    \[ 
                    p_{2,0} = \sin \left( \frac{2}{10,000^{0/4}} \right) = \sin(2) 
                    \] 
                    \[
                     p_{2,1} = \cos \left( \frac{2}{10,000^{0/4}} \right) = \cos(2) 
                    \] 
                    \[
                     p_{2,2} = \sin \left( \frac{2}{10,000^{2/4}} \right) = \sin \left( \frac{2}{100} \right) 
                     \] 
                     \[
                      p_{2,3} = \cos \left( \frac{2}{10,000^{2/4}} \right) = \cos \left( \frac{2}{100} \right) 
                      \] 
                    </p> 

                    <p> 
                        This results in the positional encoding vector for position 2 as: 
                        \[ 
                        \left[ \sin(2),\ \cos(2),\ \sin(0.02),\ \cos(0.02) \right] 
                        \]
                    </p> 

                     <p> These encodings are added element-wise to the token embeddings at each position, allowing the model to 
                        incorporate position information without relying on sequence-aware architectures like RNNs or CNNs. 
                        Moreover, the use of periodic functions enables the model to potentially generalize to sequences longer 
                        than those seen during training. 
                    </p> 
                </section>

            <section id="transformer" class="section-content">
                <h2>The Rise of Transformers</h2>
                <p>
                    The <strong>Transformer</strong> architecture (Vaswani et al., 2017) relies entirely on self-attention and forgoes recurrence. 
                    Each layer applies multi-head self-attention and feedforward sublayers:
                    \[
                    \text{MultiHead}(X) = \text{Concat}(\text{head}_1, \dots, \text{head}_h) W^O, \quad \text{head}_i = \text{Attention}(XW_i^Q, XW_i^K, XW_i^V)
                    \]
                    This enables parallel processing of entire sequences, efficient long-range context modeling, and scalability to large datasets.
                </p>

                <p>
                    Transformers now power models like BERT, GPT, and Vision Transformers, and have become the foundation of modern AI across text, images, and multimodal data.
                </p>
            </section>

            <section id="demo" class="section-content">
                <h2>Demo</h2>
                 <div id="transformer_demo"></div>
            </section>


        </div>
        <script src="/js/main.js"></script>  
        <script src="/js/deep_nn.js"></script>  
    </body>
</html>