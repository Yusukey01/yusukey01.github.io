---
layout: default
title: Intro to Deep Neural Networks
level: detail
description: Learn about deep neural networks basics via Attention mechanisms and Transformer architecture.
uses_math: true
uses_python: false
---
<!DOCTYPE html>
<html>
    <body> 
        <div class="hero-section">
            <h1 class="webpage-name">Intro to Deep Neural Networks</h1>
        </div>

        <div class="topic-nav">
            <a href="#CNN">Convolutional Neural Networks (CNNs)</a>
            <a href="#RNN">Recurrent Neural Networks (RNNs)</a>
            <a href="#attention">Attention Mechanisms</a>
            <a href="#self">Self-Attention</a>
            <a href="#multihead">Multi-Head Attention</a>
            <a href="#position">Positional Encoding</a>
            <a href="#transformer">The Rise of Transformers</a>
            <a href="#demo">Demo</a>
        </div>

        <div class="container">  

            <section id="CNN" class="section-content">
                <h2>Convolutional Neural Networks (CNNs): The First Breakthrough</h2>
                <p> 
                    Deep neural networks have undergone remarkable evolution over the past few decades. From early image 
                    classifiers to today's powerful language and multimodal models, the field has progressed through a series 
                    of foundational innovations. 
                </p>

                <p>
                    In the 1990s, <strong>Convolutional Neural Networks (CNNs)</strong> emerged as a biologically inspired architecture for processing grid-like data such as images. 
                    A CNN uses <em>convolutional layers</em> that apply learnable filters across local spatial regions. Mathematically, a 2D convolution operation is given by:
                    \[
                    (f * x)(i, j) = \sum_m \sum_n f(m, n) \cdot x(i - m, j - n)
                    \]
                    where \(f\) is the filter (or kernel) and \(x\) is the input image. This operation captures local spatial patterns while sharing parameters across the image.
                </p>

                <p>
                    The breakthrough came in 2012, when <strong>AlexNet</strong> won the ImageNet competition. It stacked convolution and pooling layers, used the ReLU nonlinearity \( \text{ReLU}(x) = \max(0, x) \), 
                    and applied dropout to reduce overfitting. AlexNet demonstrated that deep CNNs trained on GPUs could vastly outperform traditional vision pipelines.
                </p>

                <p>
                    This success led to deeper architectures:
                    <ul style="padding-left: 40px;"> 
                        <li><strong>ResNet</strong> introduced <em>residual connections</em> to learn functions of the form \( \mathcal{F}(x) + x \), helping gradients propagate through deep networks.</li> 
                        <li><strong>DenseNet</strong> connected each layer to all previous layers via concatenation, enhancing feature reuse and reducing redundancy.</li>
                    </ul> 
                    These models advanced image recognition and set the stage for modern vision architectures.
                </p>
            </section>

            <section id="RNN" class="section-content">
                <h2> Recurrent Neural Networks (RNNs): Modeling Sequences</h2>
                <p>
                    <strong>Recurrent Neural Networks (RNNs)</strong> process sequential data by maintaining a hidden state across time steps. 
                    At each time step \(t\), the hidden state \(h_t\) is updated using the previous state and the current input:
                    \[
                    h_t = \sigma(W x_t + U h_{t-1} + b)
                    \]
                    where \(\sigma\) is an activation function (e.g., \(\tanh\)), \(W\) and \(U\) are weight matrices, and \(x_t\) is the input.
                </p>

                <p>
                    While RNNs can, in principle, capture long-term dependencies, in practice they suffer from <em>vanishing and exploding gradients</em>. 
                    To mitigate this, <strong>Long Short-Term Memory (LSTM)</strong> units were introduced. LSTMs use gating mechanisms:
                    \[
                    \begin{aligned}
                    f_t &= \sigma(W_f x_t + U_f h_{t-1} + b_f) \quad &\text{(forget gate)} \\
                    i_t &= \sigma(W_i x_t + U_i h_{t-1} + b_i) \quad &\text{(input gate)} \\
                    o_t &= \sigma(W_o x_t + U_o h_{t-1} + b_o) \quad &\text{(output gate)} \\
                    \tilde{c}_t &= \tanh(W_c x_t + U_c h_{t-1} + b_c) \\
                    c_t &= f_t \odot c_{t-1} + i_t \odot \tilde{c}_t \\
                    h_t &= o_t \odot \tanh(c_t)
                    \end{aligned}
                    \]
                    These gates allow the network to learn when to forget, update, and output information, enabling stable training over longer sequences.
                </p>
            </section>

            <section id="attention" class="section-content">
                <h2>Attention Mechanisms</h2>
                <p>
                    Recurrent architectures process sequences step-by-step, which limits parallelism and makes modeling long-range dependencies challenging.  
                    <strong>Attention mechanisms</strong> address this by computing a weighted sum of all input feature vectors, allowing the model to 
                    “attend” to the most relevant parts of the sequence regardless of distance.
                </p>

                <p>
                    In traditional feedforward neural networks, each layer computes hidden activations as a linear transformation of input activations followed by a nonlinearity:
                    \[
                    \mathbf{Z} = \varphi (\mathbf{X}\mathbf{W}) \in \mathbb{R}^{m \times v'}
                    \]
                    where \(\mathbf{X} \in \mathbb{R}^{m \times v}\) is a matrix of input (or hidden) feature vectors, and 
                    \(\mathbf{W} \in \mathbb{R}^{v \times v'}\) is a fixed weight matrix learned during training.
                </p>

                <p>
                    This formulation uses the same set of weights \(\mathbf{W}\) for every input position. To allow more flexibility, 
                    we can instead make the weights input-dependent. That is, we can allow:
                    \[
                    \mathbf{Z} = \varphi (\mathbf{X}\mathbf{W}(\mathbf{X})),
                    \]
                    where the transformation matrix \(\mathbf{W}(\mathbf{X})\) varies depending on the input itself.
                    This allows the model to dynamically adjust its computations based on the input context.
                    Such input-dependent multiplicative interactions are central to <strong>attention mechanisms</strong>.
                </p>

                <p>
                    In more general settings, the input-dependent transformation matrix can be computed from separate sets of learned representations called 
                    queries, keys, and values. This leads to a broader formulation of attention:
                    \[
                    \mathbf{Z} = \varphi (\mathbf{V}\mathbf{W}(\mathbf{Q}, \mathbf{K}))
                    \]
                    where:
                    <ul style="padding-left: 40px;">
                        <li>\(\mathbf{Q} \in \mathbb{R}^{m \times q}\) is a set of <strong>queries</strong></li>
                        <li>\(\mathbf{K} \in \mathbb{R}^{m \times q}\) is a set of <strong>keys</strong></li>
                        <li>\(\mathbf{V} \in \mathbb{R}^{m \times v}\) is a set of <strong>values</strong></li>
                    </ul>
                    <br>
                    The transformation matrix \(\mathbf{W}(\mathbf{Q}, \mathbf{K})\) represents a learned or computed relationship between queries and keys.
                    Note: In practice, these matrices are often obtained by applying learned linear projections to the input sequence.
                </p>

                
                 <p>
                    In attention, the core idea is to compute each output vector \(\mathbf{z}_j\) as a weighted sum over all value vectors \(\mathbf{v}_i\), 
                    where the weights \(\alpha_{ij}\) are determined by a similarity between the query \(\mathbf{q}_j\) and each key \(\mathbf{k}_i\):
                    \[
                    \mathbf{z}_j = \sum_{i}\alpha_{ij}\mathbf{v}_i
                    \]
                    where \(0 \leq \alpha_{ij} \leq 1\) and \(\sum_i \alpha_{ij} = 1\). This allows the model to selectively attend to relevant parts of the input.
                </p>

                 <p>
                    A common similarity function is the inner product between query and key vectors. If 
                    \(\mathbf{q}, \mathbf{k} \in \mathbb{R}^d\) are independent random variables with zero mean and unit variance, 
                    then their inner product \(\mathbf{q}^\top \mathbf{k}\) has zero mean and variance \(d\). 
                    To prevent large variance from pushing the softmax into extreme regions, we normalize it:
                    \[
                    a(\mathbf{q}, \mathbf{k}) = \frac{\mathbf{q}^\top \mathbf{k}}{\sqrt{d}}.
                    \]
                    This is known as <strong>scaled dot-product attention score</strong>, where the scale factor 
                    \(\frac{1}{\sqrt{d}}\) normalizes the variance so that it remains stable(i.e., \(= 1\)) as the dimension \(d\) increases.  
                    This is used to compute <strong>attention weights</strong> \(\alpha_{ij}\) after applying the softmax function.
                </p>
               
                <p>
                    In practice, we compute attention over minibatches of \(n\) query vectors. Let:
                    \[
                    \mathbf{Q} \in \mathbb{R}^{n \times d}, \quad \mathbf{K} \in \mathbb{R}^{m \times d}, \quad \mathbf{V} \in \mathbb{R}^{m \times v}
                    \]
                    Then the attention-weighted output is computed as:
                    \[
                    \text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{softmax}\left(\frac{\mathbf{Q} \mathbf{K}^\top}{\sqrt{d}}\right) \mathbf{V} \in \mathbb{R}^{n \times v}
                    \]
                    where the softmax is applied row-wise to normalize each row of the \(n \times m\) similarity matrix.
                </p>

           </section>

            <section id="self" class="section-content">  
            <h2>Self-Attention</h2>  
            <p>  
                <strong>Self-attention</strong> is a mechanism that allows each position \(i\) in a sequence to attend to all other positions 
                in the same sequence. Given a sequence of input tokens \(\mathbf{x}_1, \cdots, \mathbf{x}_n \in \mathbb{R}^d\),
                a sequence of outputs is given by:
                \[
                \mathbf{y}_i = \text{Attention}(\mathbf{q}_i, (\mathbf{x}_1, \mathbf{x}_1), \cdots, (\mathbf{x}_n, \mathbf{x}_n )) \in \mathbb{R}^d
                \]
                where the quert is \(q_i\) and the keys and valuse are \(\mathbf{x}_1, \cdots \mathbf{x}_n\).
            </p>
            <p>
                This mechanism captures contextual relationships between tokens by allowing information to flow from any position to any other in a single layer.
                Through training, all the outputs have been known. Thus the model can evaluate \(\mathbf{y}_i\) <strong>in parallel</strong>, which 
                overcomes the sequential bottleneck of RNNs. 
            </p>

            </section>  

            <section id="multihead" class="section-content">
                <h2>Multi-Head Attention</h2>
                <p>
                    While a single self-attention mechanism can capture dependencies between tokens, using only one set of projections 
                    may limit the model's expressiveness. <strong>Multi-head attention (MHA)</strong> addresses this by computing 
                    multiple self-attention operations in parallel using different learned projections, called "heads." 
                </p>

                <p>
                    Formally, given an input matrix \( X \in \mathbb{R}^{n \times d} \), each attention head applies its own learned projections to compute queries, keys, and values:
                    \[
                    \text{head}_i = \text{Attention}\left(\mathbf{W}_i^{(q)} \mathbf{q}, \{\mathbf{W}_i^{(k)} \mathbf{k}_j, \mathbf{W}_i^{(v)}\mathbf{v}_j\}\right) \in \mathbb{R}^{p_v},
                    \]
                    where \( \mathbf{W}_i^{(q)}, \mathbf{W}_i^{(k)}, \mathbf{W}_i^{(v)} \in \mathbb{R}^{p_v \times d} \) are projection matrices specific to head \( i \).
                </p>

                <p>
                    All heads are computed in parallel and their outputs are concatenated and linearly projected to produce the final output:
                    \[
                    \text{MHA}(\mathbf{X}) = \text{MHA}\left(\mathbf{q}, \{\mathbf{k}_j, \mathbf{v}_j\}\right) 
                        = \mathbf{W}_o  \begin{bmatrix} 
                                    \text{head}_1 \\
                                    \vdots \\
                                    \text{head}_h\\
                                \end{bmatrix}.
                    \]
                    where \( \mathbf{W}_o \in \mathbb{R}^{d \times hp_v} \) is a learned output projection matrix.
                </p>

                <p>
                    By using multiple heads, the model can attend to information from different representation subspaces and capture a richer set of relationships in the data. 
                    Each head may focus on different positions or interaction patterns, improving both performance and interpretability.
                </p>
            </section>

            
            <section id="position" class="section-content"> 
                <h2>Positional Encoding</h2> 
                <p> 
                    The standard self-attention is permutation invariant, which means the model ignores the order of tokens in an 
                    input sequence. To address this, <strong>positional encoding</strong> is added to the input embeddings to inject information about 
                    the position of each token. A widely used method is to define fixed, deterministic positional encodings using 
                    sine and cosine functions of varying frequencies. 
                </p> 

                <p> 
                    The positional encoding vector for position \( i \) and dimension \( j \) is defined as follows: 
                    \[ 
                    p_{i, 2j} = \sin \left( \frac{i}{C^{2j/d}} \right) 
                    \]
                    \[ 
                    p_{i, 2j+1} = \cos \left( \frac{i}{C^{2j/d}} \right) 
                    \] 
                    where: 
                    <ul style="padding-left: 40px;"> 
                        <li>\( i \) is the position index in the sequence, starting from 0,</li> 
                        <li>\( j \) is the dimension index in the positional encoding vector,</li> 
                        <li>\( d \) is the total dimension of the model (e.g., 512),</li> 
                        <li>\( C \) is a constant (usually set to 10,000) to control the frequency scale.</li> 
                    </ul> 
                </p>

                 <p> 
                    This formulation ensures that each dimension of the positional encoding corresponds to a sinusoid of a 
                    different frequency. As a result, any two positions will have a unique positional encoding vector, and the 
                    relative position between tokens can be represented as a linear function of the encodings. 
                </p> 

                <p> 
                    <strong>Example:</strong> Suppose the model dimension is \( d = 4 \), and we compute the positional encoding for 
                    position \( i = 2 \). Then we compute each component of the encoding as:
                    \[ 
                    \begin{align*}
                    &p_{2,0} = \sin \left( \frac{2}{10,000^{0/4}} \right) = \sin(2) \\\\      
                    &p_{2,1} = \cos \left( \frac{2}{10,000^{0/4}} \right) = \cos(2) \\\\
                    &p_{2,2} = \sin \left( \frac{2}{10,000^{2/4}} \right) = \sin \left( \frac{2}{100} \right) \\\\
                    &p_{2,3} = \cos \left( \frac{2}{10,000^{2/4}} \right) = \cos \left( \frac{2}{100} \right) 
                    \end{align*}
                    \] 
                    </p> 

                    <p> 
                        This results in the positional encoding vector for position 2 as: 
                        \[ 
                        \mathbf{p}_2 = \left[ \sin(2),\ \cos(2),\ \sin(0.02),\ \cos(0.02) \right].
                        \]
                    </p> 

                     <p> These encodings are added element-wise to the token embeddings at each position, allowing the model to 
                        incorporate position information without relying on sequence-aware architectures like RNNs or CNNs. 
                        Moreover, the use of periodic functions enables the model to potentially generalize to sequences longer 
                        than those seen during training. 
                    </p> 
                </section>

            <section id="transformer" class="section-content">
                <h2>The Rise of Transformers</h2>

                <p>
                    The <strong>transformer</strong> architecture (Vaswani et al., 2017) relies on self-attention for the encoder 
                    & decoder blocks. 
                </p>

                <div class="pseudocode">
                    <span class="pseudocode-title">Transformer Encoder & Decoder</span>
                    <strong>EncoderBlock(X)</strong>
                    &emsp;&emsp;Z = LayerNorm(MHA(Q =X, K = X, V = X) + X)
                    &emsp;&emsp;E = LayerNorm(FeedForward(Z) + Z)
                    &emsp;&emsp;<strong>return</strong> E
                    <br>
                    <strong>Encoder(X, N)</strong>
                    &emsp;&emsp;E = POS(Embed(X))
                    &emsp;&emsp;for \(n\) in range (N)
                    &emsp;&emsp;&emsp;&emsp; E = EncoderBlock(E)
                    &emsp;&emsp;<strong>return</strong> E
                    <br>
                    <strong>DecoderBlock(Y, E)</strong>
                    &emsp;&emsp;Z = LayerNorm(MHA(Q =Y, K = Y, V = Y) + Y)
                    &emsp;&emsp;Z' = LayerNorm(MHA(Q = Z, K = E, V = E) + Z)
                    &emsp;&emsp;D = LayerNorm(FeedForward(Z') + Z')
                    &emsp;&emsp;<strong>return</strong> D
                    <br>
                    <strong>Decoder(Y, E, N)</strong>
                    &emsp;&emsp;D = POS(Embed(Y))
                    &emsp;&emsp;for \(n\) in range (N)
                    &emsp;&emsp;&emsp;&emsp; D = DecoderBlock(D, E)
                    &emsp;&emsp;<strong>return</strong> D
                </div>
                <p>
                    where 
                    <ul>
                        <li><strong>MHA(...)</strong> : the Multi-head self-attention. Each token attends to every other token.</li>
                        <li><strong>\(+ (...)\)</strong> : residual connection. The input is added back to the output of attention. </li>
                        <li><strong>LayerNorm(...)</strong> : Layer normalization, which is applied after the residual connection to stabilize training.</li>
                        <li><strong>FeedForward(...)</strong> : A position-wise fully connected feedforward network is applied to each token vector independently.</li>
                        <li><strong>Embed(...)</strong> : Each token is mapped to a high-dimensional embedding vector.</li>
                        <li><strong>POS(...)</strong> : Positional encoding is added to embeddings to inject token order information.</li>
                        <li><strong>N</strong>: the number of copies of the block</li>
                    </ul>
                   


                </p>

                <p>
                    Transformers now power models like BERT, GPT, and Vision Transformers, and have become the foundation of modern AI across text, images, and multimodal data.
                </p>
                
                <p>
                    This formulation was introduced in the <strong>Transformer</strong> model (Vaswani et al., 2017) and enables:
                    <ul style="padding-left: 40px;">
                        <li>Full parallelization across positions,</li>
                        <li>Effective modeling of long-range dependencies, and</li>
                        <li>Dynamic, data-dependent weighting of inputs.</li>
                    </ul>
                </p>
            </section>

            <section id="demo" class="section-content">
                <h2>Demo</h2>
                 <div id="transformer_demo"></div>
            </section>


        </div>
        <script src="/js/main.js"></script>  
        <script src="/js/deep_nn.js"></script>  
    </body>
</html>