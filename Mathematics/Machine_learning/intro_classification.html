---
layout: default
title: Intro to Classification
level: detail
description: Learn about basic classification methods such as logistic regression.
uses_math: true
uses_python: false
---
<!DOCTYPE html>
<html>
    <body> 
       <!-- LearningResource Schema for Content Pages -->
        <script type="application/ld+json">
        {
        "@context": "https://schema.org",
        "@type": "LearningResource",
        "name": "{{ page.title }}",
        "description": "{{ page.description }}",
        "learningResourceType": "lesson",
        "educationalUse": "instruction",
        "educationalLevel": "university",
        "interactivityType": "{% if page.content contains 'Interactive Demo' or page.content contains 'demo' %}active{% else %}expositive{% endif %}",
        "author": {
            "@type": "Person",
            "name": "Yusuke Yokota",
            "jobTitle": "Mathematics & Computer Science Educator"
        },
        "publisher": {
            "@type": "Organization",
            "name": "MATH-CS COMPASS",
            "url": "https://yusukey01.github.io"
        },
        "about": [
            {% if page.url contains 'classification' %}
            { "@type": "Thing", "name": "Classification" },
            { "@type": "Thing", "name": "Logistic Regression" },
            { "@type": "Thing", "name": "Machine Learning" }
            {% elsif page.url contains 'regression' %}
            { "@type": "Thing", "name": "Regression" },
            { "@type": "Thing", "name": "Statistical Learning" }
            {% elsif page.url contains 'neural' %}
            { "@type": "Thing", "name": "Neural Networks" },
            { "@type": "Thing", "name": "Deep Learning" }
            {% else %}
            { "@type": "Thing", "name": "Mathematics" },
            { "@type": "Thing", "name": "Machine Learning" }
            {% endif %}
        ],
        "teaches": [
            "Mathematical concepts",
            "Practical applications",
            "Problem-solving techniques"
        ],
        "isPartOf": {
            "@type": "Course",
            "name": "{% if page.url contains 'Machine_learning' %}Machine Learning{% elsif page.url contains 'Linear_algebra' %}Linear Algebra{% elsif page.url contains 'Calculus' %}Calculus to Optimization & Analysis{% elsif page.url contains 'Probability' %}Probability & Statistics{% elsif page.url contains 'Discrete' %}Discrete Mathematics & Algorithms{% endif %}",
            "description": "{% if page.url contains 'Machine_learning' %}Explore machine learning ideas and applications with mathematical foundations{% elsif page.url contains 'Linear_algebra' %}Explore the foundations of Linear Algebra, covering key concepts such as linear equations, vector spaces, eigenvalues, orthogonality, least squares, and stochastic matrices{% elsif page.url contains 'Calculus' %}Explore key calculus concepts essential for optimization, analysis, and machine learning{% elsif page.url contains 'Probability' %}Explore fundamental concepts of probability and statistics essential for machine learning{% elsif page.url contains 'Discrete' %}Explore the foundations of discrete mathematics and algorithms, covering graph theory, combinatorics, and the theory of computation{% endif %}",
            "provider": {
            "@type": "Organization",
            "name": "MATH-CS COMPASS",
            "url": "https://yusukey01.github.io"
            },
            "instructor": {
            "@type": "Person",
            "name": "Yusuke Yokota",
            "jobTitle": "Mathematics & Computer Science Educator"
            },
            "courseCode": "{% if page.url contains 'Machine_learning' %}V{% elsif page.url contains 'Linear_algebra' %}I{% elsif page.url contains 'Calculus' %}II{% elsif page.url contains 'Probability' %}III{% elsif page.url contains 'Discrete' %}IV{% endif %}",
            "hasCourseInstance": {
            "@type": "CourseInstance",
            "courseMode": "online",
            "courseWorkload": "{% if page.url contains 'Machine_learning' %}PT5H{% elsif page.url contains 'Linear_algebra' %}PT2H{% elsif page.url contains 'Calculus' %}PT3H{% elsif page.url contains 'Probability' %}PT2H30M{% elsif page.url contains 'Discrete' %}PT4H{% endif %}",
            "instructor": {
                "@type": "Person",
                "name": "Yusuke Yokota"
            }
            },
            "offers": {
            "@type": "Offer",
            "price": "0",
            "priceCurrency": "USD",
            "availability": "https://schema.org/InStock",
            "category": "free"
            }
        }
        }
        </script>
        <!-- WebApplication Schema for Interactive Demos -->
        <script type="application/ld+json">
        {
        "@context": "https://schema.org",
        "@type": "WebApplication",
        "name": "{{ page.title }} Interactive Demo",
        "description": "Interactive demonstration of {{ page.title | downcase }} concepts",
        "applicationCategory": "EducationalApplication",
        "operatingSystem": "Web Browser",
        "url": "https://yusukey01.github.io{{ page.url }}",
        "author": {
            "@type": "Person",
            "name": "Yusuke Yokota"
        },
        "applicationSubCategory": "Mathematical Visualization",
        "featureList": [
            "Interactive mathematical visualization",
            "Real-time parameter adjustment",
            "Educational demonstrations"
        ],
        "isAccessibleForFree": true,
        "educationalUse": "instruction",
        "educationalLevel": "university"
        }
        </script>
        <div class="hero-section">
            <h1 class="webpage-name"> Intro to Classification</h1>
        </div>

        {% include section_navigation.html %}

        <div class="topic-nav">
            <a href="#intro">Binary Logistic Regression</a>
            <a href="#demo">Binary Logistic Regression Demo</a>
             <a href="#kernel">Kernel Methods</a>
             <a href="#rbf-approx">RBF Kernel &  Random Fourier Features</a>
            <a href="#mlr"> Multinomial logistic regression </a>
        </div> 

        <div class="container">  
            <section id="intro" class="section-content">
                <h2>Binary Logistic Regression</h2>
                <p>
                    Consider discriminative classification model: 
                    \[
                    p(y \mid x, \theta)  
                    \]
                    where \(x \in \mathbb{R}^D\) is an input vector, \(y \in \{1, \cdots, C\}\) is the class label, 
                    and \(\theta\) are the parameters.
                </p>

                <p>
                    Let \(y \in \{0, 1\}\). Then the model is known as <strong>binary logistic regression</strong>, which can be represented by: 
                    \[
                     p(y \mid x, \theta) = \text{Ber }(y \mid \sigma(w^\top x + b))
                    \]
                    where \(\sigma\) is the <strong>sigmoid(logistic) function</strong>, \(w \in \mathbb{R}^D\) is the weight vector, 
                    \(b \in \mathbb{R}\) is a bias, and \(\theta = (w, b)\) are the model parameters. The notation \(\text{Ber}(y \mid p)\) denotes the 
                    <strong>Bernoulli distribution</strong>, defined as:
                    \[
                    \text{Ber}(y \mid p) = p^y (1 - p)^{1 - y}, \quad y \in \{0, 1\},\; p \in [0, 1].
                    \]
                </p>

                <p>
                    In other words, 
                    \[
                    p(y = 1 \mid x, \theta) = \sigma(a) = \frac{1}{1 + e^{-a}}
                    \]
                    where \(a\) is called the <strong>logit</strong>(or the <strong>pre-activation</strong>): 
                    \[
                     a = w^\top x + b = \log (\frac{p}{1-p}), \quad  \text{with } p = p(y = 1 \mid x, \theta).
                    \]
                </p>

                <p> 
                    The sigmoid function outputs the probability that the class label is \(y = 1\). Therefore, we define a decision rule 
                    such that we predict \(y = 1\) if and only if class 1 is more likely than class 0. That is, 
                    \[
                    \begin{align*}
                    f(x) &= \mathbb{I}(p(y=1 \mid x) > p(y=0 \mid x)) \\\\
                         &= \mathbb{I} \left( \log \frac{p(y =  1 \mid x)}{p(y =0 \mid x)} > 0 \right) \\\\
                         &= \mathbb{I}(a).
                    \end{align*}
                    \]
                    Here, \(\mathbb{I}(\cdot)\) denotes the <strong>indicator function</strong>, which returns 1 if the 
                    condition inside is true, and 0 otherwise:
                    \[
                    \mathbb{I}(\text{condition}) = 
                    \begin{cases}
                    1 & \text{if condition is true,} \\
                    0 & \text{otherwise.}
                    \end{cases}
                    \]
                    </p>

                    <p>
                    Therefore, the prediction function can be written as: 
                    \[
                    f(x ; \theta) = w^\top x + b.
                    \]
                    Since \(w^\top x\) is the <strong>inner product</strong> between the weight vector \(w\) and the feature vector \(x\), 
                    this function defines a <strong>linear hyperplane</strong> with <strong>normal</strong> vector \(w\) and an offset \(b\) from the origin. 
                    In other words, the weight vector \(w\) determines the "orientation" of the <strong>decision boundary</strong>, 
                    and the bias term \(b\) shifts the boundary. Moreover, the norm \(\|w\|\) controls the "steepness" of the 
                    sigmoid function â€” a larger norm results in a sharper transition between predicted classes, which reflects greater 
                    confidence in predictions.
                </p>

                <p>
                    To estimate the parameters \(w\), we maximize the likelihood of the observed dataset \(\mathcal{D} = \{(x_n, y_n)\}_{n=1}^N\). 
                    Equivalently, we minimize the <strong>negative log-likelihood</strong> (NLL):
                    \[
                    \begin{align*}
                    \mathrm{NLL }(w) 
                    &= - \frac{1}{N} \log p(\mathcal{D} \mid w) \\\\
                    &= - \frac{1}{N} \log \prod_{n =1}^N \text{Ber }(y_n \mid \mu_n) \\\\
                    &= - \frac{1}{N} \sum_{n=1}^N \log \left[ \mu_n^{y_n}  (1-\mu_n)^{1-y_n} \right] \\\\
                    &= - \frac{1}{N} \sum_{n=1}^N \left[ y_n \log  \mu_n  + (1-y_n) \log (1-\mu_n) \right] \\\\
                    &= - \frac{1}{N} \sum_{n=1}^N \mathbb{H}_{ce} (y_n , \mu_n) 
                    \end{align*}
                    \]
                    where \(\mu_n = \sigma(a_n) = \sigma(w^\top x_n)\) is the model's predicted probability of class 1, and 
                     \(\mathbb{H}_{ce}(y_n, \mu_n)\) is 
                    the <a href="../Probability/entropy.html"><strong>binary cross entropy</strong></a> between \(y_n\) and \(\mu_n\).
                </p>

                <p> 
                    We then find the <a href="../Probability/mle.html"><strong>maximum likelihood estimate (MLE)</strong></a> by solving:
                    \[
                    \nabla_w \text{NLL }(w) = 0
                    \]
                    via <a href="../Calculus/gradient.html"><strong>gradient-based optimization</strong></a> (in practice, using 
                    <a href="autodiff.html"><strong>automatic differentiation</strong></a>).
                </p>

                <p>
                    The gradient of the NLL with respect to \(w\) is
                    \[
                    \begin{align*}
                      \nabla_w \text{NLL }(w)
                      &= - \frac{1}{N} \sum_{n=1}^N \left[ y_n (1-\mu_n) x_n - (1-y_n) \mu_n x_n \right] \\\\
                      &= \frac{1}{N} \sum_{n=1}^N (\mu_n - y_n) x_n, \\\\
                    \end{align*}
                    \]
                    Equivalently, in matrix form:
                    \[
                     \nabla_w \text{NLL }(w) = \frac{1}{N} \left( \mathbf{1}_N^\top (\text{diag }(\mu - y)X)\right)^\top.
                    \]
                    where \(X \in \mathbb{R}^{N \times D}\) is the design matrix whose \(n\)th row is \(x_n^\top\), and 
                    \(\mu, y\in\mathbb{R}^N\) are the vectors of \(\mu_n\) and \(y_n\), respectively. Now, it is clear 
                    that each \(x_n\) is being weighted by its residual \(\mu_n - y_n\).
                </p>

                <p>
                     In practice, we never build an \(N \times N\) diagonal matrix. Instead, we simply computes
                     \[
                     \nabla_w \text{NLL }(w) = \frac{1}{N} X^\top (\mu - y)
                     \]
                     which is both more memory- and compute-efficient.
                </p>

                <div class="proof">
                    <span class="proof-title">Derivation of the gradient of \(\text{NLL }(w)\): </span> 
                    <p>
                        Recall the sigmoid function:
                        \[
                        \sigma(a) = \frac{1}{1 + e^{-a}}.
                        \]
                        We compute its derivative with respect to \(a\):
                        \[
                        \begin{aligned}
                        \frac{d\sigma}{da}
                        &= \frac{d}{da}\,(1 + e^{-a})^{-1} \\\\\
                        &= -1 \cdot (1 + e^{-a})^{-2} \;\bigl(-e^{-a}\bigr) \\\\ 
                        &= \frac{e^{-a}}{(1 + e^{-a})^2} \\\\
                        &= \left( \frac{1}{1 + e^{-a}}\right) \left(\frac{e^{-a}}{1 + e^{-a}}\right) \\\\
                        &= \sigma(a)\,\bigl(1 - \sigma(a)\bigr).
                        \end{aligned}
                        \]
                    </p>
                
                    <p>
                        Since \(\mu_n = \sigma(a_n)\), we conclude:
                        \[
                            \frac{\partial \mu_n}{\partial a_n}
                            = \mu_n\,(1 - \mu_n).
                        \]
                    </p>
                    
                    <p>
                        For a single data point \((x_n,y_n)\), let 
                        \[
                            \ell_n = -\bigl[y_n\log\mu_n + (1-y_n)\log(1-\mu_n)\bigr].
                        \]
                    
                    We compute: 
                        \[
                            \frac{\partial \ell_n}{\partial \mu_n}
                            = -\Bigl(\frac{y_n}{\mu_n} - \frac{1 - y_n}{1 - \mu_n}\Bigr)
                            = \frac{\mu_n - y_n}{\mu_n (1 -\mu_n)}.
                        \]
                    By chain rule, 
                        \[
                            \begin{aligned}
                            \frac{\partial \ell_n}{\partial a_n}
                            &= \frac{\partial \ell_n}{\partial \mu_n} \cdot \frac{\partial \mu_n}{\partial a_n} \\\\
                            &= \frac{\mu_n - y_n}{\mu_n (1 -\mu_n)} \cdot \mu_n\,(1 - \mu_n) \\\\
                            &= \mu_n - y_n.
                            \end{aligned}
                        \]
                    Also, since \(a_n = w^\top x_n + b\),
                        \[
                            \frac{\partial a_n}{\partial w}
                            = x_n.
                        \]
                    Now combine all of them: 
                        \[
                            \frac{\partial \ell_n}{\partial w}
                            = \frac{\partial \ell_n}{\partial a_n}
                            \cdot \frac{\partial a_n}{\partial w}
                            = (\mu_n - y_n)x_n.
                        \]
                    Finally, summing over all \(N\) examples and dividing by \(N\) gives
                        \[
                            \nabla_w \mathrm{NLL}(w) 
                             = \frac{1}{N}\sum_{n=1}^N (\mu_n - y_n)\,x_n.
                        \]
                    </p>
                </div>

                <p>
                    To verify <a href="../Calculus/gradient.html"><strong>convexity</strong></a>, we examine the Hessian:
                     \[
                    \begin{align*}
                    H(w) &=\nabla_w^2 \text{NLL }(w)  \\\\
                         &= \frac{1}{N} \sum_{n=1}^N (\mu_n( 1 - \mu_n)x_n) x_n^\top \\\\
                         &= \frac{1}{N} X^\top S X
                    \end{align*}
                    \]
                    where \(S = \text{diag }(\mu_1(1-\mu_1), \cdots, \mu_N(1-\mu_N))\). 
                </p>
                    For any \(v \in \mathbb{R}^D\),
                     \[
                    v^\top X^\top S X v = (v^\top X^\top S^{\frac{1}{2}})(S^{\frac{1}{2}} X v)
                    \]
                    and then 
                    \[
                    \| v^\top X^\top S^{\frac{1}{2}} \|_2^2 > 0.
                    \]
                    
                    Thus the Hessian \(H(w)\) is <a href="../Linear_algebra/symmetry.html"><strong>positive definite</strong></a>, 
                    confirming that the NLL is strictly convex and any stationary point is the global optimum.
                <p>

                <p>
                    Note: In practice, \(\mu_n\) can be very close to 0 or 1.  We need to use \(\ell_2\) regularization to make the Hessian 
                    nonsingular. 
                </p>

            </section>

            

            <section id="demo" class="section-content">
                <h2>Binary Logistic Regression Demo</h2>
                <p>
                    Given a linear scoring function:
                    \[
                    z = w_0 + w_1 x_1 + w_2 x_2
                    \]
                    and the predicted probability: 
                    \[
                    \sigma(z) = \frac{1}{1 + e^{-z}}, 
                    \]
                    we minimize the average binary cross-entropy plus an \(\ell_2\) penalty on the weights (excluding the bias):
                    \[
                    \mathrm{NLL }(w) 
                        = - \frac{1}{N} \sum_{n=1}^N \left[ y_n \log  \sigma(z_n)  + (1-y_n) \log (1-\sigma(z_n)) \right] 
                            + \frac{\lambda}{2} \sum_{j=1}^2 w_j^2.
                    \]
                    Note: The learning rate controls the step size in each gradient descent update. 
                </p>
                <div id="logistic_regression_visualizer"></div>
            </section>

           <section id="kernel" class="section-content">
                <h2>Kernel Methods</h2>

                <p>
                    In many problems the original feature vectors \(x\in\mathbb{R}^D\) are not <strong>linearly separable</strong>.  
                    One fix is to apply a nonlinear <strong>feature mapping</strong>
                    \[
                    \phi(x)\colon\mathbb{R}^D\to\mathbb{R}^P,\quad P\gg D,
                    \]
                    hoping that \(\{\phi(x_n)\}\) become separable.  But explicit mappings can blow up model complexity and lead to overfitting, even with regularization.
                </p>

                <p>
                    Alternatively, if a learning algorithm depends <em>only</em> on inner products
                    \(\langle \phi(x),\phi(x')\rangle\), we can invoke the <strong>kernel trick</strong>:
                    replace each dot-product with a <strong>kernel function</strong> 
                    \(K(x,x')\) that computes \(\langle\phi(x),\phi(x')\rangle\) implicitlyâ€”no \(\phi\) ever constructed.
                </p>

                <div class="theorem">
                    <span class="theorem-title">Kernel:</span>  
                    A function
                    \[
                    K : \mathbb{R}^D \times \mathbb{R}^D \;\to\; \mathbb{R}
                    \]
                    is a <strong>kernel</strong> for some feature map \(\phi\) if
                    \[
                    \forall\,x,x',\quad K(x,x') = \langle \phi(x),\,\phi(x')\rangle.
                    \]
                    In practice \(K\) must also be symmetric and positive semi-definite (Mercer's condition).
                </div>

                <p>
                    Concretely, we build the <strong>kernel matrix</strong>
                    \[
                    K_{ij} = K(x_i,x_j),\quad i,j=1,\dots,N,
                    \]
                    and run our algorithm using \(\{K_{ij}\}\) instead of \(\{\phi(x_i)\}\).
                </p>

                <p>
                    For large \(N\) (e.g. \(>10^5\)), storing \(N\times N\) kernels is infeasible.  
                    Two common remedies are:
                </p>
                <ul style="padding-left: 40px;">
                    <li>
                    <strong>Random Fourier Features</strong> (next section): approximate RBF kernels with \(O(ND')\) cost.
                    </li>
                    <li>
                    <strong>Representation learning</strong>: learn \(\phi(x;\theta)\) via <strong>neural networks</strong> and optimize by mini-batch SGD.
                    </li>
                </ul>
                </section>

                <section id="rbf-approx" class="section-content">
                <h2>RBF Kernel &  Random Fourier Features</h2>

                <p>
                     Among the many valid kernel functions, one of the most widely used is the <strong>RBF (Gaussian) kernel</strong>:
                    \[
                    K_{\mathrm{RBF}}(x,x') = \exp\Bigl(-\tfrac{\|x-x'\|^2}{2\sigma^2}\Bigr),
                    \]
                    which corresponds to an infinite-dimensional feature map.
                    <br>
                    Note: RBF stands for "Radical Basis Function."
                </p>

                <p>
                    By <strong>Bochner's theorem</strong>, any shift-invariant kernel admits the representation
                    \[
                    K_{\mathrm{RBF}}(x,x') 
                    = \mathbb{E}_{\omega\sim\mathcal{N}(0,\sigma^{-2}I)}\bigl[\cos(\omega^\top(x-x'))\bigr].
                    \]
                </p>

                <p>
                    To approximate, sample \(D'\) frequencies \(\{\omega_k\}\) and phases \(\{b_k\}\sim\mathrm{Uniform}[0,2\pi]\),
                    then define
                    \[
                    \phi_{\mathrm{RFF}}(x)
                    = \sqrt{\frac{2}{D'}}\,
                        \bigl[\cos(\omega_1^\top x + b_1),\,\dots,\,\cos(\omega_{D'}^\top x + b_{D'})\bigr]\in\mathbb{R}^{D'}.
                    \]
                </p>

                <p>
                    One shows
                    \[
                    \phi_{\mathrm{RFF}}(x)^\top \phi_{\mathrm{RFF}}(x') 
                    \;\approx\; K_{\mathrm{RBF}}(x,x'),
                    \]
                    so we can train a regularized <strong>linear</strong> model on these \(D'\)-dim features in \(O(ND')\) time, 
                    avoiding the \(O(N^2)\) kernel matrix.
                </p>
                </section>

                <section id="mlr" class="section-content">
                    <h2>Multinomial Logistic Regression</h2>

                    <p>
                        When there are more than two classes (\(C>2\)), we generalize the sigmoid to the
                        <strong>softmax</strong> function.  For inputs \(x\in\mathbb{R}^D\) and class weights
                        \(\{w_c,b_c\}_{c=1}^C\), the model defines
                        \[
                        p(y=c\,|\,x,\theta)
                        = \frac{\exp\!\bigl(w_c^\top x + b_c\bigr)}
                                {\sum_{c'=1}^C \exp\!\bigl(w_{c'}^\top x + b_{c'}\bigr)},
                        \quad c=1,\dots,C.
                        \]
                    </p>

                    <p>
                        We fit \(\theta=\{w_c,b_c\}\) by minimizing the average <strong>cross-entropy</strong> loss
                        \[
                        \mathrm{NLL}(\theta)
                        = -\frac{1}{N}\sum_{n=1}^N
                            \sum_{c=1}^C
                            \mathbb{I}\{y_n=c\}\,\log p(y_n=c\,|\,x_n,\theta).
                        \]
                    </p>

                    <p>
                        The gradient has a similarly clean form:
                        \[
                        \frac{\partial\mathrm{NLL}}{\partial w_c}
                        = \frac{1}{N}\sum_{n=1}^N
                            \Bigl[p(y_n=c\,|\,x_n)-\mathbb{I}\{y_n=c\}\Bigr]\,x_n,
                        \]
                        and you can optimize it via batch or (mini-batch) gradient descent.
                    </p>

                    <p>
                        The Hessian is block-structured but remains positive semi-definite, ensuring convexity.
                    </p>
                    </section>         
        </div>
        <script src="/js/main.js"></script> 
        <script src="/js/logistic_regression.js"></script>
    </body>
</html>