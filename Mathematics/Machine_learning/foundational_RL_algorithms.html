---
layout: default
title: Foundational RL Algorithms
level: detail
description: Learn about foundational reinforcement learning algorithms from dynamic programming to temporal difference methods
uses_math: true
uses_python: false
---
<!DOCTYPE html>
<html>
    <body> 
        <div class="hero-section">
            <h1 class="webpage-name">Foundational RL Algorithms</h1>
        </div>

        {% include section_navigation.html %}

        <div class="topic-nav">
            <a href="#intro">Introduction</a>
            <a href="#VI">Value Iteration</a>
            <a href="#PI">Policy Iteration</a>
            <a href="#td">Temporal Difference Learning</a>
            <a href="#qlearning">Q-Learning and SARSA</a>
            <a href="#exploration">Exploration vs. Exploitation</a>
            <a href="#"></a>
        </div>

        <div class="container">  

           <section id="intro" class="section-content">
                <h2>Introduction</h2>
                 <p>
                    This page covers the foundational algorithms that implement the theoretical concepts introduced in our 
                    <a href="reinforcement.html">Introduction to Reinforcement Learning</a>. We explore how the Bellman equations 
                    translate into practical computational methods for solving Markov Decision Processes.
                </p>

                <p>
                    We introduce fundamental algorithms starting with model-based approaches and progressing to model-free methods:
                    <ul style="padding-left: 40px;">
                    <li><strong>Value Iteration</strong>: recursive update of value functions based on Bellman optimality.</li>
                    <li><strong>Policy Iteration</strong>: alternating between policy evaluation and policy improvement.</li>
                    <li><strong>Temporal Difference Learning</strong>: bootstrapping methods that combine ideas from DP and Monte Carlo.</li>
                    <li><strong>Q-Learning and SARSA</strong>: model-free algorithms for learning optimal policies through interaction.</li>
                    </ul>
                </p>
            </section>

            <section id="" class="section-content">
                <h2>Value Iteration (VI)</h2>
                <p>
                Let the initial estimate be \(V_0\). We update it as follows: 
                \[
                V_{k+1}(s) = \max_a \left[ R(s, a) + \gamma \sum_{s'} p(s' \mid s, a) V_k (s') \right].
                \]
                This is called <strong>Bellman backup</strong>. Each iteration reduces the maximum value function error by a 
                constant factor: 
                \[
                \max_s \| V_{k+1}(s) - V_*(s) \| \leq \gamma \max_s \| V_k (s)  - V_* (s) \|.
                \]
                So, \(V_k\) will converges \(V_*\) as \(k \to \infty\). 
                </p>
                <p>
                    For all possible states \(s\), VI computes \(V_*(s)\) and \(\pi_*(s)\), averaging over all possible 
                    next states \(s'\) at each iteration. (Note: If we need the value and policy for only certain starting states, other methods can be used such as 
                    real-time dynamic programming). 
                </p>
            </section>

             <section id="" class="section-content">
                <h2>Policy Iteration (PI)</h2>
                <p>
                Let \(\mathbf{v}(s) = V_{\pi}(s)\) be the value function encoded as a vector indexed by states \(s\). Also, 
                the reward vector is represented by 
                \[
                \mathbf{r}(s) = \sum_a \pi(a \mid s) R(s, a)
                \]
                and the state transition matrix is written as 
                \[
                \mathbf{T}(s' \mid s) = \sum_a \pi(a \mid s) p(s' \mid s, a).
                \]
                Then Bellman's equation for policy evaluation can be represented as a linear system in \(\| \mathcal{S}\|\) unknowns:
                \[
                \mathbf{v} = \mathbf{r} + \gamma \mathbf{T}\mathbf{v}
                \]
                Theoretically, we can solve this by \(\mathbf{v} = (\mathbf{I} - \gamma \mathbf{T})^{-1} \mathbf{r}\), but 
                We can compute \(\mathbf{v}_{t+1} = \mathbf{r} + \gamma \mathbf{T} \mathbf{v}_t\) iteratively. This process is called 
                <strong>policy evaluation</strong>. Once we have evaluated \(V_{\pi}\) for the policy \(\pi\), we need to find "better" 
                policy \(\pi'\). 
                </p>
                <p>
                    Now we move on the <strong>policy improvement</strong> step:
                    \[
                      \pi'(s) = \arg \max_a \{R(s, a) + \gamma \mathbb{E}[V_{\pi}(s')]\}
                    \]
                    This guarantees \(V_{\pi'} \geq V_{\pi}\) because: 
                    \[
                    \begin{align*}
                    \mathbf{v} = \mathbf{r} + \gamma \mathbf{T}\mathbf{v} &\leq  \mathbf{r'} + \gamma \mathbf{T'}\mathbf{v} \\\\
                                                                          &\leq  \mathbf{r'} + \gamma \mathbf{T'}(\mathbf{r'} + \gamma \mathbf{T'}\mathbf{v})\\\\
                                                                          &\leq \cdots  \\\\
                                                                          &= (\mathbf{I} + \gamma \mathbf{T'} + \gamma^2 \mathbf{T'}^2 + \cdots)\mathbf{r} \\\\
                                                                          &= (\mathbf{I} - \gamma \mathbf{T'})^{-1} \mathbf{r} \\\\
                                                                          &= \mathbf{v'}

                    \end{align*}
                    \]
                </p>
                <p>
                    In policy iteration algorithm, starting from an initial policy \(\pi_0\), we alternate between policy evaluation step and 
                    policy improvement step. Within finite iterations, the algorithm will converge to optimal policy since there are at 
                    most \(\|\mathcal{A}^{\|\mathcal{S}\|} \|\) deterministic policies and every update improves the policy. 
                </p>
            </section>

            <section id="td" class="section-content">
                <h2>Temporal Difference Learning</h2>
                <p>
                    We now transition from <strong>model-based</strong> methods (Value and Policy Iteration) to <strong>model-free</strong> methods. 
                    The key difference is that we no longer assume knowledge of transition probabilities \(p(s' \mid s, a)\) or reward function \(R(s, a)\). 
                    Instead, the agent must learn by interacting with the environment and observing actual transitions.
                </p>

                <p>
                    <strong>Temporal Difference (TD) learning</strong> bridges the gap between Monte Carlo methods and dynamic programming. 
                    Unlike Monte Carlo methods that wait until the end of an episode, TD methods update estimates immediately after each step 
                    using <strong>bootstrapping</strong> - updating estimates based on other estimates.
                </p>
                
                <p>
                    The simplest TD method is <strong>TD(0)</strong>, which updates the value function using:
                    \[
                    V(s_t) \leftarrow V(s_t) + \alpha \left[ r_t + \gamma V(s_{t+1}) - V(s_t) \right]
                    \]
                    where \(\alpha\) is the learning rate and \(\delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)\) is called the <strong>TD error</strong>.
                </p>

                <p>
                    The TD error measures the difference between the current value estimate \(V(s_t)\) and a better estimate 
                    \(r_t + \gamma V(s_{t+1})\) based on observed experience. This better estimate is called the <strong>TD target</strong>. 
                    From Bellman's expectation equation (covered in our intro page), we know that:
                    \[
                    V_{\pi}(s) = \mathbb{E}_{\pi} \left[ r + \gamma V_{\pi}(s') \mid s \right]
                    \]
                    The TD target \(r_t + \gamma V(s_{t+1})\) is a sample-based approximation of this expectation, 
                    using the actual observed reward and next state.
                </p>

                <p>
                    Key properties of TD learning:
                    <ul style="padding-left: 40px;">
                    <li><strong>Online learning</strong>: Updates occur after each step, not requiring complete episodes</li>
                    <li><strong>Bootstrapping</strong>: Uses current value estimates to update other estimates</li>
                    <li><strong>Lower variance</strong>: Compared to Monte Carlo, as it doesn't depend on entire episode returns</li>
                    <li><strong>Biased estimates</strong>: Initially biased due to bootstrapping, but converges to true values</li>
                    </ul>
                </p>

                <p>
                    TD methods form the foundation for many practical RL algorithms, combining the best aspects of 
                    Monte Carlo methods (model-free learning) and dynamic programming (bootstrapping for efficiency).
                </p>
            </section>

            <section id="qlearning" class="section-content">
                <h2>Q-Learning and SARSA</h2>
                <p>
                    Moving from value functions to action-value functions, we can learn optimal policies without knowing 
                    the environment model. Two fundamental algorithms demonstrate different approaches to this problem.
                </p>

                <h3>Q-Learning (Off-Policy)</h3>
                <p>
                    Q-learning learns the optimal action-value function \(Q_*(s,a)\) directly using the update rule:
                    \[
                    Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left[ r_t + \gamma \max_{a'} Q(s_{t+1}, a') - Q(s_t, a_t) \right]
                    \]
                    The key insight is that Q-learning uses the <strong>maximum</strong> over next-state actions, making it 
                    <strong>off-policy</strong> - it learns about the optimal policy while potentially following a different behavioral policy.
                </p>

                <h3>SARSA (On-Policy)</h3>
                <p>
                    SARSA (State-Action-Reward-State-Action) learns the value function for the policy it's currently following:
                    \[
                    Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left[ r_t + \gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t) \right]
                    \]
                    where \(a_{t+1}\) is the action actually taken by the current policy. This makes SARSA <strong>on-policy</strong>.
                </p>

                <h3>Key Differences</h3>
                <p>
                    <ul style="padding-left: 40px;">
                    <li><strong>Convergence</strong>: Q-learning converges to \(Q_*\) regardless of policy followed; SARSA converges to \(Q_\pi\) for the policy being followed</li>
                    <li><strong>Exploration</strong>: SARSA is more conservative, considering the exploration strategy in its updates</li>
                    <li><strong>Safety</strong>: In environments with dangerous actions, SARSA may be safer as it accounts for exploration</li>
                    </ul>
                </p>

                <p>
                    Both algorithms require exploration to discover good actions, leading us to the fundamental challenge 
                    of balancing exploration and exploitation.
                </p>
            </section>

            <section id="exploration" class="section-content">
                <h2>Exploration vs. Exploitation</h2>
                <p>
                    The <strong>exploration-exploitation tradeoff</strong> is fundamental to reinforcement learning. An agent must 
                    balance between:
                    <ul style="padding-left: 40px;">
                    <li><strong>Exploitation</strong>: Choosing actions that yield high rewards based on current knowledge</li>
                    <li><strong>Exploration</strong>: Trying new actions to potentially discover better strategies</li>
                    </ul>
                </p>

                <h3>Îµ-Greedy Strategy</h3>
                <p>
                    The simplest exploration strategy chooses:
                    \[
                    a_t = \begin{cases}
                    \arg\max_a Q(s_t, a) & \text{with probability } 1-\varepsilon \\
                    \text{random action} & \text{with probability } \varepsilon
                    \end{cases}
                    \]
                    where \(\varepsilon \in [0,1]\) controls the exploration rate. Often \(\varepsilon\) is decayed over time.
                </p>

                <h3>Softmax Action Selection</h3>
                <p>
                    A more sophisticated approach uses the Boltzmann (softmax) distribution:
                    \[
                    P(a_t = a \mid s_t) = \frac{\exp(Q(s_t, a)/\tau)}{\sum_{a'} \exp(Q(s_t, a')/\tau)}
                    \]
                    where \(\tau > 0\) is the temperature parameter. Higher temperatures lead to more exploration.
                </p>

                <h3>Optimistic Initialization</h3>
                <p>
                    Initializing Q-values optimistically (higher than realistic values) encourages exploration 
                    of all actions early in learning, as the agent will be "disappointed" by actual rewards 
                    and try other actions.
                </p>

                <p>
                    The choice of exploration strategy significantly affects learning performance and is often 
                    problem-dependent. More advanced methods like UCB (Upper Confidence Bound) and Thompson 
                    sampling provide principled approaches to this tradeoff.
                </p>

            </section>
            


        </div>
        <script src="/js/main.js"></script>    
    </body>
</html>