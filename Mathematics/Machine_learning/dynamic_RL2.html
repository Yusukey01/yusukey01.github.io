---
layout: default
title: Dynamic Programming in RL
level: detail
description: Learn about dynamic programming methods for solving Markov Decision Process(MDP)
uses_math: true
uses_python: false
---
<!DOCTYPE html>
<html>
    <body> 
        <div class="hero-section">
            <h1 class="webpage-name">Dynamic Programming in RL</h1>
        </div>

        {% include section_navigation.html %}

        <div class="topic-nav">
            <a href="#intro">Introduction</a>
            <a href="#VI">Value Iteration</a>
            <a href="#PI">Policy Iteration</a>
            <a href="#"></a>
        </div>

        <div class="container">  

            <section id="intro" class="section-content">
                <h2>Introduction</h2>
                 <p>
                    In this part, we focus on <strong>Dynamic Programming (DP)</strong> methods in Reinforcement Learning. 
                    These methods assume full knowledge of the Markov Decision Process (MDP), including transition probabilities and reward models.
                    By using the Bellman equations as recursive definitions, DP algorithms can compute optimal policies and value functions 
                    through systematic updates.
                </p>

                <p>
                    Although direct access to the full model is unrealistic in most real-world settings, 
                    dynamic programming provides essential theoretical foundations for understanding 
                    modern RL algorithms. Many model-free methods can be seen as sampling-based approximations of these DP procedures.
                </p>

                <p>
                    We introduce two fundamental algorithms in this part:
                    <ul style="padding-left: 40px;">
                    <li><strong>Value Iteration</strong>: a recursive update of value functions based on Bellman optimality.</li>
                    <li><strong>Policy Iteration</strong>: alternating between policy evaluation and policy improvement.</li>
                    </ul>
                    Both algorithms are guaranteed to converge to the optimal policy in finite MDPs.
                </p>
            </section>

            <section id="" class="section-content">
                <h2>Value Iteration(VI)</h2>
                <p>
                Let the initial estimate be \(V_0\). We update it as follows: 
                \[
                V_{k+1}(s) = \max_a \left[ R(s, a) + \gamma \sum_{s'} p(s' \mid s, a) V_k (s')\rignt].
                \]
                This is called \<strong>Bellman backup</strong>. Each iteration reduces the maximum value function error by a 
                constant factor: 
                \[
                \max_s \| V_{k+1}(s) - V_*(s) \| \leq \gamma \max_s \| V_k (s)  - V_* (s) \|.
                \]
                So, \(V_k\) will converges \(V_*\) as \(k \to \infty\). 
                </p>
                <p>
                    For all possible states \(s\), VI computes \(V_*(s)\) and \(\pi_*(s)\), averaging over all possible 
                    next states \(s'\) at each iteration. (Note: If we need the value and policy for only certain starting states, other methods can be used such as 
                    real-time dynamic programming). 
                </p>
            </section>

             <section id="" class="section-content">
                <h2>Policy Iteration</h2>
                <p>
                Let \(\mathbf{v}(s) = V_{\pi}(s)\) be the value function encoded as a vector indexed by states \(s\). Also, 
                the reward vector is represented by 
                \[
                \mathbf{r}(s) = \sum_a \pi(a \mid s) R(s, a)
                \]
                and the state transition matrix is written as 
                \[
                \mathbf{T}(s' \mid s) = \sum_a \pi(a \mid s) p(s' \mid s, a).
                \]
                Then Bellman's equation for policy evaluation can be represented as a linear system in \(\| \mathcal{S}\|\) unknowns:
                \[
                \mathbf{v} = \mathbf{r} + \gamma \mathbf{T}\mathbf{v}
                \]
                Theoritically, we can solve this by \(\mathbf{v} = (\mathbf{I} - \gamma \mathbf{T})^{-1} \mathbf{r}\), but 
                We can compute \(\mathbf{v}_{t+1} = \mathbf{r} + \gamma \mathbf{T} \mathbf{v}_t\) iteratively. This process is called 
                <strong>policy evaluation</strong>. Once we have evaluated \(V_{\pi}\) for the policy \(\pi\), we need to find "better" 
                policy \(\pi'\). Now we move on the <strong>policy improvement</strong> step.
                </p>
                <p>
                    \[
                      \pi'(s) = \arg \max_a \{R(s, a) + \gamma \mathbb{E}[V_{\pi}(s')]\}
                    \]
                    This guarantees \(V_{\pi'} \geq V_{\pi}\) because: 
                    \[
                    \begin{align*}
                    \mathbf{v} = \mathbf{r} + \gamma \mathbf{T}\mathbf{v} &\leq  \mathbf{r'} + \gamma \mathbf{T'}\mathbf{v} \\\\
                                                                          &\leq  \mathbf{r'} + \gamma \mathbf{T'}(\mathbf{r'} + \gamma \mathbf{T'}\mathbf{v})\\\\
                                                                          &\leq \cdots  \\\\
                                                                          &= (\mathbf{I} - \gamma \mathbf{T'})^{-1} \mathbf{r} \\\\
                                                                          &= \mathbf{v'}

                    \end{align*}
                    \]
                </p>
                <p>
                    In policy itertion algorithm, starting form an initial policy \(\pi_0\), we alternate between policy evaluation step and 
                    policy improvement step. Within finite iterations, the algorithm will converge to optimal policy since there are at 
                    most \(\|\mathcal{A}^{\|\mathcal{S}\|} \|\) deterministic policies and every update improves the policy. 
                </p>
            </section>

             <section id="" class="section-content">
                <h2></h2>
            </section>

             <section id="" class="section-content">
                <h2></h2>
            </section>
           
            


        </div>
        <script src="/js/main.js"></script>    
    </body>
</html>