---
layout: default
title: Dynamic Programming in RL
level: detail
description: Learn about dynamic programming methods for solving Markov Decision Process(MDP)
uses_math: true
uses_python: false
---
<!DOCTYPE html>
<html>
    <body> 
        <div class="hero-section">
            <h1 class="webpage-name">Dynamic Programming in RL</h1>
        </div>

        {% include section_navigation.html %}

        <div class="topic-nav">
            <a href="#intro">Introduction</a>
            <a href="#"></a>
            <a href="#"></a>
            <a href="#"></a>
        </div>

        <div class="container">  

            <section id="intro" class="section-content">
                <h2>Introduction</h2>
                 <p>
                    In this part, we focus on <strong>Dynamic Programming (DP)</strong> methods in Reinforcement Learning. 
                    These methods assume full knowledge of the Markov Decision Process (MDP), including transition probabilities and reward models.
                    By using the Bellman equations as recursive definitions, DP algorithms can compute optimal policies and value functions 
                    through systematic updates.
                </p>

                <p>
                    Although direct access to the full model is unrealistic in most real-world settings, 
                    dynamic programming provides essential theoretical foundations for understanding 
                    modern RL algorithms. Many model-free methods can be seen as sampling-based approximations of these DP procedures.
                </p>

                <p>
                    We introduce two fundamental algorithms in this part:
                    <ul style="padding-left: 40px;">
                    <li><strong>Value Iteration</strong>: a recursive update of value functions based on Bellman optimality.</li>
                    <li><strong>Policy Iteration</strong>: alternating between policy evaluation and policy improvement.</li>
                    </ul>
                    Both algorithms are guaranteed to converge to the optimal policy in finite MDPs.
                </p>
            </section>

            <section id="" class="section-content">
                <h2></h2>
            </section>

             <section id="" class="section-content">
                <h2></h2>
            </section>

             <section id="" class="section-content">
                <h2></h2>
            </section>

             <section id="" class="section-content">
                <h2></h2>
            </section>
           
            


        </div>
        <script src="/js/main.js"></script>    
    </body>
</html>