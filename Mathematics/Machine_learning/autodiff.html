---
layout: default
title: Automatic Differentiation
level: detail
description: Learn about the automatic differentiation with analytic examples.
uses_math: true
uses_python: true
---
<!DOCTYPE html>
<html>
    <body> 
        <div class="hero-section">
            <h1 class="webpage-name"> Automatic Differentiation</h1>
        </div>

        <div class="topic-nav">
            <a href="#AD">Automatic Differentiation</a>
            <a href="#eg">Analytic Example of Reverse-Mode AD</a>
            <a href="#app">Applications of AD</a> 
            <a href="#code">Sample Code</a>   
        </div> 

        <div class="container">  

            <section id="AD" class="section-content">
                <h2>Automatic Differentiation</h2>
                <p>
                    In multilayer perceptrons (MLPs), <strong>backpropagation</strong> is an efficient application of the chain rule to compute gradients layer by layer. 
                    More generally, this technique is known as <strong>automatic differentiation (AD)</strong>. AD is not limited to sequential layers. It applies to arbitrary 
                    <strong>computational graphs</strong>, which are directed acyclic graphs (DAGs) that represent how variables are computed from inputs to outputs.
                </p>

                <p>
                    Automatic differentiation systematically applies the chain rule over this graph structure to compute exact derivatives. 
                    In <strong>reverse-mode AD</strong> — which underlies backpropagation — we begin at the output node and work backwards, 
                    accumulating gradients with respect to each variable.
                </p>
            </section>

            <section id="eg" class="section-content">
                <h2>Analytic Example of Reverse-Mode AD</h2>
                <p>
                    To make the process of automatic differentiation concrete, let's walk through an analytic example using a 
                    composite scalar-valued function of two variables. We'll decompose the function into primitive operations, 
                    represent it as a computational graph, and compute its gradients using <strong>reverse-mode automatic differentiation</strong> 
                    (i.e., backpropagation).
                </p>

                <p>
                    Consider a function
                    \[
                    f(x_1, x_2) = \log \left((x_1 + x_2)^2 + \sin(x_1 x_2) \right).
                    \]
                    We decompose this into primitive operations:
                    \[
                    \begin{align*}
                    &x_3 = x_1 + x_2 \\\\
                    &x_4 = x_3^2 \\\\
                    &x_5 = x_1 x_2 \\\\
                    &x_6 = \sin(x_5) \\\\
                    &x_7 = x_4 + x_6 \\\\
                    &x_8 = \log(x_7) = f \\\\
                    \end{align*}
                    \]
                </p>

                <div style="max-width: 800px; margin: 20px auto; background: white; padding: 20px; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1);">
                    <svg viewBox="0 0 800 650" xmlns="http://www.w3.org/2000/svg">
                      <defs>
                        <!-- Arrow marker for forward pass -->
                        <marker id="arrowhead" markerWidth="10" markerHeight="10" refX="9" refY="3" orient="auto">
                          <polygon points="0 0, 10 3, 0 6" fill="#333" />
                        </marker>
                        
                        <!-- Arrow marker for backward pass -->
                        <marker id="arrowhead-red" markerWidth="10" markerHeight="10" refX="9" refY="3" orient="auto">
                          <polygon points="0 0, 10 3, 0 6" fill="#e74c3c" />
                        </marker>
                        
                        <!-- Gradient for nodes -->
                        <linearGradient id="nodeGradient" x1="0%" y1="0%" x2="100%" y2="100%">
                          <stop offset="0%" style="stop-color:#4a90e2;stop-opacity:1" />
                          <stop offset="100%" style="stop-color:#357abd;stop-opacity:1" />
                        </linearGradient>
                        
                        <linearGradient id="inputGradient" x1="0%" y1="0%" x2="100%" y2="100%">
                          <stop offset="0%" style="stop-color:#5cb85c;stop-opacity:1" />
                          <stop offset="100%" style="stop-color:#449d44;stop-opacity:1" />
                        </linearGradient>
                        
                        <linearGradient id="outputGradient" x1="0%" y1="0%" x2="100%" y2="100%">
                          <stop offset="0%" style="stop-color:#d9534f;stop-opacity:1" />
                          <stop offset="100%" style="stop-color:#c9302c;stop-opacity:1" />
                        </linearGradient>
                      </defs>
                      
                      <!-- Title -->
                      <text x="400" y="30" text-anchor="middle" font-size="24" font-weight="bold" fill="#333">
                        Computational Graph for f(x₁, x₂) = log((x₁ + x₂)² + sin(x₁x₂))
                      </text>
                      
                      <!-- Input nodes -->
                      <g id="input-nodes">
                        <circle cx="200" cy="100" r="30" fill="url(#inputGradient)" stroke="#333" stroke-width="2"/>
                        <text x="200" y="107" text-anchor="middle" font-size="18" font-weight="bold" fill="white">x₁</text>
                        
                        <circle cx="600" cy="100" r="30" fill="url(#inputGradient)" stroke="#333" stroke-width="2"/>
                        <text x="600" y="107" text-anchor="middle" font-size="18" font-weight="bold" fill="white">x₂</text>
                      </g>
                      
                      <!-- Intermediate nodes -->
                      <g id="intermediate-nodes">
                        <!-- x3 = x1 + x2 -->
                        <circle cx="400" cy="200" r="30" fill="url(#nodeGradient)" stroke="#333" stroke-width="2"/>
                        <text x="400" y="207" text-anchor="middle" font-size="18" font-weight="bold" fill="white">x₃</text>
                        <text x="405" y="240" text-anchor="middle" font-size="14" fill="#333">x₁ + x₂</text>
                        
                        <!-- x5 = x1 * x2 -->
                        <circle cx="400" cy="300" r="30" fill="url(#nodeGradient)" stroke="#333" stroke-width="2"/>
                        <text x="400" y="307" text-anchor="middle" font-size="18" font-weight="bold" fill="white">x₅</text>
                        <text x="400" y="345" text-anchor="middle" font-size="14" fill="#333">x₁ × x₂</text>
                        
                        <!-- x4 = x3^2 -->
                        <circle cx="300" cy="400" r="30" fill="url(#nodeGradient)" stroke="#333" stroke-width="2"/>
                        <text x="300" y="407" text-anchor="middle" font-size="18" font-weight="bold" fill="white">x₄</text>
                        <text x="295" y="445" text-anchor="middle" font-size="14" fill="#333">x₃²</text>
                        
                        <!-- x6 = sin(x5) -->
                        <circle cx="500" cy="400" r="30" fill="url(#nodeGradient)" stroke="#333" stroke-width="2"/>
                        <text x="500" y="407" text-anchor="middle" font-size="18" font-weight="bold" fill="white">x₆</text>
                        <text x="500" y="445" text-anchor="middle" font-size="14" fill="#333">sin(x₅)</text>
                        
                        <!-- x7 = x4 + x6 -->
                        <circle cx="400" cy="480" r="30" fill="url(#nodeGradient)" stroke="#333" stroke-width="2"/>
                        <text x="400" y="487" text-anchor="middle" font-size="18" font-weight="bold" fill="white">x₇</text>
                        <text x="435" y="487" text-anchor="start" font-size="14" fill="#333">x₄ + x₆</text>
                      </g>
                      
                      <!-- Output node -->
                      <g id="output-node">
                        <circle cx="400" cy="580" r="30" fill="url(#outputGradient)" stroke="#333" stroke-width="2"/>
                        <text x="400" y="587" text-anchor="middle" font-size="16" font-weight="bold" fill="white">x₈ = f</text>
                        <text x="400" y="625" text-anchor="middle" font-size="14" fill="#333">log(x₇)</text>
                      </g>
                      
                      <!-- Forward edges (black) -->
                      <g id="forward-edges" fill="none" stroke="#333" stroke-width="2">
                        <!-- x1 to x3 -->
                        <path d="M 220 120 Q 300 150 380 180" marker-end="url(#arrowhead)"/>
                        
                        <!-- x2 to x3 -->
                        <path d="M 580 120 Q 500 150 420 180" marker-end="url(#arrowhead)"/>
                        
                        <!-- x1 to x5 -->
                        <path d="M 200 130 Q 200 250 370 285" marker-end="url(#arrowhead)"/>
                        
                        <!-- x2 to x5 -->
                        <path d="M 600 130 Q 600 250 430 285" marker-end="url(#arrowhead)"/>
                        
                        <!-- x3 to x4 -->
                        <path d="M 380 220 Q 340 300 315 375" marker-end="url(#arrowhead)"/>
                        
                        <!-- x5 to x6 -->
                        <path d="M 420 320 Q 460 350 485 375" marker-end="url(#arrowhead)"/>
                        
                        <!-- x4 to x7 -->
                        <path d="M 320 420 Q 360 440 380 460" marker-end="url(#arrowhead)"/>
                        
                        <!-- x6 to x7 -->
                        <path d="M 480 420 Q 440 440 420 460" marker-end="url(#arrowhead)"/>
                        
                        <!-- x7 to f -->
                        <path d="M 400 510 L 400 550" marker-end="url(#arrowhead)"/>
                      </g>
                      
                      <!-- Backward edges (red) - gradient flow -->
                      <g id="backward-edges" fill="none" stroke="#e74c3c" stroke-width="2" stroke-dasharray="5,5" opacity="0.8">
                        <!-- f to x7 -->
                        <path d="M 415 555 L 415 515" marker-end="url(#arrowhead-red)"/>
                        
                        <!-- x7 to x4 -->
                        <path d="M 385 465 Q 345 445 325 425" marker-end="url(#arrowhead-red)"/>
                        
                        <!-- x7 to x6 -->
                        <path d="M 415 465 Q 455 445 475 425" marker-end="url(#arrowhead-red)"/>
                        
                        <!-- x4 to x3 -->
                        <path d="M 285 375 Q 325 295 385 225" marker-end="url(#arrowhead-red)"/>
                        
                        <!-- x6 to x5 -->
                        <path d="M 485 380 Q 445 350 415 325" marker-end="url(#arrowhead-red)"/>
                        
                        <!-- x3 to x1 -->
                        <path d="M 385 185 Q 305 155 225 125" marker-end="url(#arrowhead-red)"/>
                        
                        <!-- x3 to x2 -->
                        <path d="M 415 185 Q 495 155 575 125" marker-end="url(#arrowhead-red)"/>
                        
                        <!-- x5 to x1 -->
                        <path d="M 385 280 Q 230 200 215 135" marker-end="url(#arrowhead-red)"/>
                        
                        <!-- x5 to x2 -->
                        <path d="M 415 280 Q 570 200 585 135" marker-end="url(#arrowhead-red)"/>
                      </g>
                      
                      <!-- Legend -->
                      <g id="legend" transform="translate(20, 480)">
                        <rect x="0" y="0" width="250" height="140" fill="#f8f8f8" stroke="#ddd" stroke-width="1" rx="5"/>
                        <text x="100" y="20" text-anchor="middle" font-size="16" font-weight="bold" fill="#333">Legend</text>
                        
                        <circle cx="20" cy="40" r="10" fill="url(#inputGradient)" stroke="#333" stroke-width="1"/>
                        <text x="40" y="45" font-size="14" fill="#333">Input variables</text>
                        
                        <circle cx="20" cy="65" r="10" fill="url(#nodeGradient)" stroke="#333" stroke-width="1"/>
                        <text x="40" y="70" font-size="14" fill="#333">Intermediate values</text>
                        
                        <circle cx="20" cy="90" r="10" fill="url(#outputGradient)" stroke="#333" stroke-width="1"/>
                        <text x="40" y="95" font-size="14" fill="#333">Output</text>
                        
                        <path d="M 10 115 L 30 115" stroke="#333" stroke-width="2" marker-end="url(#arrowhead)"/>
                        <text x="40" y="120" font-size="14" fill="#333">Forward pass</text>
                        
                        <path d="M 130 115 L 150 115" stroke="#e74c3c" stroke-width="2" stroke-dasharray="5,5" marker-end="url(#arrowhead-red)"/>
                        <text x="160" y="120" font-size="14" fill="#333">Gradient flow</text>
                      </g>
                      
                       <!-- Gradient annotations with white background for visibility -->
                      <g id="gradient-annotations" font-size="11" fill="#e74c3c" font-weight="bold">
                        <!-- Add white rectangles behind text for better visibility -->
                       
                        <text x="440" y="535">∂f/∂x₇ = 1/x₇</text>
                        
                       
                        <text x="200" y="410">∂f/∂x₄ = 1/x₇</text>
                        
                       
                        <text x="530" y="410">∂f/∂x₆ = 1/x₇</text>
                        
                      
                        <text x="285" y="210">∂f/∂x₃ = 2x₃/x₇</text>
                        
                        
                        <text x="460" y="310">∂f/∂x₅ = cos(x₅)/x₇</text>
                      </g>

                    </svg>
                </div>


                <p style="margin-top: 20px;">
                    This computational graph clearly shows the DAG (Directed Acyclic Graph) structure. Notice how:
                    <ul  style="padding-left: 40px;">
                        <li>Each input variable (x₁ and x₂) has <strong>multiple outgoing edges</strong>, contributing to different intermediate computations</li>
                        <li>The graph flows from inputs at the top to the output at the bottom</li>
                        <li>During backpropagation, gradients flow in the <strong>reverse direction</strong> (from f back to x₁ and x₂)</li>
                    </ul>
                </p>

                <p>
                    Starting from the output and working backwards:
                    \[
                    \begin{align*}
                    \frac{\partial f}{\partial x_8} &= 1  \\\\

                    \frac{\partial f}{\partial x_7} &= \frac{\partial f}{\partial x_8} \cdot \frac{\partial x_8}{\partial x_7} \\\\
                                                    &= 1 \cdot \frac{1}{x_7} = \frac{1}{x_7} \\\\

                    \frac{\partial f}{\partial x_4} &=  \frac{\partial f}{\partial x_7} \cdot \frac{\partial x_7}{\partial x_4} \\\\
                                                    &=  \frac{1}{x_7} \cdot 1 = \frac{1}{x_7} \\\\

                    \frac{\partial f}{\partial x_6} &=  \frac{\partial f}{\partial x_7} \cdot \frac{\partial x_7}{\partial x_6} \\\\
                                                    &=  \frac{1}{x_7} \cdot 1 = \frac{1}{x_7} \\\\

                    \frac{\partial f}{\partial x_3} &=  \frac{\partial f}{\partial x_4} \cdot \frac{\partial x_4}{\partial x_3} \\\\
                                                    &=  \frac{1}{x_7}  \cdot 2 x_3\\\\

                    \frac{\partial f}{\partial x_5} &=  \frac{\partial f}{\partial x_6} \cdot \frac{\partial x_6}{\partial x_5} \\\\
                                                    &=  \frac{1}{x_7} \cdot \cos(x_5)\\\\                               
                    \end{align*}
                    \]
                </p>

                <p>
                    Notice that the input variables \(x_1\) and \(x_2\) each contribute to multiple intermediate nodes:
                    <ul style="padding-left: 40px;">
                        <li>\(x_1\) influences both \(x_3\) (via addition) and \(x_5\) (via multiplication)</li>
                        <li>\(x_2\) influences both \(x_3\) (via addition) and \(x_5\) (via multiplication)</li>
                    </ul>
                    This means we need to <strong>sum the gradients</strong> from all paths when computing the final derivatives.
                </p>

                <p>
                    To find the gradients with respect to the input variables, we sum contributions from all paths:
                </p>
                
                <p>
                    For \(\frac{\partial f}{\partial x_1}\):
                    \[
                    \begin{align*}
                    \frac{\partial f}{\partial x_1} &= \frac{\partial f}{\partial x_3} \cdot \frac{\partial x_3}{\partial x_1} + \frac{\partial f}{\partial x_5} \cdot \frac{\partial x_5}{\partial x_1} \\\\
                                                    &= \frac{2x_3}{x_7} \cdot 1 + \frac{\cos(x_5)}{x_7} \cdot x_2 \\\\
                                                    &= \frac{1}{x_7} \left[2x_3 + x_2 \cos(x_5)\right]
                    \end{align*}
                    \]
                </p>

                <p>
                    For \(\frac{\partial f}{\partial x_2}\):
                    \[
                    \begin{align*}
                    \frac{\partial f}{\partial x_2} &= \frac{\partial f}{\partial x_3} \cdot \frac{\partial x_3}{\partial x_2} + \frac{\partial f}{\partial x_5} \cdot \frac{\partial x_5}{\partial x_2} \\\\
                                                    &= \frac{2x_3}{x_7} \cdot 1 + \frac{\cos(x_5)}{x_7} \cdot x_1 \\\\
                                                    &= \frac{1}{x_7} \left[2x_3 + x_1 \cos(x_5)\right]
                    \end{align*}
                    \]
                </p>

                 <p>
                    Replacing the intermediate variables with their expressions in terms of \(x_1\) and \(x_2\):
                    <ul style="padding-left: 40px;">
                        <li>\(x_3 = x_1 + x_2\)</li>
                        <li>\(x_5 = x_1 x_2\)</li>
                        <li>\(x_7 = (x_1 + x_2)^2 + \sin(x_1 x_2)\)</li>
                    </ul>
                    
                    Finally, we get the derivatives:
                    \[
                    \boxed{
                    \begin{align*}
                    \frac{\partial f}{\partial x_1} &= \frac{2(x_1 + x_2) + x_2 \cos(x_1 x_2)}{(x_1 + x_2)^2 + \sin(x_1 x_2)} \\\\
                    \frac{\partial f}{\partial x_2} &= \frac{2(x_1 + x_2) + x_1 \cos(x_1 x_2)}{(x_1 + x_2)^2 + \sin(x_1 x_2)}
                    \end{align*}
                    }
                    \]
                </p>

                 <p>
                    The power of automatic differentiation lies in its systematic approach:
                    <ol style="padding-left: 40px;">
                        <li>Decompose complex functions into simple primitive operations</li>
                        <li>Apply the chain rule mechanically through the computational graph</li>
                        <li>Sum gradients when variables contribute through multiple paths</li>
                    </ol>
                    This process can be fully automated, making it the backbone of modern deep learning frameworks.
                </p>
            </section>

              

            <section id="app" class="section-content">
                <h2>Applications of AD</h2>

                <p>
                    Automatic differentiation is a core component in modern computational systems that require efficient and 
                    accurate derivatives. In particular, it powers nearly all deep learning frameworks such as:
                </p>

                <ul style="padding-left: 40px;">
                    <li><strong>PyTorch</strong> — dynamic computational graphs with reverse-mode AD via <code>autograd</code></li>
                    <li><strong>TensorFlow</strong> — supports both eager and static (graph) modes of AD with <code>tf.GradientTape</code></li>
                    <li><strong>JAX</strong> — composable transformations like <code>grad</code>, <code>vmap</code>, <code>jit</code> based on function tracing and XLA compilation</li>
                    <li><strong>Diffrax</strong>, <strong>SciML</strong> — scientific computing libraries for differentiable differential equations (ODEs, PDEs)</li>
                </ul>

                <p>
                These systems rely on automatic differentiation to:
                <ol style="padding-left: 40px;">
                    <li>Train neural networks by computing gradients of loss functions with respect to millions (or billions) of parameters</li>
                    <li>Optimize black-box functions in physics simulation, robotics, and finance</li>
                    <li>Perform end-to-end differentiation through control flow, dynamic loops, and even solver calls (e.g., differentiable physics)</li>
                    </ol>
                </p>
            </section>

            <section id="code" class="section-content">
                <h2>Sample Code</h2>
                <div class="code-container">
                    <div class="collapsible-section">
                        <button class="collapsible-btn">Show/Hide Code</button>
                        <div class="collapsible-content">
                            <pre class="python-code">
                                import numpy as np

                                class AutoDiffNode:
                                    """Node in the computational graph for automatic differentiation"""
                                    def __init__(self, value, grad=0.0):
                                        self.value = value
                                        self.grad = grad
                                        self.children = []  # Nodes that depend on this node
                                        self.local_gradients = []  # Local gradients to children

                                def manual_autodiff_example(x1_val, x2_val):
                                    """
                                    Manual implementation of automatic differentiation for:
                                    f(x1, x2) = log((x1 + x2)^2 + sin(x1 * x2))
                                    
                                    This demonstrates the forward and backward pass explicitly.
                                    """
                                    print(f"Computing f({x1_val}, {x2_val}) = log((x1 + x2)² + sin(x1 * x2))")
                                    print("="*60)
                                    
                                    # Forward Pass - Compute function value
                                    print("FORWARD PASS:")
                                    x1 = x1_val
                                    x2 = x2_val
                                    print(f"x1 = {x1}")
                                    print(f"x2 = {x2}")
                                    
                                    x3 = x1 + x2
                                    print(f"x3 = x1 + x2 = {x3}")
                                    
                                    x4 = x3**2
                                    print(f"x4 = x3² = {x4}")
                                    
                                    x5 = x1 * x2
                                    print(f"x5 = x1 * x2 = {x5}")
                                    
                                    x6 = np.sin(x5)
                                    print(f"x6 = sin(x5) = {x6}")
                                    
                                    x7 = x4 + x6
                                    print(f"x7 = x4 + x6 = {x7}")
                                    
                                    x8 = np.log(x7)
                                    f = x8
                                    print(f"x8 = log(x7) = {f}")
                                    print(f"\nFunction value: f = {f}")
                                    
                                    # Backward Pass - Compute gradients
                                    print("\n" + "="*60)
                                    print("BACKWARD PASS:")
                                    
                                    # Initialize gradient
                                    df_dx8 = 1.0
                                    print(f"∂f/∂x8 = {df_dx8}")
                                    
                                    # x8 = log(x7)
                                    df_dx7 = df_dx8 * (1.0 / x7)
                                    print(f"∂f/∂x7 = ∂f/∂x8 * ∂x8/∂x7 = {df_dx8} * (1/{x7}) = {df_dx7}")
                                    
                                    # x7 = x4 + x6
                                    df_dx4 = df_dx7 * 1.0
                                    df_dx6 = df_dx7 * 1.0
                                    print(f"∂f/∂x4 = ∂f/∂x7 * ∂x7/∂x4 = {df_dx7} * 1 = {df_dx4}")
                                    print(f"∂f/∂x6 = ∂f/∂x7 * ∂x7/∂x6 = {df_dx7} * 1 = {df_dx6}")
                                    
                                    # x4 = x3²
                                    df_dx3 = df_dx4 * (2 * x3)
                                    print(f"∂f/∂x3 = ∂f/∂x4 * ∂x4/∂x3 = {df_dx4} * 2*{x3} = {df_dx3}")
                                    
                                    # x6 = sin(x5)
                                    df_dx5 = df_dx6 * np.cos(x5)
                                    print(f"∂f/∂x5 = ∂f/∂x6 * ∂x6/∂x5 = {df_dx6} * cos({x5}) = {df_dx5}")
                                    
                                    # Now accumulate gradients for x1 and x2
                                    # x3 = x1 + x2
                                    df_dx1_from_x3 = df_dx3 * 1.0
                                    df_dx2_from_x3 = df_dx3 * 1.0
                                    
                                    # x5 = x1 * x2
                                    df_dx1_from_x5 = df_dx5 * x2
                                    df_dx2_from_x5 = df_dx5 * x1
                                    
                                    # Sum gradients from all paths
                                    df_dx1 = df_dx1_from_x3 + df_dx1_from_x5
                                    df_dx2 = df_dx2_from_x3 + df_dx2_from_x5
                                    
                                    print(f"\n∂f/∂x1 = ∂f/∂x3 * ∂x3/∂x1 + ∂f/∂x5 * ∂x5/∂x1")
                                    print(f"       = {df_dx3} * 1 + {df_dx5} * {x2}")
                                    print(f"       = {df_dx1_from_x3} + {df_dx1_from_x5}")
                                    print(f"       = {df_dx1}")
                                    
                                    print(f"\n∂f/∂x2 = ∂f/∂x3 * ∂x3/∂x2 + ∂f/∂x5 * ∂x5/∂x2")
                                    print(f"       = {df_dx3} * 1 + {df_dx5} * {x1}")
                                    print(f"       = {df_dx2_from_x3} + {df_dx2_from_x5}")
                                    print(f"       = {df_dx2}")
                                    
                                    # Verify with the closed form
                                    print("\n" + "="*60)
                                    print("VERIFICATION WITH CLOSED FORM:")
                                    expected_df_dx1 = (2*(x1 + x2) + x2*np.cos(x1*x2)) / ((x1 + x2)**2 + np.sin(x1*x2))
                                    expected_df_dx2 = (2*(x1 + x2) + x1*np.cos(x1*x2)) / ((x1 + x2)**2 + np.sin(x1*x2))
                                    
                                    print(f"Expected ∂f/∂x1 = {expected_df_dx1}")
                                    print(f"Expected ∂f/∂x2 = {expected_df_dx2}")
                                    print(f"Error in ∂f/∂x1: {abs(df_dx1 - expected_df_dx1)}")
                                    print(f"Error in ∂f/∂x2: {abs(df_dx2 - expected_df_dx2)}")
                                    
                                    return f, df_dx1, df_dx2


                                def pytorch_autodiff_example(x1_val, x2_val):
                                    """
                                    PyTorch implementation showing how modern autodiff frameworks handle this
                                    """
                                    import torch
                                    
                                    print("\n" + "="*60)
                                    print("PYTORCH AUTOMATIC DIFFERENTIATION:")
                                    
                                    # Create tensors with gradient tracking
                                    x1 = torch.tensor(x1_val, requires_grad=True, dtype=torch.float32)
                                    x2 = torch.tensor(x2_val, requires_grad=True, dtype=torch.float32)
                                    
                                    # Define the function
                                    f = torch.log((x1 + x2)**2 + torch.sin(x1 * x2))
                                    
                                    # Compute gradients
                                    f.backward()
                                    
                                    print(f"f({x1_val}, {x2_val}) = {f.item()}")
                                    print(f"∂f/∂x1 = {x1.grad.item()}")
                                    print(f"∂f/∂x2 = {x2.grad.item()}")
                                    
                                    return f.item(), x1.grad.item(), x2.grad.item()


                                def gradient_check(f, x1, x2, epsilon=1e-7):
                                    """
                                    Numerical gradient checking using finite differences
                                    """
                                    # Compute analytical gradients
                                    df_dx1, df_dx2 = manual_autodiff_example(x1, x2)
                                    
                                    print("\n" + "="*60)
                                    print("NUMERICAL GRADIENT CHECK:")
                                    
                                    # Numerical gradient for x1
                                    def eval_f(x1_val, x2_val):
                                        return np.log((x1_val + x2_val)**2 + np.sin(x1_val * x2_val))
                                    
                                    f_plus_x1 = eval_f(x1 + epsilon, x2)
                                    f_minus_x1 = eval_f(x1 - epsilon, x2)
                                    numerical_df_dx1 = (f_plus_x1 - f_minus_x1) / (2 * epsilon)
                                    
                                    # Numerical gradient for x2
                                    f_plus_x2 = eval_f(x1, x2 + epsilon)
                                    f_minus_x2 = eval_f(x1, x2 - epsilon)
                                    numerical_df_dx2 = (f_plus_x2 - f_minus_x2) / (2 * epsilon)
                                    
                                    print(f"Analytical ∂f/∂x1: {df_dx1}")
                                    print(f"Numerical  ∂f/∂x1: {numerical_df_dx1}")
                                    print(f"Difference: {abs(df_dx1 - numerical_df_dx1)}")
                                    
                                    print(f"\nAnalytical ∂f/∂x2: {df_dx2}")
                                    print(f"Numerical  ∂f/∂x2: {numerical_df_dx2}")
                                    print(f"Difference: {abs(df_dx2 - numerical_df_dx2)}")


                                if __name__ == "__main__":
                                    # Test with specific values
                                    x1 = 1.0
                                    x2 = 0.5
                                    
                                    # Manual implementation
                                    f_manual, grad_x1_manual, grad_x2_manual = manual_autodiff_example(x1, x2)
                                    
                                    # PyTorch implementation 
                                    f_pytorch, grad_x1_pytorch, grad_x2_pytorch = pytorch_autodiff_example(x1, x2)
                                
                                    # Numerical gradient check
                                    gradient_check(None, x1, x2)
                            </pre>
                        </div>
                    </div>
                    <button class="run-button" onclick="runPythonCode(this)">Run Code</button>
                    <div class="python-output"></div>
                </div>
            </section>

        </div>
        <script src="/js/main.js"></script> 
        <script src="/js/runPythonCode.js"></script>
        <script src="/js/collapsible.js"></script>
    </body>
</html>