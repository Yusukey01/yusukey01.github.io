---
layout: default
title: Neural Networks Basics
level: detail
description: Learn about the automatic differentiation.
uses_math: true
uses_python: false
---
<!DOCTYPE html>
<html>
    <body> 
        <div class="hero-section">
            <h1 class="webpage-name"> Automatic Differentiation</h1>
        </div>

        <div class="topic-nav">
            <a href="#AD">Automatic Differentiation</a>
            <a href="#eg">Example</a>
            <a href="#"></a>
            <a href="#"></a>
              
        </div> 

        <div class="container">  

            <section id="AD" class="section-content">
                <h2>Automatic Differentiation</h2>
                In multilayer perceptrons (MLPs), <strong>backpropagation</strong> is just an application of the chain rule to compute 
                gradients efficiently. More generally, this technique is known as <strong>automatic differentiation (AD)</strong>. 
                Not just sequential layers, AD applies to arbitrary <strong>computational graphs</strong>, which is a directed acyclic graph (DAG) 
                representing how variables are computed from input to output.
            </section>

            <section id="eg" class="section-content">
                <h2>Example</h2>
                <p>
                    Consider a function
                    \[
                    f(x_1, x_2) = \log \left((x_1 + x_2)^2 + \sin(x_1 x_2) \right).
                    \]
                    we can decompose this into primitives:
                    \[
                    \begin{align*}
                    &x_3 = x_1 + x_2 \\\\
                    &x_4 = x_3^2 \\\\
                    &x_5 = x_1 x_2 \\\\
                    &x_6 = \sin(x_5) \\\\
                    &x_7 = x_4 + x_6 \\\\
                    &x_8 = \log(x_7) = f \\\\
                    \end{align*}
                    \]
                    Then
                    \[
                    
                    \]

                    \[
                    \begin{align*}
                    \frac{\partial f}{\partial x_8} &= 1  \\\\
                    \frac{\partial f}{\partial x_7} &= \frac{\partial f}{\partial x_8} \cdot \frac{\partial x_8}{\partial x_7} \\\\
                                                    &= \frac{\partial f}{\partial x_8} \cdot \frac{\partial}{\partial x_7}\log (x_7) \\\\
                                                    &= \frac{1}{x_7} \\\\
                    \frac{\partial f}

                    \end{align*}
                    \]
                </p>

                

            </section>

            <section id="" class="section-content">
                <h2></h2>

            </section>



        </div>
        <script src="/js/main.js"></script> 
    </body>
</html>