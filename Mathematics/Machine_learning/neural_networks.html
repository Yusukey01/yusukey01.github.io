---
layout: default
title: Intro to Neural Networks
level: detail
description: Learn about Neural Networks basics. 
uses_math: true
uses_python: false
---
<!DOCTYPE html>
<html>
    <body> 
        <div class="hero-section">
            <h1 class="webpage-name"> Intro to Neural Networks</h1>
        </div>

        <div class="topic-nav">
            <a href="#MLP">Multilayer Perceptron (MLP)</a>
            <a href="#activation">Activation Functions</a>
            <a href="#learning">Learning in Neural Networks</a>
            <a href="#demo">Neural Networks Demo</a>
        </div> 

        <div class="container">  

            <section id="DNNs" class="section-content">
                <h2>Multilayer Perceptron (MLP)</h2>
                <p> 
                    The key idea of <strong>deep neural networks (DNNs)</strong> is "composing" a vast number of simple functions to 
                    make a huge complex function. In this section, we focus on a specific type of DNN known as the <strong>multilayer perceptron (MLP)</strong>, 
                    also referred to as a <strong>feedforward neural network (FFNN)</strong>.
                </p>

                <p>
                    An MLP defines a composite function of the form: 
                    \[
                    f(x ; \theta) = f_L (f_{L-1}(\cdots(f_1(x))\cdots))
                    \]
                
                    where each component function \( f_\ell(x) = f(x; \theta_\ell) \) represents the transformation at 
                    <strong>layer</strong> \( \ell \,\), \( x \in \mathbb{R}^D \) is an input vector with \( D \) features, and 
                    \(\theta\) is a collection of parameters(weights and biases):
                    \[
                    \theta = \{ \theta_\ell \}_{\ell=1}^L \text{ ,where } \theta_\ell = \{ W^{(\ell)}, b^{(\ell)} \}.
                    \]
                </p>
                <p>
                     Each layer is assumed to be <strong>differentiable</strong> and consists of two operations: an <strong>affine transformation</strong> followed 
                     by a <strong>non-linear differentiable activation function</strong> \( g_\ell : \mathbb{R} \to \mathbb{R}\). An MLP consists of an <strong>input layer</strong>, 
                     one or more <strong>hidden layers</strong>, and an <strong>output layer</strong>.
                </p>

                <p>
                    We define the hidden units \(z^{(\ell)}\) at each layer \(\ell\) passed  elementwise through the activation:
                    \[
                    z^{(\ell)} = g_{\ell}(b^{(\ell)} + W^{(\ell)}z^{(\ell -1)}) = g_{\ell}(a^{(\ell)})
                    \]
                    where \(a^{(\ell)}\) is called the <strong>pre-activations</strong>, and the output of the newtwork is denoted by 
                    \(\hat{y} = h_\theta(x) = g_L(a^{(L)})\).
                </p>

                 <p>
                    <strong>Note:</strong> Input data is typically stored as an \( N \times D \) <strong>design matrix</strong>, where each 
                    row corresponds to a data point and each column to a feature. This is referred to as <strong>structured data</strong> or 
                    <strong>tabular data</strong>. In contrast, for <strong>unstructured data</strong> such as images or text, different architectures 
                    are used:
                </p>

                <ul style="padding-left: 40px;">
                    <li><strong>Convolutional Neural Networks (CNNs)</strong> for images</li>
                    <li><strong>Recurrent Neural Networks (RNNs)</strong> and <strong>Transformers</strong> for sequential data (e.g. text)</li>
                </ul>

                <p>
                    In particular, modern <strong>Large Language Models (LLMs)</strong> such as GPT are based on the transformer architecture and 
                    have replaced RNNs in many natural language processing tasks.
                </p>
            </section>

            <section id="activation" class="section-content">
                <h2>Activation Functions</h2>
                <p>
                    Without a <strong>non-linear</strong> activation function, a neural network composed of multiple layers would reduce to a 
                    single linear transformation:
                    \[
                    f(x ; \theta) = \theta^{(L)} \theta^{(L-1)}  \cdots \theta^{(2)} \theta^{(1)} x.
                    \]
                    This composition is still <strong>linear</strong> in \( x \), and therefore incapable of representing non-linear decision boundaries. 
                    Non-linear activation functions are necessary to break this linearity and allow networks to approximate arbitrary functions.
                </p>
                <p>
                    Historically, a common choice was the <strong>sigmoid (logistic)</strong> activation function:
                    \[
                    \sigma(a) = \frac{1}{1+e^{-a}}.
                    \]
                    However, sigmoid functions saturate for large positive or negative inputs: \( \sigma(a) \to 1 \) as \( a \to +\infty \), and \( \sigma(a) \to 0 \) as \( a \to -\infty \). . 
                    In these regions, the gradient becomes very small, leading to the <strong>vanishing gradient problem</strong> — gradients shrink as they propagate backward, making learning 
                    slow or unstable in deep networks.
                </p>
                <p>
                    To address this, modern networks often use the <strong>Rectified Linear Unit (ReLU)</strong>:
                    \[
                    g(a) = \max(0, a) = a \mathbb{I}(a>0)
                    \]
                    ReLU introduces non-linearity while preserving gradient magnitude for positive inputs. It is computationally simple and helps maintain gradient flow during training, which 
                    is why it is now a standard choice in modern neural networks architectures.
                </p>
            </section>

            <section id="learning" class="section-content">
                <h2>Learning in Neural Networks</h2>

                <p> 
                    Training the network is finding parameters \( \theta = \{ \theta_\ell \}_{\ell=1}^L \), where 
                    \( \theta_\ell = \{ W^{(\ell)}, b^{(\ell)} \} \), that minimize a loss function \( \mathcal{L}(\theta) \) over 
                    the training data. This is done by performing iterative <strong>gradient descent</strong> updates on 
                    each layer's weights and biases:

                    \[
                    \theta^{(\ell)} \longleftarrow \theta^{(\ell)} - \alpha \nabla_{\theta^{(\ell)}} \mathcal{L}(\theta)
                    \]
                    where \(\ell = 1, 2, \dots, L\) and \( \alpha \) is the <strong>learning rate</strong>, which controls the 
                    step size in the direction of the negative gradient. 
                </p>

                <p>
                    But how do we compute the gradients? That's where the <a href="../Calculus/jacobian.html"><strong>backpropagation</strong></a> algorithm 
                    comes in: it applies the chain rule in reverse — starting from the output layer (\(\ell = L\)) and moving backward through layers 
                    \(L-1, L-2,\dots,1\) — to propagate error signals and compute every gradient.
                </p>

                <p>
                    Our demo employs <strong>mini-batch gradient descent</strong>, which computes these gradients on a small random subset of the data at 
                    each iteration. This often leads to faster convergence and better generalization compared to using the entire dataset at once.
                </p>



                <p> 
                    Training a neural network <strong>is</strong> finding parameters \( \theta = \{ W^{(\ell)}, b^{(\ell)} \}_{\ell=1}^L \) that minimize 
                    the expected risk over the data distribution. For a neural network, the learnable parameters of the network are 
                    $\mathcal{W} = \{W_i, b_i\}_{i=1}^n$. Since we only have access to training data, we minimize the <strong>empirical risk</strong> 
                    (average loss) plus optional regularization:
                    \[
                    J(\theta) = \frac{1}{N} \sum_{i=1}^{N} \mathcal{L}(y_i, \hat{y}_i) + \lambda R(\theta)
                    \]
                    where \( \hat{y}_i = h_\theta(x_i) \) is the network's prediction for input \( x_i \).
                </p>

                <h3>Loss Functions</h3>
                <p>
                    The <strong>loss function</strong> \( \mathcal{L}(y, \hat{y}) \) measures the discrepancy between a prediction and the true label. 
                    Cross-entropy is a widely used loss function in applications. It coincides with the logistic loss applied to 
                    the outputs of a neural network, when the softmax is used. Common choices include:
                </p>
                <ul style="padding-left: 40px;">
                    <li><strong>Binary cross-entropy</strong> (for binary classification): \( \mathcal{L}(y, \hat{y}) = -y \log(\hat{y}) - (1-y) \log(1-\hat{y}) \)</li>
                    <li><strong>Categorical cross-entropy</strong> (for multi-class): \( \mathcal{L}(y, \hat{y}) = -\sum_c y_c \log(\hat{y}_c) \)</li>
                    <li><strong>Mean squared error</strong> (for regression): \( \mathcal{L}(y, \hat{y}) = \|y - \hat{y}\|^2 \)</li>
                </ul>

                <h3>Gradient-Based Optimization</h3>
                <p>
                    For gradient-based optimization, also called first-order optimization, the update function takes as input the gradient of 
                    the cost with respect to the parameters at the current operating point, $\nabla_{\theta}J(\theta)$. This reveals hugely useful 
                    information about the loss that directly tells us how to minimize it: just move in the direction of steepest descent. 
                    The basic update rule is:
                    \[
                    \theta_{t+1} = \theta_t - \alpha \nabla_\theta J(\theta_t)
                    \]
                    where \( \alpha > 0 \) is the <strong>learning rate</strong> that controls the step size. Too small and training is slow; 
                    too large and training may diverge.
                </p>

                <h3>Stochastic Gradient Descent (SGD)</h3>
                <p>
                    Computing gradients over all \( N \) training examples is expensive. SGD modifies the batch gradient descent 
                    algorithm by calculating the gradient for only one training example at every iteration. In practice, we use 
                    <strong>mini-batch SGD</strong>:
                </p>
                <ol style="padding-left: 40px;">
                    <li>Randomly sample a mini-batch \( B \subset \{1,...,N\} \) of size \( |B| \) (typically 16-256)</li>
                    <li>Compute gradient estimate: \( \nabla_\theta J_B(\theta) = \frac{1}{|B|} \sum_{i \in B} \nabla_\theta \mathcal{L}(y_i, h_\theta(x_i)) \)</li>
                    <li>Update parameters: \( \theta \leftarrow \theta - \alpha \nabla_\theta J_B(\theta) \)</li>
                </ol>
                <p>
                    Due to SGD's efficiency in dealing with large scale datasets, it is the most common method for training deep neural networks. 
                    A good starting point for the learning rate is 0.1 and adjust as necessary.
                </p>

                <h3>Backpropagation Algorithm</h3>
                <p>
                    But how do we compute the gradients efficiently? That's where the <a href="../Calculus/jacobian.html"><strong>backpropagation</strong></a> 
                    algorithm comes in. Backprop is an efficient application of the chain rule starting from the gradient of the loss w.r.t the 
                    output and working backwards. The algorithm computes all gradients in just two passes through the network:
                </p>
                
                <div class="algorithm" style="background-color: #f5f5f5; padding: 15px; border-radius: 5px; margin: 15px 0;">
                    <strong>Backpropagation:</strong>
                    <ol>
                        <li><strong>Forward pass:</strong> Compute and store all intermediate values
                            <ul>
                                <li>Set \(a^{(0)} = x\) (input)</li>
                                <li>For \(\ell = 1, ..., L\): 
                                    <ul>
                                        <li>Linear: \(z^{(\ell)} = W^{(\ell)} a^{(\ell-1)} + b^{(\ell)}\)</li>
                                        <li>Activation: \(a^{(\ell)} = g_\ell(z^{(\ell)})\)</li>
                                    </ul>
                                </li>
                                <li>Output: \(\hat{y} = a^{(L)}\)</li>
                            </ul>
                        </li>
                        <li><strong>Backward pass:</strong> Propagate error signals backward
                            <ul>
                                <li>Output layer gradient: \( \delta^{(L)} = \nabla_{a^{(L)}} \mathcal{L} \odot g'_L(z^{(L)}) \)
                                    <br>For binary cross-entropy with sigmoid: \( \delta^{(L)} = \hat{y} - y \)</li>
                                <li>For \( \ell = L-1, ..., 1 \): 
                                    <ul>
                                        <li>\( \delta^{(\ell)} = (W^{(\ell+1)})^T \delta^{(\ell+1)} \odot g'_\ell(z^{(\ell)}) \)</li>
                                    </ul>
                                </li>
                                <li>Parameter gradients: 
                                    <ul>
                                        <li>\( \frac{\partial J}{\partial W^{(\ell)}} = \frac{1}{|B|} \sum_{i \in B} \delta^{(\ell)}_i (a^{(\ell-1)}_i)^T + \lambda W^{(\ell)} \)</li>
                                        <li>\( \frac{\partial J}{\partial b^{(\ell)}} = \frac{1}{|B|} \sum_{i \in B} \delta^{(\ell)}_i \)</li>
                                    </ul>
                                </li>
                            </ul>
                        </li>
                    </ol>
                </div>

                <p>
                    Here, \( \odot \) denotes element-wise multiplication, and \( g'_\ell \) is the derivative of the activation function. 
                    For ReLU: \( g'(z) = \mathbb{I}(z > 0) \), and for sigmoid: \( g'(z) = \sigma(z)(1-\sigma(z)) \).
                </p>

                <p>
                    Our demo employs <strong>mini-batch gradient descent</strong>, which computes these gradients on a small random subset 
                    of the data at each iteration. This provides a good balance between computational efficiency and gradient quality, 
                    often leading to faster convergence and better generalization compared to using the entire dataset at once.
                </p>

                <h3>Momentum and Advanced Optimizers</h3>
                <p>
                    Momentum is an extension to the gradient descent optimization algorithm that builds inertia in a search direction 
                    to overcome local minima and oscillation of noisy gradients. It's based on the same concept of momentum in physics. 
                    The update rule with momentum is:
                    \[
                    \begin{align}
                    v_{t+1} &= \beta v_t + (1-\beta) \nabla_\theta J(\theta_t) \\
                    \theta_{t+1} &= \theta_t - \alpha v_{t+1}
                    \end{align}
                    \]
                    where \( \beta \in [0,1) \) is the momentum coefficient (typically 0.9). By taking past gradients into account, 
                    the steps become smoothed out, reducing oscillations.
                </p>

                <p>
                    Modern optimizers like <strong>Adam</strong> combine momentum with adaptive learning rates for each parameter, 
                    often leading to faster convergence. These methods are particularly effective for training deep neural networks.
                </p>

                <h3>Regularization Techniques</h3>
                <p>
                    To prevent overfitting and ensure good generalization, we employ various regularization techniques:
                </p>
                <ul style="padding-left: 40px;">
                    <li><strong>L2 regularization</strong> (weight decay): Adds penalty \( R(\theta) = \frac{1}{2} \sum_{\ell,i,j} (W^{(\ell)}_{ij})^2 \) to the loss</li>
                    <li><strong>Dropout</strong>: Randomly zeros out neurons during training with probability \( p \), forcing the network to learn redundant representations</li>
                    <li><strong>Early stopping</strong>: Stops training when validation loss plateaus or starts increasing</li>
                    <li><strong>Gradient clipping</strong>: Prevents exploding gradients by scaling them when their norm exceeds a threshold</li>
                </ul>

                <p>
                    These techniques, combined with proper initialization (like Xavier/Glorot initialization), help ensure stable training 
                    and good performance on unseen data.
                </p>
                
            </section>

            <section id="demo" class="section-content">
                <h2>Neural Networks Demo</h2>
                <div id="neural_network_visualizer"></div>
                <p>
                    This interactive demo showcases how a simple neural network can learn to classify non-linear patterns. 
                    You can generate datasets, tweak model parameters, and visualize the training process in real time.
                </p>

                <ul style="padding-left: 40px;">
                    <li><strong>Model Architecture:</strong>
                    <ul style="padding-left: 40px;">
                        <li>2 input features (\(x_1\) and \(x_2\))</li>
                        <li>1 hidden layer with ReLU activation (adjustable number of units)</li>
                        <li>1 output unit with sigmoid activation for binary classification</li>
                    </ul>
                    </li>
                    <li><strong>Forward Pass:</strong> 
                    The network computes predictions by applying matrix operations and non-linear activations. Selecting a 
                    demo point shows a step-by-step computation.
                    </li>
                    <li><strong>Training:</strong>
                    The network is trained using <strong>mini-batch gradient descent with backpropagation</strong> to minimize 
                    binary cross-entropy loss. Each iteration uses a small, randomly sampled subset of the training data to 
                    update weights.
                    <ul style="padding-left: 40px;">
                        <li>Faster and more stable than full-batch training</li>
                        <li>Helps escape flat regions and saddle points</li>
                        <li>More closely mirrors how real-world neural networks are trained</li>
                    </ul>
                    </li>
                    <li><strong>Training Optimizations:</strong>
                    <ul style="padding-left: 40px;">
                        <li>Dynamic learning rate adjustment</li>
                        <li>Gradient clipping to prevent instability</li>
                        <li>Early stopping when performance stabilizes</li>
                        <li>\(\ell_2\) regularization (λ) to reduce overfitting</li>
                    </ul>
                    </li>
                    <li><strong>Visualizations:</strong>
                    <ul style="padding-left: 40px;">
                        <li>Color-coded data points for training and test sets</li>
                        <li>Decision boundary (green) shows where prediction = 0.5</li>
                        <li>Probability contours reveal model confidence</li>
                        <li>Dynamic network graph and forward pass breakdown</li>
                    </ul>
                    </li>
                </ul>
                <br>
                <h2>Try Adjusting:</h2>
                <ul style="padding-left: 40px;">
                    <li><strong>Hidden Units:</strong> More neurons allow for more complex decision boundaries</li>
                    <li><strong>Regularization (λ):</strong> Helps prevent overfitting by discouraging large weights</li>
                    <li><strong>Learning Rate:</strong> Controls how quickly the model updates</li>
                    <li><strong>Max Iterations:</strong> Sets how long the training runs before stopping</li>
                </ul>
            </section>

        </div>
        <script src="/js/main.js"></script> 
        <script src="/js/intro_nn.js"></script>
    </body>
</html>