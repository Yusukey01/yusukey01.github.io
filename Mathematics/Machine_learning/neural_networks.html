---
layout: default
title: Intro to Neural Networks
level: detail
description: Learn about Neural Networks basics. 
uses_math: true
uses_python: false
---
<!DOCTYPE html>
<html>
    <body> 
        <div class="hero-section">
            <h1 class="webpage-name"> Intro to Neural Networks</h1>
        </div>

        <div class="topic-nav">
            <a href="#MLP">Multilayer Perceptron (MLP)</a>
            <a href="#activation">Activation Functions</a>
            <a href="#learning">Learning in Neural Networks</a>
            <a href="#demo">Neural Networks Demo</a>
        </div> 

        <div class="container">  

            <section id="DNNs" class="section-content">
                <h2>Multilayer Perceptron (MLP)</h2>
                <p> 
                    The key idea of <strong>deep neural networks (DNNs)</strong> is "composing" a vast number of simple functions to 
                    make a huge complex function. In this section, we focus on a specific type of DNN known as the <strong>multilayer perceptron (MLP)</strong>, 
                    also referred to as a <strong>feedforward neural network (FFNN)</strong>.
                </p>

                <p>
                    An MLP defines a composite function of the form: 
                    \[
                    f(x ; \theta) = f_L (f_{L-1}(\cdots(f_1(x))\cdots))
                    \]
                
                    where each component function \( f_\ell(x) = f(x; \theta_\ell) \) represents the transformation at 
                    <strong>layer</strong> \( \ell \,\), \( x \in \mathbb{R}^D \) is an input vector with \( D \) features, and 
                    \(\theta\) is a collection of parameters(weights and biases):
                    \[
                    \theta = \{ \theta_\ell \}_{\ell=1}^L \text{ ,where } \theta_\ell = \{ W^{(\ell)}, b^{(\ell)} \}.
                    \]
                </p>
                <p>
                     Each layer is assumed to be <strong>differentiable</strong> and consists of two operations: an <strong>affine transformation</strong> followed 
                     by a <strong>non-linear differentiable activation function</strong> \( g_\ell : \mathbb{R} \to \mathbb{R}\). An MLP consists of an <strong>input layer</strong>, 
                     one or more <strong>hidden layers</strong>, and an <strong>output layer</strong>.
                </p>

                <p>
                    We define the hidden units \(z_{\ell}\) at each layer \(\ell\) passed  elementwise through the activation:
                    \[
                    z_{\ell} = g_{\ell}(b_{\ell} + W_{\ell}z_{\ell -1}) = g_{\ell}(a_{\ell})
                    \]
                    where \(a_\ell\) is called the <strong>pre-activations</strong>, and the finl output of the newtwork is denoted by 
                    \(h_\theta(x) = g_L(a_L)\).
                </p>

                 <p>
                    <strong>Note:</strong> Input data is typically stored as an \( N \times D \) <strong>design matrix</strong>, where each 
                    row corresponds to a data point and each column to a feature. This is referred to as <strong>structured data</strong> or 
                    <strong>tabular data</strong>. In contrast, for <strong>unstructured data</strong> such as images or text, different architectures 
                    are used:
                </p>

                <ul style="padding-left: 40px;">
                    <li><strong>Convolutional Neural Networks (CNNs)</strong> for images</li>
                    <li><strong>Recurrent Neural Networks (RNNs)</strong> and <strong>Transformers</strong> for sequential data (e.g. text)</li>
                </ul>

                <p>
                    In particular, modern <strong>Large Language Models (LLMs)</strong> such as GPT are based on the transformer architecture and 
                    have replaced RNNs in many natural language processing tasks.
                </p>
            </section>

            <section id="activation" class="section-content">
                <h2>Activation Functions</h2>
                <p>
                    Without a <strong>non-linear</strong> activation function, a neural network composed of multiple layers would reduce to a 
                    single linear transformation:
                    \[
                    f(x ; \theta) = \theta^{(L)} \theta^{(L-1)}  \cdots \theta^{(2)} \theta^{(1)} x.
                    \]
                    This composition is still <strong>linear</strong> in \( x \), and therefore incapable of representing non-linear decision boundaries. 
                    Non-linear activation functions are necessary to break this linearity and allow networks to approximate arbitrary functions.
                </p>
                <p>
                    Historically, a common choice was the <strong>sigmoid (logistic)</strong> activation function:
                    \[
                    \sigma(a) = \frac{1}{1+e^{-a}}.
                    \]
                    However, sigmoid functions saturate for large positive or negative inputs: \( \sigma(a) \to 1 \) as \( a \to +\infty \), and \( \sigma(a) \to 0 \) as \( a \to -\infty \). . 
                    In these regions, the gradient becomes very small, leading to the <strong>vanishing gradient problem</strong> — gradients shrink as they propagate backward, making learning 
                    slow or unstable in deep networks.
                </p>
                <p>
                    To address this, modern networks often use the <strong>Rectified Linear Unit (ReLU)</strong>:
                    \[
                    g(a) = \max(0, a) = a \mathbb{I}(a>0)
                    \]
                    ReLU introduces non-linearity while preserving gradient magnitude for positive inputs. It is computationally simple and helps maintain gradient flow during training, which 
                    is why it is now a standard choice in modern neural networks architectures.
                </p>
            </section>

            <section id="learning" class="section-content">
                <h2>Learning in Neural Networks</h2>

                <p>
                    Training the network means finding parameters \( \theta = \{ \theta_\ell \}_{\ell=1}^L \), where 
                    \( \theta_\ell = \{ W^{(\ell)}, b^{(\ell)} \} \), that minimize a loss function \( J(\theta) \) over 
                    the training data. This is done by performing gradient descent updates at each layer:               
                    \[
                    \theta_\ell := \theta_\ell - \alpha \nabla_{\theta_\ell} J(\theta)
                     \quad \text{for } \ell = 1, 2, \dots, L
                    \]
                    Here, \( \alpha \) is the <strong>learning rate</strong>, which controls the step size in the direction of the negative gradient. 
                    Each layer updates its parameters independently based on its own gradients.
                </p>

                <p>
                    In practice, gradients are computed efficiently using <a href="../Calculus/jacobian.html"><strong>backpropagation</strong></a> algorithm, 
                    which applies the chain rule in reverse through the network — from the output layer back to the input — to propagate error signals and 
                    compute all partial derivatives.
                </p>

                <p>
                    Our demo uses <strong>mini-batch gradient descent</strong>, where gradients are computed using a small, randomly selected subset of training examples 
                    in each iteration. This improves convergence speed and generalization compared to full-batch training.
                </p>
            </section>

            <section id="demo" class="section-content">
                <h2>Neural Networks Demo</h2>
                <div id="neural_network_visualizer"></div>
                <p>
                    This interactive demo showcases how a simple neural network can learn to classify non-linear patterns. 
                    You can generate datasets, tweak model parameters, and visualize the training process in real time.
                </p>

                <ul style="padding-left: 40px;">
                    <li><strong>Model Architecture:</strong>
                    <ul style="padding-left: 40px;">
                        <li>2 input features (\(x_1\) and \(x_2\))</li>
                        <li>1 hidden layer with ReLU activation (adjustable number of units)</li>
                        <li>1 output unit with sigmoid activation for binary classification</li>
                    </ul>
                    </li>
                    <li><strong>Forward Pass:</strong> 
                    The network computes predictions by applying matrix operations and non-linear activations. Selecting a 
                    demo point shows a step-by-step computation.
                    </li>
                    <li><strong>Training:</strong>
                    The network is trained using <strong>mini-batch gradient descent with backpropagation</strong> to minimize 
                    binary cross-entropy loss. Each iteration uses a small, randomly sampled subset of the training data to 
                    update weights.
                    <ul style="padding-left: 40px;">
                        <li>Faster and more stable than full-batch training</li>
                        <li>Helps escape flat regions and saddle points</li>
                        <li>More closely mirrors how real-world neural networks are trained</li>
                    </ul>
                    </li>
                    <li><strong>Training Optimizations:</strong>
                    <ul style="padding-left: 40px;">
                        <li>Dynamic learning rate adjustment</li>
                        <li>Gradient clipping to prevent instability</li>
                        <li>Early stopping when performance stabilizes</li>
                        <li>\(\ell_2\) regularization (λ) to reduce overfitting</li>
                    </ul>
                    </li>
                    <li><strong>Visualizations:</strong>
                    <ul style="padding-left: 40px;">
                        <li>Color-coded data points for training and test sets</li>
                        <li>Decision boundary (green) shows where prediction = 0.5</li>
                        <li>Probability contours reveal model confidence</li>
                        <li>Dynamic network graph and forward pass breakdown</li>
                    </ul>
                    </li>
                </ul>
                <br>
                <h2>Try Adjusting:</h2>
                <ul style="padding-left: 40px;">
                    <li><strong>Hidden Units:</strong> More neurons allow for more complex decision boundaries</li>
                    <li><strong>Regularization (λ):</strong> Helps prevent overfitting by discouraging large weights</li>
                    <li><strong>Learning Rate:</strong> Controls how quickly the model updates</li>
                    <li><strong>Max Iterations:</strong> Sets how long the training runs before stopping</li>
                </ul>
            </section>

        </div>
        <script src="/js/main.js"></script> 
        <script src="/js/intro_nn.js"></script>
    </body>
</html>