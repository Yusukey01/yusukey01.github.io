---
layout: default
title: Intro to Neural Networks
level: detail
description: Learn about Neural Networks basics. 
uses_math: true
uses_python: false
---
<!DOCTYPE html>
<html>
    <body> 
        <div class="hero-section">
            <h1 class="webpage-name"> Intro to Neural Networks</h1>
        </div>

        <div class="topic-nav">
            <a href="#MLP">Multilayer Perceptron (MLP)</a>
            <a href="#activation">Activation Functions</a>
            <a href="#learning">Learning in Neural Networks</a>
            <a href="#demo">Neural Networks Demo</a>
        </div> 

        <div class="container">  

            <section id="DNNs" class="section-content">
                <h2>Multilayer Perceptron (MLP)</h2>
                <p> 
                    The key idea of <strong>deep neural networks (DNNs)</strong> is "composing" a vast number of simple functions to 
                    make a huge complex function. Here, we introduce a specific type of DNN, which is known as the <strong>multilayer perceptron (MLP)</strong>, or 
                    <strong>feedforward neural network(FFNN)</strong>.
                </p>
                <p>
                    Consider a composite function: 
                    \[
                    f(x ; \theta) = f_L (f_{L-1}(\cdots(f_1(x))\cdots))
                    \]
                    where \(f_{\ell}(x) = f(x ; \theta_{\ell})\) is the function at <strong>layer</strong> \(\ell\) and \(x \mathbb{R}^D\) 
                    is an input vector with \(D\) features and \(\theta\) is a collection of parameters(weights and biases):
                    \[
                    \theta = \{(W^{(1)}, b^{(1)}), \cdots,  (W^{(L-1)}, b^{(L-1)}).  (W^{(L)}, b^{(L)})\}.
                    \]
                    Moreover, we assume that each single layer is <strong>differentiable</strong>. A MLP consists of an <strong>input layer</strong>, 
                    one or more <strong>hidden layers</strong>, and an <strong>output layer</strong>. Each layer performs an affine transformation 
                    followed by a <strong>non-linear dfferentiable activation function</strong> \(g_{\ell}\).
                <p>
                    We define the hidden units \(z_{\ell}\) at each layer \(\ell\) passed  elementwise through the activation g: \(\mathbb{R} \to \mathbb{R}\): 
                    \[
                    z_{\ell} = g_{\ell}(b_{\ell} + W_{\ell}z_{\ell -1}) = g_{\ell}(a_{\ell})
                    \]
                    where \(a_{\ell}\) is called the <strong>pre-activations</strong>, and the finl output of the newtwork is denoted by 
                    \(h_\theta(x) = a^L\).
                </p>

                <p>
                    Note: The whole input data is stored in an \(N \times D\) design matrix where each column represents a feature. So, the data is 
                    called <strong>structured data</strong> or tabular data. If the data is <strong>unstructured</strong> such as images and text, 
                    then we need to introduce other type of DNNs. For example, <strong>convolutional neural networks (CNNs)</strong> is used for images, 
                    and <strong>recurrent neural networks (RNNs)</strong> and <strong>transformers</strong> are designed to work with sequential data 
                    such as languages. Particulary, the modern <strong>Large Language Models (LLMs)</strong> are based on the transformer architecture. 
                </p>
            </section>

            <section id="activation" class="section-content">
                <h2>Activation Functions</h2>
                <p>
                    Without a <strong>non-linear</strong> activation function, a neural network composed of multiple layers would reduce to a 
                    single linear transformation:
                    \[
                    f(x ; \theta) = \theta^{(L)} \theta^{(L-1)}  \cdots \theta^{(2)} \theta^{(1)} x = 
                    \]
                    which is still linear in \( x \).
                </p>
                <p>
                     As we have seen, a common choice <strong>WAS</strong> to use a sigmoid (logistic) function:
                     \[
                     \sigma(a) = \frac{1}{1+e^{-a}}.
                     \]
                     The issue is that this function saturates at 1 for \(a >> 0\), and at 0 for \(a << 0\), which causes 
                     the vanishing gradient problem. 
                </p>
                <p>
                    To model non-linear decision boundaries, we use non-linear activation functions. In our demo, 
                    we use the <strong>ReLU(Rectified Linear Unit)</strong> function:
                    \[
                    g(a) = \max(0, a) = a \mathbb{I}(a>0)
                    \]
                    which turns off negative inputs.
                </p>
                <p>
                    ReLU introduces non-linearity while preserving efficiency and gradient flow, making it widely used 
                    in modern neural networks.
                </p>
            </section>

            <section id="learning" class="section-content">
                <h2>Learning in Neural Networks</h2>

                <p>
                    Training the network means finding parameters \( \theta = \{ \theta_\ell \}_{\ell=1}^L \), where 
                    \( \theta_\ell = \{ W^{(\ell)}, b^{(\ell)} \} \), that minimize a loss function \( J(\theta) \) over 
                    the training data. This is done by performing gradient descent updates at each layer:               
                    \[
                    \theta_\ell := \theta_\ell - \alpha \nabla_{\theta_\ell} J(\theta)
                     \quad \text{for } \ell = 1, 2, \dots, L
                    \]
                    Here, \( \alpha \) is the <strong>learning rate</strong>, which controls the step size in the direction of the negative gradient. 
                    Each layer updates its parameters independently based on its own gradients.
                </p>

                <p>
                    In practice, gradients are computed efficiently using <a href="../Calculus/jacobian.html"><strong>backpropagation</strong></a> algorithm, 
                    which applies the chain rule in reverse through the network — from the output layer back to the input — to propagate error signals and 
                    compute all partial derivatives.
                </p>

                <p>
                    Our demo uses <strong>mini-batch gradient descent</strong>, where gradients are computed using a small, randomly selected subset of training examples 
                    in each iteration. This improves convergence speed and generalization compared to full-batch training.
                </p>
            </section>

            <section id="demo" class="section-content">
                <h2>Neural Networks Demo</h2>
                <div id="neural_network_visualizer"></div>
                <p>
                    This interactive demo showcases how a simple neural network can learn to classify non-linear patterns. 
                    You can generate datasets, tweak model parameters, and visualize the training process in real time.
                </p>

                <ul style="padding-left: 40px;">
                    <li><strong>Model Architecture:</strong>
                    <ul style="padding-left: 40px;">
                        <li>2 input features (\(x_1\) and \(x_2\))</li>
                        <li>1 hidden layer with ReLU activation (adjustable number of units)</li>
                        <li>1 output unit with sigmoid activation for binary classification</li>
                    </ul>
                    </li>
                    <li><strong>Forward Pass:</strong> 
                    The network computes predictions by applying matrix operations and non-linear activations. Selecting a 
                    demo point shows a step-by-step computation.
                    </li>
                    <li><strong>Training:</strong>
                    The network is trained using <strong>mini-batch gradient descent with backpropagation</strong> to minimize 
                    binary cross-entropy loss. Each iteration uses a small, randomly sampled subset of the training data to 
                    update weights.
                    <ul style="padding-left: 40px;">
                        <li>Faster and more stable than full-batch training</li>
                        <li>Helps escape flat regions and saddle points</li>
                        <li>More closely mirrors how real-world neural networks are trained</li>
                    </ul>
                    </li>
                    <li><strong>Training Optimizations:</strong>
                    <ul style="padding-left: 40px;">
                        <li>Dynamic learning rate adjustment</li>
                        <li>Gradient clipping to prevent instability</li>
                        <li>Early stopping when performance stabilizes</li>
                        <li>\(\ell_2\) regularization (λ) to reduce overfitting</li>
                    </ul>
                    </li>
                    <li><strong>Visualizations:</strong>
                    <ul style="padding-left: 40px;">
                        <li>Color-coded data points for training and test sets</li>
                        <li>Decision boundary (green) shows where prediction = 0.5</li>
                        <li>Probability contours reveal model confidence</li>
                        <li>Dynamic network graph and forward pass breakdown</li>
                    </ul>
                    </li>
                </ul>
                <br>
                <h2>Try Adjusting:</h2>
                <ul style="padding-left: 40px;">
                    <li><strong>Hidden Units:</strong> More neurons allow for more complex decision boundaries</li>
                    <li><strong>Regularization (λ):</strong> Helps prevent overfitting by discouraging large weights</li>
                    <li><strong>Learning Rate:</strong> Controls how quickly the model updates</li>
                    <li><strong>Max Iterations:</strong> Sets how long the training runs before stopping</li>
                </ul>
            </section>

             <section id="" class="section-content">
                <h2></h2>
            </section>

        </div>
        <script src="/js/main.js"></script> 
        <script src="/js/intro_nn.js"></script>
    </body>
</html>