---
layout: default
title: Intro to Neural Networks
level: detail
description: Learn about Neural Networks basics. 
uses_math: true
uses_python: false
---
<!DOCTYPE html>
<html>
    <body> 
        <div class="hero-section">
            <h1 class="webpage-name"> Intro to Neural Networks</h1>
        </div>

        <div class="topic-nav">
            <a href="#MLP">Multilayer Perceptron (MLP)</a>
            <a href="#activation">Activation Functions</a>
            <a href="#learning">Learning in Neural Networks</a>
            <a href="#demo">Neural Networks Demo</a>
        </div> 

        <div class="container">  

            <section id="DNNs" class="section-content">
                <h2>Multilayer Perceptron (MLP)</h2>
                <p> 
                    The key idea of <strong>deep neural networks (DNNs)</strong> is "composing" a vast number of simple functions to 
                    make a huge complex function. Here, we introduce a specific type of DNN, which is known as the <strong>multilayer perceptron (MLP)</strong>, or 
                    <strong>feedforward neural network(FFNN)</strong>.
                </p>
                <p>
                    Consider a composite function: 
                    \[
                    f(x ; \theta) = f_L (f_{L-1}(\cdots(f_1(x))\cdots))
                    \]
                    where \(f_{\ell}(x) = f(x ; \theta_{\ell})\) is the function at <strong>layer</strong> \(\ell\) and \(x \mathbb{R}^D\) 
                    is an input vector with \(D\) features and \(\theta\) is a collection of parameters(weights and biases):
                    \[
                    \theta = \{(W^{(1)}, b^{(1)}), \cdots,  (W^{(L-1)}, b^{(L-1)}).  (W^{(L)}, b^{(L)})\}.
                    \]
                    Moreover, we assume that each single layer is <strong>differentiable</strong>. A MLP consists of an <strong>input layer</strong>, 
                    one or more <strong>hidden layers</strong>, and an <strong>output layer</strong>. Each layer performs an affine transformation 
                    followed by a <strong>non-linear activation</strong>.
                <p>
                    For each unit \( i \) in layer \( j \), the pre-activation is computed as:
                    \[
                    z_i^{(j)} = \sum_k \theta_{ik}^{(j-1)} a_k^{(j-1)} + b_i^{(j)}
                    \]
                    and the activation is:
                    \[
                    a_i^{(j)} = g(z_i^{(j)})
                    \]
                </p>
                <p>
                    The final output of the network is denoted by:
                    \[
                    h_\theta(x) = a^{(L)}
                    \]
                    where \( L \) is the final layer.
                </p>

                <p>
                    Note: The whole input data is stored in an \(N \times D\) design matrix where each column represents a feature. So, the data is 
                    called structured data or tabular data. 
                </p>
            </section>

            <section id="activation" class="section-content">
                <h2>Activation Functions</h2>
                <p>
                    Without a <strong>non-linear</strong> activation function, a neural network composed of multiple layers would reduce to a 
                    single linear transformation:
                    \[
                    h_\theta(x) = \theta^{(n)} \cdots \theta^{(2)} \theta^{(1)} x
                    \]
                    which is still linear in \( x \).
                </p>
                <p>
                    To model non-linear decision boundaries, we use non-linear activation functions. In our demo, 
                    we use the <strong>ReLU(Rectified Linear Unit)</strong> function:
                    \[
                    g(z) = \max\{0, z\}
                    \]
                    which turns off negative inputs. 
                </p>
                <p>
                    ReLU introduces non-linearity while preserving efficiency and gradient flow, making it widely used 
                    in modern neural networks.
                </p>
            </section>

            <section id="learning" class="section-content">
                <h2>Learning in Neural Networks</h2>
                <p>
                    Training the network means finding parameters \( \theta \) that minimize the loss function 
                    \( J(\theta) \), typically by gradient descent:
                    <br>
                    \[
                    \theta := \theta - \alpha \nabla_\theta J(\theta)
                    \]
                    where \(\alpha\) is a learning rate (step size).
                </p>

                <p>
                    Gradients are computed efficiently using <strong>backpropagation</strong>, applying the chain rule 
                    from output to input layers. 
                </p>
                <p>
                    Our demo uses <strong>mini-batch gradient descent</strong>, which updates weights using a random 
                    subset of training samples at each step. This improves efficiency and generalization.
                </p>
            </section>

            <section id="demo" class="section-content">
                <h2>Neural Networks Demo</h2>
                <div id="neural_network_visualizer"></div>
                <p>
                    This interactive demo showcases how a simple neural network can learn to classify non-linear patterns. 
                    You can generate datasets, tweak model parameters, and visualize the training process in real time.
                </p>

                <ul style="padding-left: 40px;">
                    <li><strong>Model Architecture:</strong>
                    <ul style="padding-left: 40px;">
                        <li>2 input features (\(x_1\) and \(x_2\))</li>
                        <li>1 hidden layer with ReLU activation (adjustable number of neurons)</li>
                        <li>1 output neuron with sigmoid activation for binary classification</li>
                    </ul>
                    </li>
                    <li><strong>Forward Pass:</strong> 
                    The network computes predictions by applying matrix operations and non-linear activations. Selecting a 
                    demo point shows a step-by-step computation.
                    </li>
                    <li><strong>Training:</strong>
                    The network is trained using <strong>mini-batch gradient descent with backpropagation</strong> to minimize 
                    binary cross-entropy loss. Each iteration uses a small, randomly sampled subset of the training data to 
                    update weights.
                    <ul style="padding-left: 40px;">
                        <li>Faster and more stable than full-batch training</li>
                        <li>Helps escape flat regions and saddle points</li>
                        <li>More closely mirrors how real-world neural networks are trained</li>
                    </ul>
                    </li>
                    <li><strong>Training Optimizations:</strong>
                    <ul style="padding-left: 40px;">
                        <li>Dynamic learning rate adjustment</li>
                        <li>Gradient clipping to prevent instability</li>
                        <li>Early stopping when performance stabilizes</li>
                        <li>\(\ell_2\) regularization (λ) to reduce overfitting</li>
                    </ul>
                    </li>
                    <li><strong>Visualizations:</strong>
                    <ul style="padding-left: 40px;">
                        <li>Color-coded data points for training and test sets</li>
                        <li>Decision boundary (green) shows where prediction = 0.5</li>
                        <li>Probability contours reveal model confidence</li>
                        <li>Dynamic network graph and forward pass breakdown</li>
                    </ul>
                    </li>
                </ul>
                <br>
                <h2>Try Adjusting:</h2>
                <ul style="padding-left: 40px;">
                    <li><strong>Hidden Units:</strong> More neurons allow for more complex decision boundaries</li>
                    <li><strong>Regularization (λ):</strong> Helps prevent overfitting by discouraging large weights</li>
                    <li><strong>Learning Rate:</strong> Controls how quickly the model updates</li>
                    <li><strong>Max Iterations:</strong> Sets how long the training runs before stopping</li>
                </ul>
            </section>

             <section id="" class="section-content">
                <h2></h2>
            </section>

        </div>
        <script src="/js/main.js"></script> 
        <script src="/js/intro_nn.js"></script>
    </body>
</html>