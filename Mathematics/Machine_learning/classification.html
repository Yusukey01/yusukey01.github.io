---
layout: default
title: Classification
level: detail
description: Learn about classification methods such as logistic regression.
uses_math: true
uses_python: false
---
<!DOCTYPE html>
<html>
    <body> 
        <div class="hero-section">
            <h1 class="webpage-name"> Classification</h1>
        </div>

        <div class="topic-nav">
            <a href="#intro">Binary Logistic Regression</a>
            <a href="#demo">Binary Logistic Regression Demo</a>
            <a href="#"></a>
            <a href="#"></a>
            <a href="#"></a>
        </div> 

        <div class="container">  
            <section id="intro" class="section-content">
                <h2>Binary Logistic Regression</h2>
                <p>
                    Consider discriminative classification model: 
                    \[
                    p(y \mid x, \theta)  
                    \]
                    where \(x \in \mathbb{R}^D\) is an input vector, \(y \in \{1, \cdots, C\}\) is the class label, 
                    and \(\theta\) are the parameters.
                </p>

                <p>
                    Let \(y \in \{0, 1\}\). Then the model is known as <strong>binary logistic regression</strong>, which can be represented by: 
                    \[
                     p(y \mid x, \theta) = \text{Ber }(y \mid \sigma(w^\top x + b))
                    \]
                    where \(\sigma\) is the <strong>sigmoid(logistic) function</strong>, \(w \in \mathbb{R}^D\) is the weight vector, 
                    \(b \in \mathbb{R}\) is a bias, and \(\theta = (w, b)\) are the model parameters. The notation \(\text{Ber}(y \mid p)\) denotes the 
                    <strong>Bernoulli distribution</strong>, defined as:
                    \[
                    \text{Ber}(y \mid p) = p^y (1 - p)^{1 - y}, \quad y \in \{0, 1\},\; p \in [0, 1].
                    \]
                </p>

                <p>
                    In other words, 
                    \[
                    p(y = 1 \mid x, \theta) = \sigma(a) = \frac{1}{1 + e^{-a}}
                    \]
                    where \(a\) is called the <strong>logit</strong>(or the <strong>pre-activation</strong>): 
                    \[
                     a = w^\top x + b = \log (\frac{p}{1-p}), \quad  \text{with } p = p(y = 1 \mid x, \theta).
                    \]
                </p>

                <p> 
                    The sigmoid function outputs the probability that the class label is \(y = 1\). Therefore, we define a decision rule 
                    such that we predict \(y = 1\) if and only if class 1 is more likely than class 0. That is, 
                    \[
                    \begin{align*}
                    f(x) &= \mathbb{I}(p(y=1 \mid x) > p(y=0 \mid x)) \\\\
                         &= \mathbb{I} \left( \log \frac{p(y =  1 \mid x)}{p(y =0 \mid x)} > 0 \right) \\\\
                         &= \mathbb{I}(a).
                    \end{align*}
                    \]
                    Here, \(\mathbb{I}(\cdot)\) denotes the <strong>indicator function</strong>, which returns 1 if the 
                    condition inside is true, and 0 otherwise:
                    \[
                    \mathbb{I}(\text{condition}) = 
                    \begin{cases}
                    1 & \text{if condition is true,} \\
                    0 & \text{otherwise.}
                    \end{cases}
                    \]
                    </p>

                    <p>
                    Therefore, the prediction function can be written as: 
                    \[
                    f(x ; \theta) = w^\top x + b.
                    \]
                    Since \(w^\top x\) is the <strong>inner product</strong> between the weight vector \(w\) and the feature vector \(x\), 
                    this function defines a <strong>linear hyperplane</strong> with <strong>normal</strong> vector \(w\) and an offset \(b\) from the origin. 
                    In other words, the weight vector \(w\) determines the ""orientation" of the <strong>decision boundary</strong>, 
                    and the bias term \(b\) shifts the boundary. Moreover, the norm \(\|w\|\) controls the "steepness" of the 
                    sigmoid function â€” a larger norm results in a sharper transition between predicted classes, which reflects greater 
                    confidence in predictions.
                </p>

                <p>
                    To estimate the parameters \(w\), we maximize the likelihood of the observed dataset \(\mathcal{D} = \{(x_n, y_n)\}_{n=1}^N\). 
                    Equivalently, we minimize the <strong>negative log-likelihood</strong> (NLL):
                    \[
                    \begin{align*}
                    \mathrm{NLL }(w) 
                    &= - \frac{1}{N} \log p(\mathcal{D} \mid w) \\\\
                    &= - \frac{1}{N} \log \prod_{n =1}^N \text{Ber }(y_n \mid \mu_n) \\\\
                    &= - \frac{1}{N} \sum_{n=1}^N \log \left[ \mu_n^{y_n}  (1-\mu_n)^{1-y_n} \right] \\\\
                    &= - \frac{1}{N} \sum_{n=1}^N \left[ y_n \log  \mu_n  + (1-y_n) \log (1-\mu_n) \right] \\\\
                    &= - \frac{1}{N} \sum_{n=1}^N \mathbb{H}_{ce} (y_n , \mu_n) 
                    \end{align*}
                    \]
                    where \(\mu_n = \sigma(a_n) = \sigma(w^\top x_n)\) is the model's predicted probability of class 1, and 
                     \(\mathbb{H}_{ce}(y_n, \mu_n)\) is 
                    the <a href="../Probability/entropy.html"><strong>binary cross entropy</strong></a> bethween \(y_n\) and \(\mu_n\).
                </p>

                <p> 
                    We then find the <a href="../Probability/mle.html"><strong>maximum likelihood estimate (MLE)</strong></a> by solving:
                    \[
                    \nabla_w \text{NLL }(w) = 0
                    \]
                    via <a href="../Calculus/gradient.html"><strong>gradient-based optimization</strong></a> (in practice, using 
                    <strong>automatic differentiation</strong>).
                </p>

                <p>
                    The gradient of the NLL with respect to \(w\) is
                    \[
                    \begin{align*}
                      \nabla_w \text{NLL }(w)
                      &= - \frac{1}{N} \sum_{n=1}^N \left[ y_n (1-\mu_n) x_n - (1-y_n) \mu_n x_n \right] \\\\
                      &= \frac{1}{N} \sum_{n=1}^N (\mu_n - y_n) x_n, \\\\
                    \end{align*}
                    \]
                    Equivalently, in matrix form:
                    \[
                     \nabla_w \text{NLL }(w) = \frac{1}{N} \left( \mathbf{1}_N^\top (\text{diag }(\mu - y)X)\right)^\top.
                    \]
                    where \(X \in \mathbb{R}^{N \times D}\) is the design matrix whose \(n\)th row is \(x_n^\top\), and 
                    \(\mu, y\in\mathbb{R}^N\) are the vectors of \(\mu_n\) and \(y_n\), respectively.
                </p>

                <p>
                    To verify <a href="../Calculus/gradient.html"><strong>convexity</strong></a>, we examine the Hessian:
                     \[
                    \begin{align*}
                    H(w) &=\nabla_w \nabla_w^\top \text{NLL }(w)  \\\\
                         &= \frac{1}{N} \sum_{n=1}^N (\mu_n( 1 - \mu_n)x_n) x_n^\top \\\\
                         &= \frac{1}{N} X^\top S X
                    \end{align*}
                    \]
                    where \(S = \text{diag }(\mu_1(1-\mu_1), \cdots, \mu_N(1-\mu_N))\). 

                </p>
                    Since for any \(v \in \mathbb{R}^D\),
                     \[
                    v^\top X^\top S X v = (v^\top X^\top S^{\frac{1}{2}})(S^{\frac{1}{2}} X v)
                    \]
                    and then 
                    \[
                    \| v^\top X^\top S^{\frac{1}{2}} \|_2^2 > 0.
                    \]
                    
                    The Hessian \(H(w)\) is <a href="../Linear_algebra/symmetry.html"><strong>positive semi-definite</strong></a>, 
                    confirming that the NLL is a convex function and any stationary point is the global optimum.
                <p>

                <div class="proof">
                <span class="proof-title">Numerical Example (N=3, D=2)</span> 
                <p><strong></strong></p>
                    <ul>
                    <li>\(X = \begin{pmatrix}1 & 2 \\ 3 & 4 \\ 5 & 6\end{pmatrix}\),</li>
                    <li>\(y = \begin{pmatrix}0 \\ 1 \\ 0\end{pmatrix}\),</li>
                    <li>\(w = \begin{pmatrix}0.1 \\ 0.2\end{pmatrix}\), \(b = -0.1\).</li>
                    </ul>

                    <p>1. <strong>Compute logits</strong> \(a = Xw + b\):</p>
                    \[
                    a = 
                    \begin{pmatrix}
                    1 \\ 3 \\ 5
                    \end{pmatrix}
                    \cdot
                    \begin{pmatrix}0.1 \\ 0.2\end{pmatrix}
                    -0.1
                    =
                    \begin{pmatrix}
                    0.4 \\ 1.0 \\ 1.6
                    \end{pmatrix}.
                    \]

                    <p>2. <strong>Apply sigmoid</strong> \(\mu = \sigma(a)\):</p>
                    \[
                    \mu \approx
                    \begin{pmatrix}
                    \sigma(0.4) \\[4pt]
                    \sigma(1.0) \\[4pt]
                    \sigma(1.6)
                    \end{pmatrix}
                    \approx
                    \begin{pmatrix}
                    0.5987 \\[4pt]
                    0.7311 \\[4pt]
                    0.8320
                    \end{pmatrix}.
                    \]

                    <p>3. <strong>Compute residuals</strong> \(r = \mu - y\):</p>
                    \[
                    r \approx
                    \begin{pmatrix}
                    0.5987 \\[3pt]
                    0.7311 \\[3pt]
                    0.8320
                    \end{pmatrix}
                    -
                    \begin{pmatrix}
                    0 \\[3pt]
                    1 \\[3pt]
                    0
                    \end{pmatrix}
                    =
                    \begin{pmatrix}
                    0.5987 \\[3pt]
                    -0.2689 \\[3pt]
                    0.8320
                    \end{pmatrix}.
                    \]

                    <p>4. <strong>Gradient</strong> component-wise:</p>
                    \[
                    \nabla_w \mathrm{NLL}
                    = \frac{1}{3}\sum_{n=1}^3 r_n\,x_n
                    = \frac{1}{3}\Bigl(0.5987\,[1,2] + (-0.2689)\,[3,4] + 0.8320\,[5,6]\Bigr)
                    \approx
                    \frac{1}{3}[3.9521,\,5.1141]
                    \approx
                    [1.3174,\,1.7047].
                    \]

                    <p>Or in <strong>matrix form</strong>:</p>
                    \[
                    \nabla_w \mathrm{NLL}
                    = \frac{1}{3}\,X^\top\,r
                    = \frac{1}{3}
                    \begin{pmatrix}
                    1 & 3 & 5 \\[3pt]
                    2 & 4 & 6
                    \end{pmatrix}
                    \begin{pmatrix}
                    0.5987 \\[3pt]
                    -0.2689 \\[3pt]
                    0.8320
                    \end{pmatrix}
                    \approx
                    \begin{pmatrix}
                    1.3174 \\[3pt]
                    1.7047
                    \end{pmatrix}.
                    \]
                </div>

            </section>

            

             <section id="demo" class="section-content">
                <h2>Binary Logistic Regression Demo</h2>
                <div id="logistic_regression_visualizer"></div>

            </section>

             <section id="" class="section-content">
                <h2></h2>

            </section>

            <section id="" class="section-content">
                <h2></h2>

            </section>
            
        </div>
        <script src="/js/main.js"></script> 
        <script src="/js/logistic_regression.js"></script>
    </body>
</html>