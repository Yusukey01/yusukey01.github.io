---
layout: default
title: Principal Component Analysis (PCA)
level: detail
description: Learn about unsupervised learning basics from PCA.
uses_math: true
uses_python: false
---
<!DOCTYPE html>
<html>
    <body> 
        <div class="hero-section">
            <h1 class="webpage-name">Principal Component Analysis (PCA)</h1>
        </div>

        <div class="topic-nav">
            <a href="#intro">Recap & Introduction</a>
            <a href="#"></a>
            <a href="#"></a>
        </div> 

        <div class="container">  

            <section id="intro" class="section-content">
                <h2>Recap & Introduction</h2>
                    <p>
                    <a href="../Probability/covariance.html"><strong>Principal Component Analysis (PCA)</strong></a> is one of the most fundamental 
                    techniques in machine learning and statistics for <strong>dimensionality reduction</strong>. It provides a method to reduce the number of 
                    variables in high-dimensional datasets while retaining the most meaningful structure and variation present in the original data.
                    </p>

                    <p>
                    PCA begins by analyzing the <strong>covariance structure</strong> of the data. Given a dataset, we compute the covariance matrix to 
                    understand how different features co-vary. Since the covariance matrix is always symmetric and positive semi-definite, it can be 
                    <a href="../Linear_algebra/symmetry.html"><strong>orthogonally diagonalized</strong></a>. The resulting eigenvectors, called 
                    <strong>principal components (PCs)</strong>, form an orthonormal basis that captures the directions of maximum variance. The corresponding 
                    eigenvalues indicate how much of the total variance is captured by each component.
                    </p>

                    <p>
                    By projecting the data onto the subspace spanned by the top \(k\) principal components (those associated with the largest eigenvalues), PCA 
                    identifies a lower-dimensional representation that retains as much variance as possible. This allows us to reduce dimensionality by discarding 
                    less informative directions (i.e., those with small variance), thereby simplifying the dataset while minimizing information loss.
                    </p>

                    <p>
                    For example, if a dataset in \(\mathbb{R}^{10}\) has 3 principal components capturing 95% of the total variance, we can project the original data onto a 
                    3D subspace. This projection preserves the dominant patterns and relationships in the data and filters out noise and redundancy. The transformed vectors 
                    in the low-dimensional space are known as <strong>latent representations</strong>.
                    </p>

                    <p>
                    While PCA is a powerful tool, it identifies directions of maximum variance using <strong>linear combinations</strong> of the 
                    original features. As a result, it may fail to uncover complex, <em>nonlinear</em> structures in the data. To address this 
                    limitation, the next section introduces <strong>Kernel PCA</strong>, which extends PCA to capture nonlinear relationships by 
                    implicitly mapping the data into a high-dimensional feature space. In this space, linear PCA is applied, but due to the nonlinear 
                    nature of the mapping, the resulting projections reflect nonlinear patterns present in the original input space. This approach 
                    leverages the <em>kernel trick</em> to perform all computations without explicitly constructing the high-dimensional mapping.
                    </p>
                
            </section>

            <section id="kernel-pca" class="section-content">
                <h2>Kernel PCA</h2>
                <p>
                    <strong>Kernel PCA</strong> is a nonlinear generalization of classical PCA that uses the <strong>kernel trick</strong> 
                    to perform linear PCA in an implicit, high-dimensional feature space \(\mathcal{F}\). By choosing a positive-semidefinite 
                    kernel \(k(x,y)=\langle\phi(x),\phi(y)\rangle\), KPCA can uncover nonlinear structure in the original input space without 
                    ever computing the mapping \(\phi\) explicitly .
                </p>

                <h3>Mathematical Formulation</h3>
                <p>
                    Let \(\{x_i\}_{i=1}^m\subset\mathbb{R}^n\) be zero-mean data (or centered beforehand). KPCA proceeds by mapping 
                    each \(x_i\) into \(\mathcal{F}\) via
                    \[
                    x_i \mapsto \phi(x_i)\in\mathcal{F},
                    \]
                    and forming the kernel (Gram) matrix
                    \[
                    K_{ij} = k(x_i,x_j) = \langle\phi(x_i),\,\phi(x_j)\rangle.
                    \]
                    Since \(\phi\) may be infinite-dimensional, all computations use \(K\) directly.
                </p>

                <h3>Centering in Feature Space</h3>
                <p>
                    Centering in \(\mathcal{F}\) is required because PCA assumes zero-mean data. The centered kernel matrix 
                    \(\widetilde K\) is computed as
                    \[
                    \widetilde K = K - \mathbf{1}_m K - K\,\mathbf{1}_m + \mathbf{1}_m K\,\mathbf{1}_m,
                    \quad (\mathbf{1}_m)_{ij} = \tfrac1m.
                    \]
                </p>

                <h3>Eigenproblem and Projection</h3>
                <p>
                    Solve the eigenvalue problem
                    \[
                    m\,\lambda\,\alpha = \widetilde K\,\alpha,
                    \quad
                    \alpha^\top \widetilde K\,\alpha = 1,
                    \]
                    where each eigenvector \(\alpha\in\mathbb{R}^m\) corresponds to a principal component in \(\mathcal{F}\). 
                    The projection of a new point \(x\) onto the \(k\)th component is
                    \[
                    z_k(x) \;=\; \sum_{i=1}^m \alpha_i^{(k)}\,k(x_i,x).
                    \]
                </p>

                <h3>Common Kernel Functions</h3>
                <ul style="padding-left:40px;">
                    <li><strong>Linear:</strong> \(k(x,y)=x^\top y\) — reduces to standard PCA.</li>
                    <li><strong>Polynomial:</strong> \(k(x,y)=(\alpha\,x^\top y + c)^d\) — captures polynomial relationships up to 
                        degree \(d\).</li>
                    <li><strong>RBF (Gaussian):</strong> \(k(x,y)=\exp(-\gamma\,\|x-y\|^2)\) — maps into an infinite-dimensional space 
                        and emphasizes local similarity.</li>
                </ul>

                <h3>Advantages and Challenges</h3>
                <p>
                    <strong>Advantages:</strong>
                    <ul style="padding-left:40px;">
                    <li>Captures complex, nonlinear structure in the data.</li>
                    <li>Works with any positive semidefinite kernel—no explicit feature mapping required.</li>
                    <li>Effective for clustering, denoising, and visualization of nonlinear manifolds.</li>
                    </ul>
                    <strong>Challenges:</strong>
                    <ul style="padding-left:40px;">
                    <li>Choice and hyperparameter tuning of the kernel critically affect performance.</li>
                    <li>Computational cost \(O(m^3)\) in time and \(O(m^2)\) in memory for the kernel matrix eigendecomposition.</li>
                    <li><em>Pre-image problem:</em> recovering an approximate inverse map \( \phi^{-1}(z) \) to interpret projections 
                        in the original space is nontrivial.</li>
                    </ul>
                </p>
            </section>


            <section id="" class="section-content">
                <h2>Lipschitz Continuity in Kernel Methods</h2>
                <p>
                    <strong>Lipschitz continuity</strong> is a crucial concept for understanding the stability and 
                    generalization properties of kernel methods, including Kernel PCA. A function \(f: X \to Y\) 
                    between metric spaces is Lipschitz continuous if there exists a constant \(L \geq 0\) such that:
                    \[
                    d_Y(f(x_1), f(x_2)) \leq L \cdot d_X(x_1, x_2) \quad \forall x_1, x_2 \in X
                    \]
                    The smallest such \(L\) is called the <strong>Lipschitz constant</strong>.
                </p>
                
                <h3>Lipschitz Properties of Kernel Functions</h3>
                <p>
                    Different kernel functions have different Lipschitz properties, which affect the smoothness and 
                    stability of the resulting feature mappings:
                </p>
                <ul style="padding-left: 40px;">
                    <li><strong>Linear Kernel</strong>: Lipschitz constant equals the operator norm of the data matrix</li>
                    <li><strong>RBF Kernel</strong>: Always Lipschitz continuous with constant depending on \(\gamma\):
                        \[
                        |k(x, z) - k(y, z)| \leq 2\gamma \|x - y\| \cdot \exp(-\gamma \max(\|x-z\|^2, \|y-z\|^2))
                        \]</li>
                    <li><strong>Polynomial Kernel</strong>: Lipschitz constant grows with degree \(d\), potentially 
                        leading to instability for high degrees</li>
                </ul>
                
                <h3>Implications for Kernel PCA</h3>
                <p>
                    The Lipschitz continuity of the kernel function has several important implications:
                </p>
                <ol style="padding-left: 40px;">
                    <li><strong>Stability</strong>: Small changes in input lead to bounded changes in the kernel PCA projection</li>
                    <li><strong>Generalization</strong>: Lipschitz continuous mappings generalize better to unseen data</li>
                    <li><strong>Robustness</strong>: Lower Lipschitz constants indicate greater robustness to noise</li>
                    <li><strong>Parameter Selection</strong>: The kernel parameters (e.g., \(\gamma\) for RBF) directly 
                        affect the Lipschitz constant and thus the smoothness of the mapping</li>
                </ol>
                
                <h3>Practical Considerations</h3>
                <p>
                    When applying Kernel PCA, considering Lipschitz continuity helps in:
                </p>
                <ul style="padding-left: 40px;">
                    <li>Choosing appropriate kernel parameters to balance expressiveness and stability</li>
                    <li>Understanding why certain kernels work better for specific types of data</li>
                    <li>Designing new kernels with desired smoothness properties</li>
                    <li>Analyzing the sensitivity of dimensionality reduction to perturbations</li>
                </ul>
                
            </section>

            <section id="" class="section-content">
                 <h2>PCA vs Autoencoders</h2>
                <p>
                    While Kernel PCA extends linear PCA to non-linear spaces using the kernel trick, 
                    <strong>autoencoders</strong> provide another approach to non-linear dimensionality reduction 
                    using neural networks. Understanding the relationship between these methods is crucial for 
                    choosing the right technique for your application.
                </p>
                
                <h3>Linear Autoencoders ≈ PCA</h3>
                <p>
                    A surprising result is that a <strong>linear autoencoder</strong> with one hidden layer learns 
                    essentially the same representation as PCA:
                </p>
                <ul style="padding-left: 40px;">
                    <li>Encoder: \(z = W_e x + b_e\) (linear activation)</li>
                    <li>Decoder: \(\hat{x} = W_d z + b_d\) (linear activation)</li>
                    <li>Loss: Mean Squared Error \(\mathcal{L} = \|x - \hat{x}\|^2\)</li>
                </ul>
                <p>
                    When trained to convergence, the encoder weights \(W_e\) span the same subspace as the top 
                    principal components from PCA (though they may not be orthogonal).
                </p>
                
                <h3>Non-linear Autoencoders</h3>
                <p>
                    The real power of autoencoders comes from using <strong>non-linear activation functions</strong>:
                </p>
                <ul style="padding-left: 40px;">
                    <li>Can capture complex, non-linear manifolds in data</li>
                    <li>More flexible than Kernel PCA in choosing the architecture</li>
                    <li>Can learn hierarchical representations with deep architectures</li>
                    <li>Allow for various regularization techniques (dropout, L1/L2, etc.)</li>
                </ul>
                
                <h3>Comparison: PCA vs Kernel PCA vs Autoencoders</h3>
                <table style="width: 100%; border-collapse: collapse; margin: 20px 0;">
                    <tr style="background-color: #f0f0f0;">
                        <th style="padding: 10px; border: 1px solid #ddd;">Aspect</th>
                        <th style="padding: 10px; border: 1px solid #ddd;">PCA</th>
                        <th style="padding: 10px; border: 1px solid #ddd;">Kernel PCA</th>
                        <th style="padding: 10px; border: 1px solid #ddd;">Autoencoders</th>
                    </tr>
                    <tr>
                        <td style="padding: 10px; border: 1px solid #ddd;"><strong>Type</strong></td>
                        <td style="padding: 10px; border: 1px solid #ddd;">Linear</td>
                        <td style="padding: 10px; border: 1px solid #ddd;">Non-linear (implicit)</td>
                        <td style="padding: 10px; border: 1px solid #ddd;">Linear or Non-linear</td>
                    </tr>
                    <tr>
                        <td style="padding: 10px; border: 1px solid #ddd;"><strong>Computation</strong></td>
                        <td style="padding: 10px; border: 1px solid #ddd;">Eigendecomposition</td>
                        <td style="padding: 10px; border: 1px solid #ddd;">Kernel matrix eigen.</td>
                        <td style="padding: 10px; border: 1px solid #ddd;">Gradient descent</td>
                    </tr>
                    <tr>
                        <td style="padding: 10px; border: 1px solid #ddd;"><strong>Scalability</strong></td>
                        <td style="padding: 10px; border: 1px solid #ddd;">Good (\(O(n^3)\))</td>
                        <td style="padding: 10px; border: 1px solid #ddd;">Poor (\(O(m^3)\))</td>
                        <td style="padding: 10px; border: 1px solid #ddd;">Excellent (SGD)</td>
                    </tr>
                    <tr>
                        <td style="padding: 10px; border: 1px solid #ddd;"><strong>Interpretability</strong></td>
                        <td style="padding: 10px; border: 1px solid #ddd;">High</td>
                        <td style="padding: 10px; border: 1px solid #ddd;">Medium</td>
                        <td style="padding: 10px; border: 1px solid #ddd;">Low</td>
                    </tr>
                    <tr>
                        <td style="padding: 10px; border: 1px solid #ddd;"><strong>Reconstruction</strong></td>
                        <td style="padding: 10px; border: 1px solid #ddd;">Easy</td>
                        <td style="padding: 10px; border: 1px solid #ddd;">Difficult</td>
                        <td style="padding: 10px; border: 1px solid #ddd;">Built-in</td>
                    </tr>
                    <tr>
                        <td style="padding: 10px; border: 1px solid #ddd;"><strong>Hyperparameters</strong></td>
                        <td style="padding: 10px; border: 1px solid #ddd;">None</td>
                        <td style="padding: 10px; border: 1px solid #ddd;">Kernel choice, params</td>
                        <td style="padding: 10px; border: 1px solid #ddd;">Many (architecture, etc.)</td>
                    </tr>
                </table>
                
                <h3>When to Use Which Method?</h3>
                <p>
                    <strong>Use PCA when:</strong>
                </p>
                <ul style="padding-left: 40px;">
                    <li>Data has primarily linear relationships</li>
                    <li>Interpretability is important</li>
                    <li>You need a fast, parameter-free method</li>
                    <li>Working with relatively low-dimensional data</li>
                </ul>
                
                <p>
                    <strong>Use Kernel PCA when:</strong>
                </p>
                <ul style="padding-left: 40px;">
                    <li>Data has non-linear relationships</li>
                    <li>Dataset is small to medium-sized</li>
                    <li>You have domain knowledge about appropriate kernels</li>
                    <li>Clustering or visualization is the main goal</li>
                </ul>
                
                <p>
                    <strong>Use Autoencoders when:</strong>
                </p>
                <ul style="padding-left: 40px;">
                    <li>Data has complex non-linear relationships</li>
                    <li>Dataset is large</li>
                    <li>Reconstruction quality is important</li>
                    <li>You need to learn hierarchical features</li>
                    <li>You can afford hyperparameter tuning</li>
                </ul>
                
            </section>

            <section id="demo" class="section-content">
                <h2>Interactive Kernel PCA Demo</h2>
                <p>
                    This interactive demo allows you to explore the differences between standard PCA and Kernel PCA 
                    on various datasets. You can see how different kernels capture non-linear patterns and how 
                    parameters affect the results.
                </p>

                <div id="kernel_pca_visualizer"></div>
                         
            </section>


        </div>
        <script src="/js/main.js"></script>  
        <script src="/js/kernel_pca.js"></script>  
    </body>
</html>