---
layout: default
title: Principal Component Analysis (PCA)
level: detail
description: Learn about unsupervised learning basics from PCA.
uses_math: true
uses_python: false
---
<!DOCTYPE html>
<html>
    <body> 
        <div class="hero-section">
            <h1 class="webpage-name">Principal Component Analysis (PCA)</h1>
        </div>

        <div class="topic-nav">
            <a href="#intro">Recap & Introduction</a>
            <a href="#"></a>
            <a href="#"></a>
        </div> 

        <div class="container">  

            <section id="intro" class="section-content">
                <h2>Recap & Introduction</h2>
                    <p>
                    <a href="../Probability/covariance.html"><strong>Principal Component Analysis (PCA)</strong></a> is one of the most fundamental 
                    techniques in machine learning and statistics for <strong>dimensionality reduction</strong>. It provides a method to reduce the number of 
                    variables in high-dimensional datasets while retaining the most meaningful structure and variation present in the original data.
                    </p>

                    <p>
                    PCA begins by analyzing the <strong>covariance structure</strong> of the data. Given a dataset, we compute the covariance matrix to 
                    understand how different features co-vary. Since the covariance matrix is always symmetric and positive semi-definite, it can be 
                    <a href="../Linear_algebra/symmetry.html"><strong>orthogonally diagonalized</strong></a>. The resulting eigenvectors, called 
                    <strong>principal components (PCs)</strong>, form an orthonormal basis that captures the directions of maximum variance. The corresponding 
                    eigenvalues indicate how much of the total variance is captured by each component.
                    </p>

                    <p>
                    By projecting the data onto the subspace spanned by the top \(k\) principal components (those associated with the largest eigenvalues), PCA 
                    identifies a lower-dimensional representation that retains as much variance as possible. This allows us to reduce dimensionality by discarding 
                    less informative directions (i.e., those with small variance), thereby simplifying the dataset while minimizing information loss.
                    </p>

                    <p>
                    For example, if a dataset in \(\mathbb{R}^{10}\) has 3 principal components capturing 95% of the total variance, we can project the original data onto a 
                    3D subspace. This projection preserves the dominant patterns and relationships in the data and filters out noise and redundancy. The transformed vectors 
                    in the low-dimensional space are known as <strong>latent representations</strong>.
                    </p>

                    <p>
                    While PCA is a powerful tool, it identifies directions of maximum variance using <strong>linear combinations</strong> of the 
                    original features. As a result, it may fail to uncover complex, <em>nonlinear</em> structures in the data. 
                    </p>
                
            </section>

            <section id="kernel-pca" class="section-content">
                <h2>Kernel PCA</h2>
                <p> 
                    Given the \(N \times D\) data matrix. PCA is a problem of determining the eigenvectors of the \(D\times D\) 
                    covariance matrix \(X^\top X\). If \(D >> N\), working with the \(N \times N\) <strong>Gram matrix</strong> 
                    \(K = XX^\top\) is much efficient. The Gram matrix is just a matrix of inner products \(x_i^\top x_j\).
                </p>
                <p>
                    <strong>Kernel PCA (KPCA)</strong> is a nonlinear generalization of classical PCA that uses the 
                    <a href="intro_classification.html"><strong>kernel trick</strong></a>, which allows us to replace 
                    inner products \(x_i^\top x_j\) with a kernel function \(K_{ij} = \mathcal{K}(x_i, x_j)\).
                </p>

                <p>   
                    The kPCA implicitly replaces \(x_i\) with \(\phi(x_i) = \phi_i\). Let \(\Phi\) be the corresponding 
                    design matrix.
                    Assuming the features are centered, the covariance matrix in feature space is represented by:
                    \[
                    S_{\phi} = \frac{1}{N} \sum_i \phi_i \phi_i^T.
                    \]
                    The normalized eigenvectors of \(S\) are give by:
                    \[
                    V_{kPCA} = \Phi^\top U \Lambda^{-\frac{1}{2}}
                    \]
                    where \(U\) is an orthogonal matrix containing the eigenvectors of the Gramm matrix \(K = XX^\top\) with 
                    corresponding eigenvalues in \(\Lambda\).
                </p>
                <p>
                    Since \(\phi_i\) can be infinite dimensional, we cannot compute \(V_{kPCA}\). Instead, we compute the projection 
                    of a test vector \(x_*\) onto the feature space:
                    \[
                    \phi_*^\top V_{kPCA} = \phi_*^\top \Phi^\top U \Lambda^{1\frac{1}{2}} = k_*^\top U \Lambda^{-\frac{1}{2}}
                    \]
                    where \(k_* = \left[\mathcal{K}(x_* , x_1), \cdots, \mathcal{K}(x_* , x_N) \right]\).
                </p>

                <p>
                    Note that using \(K = \Phi \Phi^\top\) is valid only if \(\mathbb{E}[\phi_i] = 0\). However, the feature space can be 
                    infinite dimensional, so we cannot subtract off the mean. Here, we introduce the <strong>double centering trick</strong>. 
                    Let the centered feature vector be 
                    \[
                    \widetilde{\phi_i} = \phi(x_i) - \frac{1}{N} \sum_{j =1}^N \phi(x_j).
                    \]
                    Its Gram matrix is give by
                    \[
                    \widetilde{K_{ij}} =  \widetilde{\phi_i}^\top \widetilde{\phi_j}.
                    \]
                    By the double centering trick, 
                    \[
                    \begin{align*}
                    \widetilde{K} &= C_N K C_N  \\\\
                              &= K - \frac{1}{N}JK - \frac{1}{N}KJ + \frac{1}{N^2}1^\top K 1 \tag{1}
                     \end{align*}
                    \]
                    where 
                    \[
                    C_N = I_N - \frac{1}{N}1_N 1_N^\top
                    \]
                    is the centering matrix. 
                    <br>
                    Note: \(J\) denotes the \(N \times N\) all-ones matrix: \(J = 1_N 1_N^\top\).
                </p>
                <div class="proof">
                Note: To verify the double centering trick, consider the scalar form:
                \[
                \begin{align*}
                \widetilde{K_{ij}} &= \widetilde{x_i}^\top \widetilde{x_j} \\\\
                                &= \left(x_i - \frac{1}{N} \sum_{k=1}^N x_k \right)\left(x_j - \frac{1}{N} \sum_{l=1}^N x_l \right) \\\\
                                &= x_i^\top x_j - \frac{1}{N} \sum_{l=1}^N x_i^\top x_l - \frac{1}{N} \sum_{l=1}^N x_j^\top x_k 
                                    + \frac{1}{N^2} \sum_{k=1}^N \sum_{l =1}^N x_k^\top x_l.
                \end{align*}
                \]
                </div>

                <h3>Advantages and Challenges</h3>
                <p>
                    <strong>Advantages:</strong>
                    <ul style="padding-left:40px;">
                    <li>Captures complex, nonlinear structure in the data.</li>
                    <li>Works with any positive semidefinite kernelâ€”no explicit feature mapping required.</li>
                    <li>Effective for clustering, denoising, and visualization of nonlinear manifolds.</li>
                    </ul>
                    <strong>Challenges:</strong>
                    <ul style="padding-left:40px;">
                    <li>Choice and hyperparameter tuning of the kernel critically affect performance.</li>
                    <li>Computational cost \(O(m^3)\) in time and \(O(m^2)\) in memory for the kernel matrix eigendecomposition.</li>
                    <li><em>Pre-image problem:</em> recovering an approximate inverse map \( \phi^{-1}(z) \) to interpret projections 
                        in the original space is nontrivial.</li>
                    </ul>
                </p>
            </section>


            <section id="" class="section-content">
                <h2>Lipschitz Continuity in Kernel Methods</h2>
                <p>
                    <strong>Lipschitz continuity</strong> is a crucial concept for understanding the stability and 
                    generalization properties of kernel methods, including Kernel PCA. A function \(f: X \to Y\) 
                    between metric spaces is Lipschitz continuous if there exists a constant \(L \geq 0\) such that:
                    \[
                    d_Y(f(x_1), f(x_2)) \leq L \cdot d_X(x_1, x_2) \quad \forall x_1, x_2 \in X
                    \]
                    The smallest such \(L\) is called the <strong>Lipschitz constant</strong>.
                </p>
                
                <h3>Lipschitz Properties of Kernel Functions</h3>
                <p>
                    Different kernel functions have different Lipschitz properties, which affect the smoothness and 
                    stability of the resulting feature mappings:
                </p>
                <ul style="padding-left: 40px;">
                    <li><strong>Linear Kernel</strong>: Lipschitz constant equals the operator norm of the data matrix</li>
                    <li><strong>RBF Kernel</strong>: Always Lipschitz continuous with constant depending on \(\gamma\):
                        \[
                        |k(x, z) - k(y, z)| \leq 2\gamma \|x - y\| \cdot \exp(-\gamma \max(\|x-z\|^2, \|y-z\|^2))
                        \]</li>
                    <li><strong>Polynomial Kernel</strong>: Lipschitz constant grows with degree \(d\), potentially 
                        leading to instability for high degrees</li>
                </ul>
                
                <h3>Implications for Kernel PCA</h3>
                <p>
                    The Lipschitz continuity of the kernel function has several important implications:
                </p>
                <ol style="padding-left: 40px;">
                    <li><strong>Stability</strong>: Small changes in input lead to bounded changes in the kernel PCA projection</li>
                    <li><strong>Generalization</strong>: Lipschitz continuous mappings generalize better to unseen data</li>
                    <li><strong>Robustness</strong>: Lower Lipschitz constants indicate greater robustness to noise</li>
                    <li><strong>Parameter Selection</strong>: The kernel parameters (e.g., \(\gamma\) for RBF) directly 
                        affect the Lipschitz constant and thus the smoothness of the mapping</li>
                </ol>
                
                <h3>Practical Considerations</h3>
                <p>
                    When applying Kernel PCA, considering Lipschitz continuity helps in:
                </p>
                <ul style="padding-left: 40px;">
                    <li>Choosing appropriate kernel parameters to balance expressiveness and stability</li>
                    <li>Understanding why certain kernels work better for specific types of data</li>
                    <li>Designing new kernels with desired smoothness properties</li>
                    <li>Analyzing the sensitivity of dimensionality reduction to perturbations</li>
                </ul>
                
            </section>

            <section id="" class="section-content">
                 <h2>PCA vs Autoencoders</h2>
                <p>
                    While Kernel PCA extends linear PCA to non-linear spaces using the kernel trick, 
                    <strong>autoencoders</strong> provide another approach to non-linear dimensionality reduction 
                    using neural networks. Understanding the relationship between these methods is crucial for 
                    choosing the right technique for your application.
                </p>
                
                <h3>Linear Autoencoders â‰ˆ PCA</h3>
                <p>
                    A surprising result is that a <strong>linear autoencoder</strong> with one hidden layer learns 
                    essentially the same representation as PCA:
                </p>
                <ul style="padding-left: 40px;">
                    <li>Encoder: \(z = W_e x + b_e\) (linear activation)</li>
                    <li>Decoder: \(\hat{x} = W_d z + b_d\) (linear activation)</li>
                    <li>Loss: Mean Squared Error \(\mathcal{L} = \|x - \hat{x}\|^2\)</li>
                </ul>
                <p>
                    When trained to convergence, the encoder weights \(W_e\) span the same subspace as the top 
                    principal components from PCA (though they may not be orthogonal).
                </p>
                
                <h3>Non-linear Autoencoders</h3>
                <p>
                    The real power of autoencoders comes from using <strong>non-linear activation functions</strong>:
                </p>
                <ul style="padding-left: 40px;">
                    <li>Can capture complex, non-linear manifolds in data</li>
                    <li>More flexible than Kernel PCA in choosing the architecture</li>
                    <li>Can learn hierarchical representations with deep architectures</li>
                    <li>Allow for various regularization techniques (dropout, L1/L2, etc.)</li>
                </ul>
                
                <h3>Comparison: PCA vs Kernel PCA vs Autoencoders</h3>
                <table style="width: 100%; border-collapse: collapse; margin: 20px 0;">
                    <tr style="background-color: #f0f0f0;">
                        <th style="padding: 10px; border: 1px solid #ddd;">Aspect</th>
                        <th style="padding: 10px; border: 1px solid #ddd;">PCA</th>
                        <th style="padding: 10px; border: 1px solid #ddd;">Kernel PCA</th>
                        <th style="padding: 10px; border: 1px solid #ddd;">Autoencoders</th>
                    </tr>
                    <tr>
                        <td style="padding: 10px; border: 1px solid #ddd;"><strong>Type</strong></td>
                        <td style="padding: 10px; border: 1px solid #ddd;">Linear</td>
                        <td style="padding: 10px; border: 1px solid #ddd;">Non-linear (implicit)</td>
                        <td style="padding: 10px; border: 1px solid #ddd;">Linear or Non-linear</td>
                    </tr>
                    <tr>
                        <td style="padding: 10px; border: 1px solid #ddd;"><strong>Computation</strong></td>
                        <td style="padding: 10px; border: 1px solid #ddd;">Eigendecomposition</td>
                        <td style="padding: 10px; border: 1px solid #ddd;">Kernel matrix eigen.</td>
                        <td style="padding: 10px; border: 1px solid #ddd;">Gradient descent</td>
                    </tr>
                    <tr>
                        <td style="padding: 10px; border: 1px solid #ddd;"><strong>Scalability</strong></td>
                        <td style="padding: 10px; border: 1px solid #ddd;">Good (\(O(n^3)\))</td>
                        <td style="padding: 10px; border: 1px solid #ddd;">Poor (\(O(m^3)\))</td>
                        <td style="padding: 10px; border: 1px solid #ddd;">Excellent (SGD)</td>
                    </tr>
                    <tr>
                        <td style="padding: 10px; border: 1px solid #ddd;"><strong>Interpretability</strong></td>
                        <td style="padding: 10px; border: 1px solid #ddd;">High</td>
                        <td style="padding: 10px; border: 1px solid #ddd;">Medium</td>
                        <td style="padding: 10px; border: 1px solid #ddd;">Low</td>
                    </tr>
                    <tr>
                        <td style="padding: 10px; border: 1px solid #ddd;"><strong>Reconstruction</strong></td>
                        <td style="padding: 10px; border: 1px solid #ddd;">Easy</td>
                        <td style="padding: 10px; border: 1px solid #ddd;">Difficult</td>
                        <td style="padding: 10px; border: 1px solid #ddd;">Built-in</td>
                    </tr>
                    <tr>
                        <td style="padding: 10px; border: 1px solid #ddd;"><strong>Hyperparameters</strong></td>
                        <td style="padding: 10px; border: 1px solid #ddd;">None</td>
                        <td style="padding: 10px; border: 1px solid #ddd;">Kernel choice, params</td>
                        <td style="padding: 10px; border: 1px solid #ddd;">Many (architecture, etc.)</td>
                    </tr>
                </table>
                
                <h3>When to Use Which Method?</h3>
                <p>
                    <strong>Use PCA when:</strong>
                </p>
                <ul style="padding-left: 40px;">
                    <li>Data has primarily linear relationships</li>
                    <li>Interpretability is important</li>
                    <li>You need a fast, parameter-free method</li>
                    <li>Working with relatively low-dimensional data</li>
                </ul>
                
                <p>
                    <strong>Use Kernel PCA when:</strong>
                </p>
                <ul style="padding-left: 40px;">
                    <li>Data has non-linear relationships</li>
                    <li>Dataset is small to medium-sized</li>
                    <li>You have domain knowledge about appropriate kernels</li>
                    <li>Clustering or visualization is the main goal</li>
                </ul>
                
                <p>
                    <strong>Use Autoencoders when:</strong>
                </p>
                <ul style="padding-left: 40px;">
                    <li>Data has complex non-linear relationships</li>
                    <li>Dataset is large</li>
                    <li>Reconstruction quality is important</li>
                    <li>You need to learn hierarchical features</li>
                    <li>You can afford hyperparameter tuning</li>
                </ul>
                
            </section>

            <section id="demo" class="section-content">
                <h2>Interactive Kernel PCA Demo</h2>
                <p>
                    This interactive demo allows you to explore the differences between standard PCA and Kernel PCA 
                    on various datasets. You can see how different kernels capture non-linear patterns and how 
                    parameters affect the results.
                </p>

                <div id="kernel_pca_visualizer"></div>
                         
            </section>


        </div>
        <script src="/js/main.js"></script>  
        <script src="/js/kernel_pca.js"></script>  
    </body>
</html>