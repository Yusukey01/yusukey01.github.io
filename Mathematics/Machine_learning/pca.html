---
layout: default
title: Principal Component Analysis (PCA)
level: detail
description: Learn about unsupervised learning basics from PCA.
uses_math: true
uses_python: false
---
<!DOCTYPE html>
<html>
    <body> 
        <div class="hero-section">
            <h1 class="webpage-name">Principal Component Analysis (PCA)</h1>
        </div>

        <div class="topic-nav">
            <a href="#intro">Introduction</a>
            <a href="#"></a>
            <a href="#"></a>
        </div> 

        <div class="container">  

            <section id="intro" class="section-content">
                <h2>Introduction</h2>
                <p>
                    <strong>Principal Component Analysis (PCA)</strong> is one of the most fundamental techniques in machine learning for 
                    <strong>dimensionality reduction</strong> and data analysis. While the basic mathematical foundations are covered in the 
                    <a href="../Probability/covariance.html"><strong>covariance analysis</strong></a> section, here we focus on its 
                    machine learning applications and extensions.
                </p>
                <p>
                    PCA transforms high-dimensional data into a lower-dimensional representation by finding orthogonal directions 
                    (principal components) that capture the maximum variance in the data. Mathematically, given data matrix 
                    \(X \in \mathbb{R}^{m \times n}\) with \(m\) samples and \(n\) features, PCA finds:
                    \[
                    Z = XW
                    \]
                    where \(W \in \mathbb{R}^{n \times k}\) contains the top \(k\) eigenvectors of the covariance matrix 
                    \(C = \frac{1}{m-1}X^TX\), and \(Z\) represents the transformed data in the principal component space.
                </p>
                <p>
                    However, standard PCA has a critical limitation: it can only capture <strong>linear relationships</strong> 
                    in the data. For many real-world datasets with complex, non-linear structures, this linear assumption 
                    severely limits PCA's effectiveness. This limitation motivated the development of <strong>Kernel PCA</strong>.
                </p>
            </section>

            <section id="" class="section-content">
                <h2>Kernel PCA</h2>
                <p>
                    <strong>Kernel PCA</strong> extends classical PCA to capture non-linear relationships by implicitly mapping 
                    data to a higher-dimensional feature space using the <strong>kernel trick</strong>. This technique, also used 
                    in <a href="svm.html"><strong>Support Vector Machines</strong></a>, allows us to perform PCA in feature spaces 
                    where linear separation becomes possible.
                </p>
                
                <h3>Mathematical Formulation</h3>
                <p>
                    Instead of working directly with data points \(x_i \in \mathbb{R}^n\), Kernel PCA maps them to a 
                    higher-dimensional space via a (typically unknown) mapping \(\phi: \mathbb{R}^n \to \mathcal{F}\):
                    \[
                    x_i \mapsto \phi(x_i)
                    \]
                    The key insight is that we never need to compute \(\phi(x_i)\) explicitly. Instead, we only need 
                    inner products in the feature space, which can be computed using a kernel function:
                    \[
                    k(x_i, x_j) = \langle \phi(x_i), \phi(x_j) \rangle
                    \]
                </p>
                
                <h3>Common Kernel Functions</h3>
                <p>
                    The choice of kernel function determines the implicit feature space and the types of non-linear 
                    relationships that can be captured:
                </p>
                <ul style="padding-left: 40px;">
                    <li><strong>Linear Kernel</strong>: \(k(x, y) = x^T y\)
                        <br>Equivalent to standard PCA</li>
                    <li><strong>RBF (Gaussian) Kernel</strong>: \(k(x, y) = \exp(-\gamma \|x - y\|^2)\)
                        <br>Maps to infinite-dimensional space, captures local similarities</li>
                    <li><strong>Polynomial Kernel</strong>: \(k(x, y) = (\alpha x^T y + c)^d\)
                        <br>Captures polynomial relationships up to degree \(d\)</li>
                </ul>
                
                <h3>Kernel PCA Algorithm</h3>
                <p>
                    The Kernel PCA algorithm involves the following steps:
                </p>
                <ol style="padding-left: 40px;">
                    <li>Compute the kernel matrix \(K_{ij} = k(x_i, x_j)\) for all data points</li>
                    <li>Center the kernel matrix: \(\tilde{K} = K - \mathbf{1}_m K - K \mathbf{1}_m + \mathbf{1}_m K \mathbf{1}_m\)
                        <br>where \(\mathbf{1}_m\) is an \(m \times m\) matrix with all entries equal to \(1/m\)</li>
                    <li>Compute eigenvalues \(\lambda_i\) and eigenvectors \(\alpha_i\) of \(\tilde{K}\)</li>
                    <li>Normalize eigenvectors: \(\tilde{\alpha}_i = \alpha_i / \sqrt{\lambda_i}\)</li>
                    <li>Project new data point \(x\) onto the \(i\)-th principal component:
                        \[
                        z_i = \sum_{j=1}^m \tilde{\alpha}_{ij} k(x_j, x)
                        \]</li>
                </ol>
                
                <h3>Advantages and Challenges</h3>
                <p>
                    <strong>Advantages:</strong>
                </p>
                <ul style="padding-left: 40px;">
                    <li>Captures non-linear relationships in data</li>
                    <li>No need to explicitly compute high-dimensional mappings</li>
                    <li>Can work with any positive semi-definite kernel</li>
                    <li>Particularly effective for clustering and visualization of complex data</li>
                </ul>
                <p>
                    <strong>Challenges:</strong>
                </p>
                <ul style="padding-left: 40px;">
                    <li>Kernel selection and parameter tuning can be difficult</li>
                    <li>Computational complexity: \(O(m^3)\) for eigendecomposition</li>
                    <li>Memory requirements: \(O(m^2)\) for kernel matrix storage</li>
                    <li>Pre-image problem: difficult to reconstruct original data from kernel space</li>
                </ul> 
                
            </section>

              <section id="" class="section-content">
                <h2>Lipschitz Continuity in Kernel Methods</h2>
                <p>
                    <strong>Lipschitz continuity</strong> is a crucial concept for understanding the stability and 
                    generalization properties of kernel methods, including Kernel PCA. A function \(f: X \to Y\) 
                    between metric spaces is Lipschitz continuous if there exists a constant \(L \geq 0\) such that:
                    \[
                    d_Y(f(x_1), f(x_2)) \leq L \cdot d_X(x_1, x_2) \quad \forall x_1, x_2 \in X
                    \]
                    The smallest such \(L\) is called the <strong>Lipschitz constant</strong>.
                </p>
                
                <h3>Lipschitz Properties of Kernel Functions</h3>
                <p>
                    Different kernel functions have different Lipschitz properties, which affect the smoothness and 
                    stability of the resulting feature mappings:
                </p>
                <ul style="padding-left: 40px;">
                    <li><strong>Linear Kernel</strong>: Lipschitz constant equals the operator norm of the data matrix</li>
                    <li><strong>RBF Kernel</strong>: Always Lipschitz continuous with constant depending on \(\gamma\):
                        \[
                        |k(x, z) - k(y, z)| \leq 2\gamma \|x - y\| \cdot \exp(-\gamma \max(\|x-z\|^2, \|y-z\|^2))
                        \]</li>
                    <li><strong>Polynomial Kernel</strong>: Lipschitz constant grows with degree \(d\), potentially 
                        leading to instability for high degrees</li>
                </ul>
                
                <h3>Implications for Kernel PCA</h3>
                <p>
                    The Lipschitz continuity of the kernel function has several important implications:
                </p>
                <ol style="padding-left: 40px;">
                    <li><strong>Stability</strong>: Small changes in input lead to bounded changes in the kernel PCA projection</li>
                    <li><strong>Generalization</strong>: Lipschitz continuous mappings generalize better to unseen data</li>
                    <li><strong>Robustness</strong>: Lower Lipschitz constants indicate greater robustness to noise</li>
                    <li><strong>Parameter Selection</strong>: The kernel parameters (e.g., \(\gamma\) for RBF) directly 
                        affect the Lipschitz constant and thus the smoothness of the mapping</li>
                </ol>
                
                <h3>Practical Considerations</h3>
                <p>
                    When applying Kernel PCA, considering Lipschitz continuity helps in:
                </p>
                <ul style="padding-left: 40px;">
                    <li>Choosing appropriate kernel parameters to balance expressiveness and stability</li>
                    <li>Understanding why certain kernels work better for specific types of data</li>
                    <li>Designing new kernels with desired smoothness properties</li>
                    <li>Analyzing the sensitivity of dimensionality reduction to perturbations</li>
                </ul>
                
            </section>

            <section id="" class="section-content">
                 <h2>PCA vs Autoencoders</h2>
                <p>
                    While Kernel PCA extends linear PCA to non-linear spaces using the kernel trick, 
                    <strong>autoencoders</strong> provide another approach to non-linear dimensionality reduction 
                    using neural networks. Understanding the relationship between these methods is crucial for 
                    choosing the right technique for your application.
                </p>
                
                <h3>Linear Autoencoders ≈ PCA</h3>
                <p>
                    A surprising result is that a <strong>linear autoencoder</strong> with one hidden layer learns 
                    essentially the same representation as PCA:
                </p>
                <ul style="padding-left: 40px;">
                    <li>Encoder: \(z = W_e x + b_e\) (linear activation)</li>
                    <li>Decoder: \(\hat{x} = W_d z + b_d\) (linear activation)</li>
                    <li>Loss: Mean Squared Error \(\mathcal{L} = \|x - \hat{x}\|^2\)</li>
                </ul>
                <p>
                    When trained to convergence, the encoder weights \(W_e\) span the same subspace as the top 
                    principal components from PCA (though they may not be orthogonal).
                </p>
                
                <h3>Non-linear Autoencoders</h3>
                <p>
                    The real power of autoencoders comes from using <strong>non-linear activation functions</strong>:
                </p>
                <ul style="padding-left: 40px;">
                    <li>Can capture complex, non-linear manifolds in data</li>
                    <li>More flexible than Kernel PCA in choosing the architecture</li>
                    <li>Can learn hierarchical representations with deep architectures</li>
                    <li>Allow for various regularization techniques (dropout, L1/L2, etc.)</li>
                </ul>
                
                <h3>Comparison: PCA vs Kernel PCA vs Autoencoders</h3>
                <table style="width: 100%; border-collapse: collapse; margin: 20px 0;">
                    <tr style="background-color: #f0f0f0;">
                        <th style="padding: 10px; border: 1px solid #ddd;">Aspect</th>
                        <th style="padding: 10px; border: 1px solid #ddd;">PCA</th>
                        <th style="padding: 10px; border: 1px solid #ddd;">Kernel PCA</th>
                        <th style="padding: 10px; border: 1px solid #ddd;">Autoencoders</th>
                    </tr>
                    <tr>
                        <td style="padding: 10px; border: 1px solid #ddd;"><strong>Type</strong></td>
                        <td style="padding: 10px; border: 1px solid #ddd;">Linear</td>
                        <td style="padding: 10px; border: 1px solid #ddd;">Non-linear (implicit)</td>
                        <td style="padding: 10px; border: 1px solid #ddd;">Linear or Non-linear</td>
                    </tr>
                    <tr>
                        <td style="padding: 10px; border: 1px solid #ddd;"><strong>Computation</strong></td>
                        <td style="padding: 10px; border: 1px solid #ddd;">Eigendecomposition</td>
                        <td style="padding: 10px; border: 1px solid #ddd;">Kernel matrix eigen.</td>
                        <td style="padding: 10px; border: 1px solid #ddd;">Gradient descent</td>
                    </tr>
                    <tr>
                        <td style="padding: 10px; border: 1px solid #ddd;"><strong>Scalability</strong></td>
                        <td style="padding: 10px; border: 1px solid #ddd;">Good (\(O(n^3)\))</td>
                        <td style="padding: 10px; border: 1px solid #ddd;">Poor (\(O(m^3)\))</td>
                        <td style="padding: 10px; border: 1px solid #ddd;">Excellent (SGD)</td>
                    </tr>
                    <tr>
                        <td style="padding: 10px; border: 1px solid #ddd;"><strong>Interpretability</strong></td>
                        <td style="padding: 10px; border: 1px solid #ddd;">High</td>
                        <td style="padding: 10px; border: 1px solid #ddd;">Medium</td>
                        <td style="padding: 10px; border: 1px solid #ddd;">Low</td>
                    </tr>
                    <tr>
                        <td style="padding: 10px; border: 1px solid #ddd;"><strong>Reconstruction</strong></td>
                        <td style="padding: 10px; border: 1px solid #ddd;">Easy</td>
                        <td style="padding: 10px; border: 1px solid #ddd;">Difficult</td>
                        <td style="padding: 10px; border: 1px solid #ddd;">Built-in</td>
                    </tr>
                    <tr>
                        <td style="padding: 10px; border: 1px solid #ddd;"><strong>Hyperparameters</strong></td>
                        <td style="padding: 10px; border: 1px solid #ddd;">None</td>
                        <td style="padding: 10px; border: 1px solid #ddd;">Kernel choice, params</td>
                        <td style="padding: 10px; border: 1px solid #ddd;">Many (architecture, etc.)</td>
                    </tr>
                </table>
                
                <h3>When to Use Which Method?</h3>
                <p>
                    <strong>Use PCA when:</strong>
                </p>
                <ul style="padding-left: 40px;">
                    <li>Data has primarily linear relationships</li>
                    <li>Interpretability is important</li>
                    <li>You need a fast, parameter-free method</li>
                    <li>Working with relatively low-dimensional data</li>
                </ul>
                
                <p>
                    <strong>Use Kernel PCA when:</strong>
                </p>
                <ul style="padding-left: 40px;">
                    <li>Data has non-linear relationships</li>
                    <li>Dataset is small to medium-sized</li>
                    <li>You have domain knowledge about appropriate kernels</li>
                    <li>Clustering or visualization is the main goal</li>
                </ul>
                
                <p>
                    <strong>Use Autoencoders when:</strong>
                </p>
                <ul style="padding-left: 40px;">
                    <li>Data has complex non-linear relationships</li>
                    <li>Dataset is large</li>
                    <li>Reconstruction quality is important</li>
                    <li>You need to learn hierarchical features</li>
                    <li>You can afford hyperparameter tuning</li>
                </ul>
                
            </section>

            <section id="demo" class="section-content">
                <h2>Interactive Kernel PCA Demo</h2>
                <div id="kernel_pca_visualizer"></div>
                <p>
                    This interactive demo allows you to explore the differences between standard PCA and Kernel PCA 
                    on various datasets. You can see how different kernels capture non-linear patterns and how 
                    parameters affect the results.
                </p>

                <h3>Understanding the Visualization</h3>
                <p>
                    The demo shows multiple views to help understand how Kernel PCA works:
                </p>
                <ul style="padding-left: 40px;">
                    <li><strong>Original Data</strong>: The input data in 2D or 3D space</li>
                    <li><strong>Standard PCA</strong>: Linear projection onto principal components</li>
                    <li><strong>Kernel PCA</strong>: Non-linear projection using the selected kernel</li>
                    <li><strong>Explained Variance</strong>: How much variance each component captures</li>
                    <li><strong>Reconstruction Error</strong>: Visual comparison of reconstruction quality</li>
                    <li><strong>Kernel Matrix</strong>: Heatmap showing pairwise similarities</li>
                </ul>
                
                <h3>Interactive Controls</h3>
                <p>
                    Experiment with different settings to understand their effects:
                </p>
                <ul style="padding-left: 40px;">
                    <li><strong>Dataset</strong>: 
                        <ul style="padding-left: 40px;">
                            <li><em>Linear</em>: Linearly separable data (PCA = Kernel PCA with linear kernel)</li>
                            <li><em>Circles</em>: Concentric circles (requires non-linear kernel)</li>
                            <li><em>Moons</em>: Two half-moons (classic non-linear example)</li>
                            <li><em>Swiss Roll</em>: 3D manifold data</li>
                            <li><em>Spiral</em>: Intertwined spirals</li>
                        </ul>
                    </li>
                    <li><strong>Kernel Type</strong>: Choose between Linear, RBF, Polynomial, or Sigmoid kernels</li>
                    <li><strong>Kernel Parameters</strong>: Adjust γ, degree, or coefficient based on kernel type</li>
                    <li><strong>Components</strong>: Number of principal components to extract (1-3)</li>
                    <li><strong>Noise Level</strong>: Add Gaussian noise to test robustness</li>
                    <li><strong>Sample Size</strong>: Number of data points (affects computation time)</li>
                </ul>
                
                <h3>Algorithm Visualization</h3>
                <p>
                    The demo includes a step-by-step visualization of the Kernel PCA algorithm:
                </p>
                <ol style="padding-left: 40px;">
                    <li><strong>Kernel Matrix Computation</strong>: Watch as pairwise similarities are calculated</li>
                    <li><strong>Centering</strong>: See how the kernel matrix is centered in feature space</li>
                    <li><strong>Eigendecomposition</strong>: Observe the extraction of eigenvalues and eigenvectors</li>
                    <li><strong>Projection</strong>: Visualize how data points are projected onto principal components</li>
                </ol>
                
                <h3>Lipschitz Analysis</h3>
                <p>
                    The demo includes a Lipschitz continuity analysis showing:
                </p>
                <ul style="padding-left: 40px;">
                    <li>Estimated Lipschitz constant for the current kernel and parameters</li>
                    <li>Stability visualization: how projections change with input perturbations</li>
                    <li>Comparison of Lipschitz properties across different kernels</li>
                </ul>
                
                <h3>Autoencoder Comparison</h3>
                <p>
                    Toggle the "Compare with Autoencoder" option to see:
                </p>
                <ul style="padding-left: 40px;">
                    <li>Simple autoencoder with configurable architecture</li>
                    <li>Side-by-side comparison of dimensionality reduction quality</li>
                    <li>Reconstruction error comparison</li>
                    <li>Training dynamics visualization</li>
                </ul>
                
                <h3>Educational Features</h3>
                <p>
                    The demo includes several educational features to deepen understanding:
                </p>
                <ul style="padding-left: 40px;">
                    <li><strong>Mathematical Details</strong>: Toggle to show relevant equations and computations</li>
                    <li><strong>Performance Metrics</strong>: Real-time display of explained variance, reconstruction error</li>
                    <li><strong>Export Results</strong>: Download projected data and visualizations</li>
                    <li><strong>Presets</strong>: Load example configurations that highlight specific concepts</li>
                </ul>
                
            </section>


        </div>
        <script src="/js/main.js"></script>  
        <script src="/js/kernel_pca.js"></script>  
    </body>
</html>