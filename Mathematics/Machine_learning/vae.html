---
layout: default
title: Variational Autoencoder 
topic_id: ml-12
level: detail
uses_math: true
uses_python: false
noindex: true
---
<!DOCTYPE html>
<html>
    <body>
        {% include learning_resource_schema.html topic_id=page.topic_id %}
        
        <div class="hero-section">
            <h1 class="webpage-name">Variational Autoencoder</h1>
        </div>

        {% include section_navigation.html %}

        <div class="topic-nav">
            <a href="#intro">Introduction</a>
            <a href="#vi">Variational Inference</a> 
            <a href="#vae">Variational Autoencoder (VAE)</a>   
            <a href="#demo">VAE Uncertainty in Robotic Manipulation</a>          
        </div>

        <div class="container">  

            <section id="intro" class="section-content">
                <h2>Introduction</h2>
                <p>
                    In the intersection of pure mathematics and autonomous systems lies the challenge of <strong>uncertainty</strong>. 
                    While classical robotics often relies on deterministic models, real-world interaction is inherently stochastic. 
                    Sensory noise, unobservable physical properties, and environmental variances require a framework that does not 
                    merely calculate values, but manages probabilities.
                </p>
                
                <p>
                    The <strong>Variational Autoencoder (VAE)</strong> represents a profound synthesis of information theory and 
                    Bayesian inference. Unlike standard autoencoders that map data to discrete points in a latent space, the VAE 
                    learns the underlying <strong>structural manifold</strong> of the data. By encoding inputs into a continuous 
                    latent distribution characterized by a mean (\(\mu\)) and a variance (\(\sigma\)), the VAE provides a principled 
                    way to quantify what the system knows - and, more importantly, what it does not.
                </p>
            </section>

            <section id="vi" class="section-content">
                <h2>Variational Inference</h2>
                <p>
                    Deterministic control algorithms assume a single point-estimate for the state (e.g., "the center of mass is at coordinate \((x, y, z)\)"). 
                    In contrast, <strong>Variational Inference (VI)</strong> treats the world as a probability distribution. This shift from "points" to "spreads" 
                    allows the AI to quantify its own confidence.
                </p>

                <p>
                    Since calculating the true posterior distribution is often computationally intractable, VI approximates it with a simpler distribution (usually Gaussian). 
                    This mathematical approximation is what allows "intelligence" to be computable in real-time. The robot doesn't just guess where the weight is; it understands 
                    the <strong>variance</strong> of its guess, which serves as a proxy for risk.
                </p>

            </section>

            <section id="vae" class="section-content">
                
                <h2>Variational Autoencoder (VAE)</h2>

                <h3>From Factor Analysis to Nonlinear Generative Models</h3>
                <p>
                    In classical <strong>factor analysis (FA)</strong>, we model the observed data \(\boldsymbol{x} \in \mathbb{R}^D\) as a 
                    linear function of a latent variable \(\boldsymbol{z} \in \mathbb{R}^K\) with \(K \ll D\):
                    \[
                    p(\boldsymbol{x} \mid \boldsymbol{z}) = \mathcal{N}(\boldsymbol{x} \mid \boldsymbol{Wz}, \sigma^2 \boldsymbol{I})
                    \]
                    where \(\boldsymbol{W} \in \mathbb{R}^{D \times K}\) is the factor loading matrix. The linearity of \(\boldsymbol{W}\) 
                    makes posterior inference tractable: given an observation \(\boldsymbol{x}\), the posterior 
                    \(p(\boldsymbol{z} \mid \boldsymbol{x})\) is Gaussian in closed form. However, this linearity also limits the model's 
                    expressiveness - real-world data distributions are rarely well-captured by linear mappings.
                </p>
                <p>
                    The <strong>Variational Autoencoder (VAE)</strong> extends factor analysis by replacing the linear mapping 
                    \(\boldsymbol{Wz}\) with an arbitrary nonlinear function parameterized by a neural network:
                    \[
                    p_{\boldsymbol{\theta}}(\boldsymbol{x} \mid \boldsymbol{z}) = \mathcal{N}(\boldsymbol{x} \mid f_d (\boldsymbol{z}; \boldsymbol{\theta}), \, \sigma^2 \boldsymbol{I})
                    \]
                    where \(f_d(\cdot\,; \boldsymbol{\theta})\) is a <strong>decoder network</strong> with parameters \(\boldsymbol{\theta}\). 
                    This nonlinear generative model can represent far richer data distributions, but it comes at a cost: 
                    the posterior \(p_{\boldsymbol{\theta}}(\boldsymbol{z} \mid \boldsymbol{x})\) is no longer analytically tractable, 
                    because computing it requires the marginal likelihood
                    \[
                    p_{\boldsymbol{\theta}}(\boldsymbol{z} \mid \boldsymbol{x}) = \frac{p_{\boldsymbol{\theta}}(\boldsymbol{x} \mid \boldsymbol{z})\, p(\boldsymbol{z})}{p_{\boldsymbol{\theta}}(\boldsymbol{x})}
                    \quad \text{where} \quad 
                    p_{\boldsymbol{\theta}}(\boldsymbol{x}) = \int p_{\boldsymbol{\theta}}(\boldsymbol{x} \mid \boldsymbol{z})\, p(\boldsymbol{z})\, d\boldsymbol{z}
                    \]
                    which involves an intractable integral over the latent space when \(f_d\) is a deep network. This intractability is the central challenge that motivates the variational approach.
                </p>

                <h3>Amortized Variational Inference</h3>
                <p>
                    Since the true posterior is intractable, we introduce an <strong>approximate posterior</strong> - a 
                    <strong>recognition network</strong> (also called the <strong>inference network</strong> or <strong>encoder</strong>) - 
                    that is trained simultaneously with the generative model. We restrict the approximate posterior to a tractable family: 
                    a Gaussian with diagonal covariance, yielding
                    \[
                    q_{\boldsymbol{\phi}} (\boldsymbol{z} \mid \boldsymbol{x}) 
                    = \mathcal{N}\!\left(\boldsymbol{z} \;\middle|\; f_{e,\mu} (\boldsymbol{x} ; \boldsymbol{\phi}), \, \text{diag}\!\left(f_{e,\sigma}(\boldsymbol{x};\boldsymbol{\phi})\right)\right)
                    \]
                    where \(f_{e,\mu}\) and \(f_{e,\sigma}\) are the encoder's output heads that produce the mean vector and the 
                    diagonal covariance respectively, and \(\boldsymbol{\phi}\) denotes all encoder parameters.
                </p>
                <p>
                    This approach is called <strong>amortized inference</strong>: rather than running a separate optimization procedure 
                    to compute the posterior for each individual data point (as in classical variational inference), we train a single 
                    neural network that directly maps any input \(\boldsymbol{x}\) to the parameters of its approximate posterior. 
                    The cost of inference at test time is thus reduced to a single forward pass through the encoder, with the computational 
                    investment "amortized" across the entire training set.
                </p>

                <h3>The Evidence Lower Bound (ELBO)</h3>
                <p>
                    Since the marginal likelihood \(p_{\boldsymbol{\theta}}(\boldsymbol{x})\) is intractable, we cannot maximize it directly. 
                    Instead, we derive a tractable lower bound. For a single observation \(\boldsymbol{x}\), define the 
                    <strong>evidence lower bound (ELBO)</strong> as follows.
                </p>

                <div class="theorem">
                    <span class="theorem-title">Definition: Evidence Lower Bound(ELBO)</span>
                    <p>
                        For a generative model \(p_{\boldsymbol{\theta}}(\boldsymbol{x}, \boldsymbol{z})\) and 
                        approximate posterior \(q_{\boldsymbol{\phi}}(\boldsymbol{z} \mid \boldsymbol{x})\), the ELBO is
                        \[
                        L(\boldsymbol{\theta}, \, \boldsymbol{\phi}\mid \boldsymbol{x}) 
                        = \mathbb{E}_{q_{\boldsymbol{\phi}}(\boldsymbol{z} \mid \boldsymbol{x})} \left[\log p_{\boldsymbol{\theta}}(\boldsymbol{x}, \boldsymbol{z}) - \log q_{\boldsymbol{\phi}}(\boldsymbol{z} \mid \boldsymbol{x}) \right].
                        \]
                    </p>
                </div>

                <p>
                    To see that this is indeed a lower bound on the log-evidence, apply <strong>Jensen's inequality</strong>. 
                    Since \(\log\) is concave, moving it outside the expectation can only increase the value:
                    \[
                    \begin{align*}
                    L(\boldsymbol{\theta}, \, \boldsymbol{\phi}\mid \boldsymbol{x})
                    &= \int q_{\boldsymbol{\phi}}(\boldsymbol{z} \mid \boldsymbol{x}) \log \frac{p_{\boldsymbol{\theta}}(\boldsymbol{x}, \boldsymbol{z})}{q_{\boldsymbol{\phi}}(\boldsymbol{z} \mid \boldsymbol{x})}\, d\boldsymbol{z} \\\\
                    &\leq \log \int q_{\boldsymbol{\phi}}(\boldsymbol{z} \mid \boldsymbol{x}) \frac{p_{\boldsymbol{\theta}}(\boldsymbol{x}, \boldsymbol{z})}{q_{\boldsymbol{\phi}}(\boldsymbol{z} \mid \boldsymbol{x})}\, d\boldsymbol{z} \\\\
                    &= \log \int p_{\boldsymbol{\theta}}(\boldsymbol{x}, \boldsymbol{z})\, d\boldsymbol{z} \\\\
                    &= \log p_{\boldsymbol{\theta}}(\boldsymbol{x}).
                    \end{align*}
                    \]
                    That is, \(L(\boldsymbol{\theta}, \boldsymbol{\phi} \mid \boldsymbol{x}) \leq \log p_{\boldsymbol{\theta}}(\boldsymbol{x})\) 
                    for any choice of \(q_{\boldsymbol{\phi}}\). Maximizing the ELBO with respect to both \(\boldsymbol{\theta}\) and 
                    \(\boldsymbol{\phi}\) therefore simultaneously pushes up the log-likelihood and tightens the bound by making 
                    \(q_{\boldsymbol{\phi}}\) a better approximation to the true posterior.
                </p>

                <h3>ELBO Decomposition: Reconstruction + Regularization</h3>
                <p>
                    The ELBO can be decomposed into two interpretable terms by expanding the joint 
                    \(\log p_{\boldsymbol{\theta}}(\boldsymbol{x}, \boldsymbol{z}) = \log p_{\boldsymbol{\theta}}(\boldsymbol{x} \mid \boldsymbol{z}) + \log p(\boldsymbol{z})\) 
                    and rearranging.
                </p>

                <div class="theorem">
                    <span class="theorem-title">Theorem: ELBO Decomposition</span>
                    <p>
                        \[
                        L(\boldsymbol{\theta}, \, \boldsymbol{\phi}\mid \boldsymbol{x}) 
                        = \underbrace{\mathbb{E}_{q_{\boldsymbol{\phi}} (\boldsymbol{z} \mid \boldsymbol{x})} \left[\log p_{\boldsymbol{\theta}} (\boldsymbol{x} \mid \boldsymbol{z})\right]}_{\text{reconstruction}}
                            \;-\; \underbrace{D_{\mathrm{KL}}\!\left(q_{\boldsymbol{\phi}} (\boldsymbol{z} \mid \boldsymbol{x}) \,\|\, p(\boldsymbol{z})\right)}_{\text{regularization}}
                        \]
                    </p>
                </div>

                <div class="proof">
                    <span class="proof-title">Proof:</span>
                    <p>
                        Expanding the joint inside the ELBO definition,
                        \[
                        \begin{align*}
                        L(\boldsymbol{\theta}, \, \boldsymbol{\phi}\mid \boldsymbol{x}) 
                        &= \mathbb{E}_{q_{\boldsymbol{\phi}}(\boldsymbol{z} \mid \boldsymbol{x})} \left[\log p_{\boldsymbol{\theta}}(\boldsymbol{x} \mid \boldsymbol{z}) + \log p(\boldsymbol{z}) - \log q_{\boldsymbol{\phi}}(\boldsymbol{z} \mid \boldsymbol{x})\right] \\\\
                        &= \mathbb{E}_{q_{\boldsymbol{\phi}} (\boldsymbol{z} \mid \boldsymbol{x})} \left[\log p_{\boldsymbol{\theta}} (\boldsymbol{x} \mid \boldsymbol{z})\right]
                            + \mathbb{E}_{q_{\boldsymbol{\phi}}(\boldsymbol{z} \mid \boldsymbol{x})} \left[\log \frac{p(\boldsymbol{z})}{q_{\boldsymbol{\phi}}(\boldsymbol{z} \mid \boldsymbol{x})}\right] \\\\
                        &= \mathbb{E}_{q_{\boldsymbol{\phi}} (\boldsymbol{z} \mid \boldsymbol{x})} \left[\log p_{\boldsymbol{\theta}} (\boldsymbol{x} \mid \boldsymbol{z})\right]
                            - D_{\mathrm{KL}}\!\left(q_{\boldsymbol{\phi}} (\boldsymbol{z} \mid \boldsymbol{x}) \,\|\, p(\boldsymbol{z})\right).
                        \end{align*}
                        \]
                    </p>
                </div>

                <p>
                    The first term is the <strong>expected reconstruction log-likelihood</strong>: it encourages the decoder to reconstruct 
                    the input \(\boldsymbol{x}\) from latent samples \(\boldsymbol{z} \sim q_{\boldsymbol{\phi}}(\boldsymbol{z} \mid \boldsymbol{x})\). 
                    The second term is the <strong>KL divergence</strong> between the approximate posterior and the prior 
                    \(p(\boldsymbol{z}) = \mathcal{N}(\boldsymbol{0}, \boldsymbol{I})\), which regularizes the latent space by penalizing 
                    approximate posteriors that deviate from the prior. This regularization is what gives the VAE its structured, 
                    continuous latent space - without it, the encoder could collapse each data point to an isolated delta function, 
                    losing the ability to generate new data by sampling from \(p(\boldsymbol{z})\).
                </p>

                <h3>The Reparameterization Trick</h3>
                <p>
                    To train the VAE end-to-end via gradient-based optimization, we need to differentiate the ELBO with respect to 
                    both \(\boldsymbol{\theta}\) and \(\boldsymbol{\phi}\). The gradient with respect to the decoder parameters 
                    \(\boldsymbol{\theta}\) poses no difficulty, since \(\boldsymbol{\theta}\) appears only inside the expectation. 
                    However, differentiating with respect to the encoder parameters \(\boldsymbol{\phi}\) is problematic: the expectation 
                    \(\mathbb{E}_{q_{\boldsymbol{\phi}}(\boldsymbol{z} \mid \boldsymbol{x})}[\cdot]\) is taken with respect to a distribution 
                    that itself depends on \(\boldsymbol{\phi}\), so we cannot simply interchange the gradient and the expectation.
                </p>
                <p>
                    The <strong>reparameterization trick</strong> resolves this by expressing the stochastic latent variable 
                    \(\boldsymbol{z}\) as a deterministic, differentiable transformation of a parameter-free noise variable. 
                    Since the encoder outputs a diagonal Gaussian, we write
                </p>

                <div class="theorem">
                    <span class="theorem-title">Reparameterization Trick:</span>
                    <p>
                        Sample \(\boldsymbol{z}\) from \(q_{\boldsymbol{\phi}}(\boldsymbol{z} \mid \boldsymbol{x})\) via
                        \[
                        \boldsymbol{z} = \mu_{\boldsymbol{\phi}}(\boldsymbol{x}) + \sigma_{\boldsymbol{\phi}} (\boldsymbol{x})\odot \boldsymbol{\epsilon}, 
                        \quad \boldsymbol{\epsilon} \sim \mathcal{N}(\boldsymbol{0}, \boldsymbol{I})
                        \]
                        where \(\mu_{\boldsymbol{\phi}} = f_{e,\mu}(\boldsymbol{x}; \boldsymbol{\phi})\) and 
                        \(\sigma_{\boldsymbol{\phi}} = f_{e,\sigma}(\boldsymbol{x}; \boldsymbol{\phi})\) are the encoder outputs, 
                        and \(\odot\) denotes element-wise multiplication.
                    </p>
                </div>

                <p>
                    Under this reparameterization, the ELBO becomes
                    \[
                    L(\boldsymbol{\theta}, \, \boldsymbol{\phi}\mid \boldsymbol{x})
                    = \mathbb{E}_{\boldsymbol{\epsilon} \sim \mathcal{N}(\boldsymbol{0}, \boldsymbol{I})}
                      \!\left[\log p_{\boldsymbol{\theta}}\!\left(\boldsymbol{x} \mid \boldsymbol{z} = \mu_{\boldsymbol{\phi}}(\boldsymbol{x}) + \sigma_{\boldsymbol{\phi}}(\boldsymbol{x})\odot \boldsymbol{\epsilon}\right)\right]
                      - D_{\mathrm{KL}}\!\left(q_{\boldsymbol{\phi}}(\boldsymbol{z} \mid \boldsymbol{x}) \,\|\, p(\boldsymbol{z})\right).
                    \]
                    The key observation is that the expectation is now taken with respect to the fixed distribution 
                    \(\mathcal{N}(\boldsymbol{0}, \boldsymbol{I})\), which does not depend on \(\boldsymbol{\phi}\). This means we 
                    can interchange the gradient and the expectation: 
                    \(\nabla_{\boldsymbol{\phi}} \mathbb{E}_{\boldsymbol{\epsilon}}[\cdot] = \mathbb{E}_{\boldsymbol{\epsilon}}[\nabla_{\boldsymbol{\phi}}(\cdot)]\), 
                    and estimate the gradient via Monte Carlo sampling of \(\boldsymbol{\epsilon}\). In practice, even a single sample 
                    per data point provides a sufficiently low-variance gradient estimate for stochastic gradient descent.
                </p>
                <p>
                    The full VAE training objective over a dataset \(\mathcal{D}\) is therefore
                    \[
                    \min_{\boldsymbol{\theta}, \, \boldsymbol{\phi}} \;-\, \mathbb{E}_{\boldsymbol{x} \sim \mathcal{D}} \left[ L(\boldsymbol{\theta}, \, \boldsymbol{\phi}\mid \boldsymbol{x}) \right]
                    \]
                    which is optimized end-to-end using standard backpropagation through the reparameterized sampling step.
                </p>

                <h3>Putting It All Together</h3>
                <p>
                    The components developed above - the nonlinear generative model, the amortized inference network, the ELBO objective, 
                    and the reparameterization trick - collectively define the Variational Autoencoder.
                </p>

                <div class="theorem">
                    <span class="theorem-title">Variational Autoencoder (VAE):</span>
                    <p>
                        A VAE is a latent variable model consisting of:
                    </p>
                    <ol style="padding-left: 40px;">
                        <li>A <strong>prior</strong> over the latent space: \(p(\boldsymbol{z}) = \mathcal{N}(\boldsymbol{0}, \boldsymbol{I})\)</li>
                        <li>A <strong>decoder</strong> (generative model): \(p_{\boldsymbol{\theta}}(\boldsymbol{x} \mid \boldsymbol{z}) = \mathcal{N}(\boldsymbol{x} \mid f_d(\boldsymbol{z}; \boldsymbol{\theta}), \, \sigma^2 \boldsymbol{I})\)</li>
                        <li>An <strong>encoder</strong> (inference network): \(q_{\boldsymbol{\phi}}(\boldsymbol{z} \mid \boldsymbol{x}) = \mathcal{N}\!\left(\boldsymbol{z} \mid f_{e,\mu}(\boldsymbol{x}; \boldsymbol{\phi}), \, \text{diag}(f_{e,\sigma}(\boldsymbol{x}; \boldsymbol{\phi}))\right)\)</li>
                    </ol>
                    <p>
                        The parameters \(\boldsymbol{\theta}\) and \(\boldsymbol{\phi}\) are trained jointly by maximizing the ELBO
                        \[
                        \max_{\boldsymbol{\theta}, \, \boldsymbol{\phi}} \; \mathbb{E}_{\boldsymbol{x} \sim \mathcal{D}} \left[ 
                            \mathbb{E}_{\boldsymbol{\epsilon} \sim \mathcal{N}(\boldsymbol{0}, \boldsymbol{I})}
                            \!\left[\log p_{\boldsymbol{\theta}}\!\left(\boldsymbol{x} \mid \boldsymbol{z} = \mu_{\boldsymbol{\phi}}(\boldsymbol{x}) + \sigma_{\boldsymbol{\phi}}(\boldsymbol{x}) \odot \boldsymbol{\epsilon}\right)\right]
                            - D_{\mathrm{KL}}\!\left(q_{\boldsymbol{\phi}}(\boldsymbol{z} \mid \boldsymbol{x}) \,\|\, p(\boldsymbol{z})\right)
                        \right]
                        \]
                        where gradients with respect to both \(\boldsymbol{\theta}\) and \(\boldsymbol{\phi}\) are computed via 
                        backpropagation through the reparameterized sampling step.
                    </p>
                </div>
             
            </section>
           
            


            <section id="demo" class="section-content">
                <h2>VAE Uncertainty in Robotic Manipulation</h2>

                <h3>Overview</h3>
                <p>
                    This simulation demonstrates how a <strong>Variational Autoencoder (VAE)</strong> can be used as a real-time safety monitor during 
                    robotic manipulation. A 2-link planar arm attempts to lift a box with an off-center mass. The VAE's posterior distribution 
                    \(q(\mathbf{z} \mid \mathbf{x})\) encodes the robot's internal uncertainty about the physical state of the grasp. When this uncertainty exceeds a 
                    learned threshold, the system aborts the lift to prevent a catastrophic drop.
                </p>

                <h3>The Physical Setup</h3>
                <p>
                    The robot is a <strong>3-DOF articulated arm</strong>. While its spatial positioning is achieved via a rotating base and two revolute joints, 
                    its reaching kinematics are governed by two primary links with lengths \(L_1 = 24\) and \(L_2 = 22\). The end-effector pose is 
                    calculated through <strong>analytic inverse kinematics</strong> that maps 3D target coordinates \((x, y, z)\) to the arm's joint angles.
                </p>
                
                <p>
                    A rigid box of adjustable mass \(m\) sits on the ground plane. Its <strong>center of mass (CoM)</strong> 
                    is offset from its geometric center by a user-controlled displacement \(\mathbf{c} = (c_x, c_y, c_z)\). 
                    This offset serves as the primary source of <strong>epistemic uncertainty</strong>: the robot 
                    cannot directly observe the true CoM location and must infer the resulting torque anomalies 
                    from sensor feedback during the initial lift phase.
                </p>

                <h3>Torque as Sensory Input</h3>
                <p>
                    Once the gripper secures the box and begins to lift, gravity acting on the offset CoM produces a <strong>torque</strong> about 
                    the grip point:
                </p>
                <p>
                    \[
                    \boldsymbol{\tau} = (\mathbf{r}_{\text{CoM}} - \mathbf{r}_{\text{grip}}) \times (-mg\,\hat{\mathbf{y}})
                    \]
                </p>
                <p>
                    where \(\mathbf{r}_{\text{CoM}}\) is the world-space CoM position and \(\mathbf{r}_{\text{grip}}\) is the grip point. 
                    This torque is the robot's primary sensory signal - it reveals how much the CoM deviates from the grip axis. 
                    The torque vector is displayed as a green arrow on the box during the lift.
                </p>

                <h3>Rotational Dynamics</h3>
                <p>
                    The torque drives the box's rotational dynamics via the damped Euler equation:
                </p>
                <p>
                    \[
                    I\,\dot{\boldsymbol{\omega}} = \boldsymbol{\tau} - C_d\,\boldsymbol{\omega}
                    \]
                </p>
                <p>
                    where \(I\) is the moment of inertia, \(\boldsymbol{\omega}\) is the angular velocity, and \(C_d\) is the damping coefficient. 
                    This produces the visible tilt of the box during lifting. Higher mass or larger CoM offset leads to larger torques, faster rotation, 
                    and more pronounced tilt - all signals that feed into the VAE.
                </p>

                <h3>VAE Posterior: \(q(\mathbf{z} \mid \mathbf{x})\)</h3>
                <p>
                    The sensory input \(\mathbf{x}\) (here, the torque feedback) is encoded into a 2D latent space \(\mathbf{z} = (z_1, z_2)\) via a 
                    diagonal Gaussian posterior:
                </p>
                <p>
                    \[
                     q(\mathbf{z} \mid \mathbf{x}) = \mathcal{N}(\mathbf{z};\; \boldsymbol{\mu}(\mathbf{x}),\; \sigma(\mathbf{x})^2 \mathbf{I})
                    \]
                </p>
                <p>
                    The encoder maps torque to the posterior parameters as follows:
                </p>
                <ul style="padding-left: 40px;">
                    <li>\(\boldsymbol{\mu} = (0.07\,\tau_x,\; 0.07\,\tau_z)\) - the posterior mean shifts proportionally to the horizontal torque components</li>
                    <li>\(\sigma = 0.011\,\|\boldsymbol{\tau}\|\) - the posterior spread grows with the torque magnitude</li>
                </ul>
                <p>
                    The key insight is that <strong>\(\sigma\) encodes uncertainty</strong>. 
                    When the torque is small (CoM near grip axis), \(\sigma\) is small and the robot is confident in its grasp. 
                    When the torque is large (CoM far off-center), \(\sigma\) grows, reflecting the robot's increasing uncertainty 
                    about whether the grasp is stable enough to complete the lift.
                </p>

                <h3>The Latent Space Visualization</h3>
                <p>
                    The top-right canvas shows the 2D latent space in real time:
                </p>
                <ul style="padding-left: 40px;">
                    <li><strong style="color:#64b4ff;">Blue dot</strong> at the origin - the prior \(p(\mathbf{z}) = \mathcal{N}(\mathbf{0}, \mathbf{I})\)</li>
                    <li><strong style="color:#ff9800;">Orange dot</strong> - the posterior mean \(\boldsymbol{\mu}\), which drifts as torque changes</li>
                    <li><strong style="color:#ff9800;">Orange circle</strong> - the 1 \(\sigma\)  contour of the posterior distribution</li>
                    <li><strong style="color:#ffa726;">Orange scatter points</strong> - stochastic samples \(\mathbf{z}^{(i)} \sim q(\mathbf{z} \mid \mathbf{x})\), visualized as ghost arms in 3D</li>
                    <li><strong style="color:#ff5252;">Red dashed circle</strong> - the abort threshold on \(\sigma\) </li>
                </ul>
                <p>
                    The display uses <strong>dynamic scaling</strong>: both the \(\sigma\)  circle and the threshold circle always fit 
                    within the canvas, preserving their true ratio. When the lift fails, the orange circle is visibly larger than the 
                    red circle, making the safety violation immediately apparent.
                </p>

                <h3>KL Divergence</h3>
                <p>
                    The telemetry panel also reports the KL divergence between the posterior and prior:
                </p>
                <p>
                    \[
                    D_{\mathrm{KL}}\!\left[\,q(\mathbf{z} \mid \mathbf{x})\;\|\;p(\mathbf{z})\,\right]
                     = \frac{1}{2}\!\left(\|\boldsymbol{\mu}\|^2 + 2\sigma^2 - 2\ln\sigma^2 - 2\right)
                    \]
                </p>
                <p>
                    This quantity measures how far the posterior has diverged from the prior. A large KL divergence indicates that the 
                    sensory feedback has pushed the robot's internal representation far from its initial beliefs - a sign of an unusual 
                    or difficult grasp configuration.
                </p>

                <h3>Ghost Arms: Sampling Uncertainty</h3>
                <p>
                    The five semi-transparent <strong>ghost arms</strong> visualize samples from the posterior. Each ghost arm \(i\) receives a 
                    perturbed end-effector target:
                </p>
                <p>
                    \[
                    \mathbf{t}^{(i)} = \mathbf{t}_{\text{primary}} + \boldsymbol{\mu} \cdot s_{\mu} + \sigma \cdot \boldsymbol{\epsilon}^{(i)} \cdot s_{\sigma}
                    \]
                </p>
                <p>
                    where \(\boldsymbol{\epsilon}^{(i)} \sim \mathcal{N}(\mathbf{0}, \mathbf{I})\) are smoothed Gaussian samples 
                    (using exponential moving average with \(\alpha = 0.92\) for visual stability). When \(\sigma\) is small, the ghosts cluster 
                    tightly around the primary arm. When \(\sigma\) is large, they splay outward, creating a visual "cloud of possible robot states" 
                    that communicates the degree of uncertainty.
                </p>

                <h3>The Safety Decision</h3>
                <p>
                    The lift sequence proceeds through a state machine:
                </p>
                <ol style="padding-left: 40px;">
                    <li><strong>APPROACH</strong> &rarr; Arm moves from home to hover above the box</li>
                    <li><strong>DESCEND</strong> &rarr; Arm lowers to grasp height</li>
                    <li><strong>GRASP</strong> &rarr; Fingers close on the box</li>
                    <li><strong>PRE_LIFT</strong> &rarr; Small test lift (5 units). During this phase, the system assesses \(\sigma\) for 0.5 seconds. If \(\sigma > \sigma_{\text{thr}}\), the lift is aborted immediately.</li>
                    <li><strong>LIFT_OK</strong> &rarr; Full lift if \(\sigma\) stayed below threshold. Continuous safety monitoring continues; an abort can still trigger if \(\sigma\) spikes during the lift.</li>
                    <li><strong>ABORT</strong> &rarr; Three-phase emergency: lower the box, release grip, return home.</li>
                </ol>
                <p>
                    The critical decision is the comparison \(\sigma \lessgtr \sigma_{\text{thr}}\). This is a direct analogy to how real-world 
                    autonomous systems use learned uncertainty estimates for <strong>out-of-distribution detection</strong>: if the VAE posterior 
                    is too diffuse, the current observation lies far from the training distribution, and the robot should not trust its control policy.
                </p>

                <h3>What the Parameters Control</h3>
                <p>
                    <strong>CoM Offset</strong> \((c_x, c_y, c_z)\): Shifts the center of mass away from the box's 
                    geometric center. Larger offsets produce larger torques, higher \(\sigma\) and a higher likelihood of abort. This simulates 
                    real-world scenarios where the load distribution inside a package is unknown.
                </p>
                <p>
                    <strong>Box Mass</strong>: Scales the gravitational force \(mg\), amplifying the torque for a given CoM offset. Heavy objects 
                    are harder to lift safely.
                </p>
                <p>
                    <strong>\(\sigma\) Threshold</strong>: The abort boundary. Lowering it makes the robot more cautious (aborting earlier); 
                    raising it makes the robot more risk-tolerant. This models the engineering trade-off between safety and task completion 
                    in autonomous systems.
                </p>
                <p>
                    <strong>Damping</strong> \(C_d\): Controls how quickly the box's rotational oscillation decays. High damping suppresses 
                    wobble; low damping allows the box to swing more freely, producing a more dynamic and potentially uncertain lift.
                </p>

                <h3>Key Takeaways</h3>
                <p>
                    This demo illustrates three interconnected ideas from modern <strong>physical AI</strong>:
                </p>
                <p>
                    <strong>1. Uncertainty quantification through latent representations:</strong><br>
                    The VAE does not output a single point estimate of the physical state - it outputs a <em>distribution</em>. 
                    The spread of this distribution (\(\sigma\)) is a principled measure of epistemic uncertainty derived from sensory feedback.
                </p>
                <p>
                    <strong>2. Safety-critical decision-making under uncertainty:</strong><br>
                    Rather than blindly executing a motion plan, the robot uses its uncertainty estimate as a gating signal. When \(\sigma\) exceeds 
                    the threshold, the system recognizes that it is operating outside its confident regime and aborts. This is directly analogous to 
                    uncertainty-aware reinforcement learning and Bayesian safe control.
                </p>
                <p>
                    <strong>3. The connection between physics and information:</strong><br>
                    The torque - a purely physical quantity governed by Newton's laws - becomes the input to a probabilistic inference engine. 
                    The KL divergence between posterior and prior quantifies how much information the robot has gained (or how surprised it is) 
                    from the physical interaction. Large KL divergence means the physical situation deviates significantly from the robot's expectations.
                </p>
                
                <div id="simulation-container"></div>

            </section>

        </div>

        <script src="/js/main.js"></script> 
        <script src="/js/sec5_p12_vae_uncertainty.js"></script>

    </body>
</html>



