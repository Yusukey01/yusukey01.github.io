---
layout: default
title: Regression
level: detail
description: Learn about regression methods such as ridge regression and Lasso regression.
uses_math: true
uses_python: false
---
<!DOCTYPE html>
<html>
    <body> 
        <div class="hero-section">
            <h1 class="webpage-name">Regression
            </h1>
        </div>

        <div class="topic-nav">
            <a href="#ridge">Ridge Regression</a></li>
            <a href="#lasso">Lasso Regression</a>
            <a href="#Demo">Demo: Ridge Regression</a>
            <a href="#CV">Cross Validation</a>
            <a href="#b-v">Bias-Variance Tradeoff in Ridge Regression</a>
        </div> 

        <div class="container">  
            <section id="ridge" class="section-content">
                <h2>Ridge Regression</h2>
                <p>
                    Let's first review <a href="../Probability/linear_regression.html"><strong>linear regression</strong></a>. 
                    Suppose we are given observed data points \({\{(x_i, y_i)\}_{i = 1}^n}\), where each \(x_i \in \mathbb{R}^d\) is a 
                    feature vector, and \(y_i \in \mathbb{R}\) is the corresponding target value. The linear regression model assumes a 
                    linear relationship:
                    \[
                    y = XW + \epsilon, 
                    \]
                    where \(X \in \mathbb{R}^{n \times d}\) is the design matrix, \(W \in \mathbb{R}^d\) is the weight vector to be estimated, 
                    and \(\epsilon \sim \mathcal{N}(0, \sigma^2 I)\) is a Gaussian noise vector.
                </p>
                   
                <p>
                    In linear regression, the <a href="../Probability/mle.html"><strong>maximum likelihood estimator (MLE)</strong></a> 
                    of \(W\) coincides with the <a href="../Linear_algebra/leastsquares.html"><strong>least-squares solution</strong></a>: 
                    \[
                    \begin{align*}
                    \widehat{W}_{LS}    &= \arg \min_W \sum_{i=1}^n (y_i - {x_i}^T W )^2 \\\\
                                        &= \arg \min_W \|y - XW\|_2^2 \\\\
                                        &= \arg \min_W (y - XW)^T (y - XW) \\\\
                                        &= \arg \min_W y^T y -y^T XW - W^T X^T y + W^T X^T X W \\\\
                                        &= \arg \min_W f(W) \\\\
                    \end{align*}
                    \]
                    
                    Differentiating with respect to \(W\) and setting the gradient to zero:
                    \[
                    \begin{align*}
                        \nabla f(W) &= -X^T y -X^T y + 2X^T XW \\\\
                                    &= -2X^T y + 2 X^T X W  = 0 
                    \end{align*}
                    \]
                    Thus, if \((X^TX)^{-1}\) exists,
                    \[
                    \begin{align*}
                    \widehat{W}_{LS} &= (X^TX)^{-1}X^Ty \\\\
                                     &= \widehat{W}_{MLE}.
                    \end{align*}
                    \]
                </p>

                <p>
                    However, when \(d \gg n\), the matrix \(X^T X\) becomes ill-conditioned or singular. This leads to a flat objective 
                    surface in some directions, making the solution non-unique and highly sensitive to noise â€” a phenomenon known as 
                    <strong>overfitting</strong>.
                </p>

                <p>     
                    To mitigate this issue, we add a penalty term on the objective. This technique is called <strong>regularization</strong>, 
                    and the modified version of linear regression is known as <strong>ridge regression</strong>:
                    \[
                    \begin{align*}
                    \widehat{W}_{ridge} &= \arg \min_W \sum_{i=1}^n (y_i - {x_i}^T W )^2 +  \lambda \|W\|_2^2.\\\\
                                        &=  \arg \min_W \|y - XW\|_2^2 + \lambda W^T W \\\\
                                        &= \arg \min_W (XW -y)^T (XW-y)+\lambda W^T W \\\\
                                        &= \arg \min_W y^Ty -2W^T Xy +W^T X^T XW + W^T (\lambda I)W \\\\
                                        &= \arg \min_W y^Ty -2W^T Xy +W^T  (X^T X + \lambda I)W \\\
                                        &= \arg \min_W J(W)            
                    \end{align*}
                    \]
                    Then
                    \[
                    \nabla_W J(W) = -2X^T y +2(X^T X + \lambda I )W = 0
                    \]
                    Solve this equation for \(W\), we botain 
                    \[
                    \widehat{W}_{ridge} = (X^T X + \lambda I)^{-1}X^T y.
                    \]
                </p>

                <p>
                    The hyperparameter \(\lambda \geq 0\) controls the strength of the regularization:
                    <ul style="padding-left: 40px;">
                        <li>As \(\lambda \to 0\), the solution approaches the least-squares solution: \(\widehat{W}_{ridge} \to \widehat{W}_{LS}\).</li>
                        <li>As \(\lambda \to \infty\), the solution is pushed toward zero: \(\widehat{W}_{ridge} \to 0\), leading to high bias but low variance.</li>
                    </ul>
                    <br>
                    This illustrates the classic <strong>bias-variance tradeoff</strong> in statistical learning. 
                </p>

            </section>

            <section id="Demo" class="section-content">
              
                
                <h2>Demo: Ridge Regression</h2>
                <div id="ridge_regression_visualizer"></div>
                <p>
                    Note: Tiny regularization \(\lambda = 0.0001\) has been added on linear regression for numerical stability.
                </p>

                <p> 
                    After running the demo above, we observe that linear regression tends to achieve lower training error but at the cost of higher test error and wildly large 
                    coefficients, especially with high polynomial degrees. This means the model memorizes the training data too closely and fails to generalize 
                    well to new, unseen data. 
                </p>
                <p>
                    In contrast, ridge regression, by constraining the weights, may have slightly higher training error but typically generalizes 
                    better to new data. This trade-off between fitting the training data and maintaining model simplicity is known as the <strong>bias-variance tradeoff</strong>. 
                    Increasing the model complexity (e.g., by raising the polynomial degree) monotonically reduces training error, but <strong>after a certain point, the test error begins to increase.</strong>
                    This turning point highlights the danger of <strong>overfitting</strong>.
                </p>
            </section>

            <section id="CV" class="section-content">
                <h2>Cross Validation</h2>
                <p>
                    To build a <strong>generalized</strong> model and select an appropriate <strong>regularization</strong> strength (\(\lambda\)), 
                    we employ <strong>K-fold cross-validation (CV)</strong> on the <strong>training set</strong>. The training data is partitioned into \(K\) 
                    equal-sized folds. For each fold \(k \in \{1, \cdots, K\}\), the model is trained on the \(K - 1\) other folds and validated on the \(k\)th fold. 
                    This process is repeated \(K\) times, ensuring that each data point in the training set is used once for validation. The average validation 
                    error is used to select the optimal hyperparameter \(\lambda\). Finally, the model is evaluated on the <strong>held-out test set</strong>.
                </p>

                <p>
                    Here, we demonstrate how k-fold CV is used to tune the regularization parameter \(\lambda\) in Ridge Regression.
                </p>
               
                <p>
                    An special case of K-fold CV is <strong>Leave-One-Out Cross-Validation (LOOCV)</strong>, where the number of folds \(K\) equals the 
                    number of data points in the training set. That is, the model is trained on all training data points except one, which is used for validation, 
                    and this process is repeated for each data point. Thus, LOOCV tends to produce a low-bias estimate of the validation error. However, 
                    LOOCV is computationally expensive for large datasets. In contrast, smaller values of \(K\) (like 5 ~ 10) offer a good tradeoff between bias, 
                    variance, and computational cost, making them a popular choice in practice.
                </p>

                <div id="cross_validation_visualizer"></div>

            </section>

            <section id="b-v">
                <h2>Bias-Variance Tradeoff in Ridge Regression</h2>
                
                </section>

            <section id="LASSO" class="section-content">
                <h2>Lasso Regression</h2>
                    
            </section>

    

        
            
        </div>
        <script src="/js/main.js"></script> 
        <script src="/js/ridge_regression.js"></script>
        <script src="/js/cross_validation.js"></script>
    </body>
</html>