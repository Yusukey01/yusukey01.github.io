---
layout: default
title: Regression
level: detail
description: Learn about regression methods such as ridge regression and Lasso regression.
uses_math: true
uses_python: false
---
<!DOCTYPE html>
<html>
    <body> 
        <div class="hero-section">
            <h1 class="webpage-name">Regression
            </h1>
        </div>

        <div class="topic-nav">
            <a href="#ridge">Ridge Regression</a></li>
            <a href="#lasso">Lasso Regression</a>
            <a href="#"></a>
            <a href="#"></a>
        </div> 

        <div class="container">  
            <section id="ridge" class="section-content">
                <h2>Ridge Regression</h2>
                <p>
                    In <a href="../Probability/linear_regression.html"><strong>linear regression</strong></a>, 
                    if \(x_i \in \mathbb{R}^d\) and \(d >> n\), then the objective function becomes flat in some directions. So, 
                    the optimal solution is NOT unique and unstable due to lack of curvature. This means that small changes in training data 
                    result in large changes in solution. 
                    Recall that
                    \[
                    \begin{align*}
                    \hat{W}_{ls} &= \arg \min_W \sum_{i=1}^n (y_i - {x_i}^T W )^2 \\\\
                                 &= \arg \min_W (y - XW)^T (y - XW) \\\\
                                 &= (X^TX)^{-1}X^Ty.
                    \end{align*}
                    \]
                    In general, 
                    \[
                    \hat{W}_{ls} &= \arg \min_W W^T (X^TX)W-2y^TXW 
                    \]
                    In most cases, the empirical distribution is not the same as the true distribution. This is overfitting. To avoid ovefitting, 
                    simply we add a penalty term(this is called regularization). This is ridge regression. Now, we have 
                    \[
                    \begin{align*}
                    \hat{W}_{ridge} &= \arg \min_W \sum_{i=1}^n (y_i - {x_i}^T W )^2 + \lambda {\| W \|_2}^2 \\\\
                                    &= \arg \min_W (XW -y)^T (XW-y)+\lambda W^T W \\\\
                                    &= \arg \min_W y^Ty -2W^T Xy +W^T X^T XW + W^T (\lambda I)W \\\\
                                    &= \arg \min_W y^Ty -2W^T Xy +W^T  (X^T X + \lambda I)W \\\
                                    &= \arg \min_W J(W) 
                                
                    \end{align*}
                    \]
                    Then
                    \[
                    \nabla_W J(W) = -2X^T y +2(X^T X + \lambda I )W = 0
                    \]
                    Solve this equation for \(W\), we botain 
                    \[
                    \hat{W}_{ridge} = (X^T X + \lambda I)^{-1}X^T y.
                    \]
                    \(\lambda\) represents the strength of the regularizer. Larger \(\lambda\) gives more bias and then decreases 
                    variance. In other words, as \(\lambda \to \infty\), \( \hat{W_{ridge}} \to 0\). On the other hand,  
                    smaller \(\lambda\) gives less bias and then increases variance. In other words, as \(\lambda \to 0\), 
                    \( \hat{W}_{ridge} \to \hat{W_{LS}}\).
                </p>
               


                
                <a href="../Probability/mle.html"><strong>Maximum Likelihood Estimation</strong></a> 
                <a href="../Linear_algebra/leastsquares.html"><strong>least-squares</strong></a>  
               
            </section>

            <section id="" class="section-content">
                <h2>Lasso Regression</h2>
                    
            </section>

            <section id="" class="section-content">
                <h2></h2>
                    
            </section>

            <section id="" class="section-content">
                <h2></h2>
                    
            </section>

    

        
            
        </div>
        <script src="/js/main.js"></script> 
    </body>
</html>