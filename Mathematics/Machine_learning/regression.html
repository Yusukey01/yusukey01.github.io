---
layout: default
title: Regression
level: detail
description: Learn about regression methods such as ridge regression and Lasso regression.
uses_math: true
uses_python: false
---
<!DOCTYPE html>
<html>
    <body> 
        <div class="hero-section">
            <h1 class="webpage-name">Regression
            </h1>
        </div>

        <div class="topic-nav">
            <a href="#ridge">Ridge Regression</a></li>
            <a href="#lasso">Lasso Regression</a>
            <a href="#"></a>
            <a href="#"></a>
        </div> 

        <div class="container">  
            <section id="ridge" class="section-content">
                <h2>Ridge Regression</h2>
                <p>
                    Let's first review <a href="../Probability/linear_regression.html"><strong>linear regression</strong></a>. 
                    Suppose we are given observed data points \({\{(x_i, y_i)\}_{i = 1}^n}\), where each \(x_i \in \mathbb{R}^d\) is a 
                    feature vector, and \(y_i \in \mathbb{R}\) is the corresponding target value. The linear regression model assumes a 
                    linear relationship:
                    \[
                    y = XW + \epsilon, 
                    \]
                    where \(X \in \mathbb{R}^{n \times d}\) is the design matrix, \(W \in \mathbb{R}^d\) is the weight vector to be estimated, 
                    and \(\epsilon \sim \mathcal{N}(0, \sigma^2 I)\) is a Gaussian noise vector.
                </p>
                   
                <p>
                    In linear regression, the <a href="../Probability/mle.html"><strong>maximum likelihood estimator (MLE)</strong></a> 
                    of \(W\) coincides with the <a href="../Linear_algebra/leastsquares.html"><strong>least-squares solution</strong></a>: 
                    \[
                    \begin{align*}
                    \widehat{W}_{LS}    &= \arg \min_W \sum_{i=1}^n (y_i - {x_i}^T W )^2 \\\\
                                        &= \arg \min_W \|y - XW\|_2^2 \\\\
                                        &= \arg \min_W (y - XW)^T (y - XW) \\\\
                                        &= \arg \min_W y^T y -y^T XW - W^T X^T y + W^T X^T X W \\\\
                                        &= \arg \min_W f(W) \\\\
                    \end{align*}
                    \]
                    
                    Differentiating with respect to \(W\) and setting the gradient to zero:
                    \[
                    \begin{align*}
                        \nabla f(W) &= -X^T y -X^T y + 2X^T XW \\\\
                                    &= -2X^T y + 2 X^T X W  = 0 
                    \end{align*}
                    \]
                    Thus, if \((X^TX)^{-1}\) exists,
                    \[
                    \begin{align*}
                    \widehat{W}_{LS} &= (X^TX)^{-1}X^Ty \\\\
                                     &= \widehat{W}_{MLE}.
                    \end{align*}
                    \]
                </p>

                <p>
                    However, when \(d \gg n\), the matrix \(X^T X\) becomes ill-conditioned or singular. This leads to a flat objective 
                    surface in some directions, making the solution non-unique and highly sensitive to noise â€” a phenomenon known as 
                    <strong>overfitting</strong>.
                </p>

                <p>     
                    To mitigate this issue, we add a penalty term on the objective. This technique is called <strong>regularization</strong>, 
                    and the modified version of linear regression is known as <strong>ridge regression</strong>:
                    \[
                    \begin{align*}
                    \widehat{W}_{ridge} &= \arg \min_W \sum_{i=1}^n (y_i - {x_i}^T W )^2 +  \lambda \|W\|_2^2.\\\\
                                        &=  \arg \min_W \|y - XW\|_2^2 + \lambda W^T W \\\\
                                        &= \arg \min_W (XW -y)^T (XW-y)+\lambda W^T W \\\\
                                        &= \arg \min_W y^Ty -2W^T Xy +W^T X^T XW + W^T (\lambda I)W \\\\
                                        &= \arg \min_W y^Ty -2W^T Xy +W^T  (X^T X + \lambda I)W \\\
                                        &= \arg \min_W J(W)            
                    \end{align*}
                    \]
                    Then
                    \[
                    \nabla_W J(W) = -2X^T y +2(X^T X + \lambda I )W = 0
                    \]
                    Solve this equation for \(W\), we botain 
                    \[
                    \widehat{W}_{ridge} = (X^T X + \lambda I)^{-1}X^T y.
                    \]
                </p>

                <p>
                    The hyperparameter \(\lambda \geq 0\) controls the strength of the regularization:
                    <ul style="padding-left: 40px;">
                        <li>As \(\lambda \to 0\), the solution approaches the least-squares solution: \(\widehat{W}_{ridge} \to \widehat{W}_{LS}\).</li>
                        <li>As \(\lambda \to \infty\), the solution is pushed toward zero: \(\widehat{W}_{ridge} \to 0\), leading to high bias but low variance.</li>
                    </ul>
                    This illustrates the classic <strong>bias-variance tradeoff</strong> in statistical learning.
                </p>

                <p>
                    So, ridge regression helps control variance by penalizing weights. As the regularization parameter \(\lambda\) increases:
                    <ul>
                      <li>The model becomes simpler (weights are shrunk), so bias increases</li>
                      <li>The model becomes more stable across different datasets (variance decreases)</li>
                    </ul>
                    The optimal value of \(\lambda\) balances these two sources of error to minimize overall prediction error.
                </p>

            </section>

            <section id="" class="section-content">
                <h2>Lasso Regression</h2>
                    
            </section>

            <section id="" class="section-content">
                <h2></h2>
                    
            </section>

            <section id="" class="section-content">
                <h2></h2>
                    
            </section>

    

        
            
        </div>
        <script src="/js/main.js"></script> 
    </body>
</html>