---
layout: default
title: Regression
level: detail
description: Learn about regression methods such as ridge regression and Lasso regression.
uses_math: true
uses_python: false
---
<!DOCTYPE html>
<html>
    <body> 
        <div class="hero-section">
            <h1 class="webpage-name">Regression
            </h1>
        </div>

        <div class="topic-nav">
            <a href="#ridge">Ridge Regression</a></li>
            <a href="#lasso">Lasso Regression</a>
            <a href="#"></a>
            <a href="#"></a>
        </div> 

        <div class="container">  
            <section id="ridge" class="section-content">
                <h2>Ridge Regression</h2>
                <p>
                    Let's first review <a href="../Probability/linear_regression.html"><strong>linear regression</strong></a>. 
                    Suppose we are given observed data points \({\{(x_i, y_i)\}_{i =1}}^n\), , where each \(x_i \in \mathbb{R}^d\) is a 
                    feature vector, and \(y_i \in \mathbb{R}\) is the corresponding target value. The linear regression model assumes a 
                    linear relationship:
                    \[
                    y = XW + \epsilon,
                    \]
                    where \(X \in \mathbb{R}^{n \times d}\) is the design matrix, \(W \in \mathbb{R}^d\) is the weight vector to be estimated, 
                    and \(\epsilon \sim \mathcal{N}(0, \sigma^2 I)\) is a Gaussian noise vector.
                </p>
                   
                <p>
                    In linear regression, the <a href="../Probability/mle.html"><strong>maximum likelihood estimator (MLE)</strong></a> 
                    of \(W\) coincides with the <a href="../Linear_algebra/leastsquares.html"><strong>least-squares solution</strong></a>: 
                    \[
                    \begin{align*}
                    \widehat{W}_{LS}    &= \arg \min_W \sum_{i=1}^n (y_i - {x_i}^T W )^2 \\\\
                                        &= \arg \min_W \|y - XW\|_2^2 \\\\
                                        &= \arg \min_W (y - XW)^T (y - XW) \\\\
                                        &= \arg \min_W y^T y -y^T XW - W^T X^T y + W^T X^T X W \\\\
                                        &= \arg \min_W f(W) \\\\
                    \end{align*}
                    \]
                    
                    Differentiating with respect to \(W\) and setting the gradient to zero:
                    \[
                    \begin{align*}
                        \nabla f(W) &= -X^T y -X^T y + 2X^T XW \\\\
                                    &= -2X^T y + 2 X^T X W  = 0 
                    \end{align*}
                    \]
                    Thus, if \((X^TX)^{-1}\) exists,
                    \[
                    \begin{align*}
                    \widehat{W}_{LS} &= (X^TX)^{-1}X^Ty \\\\
                                     &= \widehat{W}_{MLE}.
                    \end{align*}
                    \]
                </p>
                
                <p>
                    If \(d >> n\), the objective function becomes flat in some directions. So, the optimal solution is NOT unique and 
                    unstable due to lack of curvature. This means that small changes in training data result in large changes in solution. 
                </p>

                <p>     
                    In most cases, the empirical distribution is not the same as the true distribution. To avoid this <strong>ovefitting</strong>, 
                    simply we add a <strong>penalty term</strong>. (This process is called <strong>>regularization</strong>). 
                    This modified version of linear rigression is called the <strong>ridge regression</strong>. Now, we have 
                    \[
                    \begin{align*}
                    \widehat{W}_{ridge} &= \arg \min_W \sum_{i=1}^n (y_i - {x_i}^T W )^2 + \lambda {\| W \|_2}^2 \\\\
                                        &= \arg \min_W (XW -y)^T (XW-y)+\lambda W^T W \\\\
                                        &= \arg \min_W y^Ty -2W^T Xy +W^T X^T XW + W^T (\lambda I)W \\\\
                                        &= \arg \min_W y^Ty -2W^T Xy +W^T  (X^T X + \lambda I)W \\\
                                        &= \arg \min_W J(W)            
                    \end{align*}
                    \]
                    Then
                    \[
                    \nabla_W J(W) = -2X^T y +2(X^T X + \lambda I )W = 0
                    \]
                    Solve this equation for \(W\), we botain 
                    \[
                    \widehat{W}_{ridge} = (X^T X + \lambda I)^{-1}X^T y.
                    \]
                    \(\lambda\) represents the strength of the regularizer. Larger \(\lambda\) gives more bias and then decreases 
                    variance. In other words, as \(\lambda \to \infty\), \( \widehat{W}_{ridge} \to 0\). On the other hand,  
                    smaller \(\lambda\) gives less bias and then increases variance. In other words, as \(\lambda \to 0\), 
                    \( \widehat{W}_{ridge} \to \widehat{W}_{LS}\).
                </p>
                  
            </section>

            <section id="" class="section-content">
                <h2>Lasso Regression</h2>
                    
            </section>

            <section id="" class="section-content">
                <h2></h2>
                    
            </section>

            <section id="" class="section-content">
                <h2></h2>
                    
            </section>

    

        
            
        </div>
        <script src="/js/main.js"></script> 
    </body>
</html>