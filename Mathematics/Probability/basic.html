<!DOCTYPE html>
<html>
    <head> 
        <title>Basic Ideas</title>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <link rel="stylesheet" href="../styles.css">
        <meta name="viewport" content="width=device-width, initial-scale=1">
    </head>
    <body> 
        <div class="toc-container">
            <h2>Contents</h2>
            <ul>
                <li><a href="#pro">Probability</a></li>
                <li><a href="#conditional">Conditional Probability</a></li>
                <li><a href="#total">Law of Total Probability</a></li>
                <li><a href="#bayes">Bayes' Theorem</a></li>
            </ul>
        </div>
        <h1 id="pro">Probability</h1>
        <blockquote>
            The collection of every possible outcome of an experiment is called a <strong>sample space</strong> denoted as \(S\).
            It can be discrete or continuous. An <strong>event</strong> \(A\) is a set of outcomes of an experiment, or a 
            subset of the sample space \(A \subseteq S\). 
            <br><br>
            The <strong>probability</strong> of \(A\) denoted as \(P(A)\) satisfies the following axioms:
            <ol>
                <li>\( 0 \leq P(A) \leq 1\)</li>
                <li>\(P(S) = 1\) and \(P(\emptyset) = 0\) </li>
                <li>If the events \(A\) and \(B\) are <strong>mutually exclusive</strong>, \(P(A \cup B) = P(A) + P(B)\)</li>
                Note: Mutually exclusive means that the two events \(A\) and \(B\) cannot occur at the same time.
            </ol>
            and can be calculated as: 
            \[
            P(A) = \frac{\text{number of outcomes in } A}{\text{number of outocomes in } S}.
            \]
            <br>
            Using event algebra, the follwing basic facts can be derived: 
            <ol>
                <li><strong>Complement</strong>: \(P(\bar{A}) = 1 - P(A)\)</li>
                Note: \(1 = P(S) = P(A \cup \bar{A}) = P(A) + P(\bar{A})\)
                <br><br>
                <li><strong>Addition</strong>:\(P(A \cup B) = P(A) + P(B) - P(A \cap B)\)</li>
                Note: If \(A\) and \(B\) are mutually exclusive , \(P(A \cap B) = P(\emptyset) = 0\).
                <br><br>
                <li><strong>Inclusion</strong>: If \(B \subset A\), then \(A \cap B = B\), and so \(P(A) - P(B) = P(A \cap \bar{B})\)</li>
                <br>
                <li><strong>de Morgan's laws</strong>: <br>
                    \(P(\overline{A \cup B}) = P( \bar{A} \cap \bar{B})\) <br>
                    \(P(\overline{A \cap B}) = P( \bar{A} \cup \bar{B})\)</li>
            </ol>
            There are some counting rules to find all possible outcomes in \(S\) quickly.
            <ol>
                <li><strong>Multiplication principle</strong>:</li>
                If an experiment consists of a sequence of operations \(O_1, O_2, \cdots, O_r\) 
                with \(n_1, n_2, \cdots, n_r\) outcomes respectively, then the total mumber of possible outcomes 
                is the product \(n_1n_2\cdots n_r\).
                <br><br>
                <li><strong>Permutation</strong>:</li>
                Sampling without replacement where the oder of sampling matters. 
                \[
                {}_n P_r = n(n-1)(n-2)\cdots (n-1+1) = \frac{n!}{(n-r)!}
                \] 
                (Multiplying the number of possible outcomes at each step until \(r\)th one is made.)
                <br><br>
                <li><strong>Combinations</strong>:</li>
                Sampling without replacement where the oder of sampling does not matter. 
                \[
                {}_n C_r = \binom{n}{r} = \frac{n!}{r!(n-r)!} = \frac{ {}_n P_r }{r!}
                \] 
                Note: This is the <strong>Binomial coefficient</strong> for the binomial expansion. 
                <br><br>
                We can generalize the binomial coefficient in \(k \geq 2\) groups so that there are \(r_i\) in 
                group \(i \quad (1 \leq i \leq k)\) and \(r_1 + r_2 + \cdots + r_k = n\):
                \[
                \frac{n!}{r_1 ! r_2 ! \cdots r_k !}
                \]
                This is called the <strong>multinomial coefficient</strong>. 
                <br>For example, if we deal 52 cards evenly among 4 players, giving each player 13 cards, the total 
                number of different hands within the 4 players is caluculated by  
                \[
                \frac{52!}{13!13!13!13!}
                \]  
            </ol>
        </blockquote>

        <h1 id="conditional">Conditional Probability</h1>
        <blockquote>
            The <strong>conditional probability</strong> of an event \(A\) given an event \(B\) is defined as 
            \[
            P(A \mid B) = \frac{P(A \cap B)}{P(B)}. \tag{1}
            \]
            which means 
            \[
            P(A \cap B) = P(A \mid B) P(B).  \tag{2}
            \]
            Also, from Equation (1),
            \[
            P(B \mid A) = \frac{P(B \cap A)}{P(A)} = \frac{P(A \cap B)}{P(A)}.
            \]
            Using Equation (2), 
            \[
            P(B \mid A) = \frac{P(A \mid B) P(B)}{P(A)}.  \tag{3}
            \]
            This is known as <strong>Bayes' theorem</strong> or inverse probability law. We will discuss it in a more 
            useful form later. 
            <br><br>
            If \(A\) and \(B\)  are mutually <strong>independent</strong>, 
            \[
            P(A \mid B) = P(A), \text{ and } P(B \mid A) = P(B).
            \]
            Thus 
            \[
            P(A \cap B) = P(A) P(B).
            \]
            In addition, in this case, 
            \[
            \begin{align*}
            P(A)P(\bar{B}) &= P(A)[1 - P(B)] \\\\
                           &= P(A) - P(A)P(B) \\\\
                           &= P(A) - P(A \cap B) \\\\
                           &= P(A \cap \bar{B})
            \end{align*}
            \]
            Similarly, \( P(\bar{A})P(B) = P(\bar{A} \cap B)\) and \( P(\bar{A})P(\bar{B}) = P(\bar{A} \cap \bar{B})\).
            <br><br>
            Note: Two events are mutually independent if the occurrence of one event does not affect the probability 
            of the occurrence of the other event. Be careful not to confuse it with mutually exclusive.
        </blockquote>

        <h1 id="total">Law of Total Probability</h1>
        <blockquote>
            <div class="theorem">
                <span class="theorem-title">Theorem 1: Law of Total Probability</span> 
                Let the sample space \(S\) be decomposed into \(k\) mutually exclusive events \(B_1, B_2, \cdots B_k\).
                Then for any event \(A\), 
                \[
                \begin{align*}
                P(A) &= P(A \mid B_1)P(B_1) + P(A \mid B_2)P(B_2) + \cdots +  P(A \mid B_k)P(B_k) \\\\
                     &= \sum_{i =1} ^k P(A \mid B_i)P(B_i)
                \end{align*}
                \]
            </div>
            <div class="proof">
                <span class="proof-title">Proof:</span>
                \begin{align*}
                P(A) &= P[(A \cap B_1) \cup (A \cap B_2) \cup \cdots \cup (A \cap B_k)] \\\\
                     &= P(A \cap B_1) +  P(A \cap B_2) + \cdots +  P(A \cap B_k) \\\\
                     &= P(A \mid B_1)P(B_1) + P(A \mid B_2)P(B_2) + \cdots +  P(A \mid B_k)P(B_k) \\\\
                     &= \sum_{i =1} ^k P(A \mid B_i)P(B_i)
                \end{align*}
            </div>
        </blockquote>

        <h1 id="bayes">Bayes' Theorem</h1>
        <blockquote>
            We revisit Equation (3): 
            \[
            P(B \mid A) = \frac{P(A \mid B) P(B)}{P(A)}.
            \]
            \(P(B)\) is called the <strong>prior probability</strong> of \(B\) and \(P(B \mid A)\) is called the 
            <strong>posterior probability</strong> of \(B\). This is the foundation of <strong>Bayesian statistics</strong>.
            <br><br>
            Here, using the Law of Total Probability, we can get a more general form of Bayes' Theorem.
            <div class="theorem">
                <span class="theorem-title">Theorem 2: Bayes' Theorem</span>
                Let mutually exclusive events \(B_1, B_2, \cdots B_k\) be partitions of the sample space \(S\) with the 
                condition that \(P(B_i) > 0\) for \(i = 1, 2, \cdots, k\). Then for \(j = 1, 2, \cdots, k\),
                \[
                \begin{align*}
                P(B_j \mid A) &= \frac{P(A \mid B_j)P(B_j)}{\sum_{i=1}^k P(A \mid B_i)P(B_i)} \\\\
                              &= \frac{P(B_j \cap A)}{P(A)}
                \end{align*}
                \]
            </div>
            This is a powerful tool in machine learning such as <strong>classification</strong>. For example, in medical 
            diagnosis, \(B_1, B_2, \cdots B_k\) can be possible diseases and let \(A\) be observed symptoms, then \(P(B_j \cap A)\) 
            represents the probability of having diseas \(B_j\) given the symptoms \(A\).
           
        </blockquote>


        <a href="../../index.html">Back to Home </a>
        <br> <a href="probability.html">Back to Probability </a>   
    </body>
</html>