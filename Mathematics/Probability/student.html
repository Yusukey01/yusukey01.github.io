---
layout: default
title: Student's t-Distribution
topic_id: prob-5
level: detail
uses_math: true
uses_python: false
---
<!DOCTYPE html>
<html>
    <body>
        {% include learning_resource_schema.html topic_id=page.topic_id %}
        
        <div class="hero-section">
            <h1 class="webpage-name">Student's \(t\)-Distribution</h1>
        </div>

        {% include section_navigation.html %}

        <div class="topic-nav">
            <a href="#student">Student's \(t\)-Distribution</a>
            <a href="#cauchy">Cauchy Distribution</a>
            <a href="#laplace">Laplace Distribution</a>
        </div> 

        <div class="container">  
           
            <section id="student" class="section-content">
                <h2>Student's \(t\)-Distribution</h2>

                <p>
                    In <a href="gaussian.html"><strong>Part 4</strong></a>, we saw that the chi-squared distribution governs 
                    sums of squared standard normals, and that the sample variance \(s^2\) satisfies \((n-1)s^2/\sigma^2 \sim \chi^2_{n-1}\). 
                    A natural question follows: what happens to our inference when we replace the unknown population standard deviation 
                    \(\sigma\) with its estimate \(s\)? The answer is the <strong>Student's \(t\)-distribution</strong>, which arises as the 
                    ratio of a standard normal to the square root of an independent chi-squared variable divided by its degrees of freedom.
                </p>

                <div class="theorem">
                    <span class="theorem-title">Definition: Student's \(t\)-Distribution</span>
                    <p>
                        A random variable \(Y\) has a <strong>Student's \(t\)-distribution</strong> with location 
                        \(\mu\), scale \(\sigma > 0\) (not the standard deviation), and \(\nu > 0\) <strong>degrees 
                        of freedom</strong> if its p.d.f. is:
                        \[
                        f(y \mid \mu, \sigma^2, \nu) \propto \left[1 + \frac{1}{\nu}\left(\frac{y - \mu}{\sigma}\right)^2\right]^{-\frac{\nu+1}{2}}.
                        \]
                        The moments are:
                        \[
                        \text{mean} = \text{mode} = \mu \;(\text{exists if } \nu > 1), \qquad 
                        \text{Var}(Y) = \frac{\nu\sigma^2}{\nu - 2} \;(\text{exists if } \nu > 2).
                        \]
                    </p>
                </div>

                <p>
                    The key feature of the \(t\)-distribution is its <strong>heavy tails</strong>: compared to a normal distribution 
                    with the same location and scale, the \(t\)-distribution assigns substantially more probability mass to extreme values. 
                    This makes parameter estimates based on the \(t\)-distribution more robust to outliers.
                </p>
                <p>
                    The parameter \(\nu\) controls the tail heaviness. As \(\nu \to \infty\), the \(t\)-distribution converges to the 
                    normal distribution \(\mathcal{N}(\mu, \sigma^2)\). In practice, for  \(\nu \gg 5\), the \(t\)-distribution is nearly 
                    indistinguishable from the normal and loses its robustness advantage.
                </p>

            </section>

            <section id="cauchy" class="section-content">
                <h2>Cauchy Distribution</h2>

                <p>
                    An important special case of the \(t\)-distribution arises at the extreme end of heavy-tailed behavior, 
                    when the degrees of freedom parameter takes its smallest meaningful value.
                </p>

                <div class="theorem">
                    <span class="theorem-title">Definition: Cauchy Distribution</span>
                    <p>
                        When \(\nu = 1\), the Student's \(t\)-distribution reduces to the <strong>Cauchy distribution</strong> 
                        with location \(\mu\) and scale \(\gamma > 0\) that has p.d.f.:
                        \[
                        f(x \mid \mu, \gamma) = \frac{1}{\gamma\pi}\left[1 + \left(\frac{x - \mu}{\gamma}\right)^2\right]^{-1}.
                        \]
                    </p>
                </div>

                <p>
                    Consider the standard Cauchy distribution (\(\mu = 0, \gamma = 1\)). When we attempt to calculate its 
                    expected value:
                    \[
                    \mathbb{E}[X] = \frac{1}{\pi} \int_{-\infty}^{\infty} \frac{x}{1 + x^2} dx
                    \]
                    we find that the integral does not converge. Because the tails decay at a rate of only \(\frac{1}{x^2}\), 
                    the integrand \(\frac{x}{(1+x^2)}\) behaves like \(\frac{1}{x}\) for large \(x\). The integral \(\int \frac{1}{x} \, dx\) 
                    diverges logarithmically. (See <a href="../Calculus/riemann.html"><strong>Improper Riemann integrals</strong></a>).
                </p>

                <p>
                    <strong>Why this matters:</strong> Since the mean and variance are undefined, the <strong>Law of Large Numbers</strong> 
                    fails. If you average \(n\) independent Cauchy variables, the sample mean \(\bar{X}_n\) does not settle down; 
                    it follows the <em>exact same</em> Cauchy distribution as the individual observations. We will explore 
                    this behavior rigorously in <a href="convergence.html"><strong>Part 13: Convergence</strong></a>.
                </p>

                <p>
                    Despite these theoretical challenges, the Cauchy distribution is highly useful. In Bayesian modeling, 
                    when we need a heavy-tailed prior over \(\mathbb{R}^+\) that allows for large values while maintaining 
                    finite density at the origin, we use the <strong>Half-Cauchy distribution</strong> (\(x \geq 0\)):
                    \[
                    f(x \mid \gamma) = \frac{2}{\pi \gamma} \left[ 1 + \left(\frac{x}{\gamma}\right)^2\right]^{-1}.
                    \]
                    This is often the default choice for scale parameters (hierarchical priors) because it is more 
                    robust than the Inverse-Gamma distribution.
                </p>

            </section>

            <section id="laplace" class="section-content">
                <h2>Laplace Distribution</h2>

                 <p>
                    The Cauchy distribution demonstrates that heavy tails can be extreme enough to prevent even the mean 
                    from existing. A more moderate heavy-tailed alternative, which retains finite moments of all orders 
                    while still placing more mass in the tails than the normal distribution, is the <strong>Laplace distribution</strong>.
                </p>

                <div class="theorem">
                    <span class="theorem-title">Definition: Laplace Distribution</span>
                    <p>
                        The <strong>Laplace distribution</strong> (also called the double-sided exponential distribution) 
                        with location \(\mu\) and scale \(b > 0\) has p.d.f.:
                        \[
                        f(y \mid \mu, b) = \frac{1}{2b}\exp\!\left(-\frac{|y - \mu|}{b}\right).
                        \]
                        Its moments are:
                        \[
                        \text{mean} = \text{mode} = \mu, \qquad \text{Var}(Y) = 2b^2.
                        \]
                    </p>
                </div>
                
                <div class="insight-box">
                    <h3>Insight: Heavy Tails and Regularization</h3>
                    <p>
                        In machine learning, the choice of distribution often corresponds to the choice of <strong>regularization</strong>. 
                        While a Gaussian prior on weights leads to <strong>\(L_2\) regularization (Ridge)</strong>, 
                        a Laplace prior leads to <strong>\(L_1\) regularization (Lasso)</strong>, promoting sparsity. 
                        Additionally, using the Student's t-distribution in regression makes the model more 
                        <strong>robust</strong> to outliers compared to standard least-squares (Gaussian) regression.
                    </p>
                </div>

                 <p>
                    The Student's \(t\), Cauchy, and Laplace distributions complete our toolkit of univariate distributions for robust modeling. 
                    In the next part, we move from single random variables to <em>pairs</em> of random variables, introducing 
                    <a href="covariance.html"><strong>covariance</strong></a> as the fundamental measure of 
                    linear dependence between variables.
                </p>
            </section>
        </div> 
        <script src="/js/main.js"></script>  
    </body>
</html>