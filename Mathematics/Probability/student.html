---
layout: default
title: Student's t-Distribution
topic_id: prob-5
level: detail
uses_math: true
uses_python: false
---
<!DOCTYPE html>
<html>
    <body>
        {% include learning_resource_schema.html topic_id=page.topic_id %}
        
        <div class="hero-section">
            <h1 class="webpage-name">Student's \(t\)-Distribution</h1>
        </div>

        {% include section_navigation.html %}

        <div class="topic-nav">
            <a href="#student">Student's \(t\)-Distribution</a>
            <a href="#cauchy">Cauchy Distribution</a>
            <a href="#laplace">Laplace Distribution</a>
        </div> 

        <div class="container">  
           
            <section id="student" class="section-content">
                <h2>Student's \(t\)-Distribution</h2>

                <p>
                    In general, the normal distribution is sensitive to outliers. A robust alternative to the normal distribution is 
                    the <strong>Student's \(t\)-distribution</strong>. Its probability density function is given by:
                    \[
                    f(y | \mu, \sigma^2, \nu) \propto \left[ 1 + \frac{1}{\nu}\left(\frac{y-\mu}{\sigma}\right)^2 \right]^{-\frac{\nu+1}{2}}
                    \]
                    where \(\mu\) is the mean, \(\sigma > 0\) is the scale parameter(NOT standard deviation), and \(\nu > 0\) is the 
                    <strong>degrees of freedom</strong>.
                </p>
                <p>
                    The Student \(t\)-distribution has heavy tails(There is more probability mass in the tail than
                    with a normal distribution), which makes it robust to outliers.
                </p>

                <p>
                    As the degrees of freedom \(\nu\) gets larger, the function acts like the normal distribution. For \(\nu \gg 5\), the pdf rapidly approaches 
                    a normal distribution and loses its robustness properties.
                </p>
                <p>
                    Note:
                    \[
                    \text{mean } = \text{mode } = \mu, \quad \text{variance } = \frac{\nu \sigma^2}{\nu - 2}
                    \]
                    and the mean only exists if \(\nu > 1\) and the variance only exists if \(\nu > 2\).
                </p>

            </section>

            <section id="cauchy" class="section-content">
                <h2>Cauchy Distribution</h2>

                <p>
                    When \(\nu = 1\), the Student's \(t\)-distribution becomes the <strong>Cauchy distribution</strong>. 
                    Its probability density function is given by:
                    \[
                    f(x | \mu, \gamma) = \frac{1}{\gamma \pi}\left[ 1 + \left(\frac{x - \mu}{\gamma}\right)^2\right]^{-1}.
                    \]
                    This pdf has heavy tails compared to of the normal distribution. 
                </p>

                <p>
                    Consider the standard Cauchy distribution (\(\mu = 0, \quad \gamma = 1\)). Then 
                    \[
                    \begin{align*}
                    \mathbb{E }[X] &= \int_{-\infty}^{\infty} x f(x)dx \\\\
                                &= \frac{1}{\pi} \int_{-\infty}^{\infty} \frac{x}{1 + x^2} dx.
                    \end{align*}
                    \]
                    In short, since the tails are so heavy, <strong>the mean does not converge</strong>.
                    (See <a href="../Calculus/riemann.html"><strong>Improper Riemann integrals</strong></a> for the details.)
                </p>
                <p>
                    In Bayesian modeling, we want to use a distribution over \(\mathbb{R}^+\) with heavy tails, but finite density 
                    at the origin. In such a case, the <strong>half Cauchy distribution</strong> is useful. Its probability density 
                    function is given by:
                    \[
                    f(x | \gamma) = \frac{2}{\pi \gamma} \left[ 1 + \left(\frac{x}{\gamma}\right)^2\right]^{-1}.
                    \]
                    (This is the Cauchy distribution with \(\mu = 0\).)
                </p>
            </section>

            <section id="laplace" class="section-content">
                <h2>Laplace Distribution</h2>
                <p>
                    Like the half Cauchy distribution, because of its heavy tails, the 
                    <strong>Laplace distribution (Double sided exponential distribution)</strong> is also popular 
                    in some machine learning models such as robust linear regression.
                </p>
                <p>
                    Its pdf is given by:
                    \[
                    \text{Laplace } ( y | \mu, b) = \frac{1}{2b}\exp \left( - \frac{|y - \mu|}{b}\right)
                    \]
                    where \(\mu\) is a location parameter and \(b > 0\) is a scale parameter. 
                    Note:
                    \[
                    \text{mean } = \text{mode } = \mu, \quad \text{variance } = 2b^2.
                    \]
                </p>

                <div class="insight-box">
                    <h3>Insight: Heavy Tails and Regularization</h3>
                    <p>
                        In machine learning, the choice of distribution often corresponds to the choice of <strong>regularization</strong>. 
                        While a Gaussian prior on weights leads to <strong>\(L_2\) regularization (Ridge)</strong>, 
                        a Laplace prior leads to <strong>\(L_1\) regularization (Lasso)</strong>, promoting sparsity. 
                        Additionally, using the Student's t-distribution in regression makes the model more 
                        <strong>robust</strong> to outliers compared to standard least-squares (Gaussian) regression.
                    </p>
                </div>
            </section>
        </div> 
        <script src="/js/main.js"></script>  
    </body>
</html>