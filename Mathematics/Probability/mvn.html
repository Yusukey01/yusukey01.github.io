---
layout: default
title: Multivariate Distributions
topic_id: prob-8
level: detail
uses_math: true
uses_python: false
---
<!DOCTYPE html>
<html>
    <body>
        {% include learning_resource_schema.html topic_id=page.topic_id %}
        
        <div class="hero-section">
            <h1 class="webpage-name">Multivariate Distributions</h1>
        </div>

        {% include section_navigation.html %}

        <div class="topic-nav">
            <a href="#mn">Multivariate Normal Distribution</a>
            <a href="#cholesky">Cholesky Decomposition</a>
            <a href="#dirichlet">Dirichlet Distribution</a>
            <a href="#wishart"> Wishart Distribution</a>
        </div> 

        <div class="container">  
            <section id="mn" class="section-content">

                <h2>Multivariate Normal Distribution</h2>

                <p>
                    In the previous two parts, we developed the <a href="covariance.html"><strong>covariance matrix</strong></a> and the 
                    <a href="correlation.html"><strong>correlation coefficient</strong></a> as tools for quantifying linear relationships 
                    among random variables. We also saw in <a href="gaussian.html"><strong>Part 4</strong></a> that the univariate Gaussian 
                    \(\mathcal{N}(\mu, \sigma^2)\) is characterized by its mean and variance. The  <strong>multivariate normal distribution</strong> 
                    (MVN) is the natural extension of the Gaussian to random vectors, and it is arguably the most important joint distribution 
                    in all of statistics and machine learning. Its shape is entirely determined by a mean vector and a covariance matrix,
                    making the concepts from Parts 6 and 7 indispensable for understanding its geometry.
                </p>

                <div class="theorem">
                    <span class="theorem-title">Definition: Multivariate Normal Distribution</span>
                    <p>
                        An \(n\)-dimensional random vector \(\boldsymbol{x} \in \mathbb{R}^n\) follows a <strong>multivariate normal distribution</strong>, 
                        written \(\boldsymbol{x} \sim \mathcal{N}(\boldsymbol{\mu}, \Sigma)\), if its p.d.f. is 

                        \[
                        f(\boldsymbol{x}) = \frac{1}{\sqrt{(2\pi)^n \det(\Sigma)}} 
                        \exp\!\left[ 
                                    -\frac{1}{2}(\boldsymbol{x} - \boldsymbol{\mu})^\top \Sigma^{-1} (\boldsymbol{x} - \boldsymbol{\mu}) 
                                \right], \tag{1}
                        \]

                        where \(\boldsymbol{\mu} = \mathbb{E}[\boldsymbol{x}] \in \mathbb{R}^n\) is the mean vector 
                        and \(\Sigma = \text{Cov}[\boldsymbol{x}] \in \mathbb{R}^{n \times n}\) is the 
                        (symmetric, positive definite) covariance matrix.
                    </p>
                </div>

                <p>
                    The expression inside the exponential (ignoring the factor of \(-\tfrac{1}{2}\)) is the squared 
                    <strong>Mahalanobis distance</strong> between \(\boldsymbol{x}\) and \(\boldsymbol{\mu}\):
                    \[
                    d_{\Sigma}(\boldsymbol{x}, \boldsymbol{\mu})^2 
                    = (\boldsymbol{x} - \boldsymbol{\mu})^\top \Sigma^{-1} (\boldsymbol{x} - \boldsymbol{\mu}).
                    \]
                    Unlike the Euclidean distance, the Mahalanobis distance accounts for the correlation 
                    structure among variables. The contours of constant density are 
                    <strong>ellipsoids</strong> defined by 
                    \[
                    (\boldsymbol{x} - \boldsymbol{\mu})^\top \Sigma^{-1} (\boldsymbol{x} - \boldsymbol{\mu}) = c,
                    \]
                    whose axes are aligned with the eigenvectors of \(\Sigma\) and whose lengths are 
                    proportional to the square roots of the corresponding eigenvalues.
                </p>

                <p>
                    To build concrete intuition, let us work out the special case \(n = 2\) in full detail. 
                    When \(\boldsymbol{x} \in \mathbb{R}^2\), the MVN is called the <strong>bivariate normal distribution</strong>.
                    In this case,
                    \[
                    \begin{align*}
                    \Sigma  &=  \begin{bmatrix}
                                    \text{Var } (X_1) & \text{Cov }[X_1, X_2] \\
                                    \text{Cov }[X_2, X_1] & \text{Var } (X_2)
                                \end{bmatrix} \\\\
                            &=  \begin{bmatrix}
                                    \sigma_1^2 & \rho \sigma_1 \sigma_2 \\
                                    \rho \sigma_1 \sigma_2 & \sigma_2^2
                                \end{bmatrix} \\\\
                    \end{align*}
                    \]
                    where \(\rho\) is the correlation coefficient defined by 
                    \[
                    \text{Corr }[X_1, X_2] = \frac{\text{Cov }[X_1, X_2]}{\sqrt{\text{Var }(X_1)\text{Var }(X_2)}}.
                    \]
                </p>

                <p>
                    Then 
                    \[
                    \begin{align*}
                    \det (\Sigma) &= \sigma_1^2 \sigma_2^2 - \rho^2 \sigma_1^2 \sigma_2^2 \\\\
                                &= \sigma_1^2 \sigma_2^2 (1 - \rho^2) 
                    \end{align*}
                    \]
                    and 
                    \[
                    \begin{align*}
                    \Sigma^{-1} &= \frac{1}{\det (\Sigma )}
                                \begin{bmatrix}
                                \sigma_2^2 & -\rho \sigma_1 \sigma_2 \\
                                -\rho \sigma_1 \sigma_2 & \sigma_1^2
                                \end{bmatrix} \\\\
                                &= \frac{1}{1 - \rho^2}
                                \begin{bmatrix}
                                \frac{1}{\sigma_1^2 } & \frac{-\rho} {\sigma_1 \sigma_2} \\
                                \frac{-\rho} {\sigma_1 \sigma_2} & \frac{1}{\sigma_2^2 }
                                \end{bmatrix} 
                    \end{align*}
                    \]
                </p>

                <p>
                    Note that in Expression (1), 
                    \[
                    (\boldsymbol{x} - \boldsymbol{\mu})^T \Sigma^{-1} (\boldsymbol{x} -\boldsymbol{\mu})
                    \]
                    is a <strong>quadratic form</strong>. So, 
                    \[
                    \begin{align*}
                    (\boldsymbol{x} - \boldsymbol{\mu})^T \Sigma^{-1} (\boldsymbol{x} -\boldsymbol{\mu}) 
                        &= \frac{1}{1 - \rho^2} \begin{bmatrix} X_1 - \mu_1 & X_2 - \mu_2 \end{bmatrix}
                            \begin{bmatrix}
                            \frac{1}{\sigma_1^2 } & \frac{-\rho} {\sigma_1 \sigma_2} \\
                            \frac{-\rho} {\sigma_1 \sigma_2} & \frac{1}{\sigma_2^2 }
                            \end{bmatrix} 
                            \begin{bmatrix} X_1 - \mu_1 \\ X_2 - \mu_2 \end{bmatrix} \\\\
                            &= \frac{1}{1 - \rho^2}\left[\frac{1}{\sigma_1^2 }(X_1 - \mu_1)^2 
                                                    -\frac{2\rho} {\sigma_1 \sigma_2}(X_1 - \mu_1)(X_2 - \mu_2) 
                                                    +\frac{1}{\sigma_2^2 }(X_2 - \mu_2)^2 \right].
                    \end{align*}
                    \] 
                    Therefore, we obtain the p.d.f for the bivariate normal distribution:
                    \[
                    f(\boldsymbol{x}) = \frac{1}{2\pi \sigma_1 \sigma_2 \sqrt{(1 - \rho^2)}} 
                        \exp\Big\{-\frac{1}{2(1 - \rho^2)}
                                                        \Big[\Big(\frac{X_1 - \mu_1}{\sigma_1}\Big)^2
                                                        -2\rho \Big(\frac{X_1 - \mu_1} {\sigma_1}\Big)  \Big(\frac{X_2 - \mu_2} {\sigma_2}\Big)
                                                        +\Big(\frac{X_2 - \mu_2}{\sigma_2}\Big)^2
                                                        \Big]
                            \Big\}
                    \]
                    When \(\rho = -1 \text{ or } 1\), this p.d.f is undefined and \(f\) is said to be <strong>degenerate</strong>.
                </p>
            </section>

            <section id="cholesky" class="section-content">
                <h2>Cholesky Decomposition</h2>

                 <p>
                    The previous section defined the MVN in terms of its density function, but 
                    in practice we frequently need to <em>sample</em> from 
                    \[
                    \mathcal{N}(\boldsymbol{\mu}, \boldsymbol{\Sigma}).
                    \] 
                    For instance, when training variational autoencoders, implementing the reparameterization trick, 
                    or running Monte Carlo simulations. Generating independent standard normal samples is straightforward, 
                    but we need a way to introduce the correlation structure encoded in \(\boldsymbol{\Sigma}\). 
                    The <strong>Cholesky decomposition</strong> provides a numerically stable factorization that transforms 
                    uncorrelated samples into correlated ones.
                </p>

                <div class="theorem">
                    <span class="theorem-title">Cholesky Decomposition:</span>
                    <p>
                        Let \( \boldsymbol{A} \in \mathbb{R}^{d \times d} \) be a 
                        <a href="../Linear_algebra/symmetry.html"><strong>symmetric positive definite</strong></a> matrix. 
                        Then there exists a unique lower triangular matrix \( \boldsymbol{L} \) with positive diagonal 
                        entries such that
                        \[
                        \boldsymbol{A} = \boldsymbol{L}\boldsymbol{L}^\top.
                        \]
                        (Equivalently, \( \boldsymbol{A} = \boldsymbol{R}^\top \boldsymbol{R} \) where
                        \( \boldsymbol{R} = \boldsymbol{L}^\top \) is upper triangular.)
                    </p>
                </div>

                <p>
                    Let \( \boldsymbol{y} \sim \mathcal{N}(\boldsymbol{\mu}, \boldsymbol{\Sigma}) \) and apply the
                    Cholesky decomposition to its covariance matrix:
                    \[
                    \boldsymbol{\Sigma} = \boldsymbol{L}\boldsymbol{L}^\top.
                    \]
                    If \( \boldsymbol{x} \sim \mathcal{N}(\boldsymbol{0}, \boldsymbol{I}) \), which can be obtained by sampling
                    \( d \) independent standard normal variables, then
                    \[
                    \boldsymbol{y} = \boldsymbol{L}\boldsymbol{x} + \boldsymbol{\mu}
                    \]
                    has the desired covariance:
                    \[
                    \begin{align*}
                    \text{Cov}[\boldsymbol{y}]
                    &= \boldsymbol{L}\,\text{Cov}[\boldsymbol{x}]\,\boldsymbol{L}^\top \\
                    &= \boldsymbol{L}\boldsymbol{I}\boldsymbol{L}^\top \\
                    &= \boldsymbol{\Sigma}.
                    \end{align*}
                    \]
                </p>

                <div class="proof">
                    <span class="proof-title">Proof:</span>
                    <p>
                        By linearity of expectation,
                        \[
                        \mathbb{E}[\boldsymbol{y}] 
                        = \mathbb{E}[\boldsymbol{L}\boldsymbol{x} + \boldsymbol{\mu}]
                        = \boldsymbol{L}\mathbb{E}[\boldsymbol{x}] + \boldsymbol{\mu}
                        = \boldsymbol{\mu}.
                        \]
                        The covariance of a random vector is given by
                        \[
                        \text{Cov}[\boldsymbol{y}]
                        = \mathbb{E}\!\left[(\boldsymbol{y} - \mathbb{E}[\boldsymbol{y}])
                            (\boldsymbol{y} - \mathbb{E}[\boldsymbol{y}])^\top \right].
                        \]
                        Substituting \( \boldsymbol{y} = \boldsymbol{L}\boldsymbol{x} + \boldsymbol{\mu} \) and using 
                        \( \mathbb{E}[\boldsymbol{x}] = \boldsymbol{0} \),
                        \[
                        \begin{align*}
                        \text{Cov}[\boldsymbol{y}]
                        &= \mathbb{E}\!\left[ (\boldsymbol{L}\boldsymbol{x})
                                            (\boldsymbol{L}\boldsymbol{x})^\top \right] \\\\
                        &= \boldsymbol{L}\,\mathbb{E}\!\left[\boldsymbol{x}\boldsymbol{x}^\top\right]\boldsymbol{L}^\top \\\\
                        &= \boldsymbol{L}\,\text{Cov}[\boldsymbol{x}]\,\boldsymbol{L}^\top \\\\
                        &= \boldsymbol{L}\boldsymbol{I}\boldsymbol{L}^\top \\\\
                        &= \boldsymbol{L}\boldsymbol{L}^\top \\\\
                        &= \boldsymbol{\Sigma}.
                        \end{align*}
                        \]
                        Therefore, \( \boldsymbol{y} \sim \mathcal{N}(\boldsymbol{\mu}, \boldsymbol{L}\boldsymbol{L}^\top) \).
                    </p>
                </div>
            </section>

        
            <section id="dirichlet" class="section-content">
                <h2>Dirichlet Distribution</h2>

                 <p>
                    The multivariate normal distribution models random vectors taking values in all of \(\mathbb{R}^n\). 
                    However, many quantities of interest in machine learning are <strong>probability vectors</strong>, which 
                    entries are non-negative and sum to one. For example, the class probabilities output by a softmax layer, 
                    the topic proportions in a document, or the mixing weights in a mixture model all live on a 
                    <strong>probability simplex</strong>. We need a distribution that respects this constraint.
                </p>

                <p>
                    The <strong>Dirichlet distribution</strong> is a multivariate generalization of <a href="gamma.html"><strong>beta distribution</strong></a>. 
                    It has support over the the \((K - 1)\)-dimensional <strong>probability simplex</strong>, defined by
                    \[ 
                    S_K = \Bigl\{(x_1, x_2, \dots, x_K) \in \mathbb{R}^K: x_k \ge 0,\ \sum_{k=1}^K x_k = 1 \Bigr\}. 
                    \] 
                    A random vector \(\boldsymbol{x} \in \mathbb{R}^K\) is said to have a Dirichlet distribution with parameters 
                    \(\boldsymbol{\alpha} = (\alpha_1, \alpha_2, \dots, \alpha_K)\) (with each \(\alpha_k > 0\)) if its probability 
                    density function is given by 
                    \[ 
                    f(x_1, \dots, x_K; \boldsymbol{\alpha}) = \frac{1}{B(\boldsymbol{\alpha})} \prod_{k=1}^K x_k^{\alpha_k - 1}, \quad (x_1, \dots, x_K) \in S_K, 
                    \]
                    or
                    \[
                    \text{Dir }(\boldsymbol{\alpha}) = \frac{1}{B(\boldsymbol{\alpha})} \prod_{k=1}^K x_k^{\alpha_k - 1} \mathbb{I}(\boldsymbol{x} \in S_k),
                    \] 
                    where the <strong>multivariate beta function</strong> \(B(\boldsymbol{\alpha})\) is defined as 
                    \[ 
                    B(\boldsymbol{\alpha}) = \frac{\prod_{k=1}^K \Gamma(\alpha_k)}{\Gamma\Bigl(\sum_{k=1}^K \alpha_k\Bigr)}. 
                    \]
                </p>

                <div class="theorem">
                    <span class="theorem-title">Moments:</span>
                    <p>
                        Let \(\alpha_0 = \sum_{k=1}^K \alpha_k\). Then for each \(k\),
                    </p>
                  
                    <ul style="padding-left: 40px;">
                        <li><strong>Mean:</strong><br>
                            \[
                            \mathbb{E}[x_k] = \frac{\alpha_k}{\alpha_0}.
                            \]
                        </li>
                        <li><strong>Variance:</strong><br>
                            \[
                            \operatorname{Var}[x_k] = \frac{\alpha_k (\alpha_0 - \alpha_k)}{\alpha_0^2 (\alpha_0+1)}.
                            \]
                        </li>
                        Note: Often we use a symmetric Dirichlet prior of the \(\alpha_k = \frac{\alpha}{K}\). Then 
                        \[
                        \mathbb{E}[x_k] = \frac{1}{K}, \quad \operatorname{Var}[x_k] = \frac{K-1}{K^2 (\alpha +1)}.
                        \]
                        We can see that increasing \(\alpha\) increases the precision(decreases the variance) of the distribution. 
                        <li><strong>Covariance:</strong><br>
                            For \(i \neq j\),
                            \[
                            \operatorname{Cov}[x_i, x_j] = \frac{-\alpha_i \alpha_j}{\alpha_0^2 (\alpha_0+1)}.
                            \]
                        </li>
                    </ul>
                </div>
                <p>
                    The parameters \(\alpha_k\) can be thought of as "pseudocounts" or prior observations of each category. When all 
                    \(\alpha_k\) are equal (i.e., \(\boldsymbol{\alpha} = \alpha\, \mathbf{1}\)), the distribution is said to be <strong>uniform</strong>
                    over the simplex when \(\alpha = 1\) or symmetric if \(\alpha \ne 1\). This symmetry makes the Dirichlet distribution a 
                    natural <a href="markov.html"><strong>prior in Bayesian models</strong></a> where no category is favored a priori.
                </p>
                <p>
                    The Dirichlet distribution is widely used as a conjugate prior for the parameters 
                    of a multinomial distribution in <a href="bayesian.html"><strong>Bayesian statistics</strong></a>, as well as in machine learning models 
                    such as latent Dirichlet allocation (LDA) for topic modeling.
                </p>
            </section>

            <section id="wishart" class="section-content">
                <h2>Wishart Distribution</h2>

                 <p>
                    The Dirichlet distribution placed a prior on probability vectors. In Bayesian multivariate analysis, 
                    we also need priors on <em>covariance matrices</em> themselves. For example, when inferring the parameters 
                    of a multivariate normal model. The <strong>Wishart distribution</strong> fills this role: it generalizes the 
                    <a href="gamma.html"><strong>gamma distribution</strong></a> from positive scalars to 
                    <a href="../Linear_algebra/symmetry.html"><strong>positive definite matrices</strong></a>.
                </p>

                <div class="theorem">
                    <span class="theorem-title">Definition: Wishart Distribution</span>

                    <p>
                        The p.d.f. of Wishart distribution is defined as follows:
                        \[
                        \operatorname{Wi}(\boldsymbol{\Sigma} | \boldsymbol{S}, \nu) 
                        = \frac{1}{Z} |\boldsymbol{\Sigma}| ^{(\nu-D-1)/2} \exp \Bigl( - \frac{1}{2} \text{ tr } (\boldsymbol{S}^{-1} \boldsymbol{\Sigma})\Bigr)
                        \]
                        where 
                        \[
                        Z = 2^{\nu D/2} | \boldsymbol{S} |^{-\nu / 2}\,\Gamma_D (\frac{\nu}{2}).
                        \] 
                        Here, \(\Gamma_D(\cdot)\) denotes the multivariate gamma function. The parameters are:
                    </p>

                    <ul style="padding-left: 40px;">
                        <li>\(\nu\): the <strong>degrees of freedom</strong> (which must satisfy \(\nu \ge D\) for the distribution to be well-defined),</li>
                        <li>\(\boldsymbol{S}\): the <strong>scale matrix</strong> (a \(D \times D\) positive definite matrix).
                        Note: \(\text{ mean } = \nu \boldsymbol{S}\).
                        </li>
                    </ul>

                </div>
                
                <p>
                    If \(D=1\), ir reduces to the gamma distribution:
                    \[
                    \operatorname{Wi}(\lambda, s^{-1}, \nu) = \operatorname{Gamma}(\lambda | \text{ shape } = \frac{\nu}{2}, \text{ rate } = \frac{1}{2s}).
                    \]
                    If we set \(s=2\), this further reduces to the <strong>chi-squared distribution</strong>.
                </p>

                <h3>Applications:</h3>
                <ul style="padding-left: 40px;">
                    <li><strong>Covariance Matrix Estimation:</strong><br>
                        <p>
                            In multivariate statistics, the Wishart distribution is used to model the uncertainty in 
                            <a href="covariance.html"><strong>covariance matrix</strong></a> estimates.
                        </p>
                        <p>
                            Given \(D\) i.i.d. samples from \(\mathcal{N}(0, \boldsymbol{\Sigma})\), the sample covariance matrix
                            \[
                            \boldsymbol{S} = \sum_{i=1}^D x_i {x_i}^T
                            \]
                            follows a Wishart distribution:
                            \[
                            \boldsymbol{S} \sim \operatorname{Wi}( \boldsymbol{\Sigma}, D).
                            \]
                        </p>
                    </li>
                    <li><strong>Bayesian Inference:</strong>
                        <p>
                            It serves as a conjugate prior for the covariance matrix in multivariate normal models, enabling closed-form 
                            updates of the posterior distribution. We often use the <strong>inverse Wishart distribution</strong>:
                        </p>
                        <p>
                            For \(\nu > D -1\) and \(\boldsymbol{S} \succ 0\),
                            \[
                            \operatorname{IW}(\boldsymbol{\Sigma} | \boldsymbol{S}^{-1}, \nu) 
                            = \frac{1}{Z_{IW}} |\boldsymbol{\Sigma}| ^{-(\nu+D+1)/2} \exp \Bigl( - \frac{1}{2} \text{ tr } (\boldsymbol{S} \boldsymbol{\Sigma}^{-1})\Bigr)
                            \]
                            where 
                            \[
                            Z_{IW} = 2^{\nu D/2} | \boldsymbol{S} |^{\nu / 2}\,\Gamma_D (\frac{\nu}{2}).
                            \] 
                            Then 
                            \[
                            \boldsymbol{\Sigma} \sim \operatorname{IW}( \boldsymbol{S}, \nu).
                            \]
                            (Remember, \(\lambda \sim \text{Gamma }(a, b)\) then \(\frac{1}{\lambda} \sim  \operatorname{IG}(a,b)\). 
                            So, similarly, the IW is multivariate generalization of the IG.)
                        </p>
                    </li>
                </ul>

                <p>
                    Note that one is about sampling properties of the estimator (<strong>frequentist</strong>), and the other is about 
                    prior beliefs and posterior inference (<strong>Bayesian</strong>). In practice, both deal with uncertainty in 
                    covariance matrices, and the same distribution arises in both roles - just with different interpretations.
                </p>

                <div class="insight-box">
                    <h3>Connections to Machine Learning</h3>
                    <p>
                        The multivariate distributions introduced in this part form the backbone of probabilistic machine learning. 
                        The MVN appears in <a href="gaussian_process.html"><strong>Gaussian processes</strong></a> (where it defines a 
                        prior over functions), in the E-step of <a href="../Machine_learning/clustering.html"><strong>Gaussian mixture models</strong></a>, 
                        and in the reparameterization trick for <strong>variational autoencoders</strong>. The Dirichlet distribution is the standard prior 
                        for topic models (LDA) and categorical distributions in Bayesian inference. The Wishart and inverse Wishart distributions are essential 
                        whenever the covariance matrix itself is a parameter to be inferred, as in Bayesian multivariate regression and Bayesian Gaussian mixture models.
                    </p>
                </div>

                <p>
                    With the multivariate framework now in place from covariance matrices (Part 6) through 
                    correlation (Part 7) to multivariate distributions (this part), we have the probabilistic 
                    foundations needed for statistical inference. In 
                    <a href="mle.html"><strong>Part 9</strong></a>, we turn to <strong>maximum likelihood estimation</strong>, 
                    the principal method for fitting probabilistic models to observed data.
                </p>
                
            </section>
        </div> 
        <script src="/js/main.js"></script>   
    </body>
</html>