<!DOCTYPE html>
<html>
    <head> 
        <title>Multivariate Distributions</title>
        <link rel="stylesheet" href="../styles.css">
        <script id="MathJax-script" defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <meta name="viewport" content="width=device-width, initial-scale=1">
    </head>
    <body> 
        <div class="toc-container">
            <h2>Contents</h2>
            <ul>
                <li><a href="#mn">Multivariate Normal Distribution</a></li>
                <li><a href="#dirichlet">Dirichlet Distribution</a></li>
            </ul>
        </div>
        <h1 id="mn">Multivariate Normal Distribution</h1>
        <blockquote>
            In machine learning, the most important joint probability distribution for continuous random variables is the 
            <strong>multivariate normal distribution</strong> (MVN). 
            The multivariate normal distribution of a \(n\) dimentional random vector \(x \in \mathbb{R}^n \) is denoted as 
            \[
            X \sim N(\mu, \Sigma)
            \]
            where \(\mu = \text{E } [x] \in \mathbb{R}^n\) is the mean vector, and 
            \(\Sigma = \text{Cov }[x] \in \mathbb{R}^{n \times n}\) is the covariance matrix. 
            <br>
            The p.d.f. is given by
            \[
            f(x) = \frac{1}{\sqrt{(2\pi)^n \det(\Sigma)}} \exp \Big[-\frac{1}{2}(x - \mu)^T \Sigma^{-1} (x -\mu) \Big]. \tag{1}
            \]
            <br>
            For example, if \(x \in \mathbb{R}^2\), the MVN is known as the <strong>bivariate normal distribution</strong>.
            In this case, 
            \[
            \begin{align*}
            \Sigma  &=  \begin{bmatrix}
                            \text{Var } (X_1) & \text{Cov }[X_1, X_2] \\
                            \text{Cov }[X_2, X_1] & \text{Var } (X_2)
                        \end{bmatrix} \\\\
                    &=  \begin{bmatrix}
                            \sigma_1^2 & \rho \sigma_1 \sigma_2 \\
                            \rho \sigma_1 \sigma_2 & \sigma_2^2
                        \end{bmatrix} \\\\
            \end{align*}
            \]
            where \(\rho\) is the correlation coefficient defined by 
            \[
            \text{Corr }[X_1, X_2] = \frac{\text{Cov }[X_1, X_2]}{\sqrt{\text{Var }(X_1)\text{Var }(X_2)}}.
            \]
            Then 
            \[
            \begin{align*}
            \det (\Sigma) &= \sigma_1^2 \sigma_2^2 - \rho^2 \sigma_1^2 \sigma_2^2 \\\\
                          &= \sigma_1^2 \sigma_2^2 (1 - \rho^2) 
            \end{align*}
            \]
            and 
            \[
            \begin{align*}
            \Sigma^{-1} &= \frac{1}{\det (\Sigma )}
                          \begin{bmatrix}
                           \sigma_2^2 & -\rho \sigma_1 \sigma_2 \\
                           -\rho \sigma_1 \sigma_2 & \sigma_1^2
                          \end{bmatrix} \\\\
                        &= \frac{1}{1 - \rho^2}
                          \begin{bmatrix}
                           \frac{1}{\sigma_1^2 } & \frac{-\rho} {\sigma_1 \sigma_2} \\
                           \frac{-\rho} {\sigma_1 \sigma_2} & \frac{1}{\sigma_2^2 }
                          \end{bmatrix} 
            \end{align*}
            \]
            Note that in (1), \((x - \mu)^T \Sigma^{-1} (x -\mu)\) is a <strong>quadratic form</strong>. So, 
            \[
            \begin{align*}
            (x - \mu)^T \Sigma^{-1} (x -\mu) &= \frac{1}{1 - \rho^2} \begin{bmatrix} X_1 - \mu_1 & X_2 - \mu_2 \end{bmatrix}
                                                \begin{bmatrix}
                                                \frac{1}{\sigma_1^2 } & \frac{-\rho} {\sigma_1 \sigma_2} \\
                                                \frac{-\rho} {\sigma_1 \sigma_2} & \frac{1}{\sigma_2^2 }
                                                \end{bmatrix} 
                                                \begin{bmatrix} X_1 - \mu_1 \\ X_2 - \mu_2 \end{bmatrix} \\\\
                                             &= \frac{1}{1 - \rho^2}\Big[\frac{1}{\sigma_1^2 }(X_1 - \mu_1)^2 
                                                                     -\frac{2\rho} {\sigma_1 \sigma_2}(X_1 - \mu_1)(X_2 - \mu_2) 
                                                                     +\frac{1}{\sigma_2^2 }(X_2 - \mu_2)^2 \Big]
            \end{align*}
            \] 
            Therefore, from (1), we obtain the p.d.f for the bivariate normal distribution:
            \[
            f(x) = \frac{1}{2\pi \sigma_1 \sigma_2 \sqrt{(1 - \rho^2)}} 
                   \exp\Big\{-\frac{1}{2(1 - \rho^2)}
                                                \Big[\Big(\frac{X_1 - \mu_1}{\sigma_1}\Big)^2
                                                 -2\rho \Big(\frac{X_1 - \mu_1} {\sigma_1}\Big)  \Big(\frac{X_2 - \mu_2} {\sigma_2}\Big)
                                                 +\Big(\frac{X_2 - \mu_2}{\sigma_2}\Big)^2
                                                \Big]
                    \Big\}
            \]
            When \(\rho = -1 \text{ or } 1\), this p.d.f is undefined and \(f\) is said to be <strong>degenerate</strong>.
        </blockquote>

        <h1 id="dirichlet">Dirichlet Distribution</h1> 
        <blockquote> 
            The <strong>Dirichlet distribution</strong> is a multivariate generalization of <a href="gamma.html"><strong>beta distribution</strong></a>. 
            It has support over the the \((K - 1)\)-dimensional <strong>probability simplex</strong>, defined by
            \[ 
            S_K = \Bigl\{(x_1, x_2, \dots, x_K) \in \mathbb{R}^K: x_k \ge 0,\ \sum_{k=1}^K x_k = 1 \Bigr\}. 
            \] 
            A random vector \(\boldsymbol{x} \in \mathbb{R}^K\) is said to have a Dirichlet distribution with parameters 
            \(\boldsymbol{\alpha} = (\alpha_1, \alpha_2, \dots, \alpha_K)\) (with each \(\alpha_k > 0\)) if its probability 
            density function is given by 
            \[ 
            f(x_1, \dots, x_K; \boldsymbol{\alpha}) = \frac{1}{B(\boldsymbol{\alpha})} \prod_{k=1}^K x_k^{\alpha_k - 1}, \qquad (x_1, \dots, x_K) \in S_K, 
            \]
            or
            \[
            \text{Dir }(\boldsymbol{x} | \boldsymbol{\alpha}) = \frac{1}{B(\boldsymbol{\alpha})} \prod_{k=1}^K x_k^{\alpha_k - 1} \mathbb{I}(\boldsymbol{x} \in S_k),
            \] 
            where the <strong>multivariate beta function</strong> \(B(\boldsymbol{\alpha})\) is defined as 
            \[ 
            B(\boldsymbol{\alpha}) = \frac{\prod_{k=1}^K \Gamma(\alpha_k)}{\Gamma\Bigl(\sum_{k=1}^K \alpha_k\Bigr)}. 
            \]
            <div class="theorem">
                <span class="theorem-title">Moments:</span>
                Let \(\alpha_0 = \sum_{k=1}^K \alpha_k\). Then 
                <ul>
                    <li><strong>Mean:</strong> For each \(k\),
                        \[
                        \mathbb{E}[x_k] = \frac{\alpha_k}{\alpha_0}.
                        \]
                    </li>
                    <li><strong>Variance:</strong>
                        \[
                        \operatorname{Var}[x_k] = \frac{\alpha_k (\alpha_0 - \alpha_k)}{\alpha_0^2 (\alpha_0+1)}.
                        \]
                    </li>
                    Note: Often we use a symmetric Dirichlet prior of the \(\alpha_k = \frac{\alpha}{K}\). Then 
                    \[
                    \mathbb{E}[x_k] = \frac{1}{K}, \quad \operatorname{Var}[x_k] = \frac{K-1}{K^2 (\alpha +1)}.
                    \]
                    We can see that increasing \(\alpha\) increases the precision(decreases the variance) of the distribution. 
                    <li><strong>Covariance:</strong> For \(i \neq j\),
                        \[
                        \operatorname{Cov}[x_i, x_j] = \frac{-\alpha_i \alpha_j}{\alpha_0^2 (\alpha_0+1)}.
                        \]
                    </li>
                </ul>
            </div>

            <div class="proof">
                <span class="proof-title">Relationship to the Gamma Distribution:</span>
                A common way to construct a Dirichlet random vector is as follows. Let \(Y_1, Y_2, \dots, Y_K\) be independent 
                random variables with
                \[
                Y_k \sim \text{Gamma}(\alpha_k, 1), \quad k = 1, \dots, K.
                \]
                Then define
                \[
                X_k = \frac{Y_k}{\sum_{j=1}^K Y_j}, \quad k = 1, \dots, K.
                \]
                It can be shown that \(\mathbf{X} = (X_1, \dots, X_K)\) has the Dirichlet distribution with parameters 
                \(\boldsymbol{\alpha}\). Moreover, when \(K=2\), the Dirichlet distribution reduces to the beta distribution:
                \[
                \text{Beta}(a,b) \equiv \text{Dir}(a,b).
                \]
            </div>
            The parameters \(\alpha_k\) can be thought of as "pseudocounts" or prior observations of each category. When all 
            \(\alpha_k\) are equal (i.e., \(\boldsymbol{\alpha} = \alpha\, \mathbf{1}\)), the distribution is said to be <strong>uniform</strong>
            over the simplex when \(\alpha = 1\) or symmetric if \(\alpha \ne 1\). This symmetry makes the Dirichlet distribution a 
            natural prior in Bayesian models where no category is favored a priori.

            <br><br>
            The Dirichlet distribution is widely used as a <a href="markov.html"><strong>conjugate prior(Dirichlet prior)</strong></a> for the parameters 
            of a multinomial distribution in <a href="bayesian.html"><strong>Bayesian statistics</strong></a>, as well as in machine learning models 
            such as latent Dirichlet allocation (LDA) for topic modeling.
        </blockquote>

        <a href="../../index.html">Back to Home </a>
        <br> <a href="probability.html">Back to Probability </a>   
    </body>
</html>