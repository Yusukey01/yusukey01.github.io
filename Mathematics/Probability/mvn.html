<!DOCTYPE html>
<html>
    <head> 
        <title>Multivariate Distributions</title>
        <link rel="stylesheet" href="../../css/styles.css">
        <script id="MathJax-script" defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <meta name="viewport" content="width=device-width, initial-scale=1">
    </head>
    <body>
        <nav class="navbar">
            <div class="logo">
                <h1>Section III - Probability & Statistics</h1>
            </div>
            <ul class="nav-links">
                <li><a href="../../index.html">Home</a></li>
                <li><a href="../Linear_algebra/linear_algebra.html">Linear Algebra</a></li>
                <li><a href="../Calculus/calculus.html">Calculus to Optimization & Analysis</a></li>
                <li><a href="probability.html">Probability & Statistics</a></li>
                <li><a href="../Discrete/discrete_math.html">Discrete Mathematics & Algorithms</a></li>
            </ul>
            <div class="menu-toggle">&#9776;</div>
        </nav>

        <div class="hero-section">
            <h1 class="webpage-name">Multivariate Distributions
            </h1>
        </div>

        <div class="topic-nav">
            <a href="#mn">Multivariate Normal Distribution</a>
            <a href="#dirichlet">Dirichlet Distribution</a>
            <a href="#wishart"> Wishart distribution</a>
        </div> 

        <div class="container">  
           
            <section id="mn" class="section-content">
            <h2>Multivariate Normal Distribution</h2>
            <p>
            In machine learning, the most important joint probability distribution for continuous random variables is the 
            <strong>multivariate normal distribution</strong> (MVN). 
            The multivariate normal distribution of a \(n\) dimentional random vector \(\boldsymbol{x} \in \mathbb{R}^n \) is denoted as 
            \[
            \boldsymbol{X} \sim \mathcal{N}(\boldsymbol{\mu}, \Sigma)
            \]
            where \(\boldsymbol{\mu} = \mathbb{E} [\boldsymbol{x}] \in \mathbb{R}^n\) is the mean vector, and 
            \(\Sigma = \text{Cov }[\boldsymbol{x}] \in \mathbb{R}^{n \times n}\) is the covariance matrix. 
            <br>
            The p.d.f. is given by
            \[
            f(\boldsymbol{x}) = \frac{1}{\sqrt{(2\pi)^n \det(\Sigma)}} \exp \Big[-\frac{1}{2}(\boldsymbol{x} - \boldsymbol{\mu})^T \Sigma^{-1} (\boldsymbol{x} -\boldsymbol{\mu}) \Big]. \tag{1}
            \]
            The expression inside the exponential (ignoring the factor of -\frac{1}{2}) is the squared <strong>Mahalanobis distance</strong> between 
            the data vector \(\boldsymbol{x}\) and the mean vector \(\boldsymbol{\mu}\), given by 
            \[
            d_{\Sigma} (\boldsymbol{x}, \boldsymbol{\mu})^2 = (\boldsymbol{x} - \boldsymbol{\mu})^T \Sigma^{-1} (\boldsymbol{x} -\boldsymbol{\mu}).
            \]
            <br>
            If \(\boldsymbol{x} \in \mathbb{R}^2\), the MVN is known as the <strong>bivariate normal distribution</strong>.
            In this case, 
            \[
            \begin{align*}
            \Sigma  &=  \begin{bmatrix}
                            \text{Var } (X_1) & \text{Cov }[X_1, X_2] \\
                            \text{Cov }[X_2, X_1] & \text{Var } (X_2)
                        \end{bmatrix} \\\\
                    &=  \begin{bmatrix}
                            \sigma_1^2 & \rho \sigma_1 \sigma_2 \\
                            \rho \sigma_1 \sigma_2 & \sigma_2^2
                        \end{bmatrix} \\\\
            \end{align*}
            \]
            where \(\rho\) is the correlation coefficient defined by 
            \[
            \text{Corr }[X_1, X_2] = \frac{\text{Cov }[X_1, X_2]}{\sqrt{\text{Var }(X_1)\text{Var }(X_2)}}.
            \]
            Then 
            \[
            \begin{align*}
            \det (\Sigma) &= \sigma_1^2 \sigma_2^2 - \rho^2 \sigma_1^2 \sigma_2^2 \\\\
                          &= \sigma_1^2 \sigma_2^2 (1 - \rho^2) 
            \end{align*}
            \]
            and 
            \[
            \begin{align*}
            \Sigma^{-1} &= \frac{1}{\det (\Sigma )}
                          \begin{bmatrix}
                           \sigma_2^2 & -\rho \sigma_1 \sigma_2 \\
                           -\rho \sigma_1 \sigma_2 & \sigma_1^2
                          \end{bmatrix} \\\\
                        &= \frac{1}{1 - \rho^2}
                          \begin{bmatrix}
                           \frac{1}{\sigma_1^2 } & \frac{-\rho} {\sigma_1 \sigma_2} \\
                           \frac{-\rho} {\sigma_1 \sigma_2} & \frac{1}{\sigma_2^2 }
                          \end{bmatrix} 
            \end{align*}
            \]
            Note that in Expression (1), \((\boldsymbol{x} - \boldsymbol{\mu})^T \Sigma^{-1} (\boldsymbol{x} -\boldsymbol{\mu})\) is a <strong>quadratic form</strong>. So, 
            \[
            \begin{align*}
            (\boldsymbol{x} - \boldsymbol{\mu})^T \Sigma^{-1} (\boldsymbol{x} -\boldsymbol{\mu}) 
                &= \frac{1}{1 - \rho^2} \begin{bmatrix} X_1 - \mu_1 & X_2 - \mu_2 \end{bmatrix}
                    \begin{bmatrix}
                    \frac{1}{\sigma_1^2 } & \frac{-\rho} {\sigma_1 \sigma_2} \\
                    \frac{-\rho} {\sigma_1 \sigma_2} & \frac{1}{\sigma_2^2 }
                    \end{bmatrix} 
                    \begin{bmatrix} X_1 - \mu_1 \\ X_2 - \mu_2 \end{bmatrix} \\\\
                    &= \frac{1}{1 - \rho^2}\Big[\frac{1}{\sigma_1^2 }(X_1 - \mu_1)^2 
                                            -\frac{2\rho} {\sigma_1 \sigma_2}(X_1 - \mu_1)(X_2 - \mu_2) 
                                            +\frac{1}{\sigma_2^2 }(X_2 - \mu_2)^2 \Big].
            \end{align*}
            \] 
            Therefore, we obtain the p.d.f for the bivariate normal distribution:
            \[
            f(\boldsymbol{x}) = \frac{1}{2\pi \sigma_1 \sigma_2 \sqrt{(1 - \rho^2)}} 
                   \exp\Big\{-\frac{1}{2(1 - \rho^2)}
                                                \Big[\Big(\frac{X_1 - \mu_1}{\sigma_1}\Big)^2
                                                 -2\rho \Big(\frac{X_1 - \mu_1} {\sigma_1}\Big)  \Big(\frac{X_2 - \mu_2} {\sigma_2}\Big)
                                                 +\Big(\frac{X_2 - \mu_2}{\sigma_2}\Big)^2
                                                \Big]
                    \Big\}
            \]
            When \(\rho = -1 \text{ or } 1\), this p.d.f is undefined and \(f\) is said to be <strong>degenerate</strong>.
            </p>
            </section>

            <section id="dirichlet" class="section-content">
            <h2>Dirichlet Distribution</h2>
            <p>
            The <strong>Dirichlet distribution</strong> is a multivariate generalization of <a href="gamma.html"><strong>beta distribution</strong></a>. 
            It has support over the the \((K - 1)\)-dimensional <strong>probability simplex</strong>, defined by
            \[ 
            S_K = \Bigl\{(x_1, x_2, \dots, x_K) \in \mathbb{R}^K: x_k \ge 0,\ \sum_{k=1}^K x_k = 1 \Bigr\}. 
            \] 
            A random vector \(\boldsymbol{x} \in \mathbb{R}^K\) is said to have a Dirichlet distribution with parameters 
            \(\boldsymbol{\alpha} = (\alpha_1, \alpha_2, \dots, \alpha_K)\) (with each \(\alpha_k > 0\)) if its probability 
            density function is given by 
            \[ 
            f(x_1, \dots, x_K; \boldsymbol{\alpha}) = \frac{1}{B(\boldsymbol{\alpha})} \prod_{k=1}^K x_k^{\alpha_k - 1}, \qquad (x_1, \dots, x_K) \in S_K, 
            \]
            or
            \[
            \text{Dir }(\boldsymbol{\alpha}) = \frac{1}{B(\boldsymbol{\alpha})} \prod_{k=1}^K x_k^{\alpha_k - 1} \mathbb{I}(\boldsymbol{x} \in S_k),
            \] 
            where the <strong>multivariate beta function</strong> \(B(\boldsymbol{\alpha})\) is defined as 
            \[ 
            B(\boldsymbol{\alpha}) = \frac{\prod_{k=1}^K \Gamma(\alpha_k)}{\Gamma\Bigl(\sum_{k=1}^K \alpha_k\Bigr)}. 
            \]
            <div class="theorem">
                <span class="theorem-title">Moments:</span>
                Let \(\alpha_0 = \sum_{k=1}^K \alpha_k\). Then for each \(k\),
                <ul style="padding-left: 40px;">
                    <li><strong>Mean:</strong> 
                        \[
                        \mathbb{E}[x_k] = \frac{\alpha_k}{\alpha_0}.
                        \]
                    </li>
                    <li><strong>Variance:</strong>
                        \[
                        \operatorname{Var}[x_k] = \frac{\alpha_k (\alpha_0 - \alpha_k)}{\alpha_0^2 (\alpha_0+1)}.
                        \]
                    </li>
                    Note: Often we use a symmetric Dirichlet prior of the \(\alpha_k = \frac{\alpha}{K}\). Then 
                    \[
                    \mathbb{E}[x_k] = \frac{1}{K}, \quad \operatorname{Var}[x_k] = \frac{K-1}{K^2 (\alpha +1)}.
                    \]
                    We can see that increasing \(\alpha\) increases the precision(decreases the variance) of the distribution. 
                    <li><strong>Covariance:</strong> For \(i \neq j\),
                        \[
                        \operatorname{Cov}[x_i, x_j] = \frac{-\alpha_i \alpha_j}{\alpha_0^2 (\alpha_0+1)}.
                        \]
                    </li>
                </ul>
            </div>
            The parameters \(\alpha_k\) can be thought of as "pseudocounts" or prior observations of each category. When all 
            \(\alpha_k\) are equal (i.e., \(\boldsymbol{\alpha} = \alpha\, \mathbf{1}\)), the distribution is said to be <strong>uniform</strong>
            over the simplex when \(\alpha = 1\) or symmetric if \(\alpha \ne 1\). This symmetry makes the Dirichlet distribution a 
            natural <a href="markov.html"><strong>prior in Bayesian models</strong></a> where no category is favored a priori.
            <br><br>
            The Dirichlet distribution is widely used as a conjugate prior for the parameters 
            of a multinomial distribution in <a href="bayesian.html"><strong>Bayesian statistics</strong></a>, as well as in machine learning models 
            such as latent Dirichlet allocation (LDA) for topic modeling.
            </p>
            </section>

            <section id="wishart" class="section-content">
            <h2>Wishart Distribution</h2>
            <p>
            The <strong>Wishart distribution</strong> is a fundamental multivariate distribution that generalizes the <a href="gamma.html"><strong>gamma distribution</strong></a> to 
            <a href="../Linear_algebra/symmetry.html"><strong>positive definite matrices</strong></a>. 
         
            The p.d.f. of Wishart distribution is defined as follows:
            \[
            \operatorname{Wi}(\boldsymbol{\Sigma} | \boldsymbol{S}, \nu) 
            = \frac{1}{Z} |\boldsymbol{\Sigma}| ^{(\nu-D-1)/2} \exp \Bigl( - \frac{1}{2} \text{ tr } (\boldsymbol{S}^{-1} \boldsymbol{\Sigma})\Bigr)
            \]
            where 
            \[
            Z = 2^{\nu D/2} | \boldsymbol{S} |^{-\nu / 2}\,\Gamma_D (\frac{\nu}{2}).
            \] 
            Here, \(\Gamma_D(\cdot)\) denotes the multivariate gamma function. The parameters are:
            <ul style="padding-left: 40px;">
                <li>\(\nu\): the <strong>degrees of freedom</strong> (which must satisfy \(\nu \ge D\) for the distribution to be well-defined),</li>
                <li>\(\boldsymbol{S}\): the <strong>scale matrix</strong> (a \(D \times D\) positive definite matrix).</li>
            </ul>
            Note: \(\text{ mean } = \nu \boldsymbol{S}\).
            <br><br>
            If \(D=1\), ir reduces to the gamma distribution:
            \[
            \operatorname{Wi}(\lambda, s^{-1}, \nu) = \operatorname{Gamma}(\lambda | \text{ shape } = \frac{\nu}{2}, \text{ rate } = \frac{1}{2s}).
            \]
            If we set \(s=2\), this further reduces to the <strong>chi-squared distribution</strong>.
            <br><br>
            <strong>Applications:</strong>
            <ul style="padding-left: 40px;">
                <li>Covariance Matrix Estimation:</li>
                In multivariate statistics, the Wishart distribution is used to model the uncertainty in <a href="covariance.html"><strong>covariance matrix</strong></a> estimates.
                <br>
                Given \(D\) i.i.d. samples from \(\mathcal{N}(0, \boldsymbol{\Sigma})\), the sample covariance matrix
                \[
                \boldsymbol{S} = \sum_{i=1}^D x_i {x_i}^T
                \]
                follows a Wishart distribution:
                \[
                \boldsymbol{S} \sim \operatorname{Wi}( \boldsymbol{\Sigma}, D).
                \]
                <li>Bayesian Inference:</li>
                    It serves as a conjugate prior for the covariance matrix in multivariate normal models, enabling closed-form 
                    updates of the posterior distribution. We often use the <strong>inverse Wishart distribution</strong>:
                    <br>
                    For \(\nu > D -1\) and \(\boldsymbol{S} \succ 0\),
                    \[
                    \operatorname{IW}(\boldsymbol{\Sigma} | \boldsymbol{S}^{-1}, \nu) 
                    = \frac{1}{Z_{IW}} |\boldsymbol{\Sigma}| ^{-(\nu+D+1)/2} \exp \Bigl( - \frac{1}{2} \text{ tr } (\boldsymbol{S} \boldsymbol{\Sigma}^{-1})\Bigr)
                    \]
                    where 
                    \[
                    Z_{IW} = 2^{\nu D/2} | \boldsymbol{S} |^{\nu / 2}\,\Gamma_D (\frac{\nu}{2}).
                    \] 
                    Then 
                    \[
                    \boldsymbol{\Sigma} \sim \operatorname{IW}( \boldsymbol{S}, \nu).
                    \]
                    (Remember, \(\lambda \sim \text{Gamma }(a, b)\) then \(\frac{1}{\lambda} \sim  \operatorname{IG}(a,b)\). 
                    So, similarly, the IW is multivariate generalization of the IG.)
            </ul>
            Note: One is about sampling properties of the estimator (frequentist), and the other is about prior beliefs and posterior 
            inference (Bayesian). In practice, both deal with uncertainty in covariance matrices, and the same distribution arises in 
            both roles — just with different interpretations. 
            </p>
            </section>
        </div> 
        <!-- Footer -->
        <footer>
            <div class="footer-content">
                <div class="footer-links">
                    <h3>Quick Links</h3>
                    <ul>
                        <li><a href="../../index.html">Home</a></li>
                        <li> <a href="probability.html">Section III - Probability & Statistics </a> </li>
                        <li><a href="basic.html">Basic Probability Ideas</a></li>   
                        <li><a href="random_variables.html">Random Variables</a></li> 
                        <li><a href="gamma.html">Gamma & Beta Distribution</a></li>
                        <li><a href="gaussian.html">Normal (Gaussian) Distribution</a></li> 
                        <li><a href="student.html">Student's \(t\)-Distribution</a></li>
                        <li><a href="covariance.html">Covariance</a></li>   
                        <li><a href="correlation.html">Correlation</a></li> 
                        <li><a href="mvn.html">Multivariate Distributions</a></li>
                        <li><a href="mle.html">Maximum Likelihood Estimate</a></li> 
                        <li><a href="hypothesis_testing.html">Statistical Inference & Hypothesis Testing</a></li>
                        <li><a href="linear_regression.html">Linear Regression</a></li>   
                        <li><a href="entropy.html">Entropy</a></li> 
                        <li><a href="convergence.html">Convergence</a></li>
                        <li><a href="bayesian.html">Intro to Bayesian Statistics</a></li> 
                        <li><a href="expfamily.html">The Exponential Family</a></li>
                        <li><a href="fisher_info.html">Fisher Information Matrix</a></li>
                        <li><a href="decision_theory.html">Bayesian Decision Theory</a></li>
                        <li><a href="markov.html">Markov Chains</a></li>          
                    </ul>
                </div>
            </div>
            
            <div class="footer-bottom">
                <p>&copy; 2025 Math-CS Compass. All rights reserved.</p>
            </div>
        </footer>
        <script src="/js/main.js"></script>   
    </body>
</html>