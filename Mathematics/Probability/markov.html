<!DOCTYPE html>
<html>
    <head> 
        <title>Markov Chain</title>
        <link rel="stylesheet" href="../styles.css">
        <script id="MathJax-script" defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <meta name="viewport" content="width=device-width, initial-scale=1">
    </head>
    <body> 
        <div class="toc-container">
            <h2>Contents</h2>
            <ul>
                <li><a href="#lm">Language Model</a></li>
                <li><a href="#MLE">Parameter estimation of Markov models</a></li>
            </ul>
        </div>
        <h1 id="lm">Language Model</h1>
        <blockquote>
            The concept of the <strong>Markov chain</strong> is fundamental in various fields. while the core idea remains 
            the same, its applications and methods of analysis can differ depending on the field. Here, we'll explore a specific application in 
            the context of <strong>language modeling</strong>.
            <br><br>
            Suppose our goal is to model a joint probability distribution over variable-length sequences: \(P(y_{1:T})\), where each 
            \(y_t \in \{1, \cdots, K\}\) represents a <strong>word</strong> from a <strong>vocabulary</strong> of size \(K\).
            A language model assigns probabilities to possible sentences (sequences) of length \(T\).
            <br><br>
            By the chain rule of probability, 
            \[
            P(y_{1:T}) = P(y_1)P(y_2 | y_1)P(y_3 | y_2, y_1)\cdots = \prod_{t = 1}^{T} P(y_t | y_{t_1 : t-1}).
            \]
            However, as \(T\) increases, this formulation becomes computationally expensive due to the need to condition on the 
            entire history. To address this, we make the <strong>Markov assumption</strong>: the next state depends only on the 
            current state.
            <br><br>
            Under the <strong>first-order Markov assumption</strong>,
            \[
            P(y_{1:T}) = P(y_1)P(y_2 | y_1)P(y_3 | y_2)P(y_4 | y_3)\cdots =  P(y_1)\prod_{t = 2}^{T} P(y_t | y_{t-1}).
            \]
            This <strong>memoryless property</strong> simplifies both analysis and computation of probabilities. 
            The function \(P(y_t | y_{t-1})\) is called the <strong>transition function</strong> or <strong>transition kernel</strong>, 
            and it satisfies:
            <ul>
                <li>\(P(y_t | y_{t-1}) \geq 0\)</li>
                <li>\(\sum_{k=1}^K P(y_t = k | y_{t-1} = j) = 1\) for each \(j\)</li>
            </ul>
            We can represent the transition probabilities using a
            <a href="../Linear_algebra/stochastic.html"><strong>stochastic matrix</strong></a>:
            \[
            A_{jk} = P(y_t = k | y_{t-1} = j),
            \]
            where each row of \(A\) sums to 1. This matrix is considered as the conditional probability Table (CPT). 
            Since we assume the same transition probabilities at all time steps, the model is said to be <strong>time-invariant</strong>. 
            <br><br>
            The Markov assumption can be extended to consider the last \(M\) states( or memory length):
            \[
            P(y_{1:T}) = P(y_{1 : M}) \prod_{t = M + 1}^T P(y_t | y_{t - M : t - 1}).
            \]
            This is known as an <strong>M'th order Markov model</strong>. In language modeling, this is equivalent to an \(M+1\)-gram model.
            For example, if \(M = 2\), 
            each word depends on the two preceding words, leading to a <strong>trigram model</strong>:
            \[
            P(y_t | y_{t-1}, y_{y-2}).
            \] 
            <br>
            Any Higher-order Markov model can be converted into the first-order Markov model by redefining the state to include the past \(M\) observations. 
            For \(M = 2\), define \(\tilde{y}_t = (y_{t-1}, y_t)\), then:
            \[
            \begin{align*}
            P(\tilde{y}_{1:T}) &= P(\tilde{y}_2) \prod_{t = 3}^T P(\tilde{y}_t | \tilde{y}_{t-1}) \\\\
                               &= P(y_1, y_2) \prod_{t = 3}^T P(y_t | y_{t-1}, y_{t-2}).
            \end{align*}
            \]
            <br>
            In practice, with large vocabularies, modeling all possible transitions becomes infeasible, and we need additional techniques such as 
            <strong>neural language models</strong> to approximate the distribution efficiently. Not only language modeling, <strong>Markov chains</strong> 
            are widely used in <strong>sequential data modeling</strong>. Additionally, <strong>Markov Chain Monte Carlo (MCMC)</strong> methods are crucial 
            in Bayesian statistics for performing approximate inference when direct sampling is challenging.
        </blockquote>

        <h1 id="MLE">Parameter estimation of Markov models</h1>
        <blockquote>
            The probability of any particular sequence of length T is given by 
            \[
            \begin{align*}
            P(x_{1:T} | \theta) &= \pi (x_1)A(x_1, x_2)\cdots A(x_{T-1}, x_T) \\\\
                                &= \prod_{j=1}^K (\pi_j)^{\mathbb{I}(x_1 =j)} \prod_{t=2}^T \prod_{j=1}^K \prod_{k=1}^K 
                                (A_{jk})^{\mathbb{I}(x_t=k,\, x_{t-1}=j)}, \tag{1}
            \end{align*}
            \]
            where \(\pi_j\) is probabilty that the first symbol is \(j\), and \(A_{jk}\) is probability of going from symbol 
            \(j\) to symbol \(k\). This is the transition probability matrix. In addition, \(\mathbb{I}(\cdot)\) is the indicator function. 
            For example, 
            \[
            \mathbb{I}(x_1 =j) =  \begin{cases}
                                    1 &\text{if \(x_1 = j\)} \\
                                    0 &\text{otherwise}
                                    \end{cases}.
            \]
            This lets us convert sums and products into counts. In Equation 1, only one transition happens at a time, so only one term contributes 
            in each time step.
            <br><br>
            Thus, the log-likelihood of a set of sequences \(\mathcal{D} = (x_1, \cdots, x_N)\), where 
            \(x_i = (x_{i\,1}, \cdots, x_{i \, T_{i}})\) is a sequence of length \(T_i\) is given by 
            \[
            \begin{align*}
            \log P(\mathcal{D} | \theta) &= \sum_{i=1}^N \log P(x_1 | \theta) \\\\
                                         &= \sum_j  N_j^1 \log \pi_i + \sum_j \sum_k N_{jk} \log A_{jk}
            \end{align*}
            \]
            where we define the following counts: 
            \[
            \begin{align*}
            & N_j^1 = \sum_{i=1}^N \mathbb{I}(x_{i1} = j)\\\\
            & N_{jk}  = \sum_{i=1}^N \sum_{t=1} ^{T_i -1} \mathbb{I}(x_{i,t}  = j, x_{i, t+1} = k)\\\\
            & N_j = \sum_k N_{jk}.
            \end{align*}
            \]
            Now, we want to get \(\hat{\pi}_j\) and \( \hat{A}_{jk}\) that is maximum likelihood estimation.
            <br>
            Note that 
            \[
            \begin{align*}
            &\sum_j \pi_j = 1  \\\\
            &\sum_k A_{jk} = 1 \text{ for each } j
            \end{align*}
            \]
            By adding Lagrange multipliers to enforce the sum to one constraint, the MLE is given by the 
            normalized counts:
            \[
            \hat{\pi}_j = \frac{N_j^1}{\sum_{j^{\prime}} {N_{j^{\prime}}^1}}, \quad \hat{A}_{jk} = \frac{N_{jk}}{N_j}
            \]
            \(N_j^1\) represents how often symbol \(j\) is seen at the "start" of a sequence. Instead, we often use \(N_j\), which 
            is how often symbol \(j\) is seen "anywhere" in a sequence. This is called <strong>unigram statistics</strong>, whereas 
            \(N_{jk}\) is called <strong>bigram statistics</strong>.
        </blockquote>

        <h1></h1>
        <blockquote>
          
            
        </blockquote>

        <br><a href="../../index.html">Back to Home </a>
        <br> <a href="probability.html">Back to Probability </a>   
    </body>
</html>