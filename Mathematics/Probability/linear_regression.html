<!DOCTYPE html>
<html>
    <head> 
        <title>Linear Regression</title>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <link rel="stylesheet" href="../styles.css">
        <meta name="viewport" content="width=device-width, initial-scale=1">
    </head>
    <body> 
        <h1>Recap from Linear Algebra</h1>
        <blockquote>
            Given a set of observed data points \(\{(x_i, y_i)\}_{i = 1}^{n}\) where \(x_i \in \mathbb{R}^d, \quad y_i \in \mathbb{R}\),
            we assume that the given data can be explained by the <strong>linear model</strong>:
            \[y = X\beta + \epsilon \]
            where \(X \in \mathbb{R}^{n \times d}\) is the <strong>design matrix</strong>, \(\beta \in \mathbb{R}^d\) is the <strong>parameter(weight) vector</strong>, \(y \in \mathbb{R}^n\) is the
            <strong>observation vector</strong>, and \(\epsilon = y - X\beta\) is a <strong>residual vector</strong>. 
            <br><br>
            The dimension \(d\) is the number of features, and \(n\) is the number of data points. 
            The residual \(\epsilon\) is the "difference" between each observed value \(y_i\) and its corresponding predicted \(y\) value.
            The <strong>least-squares hyperplane</strong> represents the set of predicted \(y\) values based on "estimated" parameters(weights) \(\hat{\beta}\), 
            and it must satisfy the <strong>normal equations</strong>:
            \[
            X^TX\hat{\beta} = X^Ty \tag{1}
            \]
            <br><br>
            Note: the linear model is linear in terms of <strong>"parameters \(\beta \, \)</strong> , not \(X\). We can choose any non-linear transformation
            for each \(x_i\). For example, \(y = \beta_0 + \beta_1 x^2 + \beta_2 \sin(2\pi x)\) is linear. So, \(Y\) is modeled as a 
            <strong>linear combination</strong> of features(predictors) \(X\) with respect to the coefficients(weights) \(\beta\).
        </blockquote>

        <h1>MLE for Linear Regression</h1>
        <blockquote>
            Here, we update the linear regression by introducing probabilistic perspectives. We assume that \(y_i\) is the observed value of 
            the random variable \(Y_i\) and it depends on \(x_i\) and \(\epsilon_i\) is said to be a random error that is an i.i.d. random variable with 
            \(\epsilon_i \sim N(0, \sigma^2)\). Then since \(\mathbb{E}[\epsilon_i]=0\), the unknown mean of \(Y_i\) is represented as
            \[
            \mathbb{E }[Y_i] = \mu_i = x_i^T \beta.
            \]
            This is the true regression line between \(\mathbb{E }[Y_i]\) and \(x_i\) with unknown parameters \(\beta \in \mathbb{R}^d\) and 
            \(Y_i\) is the independent random variable \(Y_i \sim N(\mu_i, \sigma^2)\). 
            <br>
            In other world, 
            \[
            p(y_i | x_i, \beta, \sigma^2) = \frac{1}{\sigma \sqrt{2\pi}} \exp \Big\{-\frac{1}{2\sigma^2}(y_i - x_i^T \beta)^2 \Big\}
            \]
            and its likelihood function of \(\beta\) for fixed \(\sigma^2\) is given by 
            \[
            \begin{align*}
            L(\beta) &= \prod_{i =1}^n \Big[ \frac{1}{\sigma \sqrt{2\pi}} \exp \Big\{-\frac{1}{2\sigma^2}(y_i - x_i^T \beta)^2 \Big\} \Big] \\\\
                     &=  \Big(\frac{1}{\sigma \sqrt{2\pi}}\Big)^n \exp \Big\{-\frac{1}{2\sigma^2} \sum_{i=1}^n (y_i - x_i^T \beta)^2 \Big\}.
            \end{align*}
            \]
            The log-likelihood fuction is given by 
            \[
            \ln L(\beta) = n \ln \Big(\frac{1}{\sigma \sqrt{2\pi}}\Big) -\frac{1}{2\sigma^2} \sum_{i=1}^n (y_i - x_i^T \beta)^2
            \]
            Setting the derivative with respect to \(\beta\) equal to zero: 
            \[
            \begin{align*}
            & \nabla_{\beta} = -\frac{1}{\sigma^2} \sum_{i=1}^n (y_i - x_i^T \beta) \cdot x_i = 0 \\\\
            &\Longrightarrow \sum_{i=1}^n (x_i^T \beta -y_i) \cdot x_i = 0 \\\\
            &\Longrightarrow (\sum_{i=1}^n x_ix_i^T)\beta - \sum_{i=1}^n x_i y_i = 0 \\\\
            &\Longrightarrow X^TX \beta = X^Ty
            \end{align*}
            \]
            This is equivalent to the normal equations (1) and thus, the MLE of linear regression model is actually 
            the same as the least-square estimate (LSE):
            \[
            \begin{align*}
            \hat{\beta}_{MLE} &= \Big(\sum_{i=1}^n x_ix_i^T \Big)^{-1}\Big(\sum_{i=1}^n x_i y_i \Big) \\\\
                              &=(X^TX)^{-1}X^Ty \\\\
                              &= \hat{\beta}_{LSE} 
            \end{align*}
            \]
            <br><br>
            <div class="proof">
                <span class="proof-title">Minimizing Least-Square</span>
                \[
                \begin{align*}
                \hat{\beta}_{LSE} &= \arg \min_{\beta} \| y - X \beta \|^2 \\\\
                                  &= \arg \min_{\beta} (y - X \beta)^T (y - X \beta) \\\\
                                  &= \arg \min_{\beta} y^T y - y^T X \beta - \beta^T X^T y + \beta^T X^T X \beta \\\\
                                  &= \arg \min_{\beta} f(\beta)

                \end{align*}
                \]
                Differenciating \(f(\beta)\):
                \[
                \begin{align*}
                df &= - X^Ty - X^Ty + 2X^TX\beta \\\\
                   &= -2 X^Ty + 2X^TX\beta  \\\\
                \end{align*}
                \] 
                
            </div>
        </blockquote>

      

        <br><a href="../../index.html">Back to Home </a>
        <br> <a href="probability.html">Back to Probability </a>   
    </body>
</html>