<!DOCTYPE html>
<html>
    <head> 
        <title>Linear Regression</title>
        <link rel="stylesheet" href="../../css/styles.css">
        <script id="MathJax-script" defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <meta name="viewport" content="width=device-width, initial-scale=1">
    </head>
    <body>
        <nav class="navbar">
            <div class="logo">
                <h1>Section III - Probability & Statistics</h1>
            </div>
            <ul class="nav-links">
                <li><a href="../../index.html">Home</a></li>
                <li><a href="../Linear_algebra/linear_algebra.html">Linear Algebra</a></li>
                <li><a href="../Calculus/calculus.html">Calculus to Optimization & Analysis</a></li>
                <li><a href="probability.html">Probability & Statistics</a></li>
                <li><a href="../Discrete/discrete_math.html">Discrete Mathematics & Algorithms</a></li>
            </ul>
            <div class="menu-toggle">&#9776;</div>
        </nav>

        <div class="hero-section">
            <h1 class="webpage-name">Linear Regression
            </h1>
        </div>

        <div class="topic-nav">
            <a href="#recap">Recap from Linear Algebra</a>
            <a href="#lr">Linear Regression: A Probabilistic Perspective</a>
        </div> 

        <div class="container">  
           
            <section id="recap" class="section-content">
            <h2>Recap from Linear Algebra</h2>
            <p>
            Given a set of observed data points \(\{(x_i, y_i)\}_{i = 1}^{n}\) where \(x_i \in \mathbb{R}^d, \quad y_i \in \mathbb{R}\),
            we assume that the given data can be explained by the <strong>linear model</strong>:
            \[y = X\beta + \epsilon \tag{1} \]
            where \(X \in \mathbb{R}^{n \times d}\) is the <strong>design matrix</strong>, \(\beta \in \mathbb{R}^d\) is the <strong>parameter(weight) vector</strong>, \(y \in \mathbb{R}^n\) is the
            <strong>observation vector</strong>, and \(\epsilon = y - X\beta\) is a <strong>residual vector</strong>. 
            <br><br>
            The dimension \(d\) is the number of features, and \(n\) is the number of data points. 
            The residual \(\epsilon\) is the "difference" between each observed value \(y_i\) and its corresponding predicted \(y\) value.
            The <strong>least-squares hyperplane</strong> represents the set of predicted \(y\) values based on "estimated" parameters(weights) \(\hat{\beta}\), 
            and it must satisfy the <strong>normal equations</strong>:
            \[
            X^TX\hat{\beta} = X^Ty \tag{2}
            \]
            <br><br>
            Note: the linear model is linear in terms of <strong>parameters \(\beta \, \)</strong> , not \(X\). We can choose any non-linear transformation
            for each \(x_i\). For example, \(y = \beta_0 + \beta_1 x^2 + \beta_2 \sin(2\pi x)\) is linear. So, \(y\) is modeled as a 
            <strong>linear combination</strong> of features(predictors) \(X\) with respect to the coefficients(weights) \(\beta\).
            </p>
            </section>

            <section id="lr" class="section-content">
            <h2>Linear Regression: A Probabilistic Perspective</h2>
            <p>
            We consider a probabilistic model for linear regression.  We assume that \(y_i\) is the observed value of 
            the random variable \(Y_i\) and it depends on the predictor \(x_i\). In addition, the random error\(\epsilon_i\) is 
            an i.i.d. random variable following \(\epsilon_i \sim N(0, \sigma^2)\). Then since \(\mathbb{E}[\epsilon_i]=0\), the unknown 
            mean of \(Y_i\) can be represented as
            \[
            \mathbb{E }[Y_i] = \mu_i = x_i^T \beta
            \]
            where \(\beta \in \mathbb{R}^d\) represents the unknown parameters of the regression model. 
            <br>
            This relationship defines the true regression line between \(\mathbb{E }[Y_i]\) and \(x_i\). Here, \(Y_i\) is 
            the independent random variable \(Y_i \sim N(\mu_i, \sigma^2)\). 
            <br>
            In other world, the conditional p.d.f of \(y_i\) is given by 
            \[
            p(y_i | x_i, \beta, \sigma^2) = \frac{1}{\sigma \sqrt{2\pi}} \exp \Big\{-\frac{1}{2\sigma^2}(y_i - x_i^T \beta)^2 \Big\}
            \]
            and its likelihood function of \(\beta\) for fixed \(\sigma^2\) is given by 
            \[
            \begin{align*}
            L(\beta) &= \prod_{i =1}^n \Big[ \frac{1}{\sigma \sqrt{2\pi}} \exp \Big\{-\frac{1}{2\sigma^2}(y_i - x_i^T \beta)^2 \Big\} \Big] \\\\
                     &=  \Big(\frac{1}{\sigma \sqrt{2\pi}}\Big)^n \exp \Big\{-\frac{1}{2\sigma^2} \sum_{i=1}^n (y_i - x_i^T \beta)^2 \Big\}.
            \end{align*}
            \]
            The log-likelihood fuction is given by 
            \[
            \ln L(\beta) = n \ln \Big(\frac{1}{\sigma \sqrt{2\pi}}\Big) -\frac{1}{2\sigma^2} \sum_{i=1}^n (y_i - x_i^T \beta)^2
            \]
            Setting the derivative with respect to \(\beta\) equal to zero: 
            \[
            \begin{align*}
            & \nabla_{\beta} = -\frac{1}{\sigma^2} \sum_{i=1}^n (y_i - x_i^T \beta) \cdot x_i = 0 \\\\
            &\Longrightarrow \sum_{i=1}^n (x_i^T \beta -y_i) \cdot x_i = 0 \\\\
            &\Longrightarrow (\sum_{i=1}^n x_ix_i^T)\beta - \sum_{i=1}^n x_i y_i = 0 \\\\
            &\Longrightarrow X^TX \beta = X^Ty
            \end{align*}
            \]
            This is equivalent to the normal equations (2) and thus, the MLE solution for linear regression corresponds to 
            the <strong>least-squares</strong> solution:
            \[
            \begin{align*}
            \hat{\beta}_{MLE} &= \Big(\sum_{i=1}^n x_ix_i^T \Big)^{-1}\Big(\sum_{i=1}^n x_i y_i \Big) \\\\
                              &=(X^TX)^{-1}X^Ty \\\\
                              &= \hat{\beta}_{LS} 
            \end{align*}
            \]
            <br>
            <div class="proof">
                <span class="proof-title">Least-Squares Solution</span>
                Consider the linear model (1) where \(X \in \mathbb{R}^{n \times d}\), \(\, y \in \mathbb{R}^n\), 
                \(\, \beta \in \mathbb{R}^d\).
                <br>
                To obtain the least-squares solution \(\hat{\beta}_{LS}\), we minimize the <strong>least-squares error</strong>:
                \[
                \begin{align*}
                \hat{\beta}_{LS} &= \arg \min_{\beta} \| y - X \beta \|_{2}^2 \\\\
                                  &= \arg \min_{\beta} (y - X \beta)^T (y - X \beta) \\\\
                                  &= \arg \min_{\beta} y^T y - y^T X \beta - \beta^T X^T y + \beta^T X^T X \beta \\\\
                                  &= \arg \min_{\beta} f(\beta)

                \end{align*}
                \]
                Differenciating \(f(\beta)\) and setting it equal to zero: 
                \[
                \begin{align*}
                & df = - X^Ty - X^Ty + 2X^TX\beta \\\\
                &\Longrightarrow -2 X^Ty + 2X^TX\beta = 0 \\\\
                &\Longrightarrow \hat{\beta}_{LS}  = (X^TX)^{-1}X^Ty \\\\
                \end{align*}
                \] 
                Note: The matrix \(X^TX\) is symmetric. 
                <br>
                If you are not familiar with Matrix Calculus, See: <a href="../Calculus/linear_approximation.html">Matrix Calculus</a>.
            </div>
            </p>
            </section>
        </div>
        <!-- Footer -->
        <footer>
            <div class="footer-content">
                <div class="footer-links">
                    <h3>Quick Links</h3>
                    <ul>
                        <li><a href="../../index.html">Home</a></li>
                        <li> <a href="probability.html">Section III - Probability & Statistics</a> </li>
                        <li><a href="basic.html">Basic Probability Ideas</a></li>   
                        <li><a href="random_variables.html">Random Variables</a></li> 
                        <li><a href="gamma.html">Gamma & Beta Distribution</a></li>
                        <li><a href="gaussian.html">Normal (Gaussian) Distribution</a></li> 
                        <li><a href="student.html">Student's \(t\)-Distribution</a></li>
                        <li><a href="covariance.html">Covariance</a></li>   
                        <li><a href="correlation.html">Correlation</a></li> 
                        <li><a href="mvn.html">Multivariate Distributions</a></li>
                        <li><a href="mle.html">Maximum Likelihood Estimate</a></li> 
                        <li><a href="hypothesis_testing.html">Statistical Inference & Hypothesis Testing</a></li>
                        <li><a href="linear_regression.html">Linear Regression</a></li>   
                        <li><a href="entropy.html">Entropy</a></li> 
                        <li><a href="convergence.html">Convergence</a></li>
                        <li><a href="bayesian.html">Intro to Bayesian Statistics</a></li> 
                        <li><a href="expfamily.html">The Exponential Family</a></li>
                        <li><a href="fisher_info.html">Fisher Information Matrix</a></li>
                        <li><a href="decision_theory.html">Bayesian Decision Theory</a></li>
                        <li><a href="markov.html">Markov Chains</a></li>          
                    </ul>
                </div>
            </div>
            
            <div class="footer-bottom">
                <p>&copy; 2025 Math-CS Compass. All rights reserved.</p>
            </div>
        </footer> 
        <script src="/js/main.js"></script>   
    </body>
</html>