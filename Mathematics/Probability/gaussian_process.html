---
layout: default
title: Gaussian Processes
level: detail
description: Learn about Gaussian processes as distributions over functions, kernel functions and GP regression.
uses_math: true
uses_python: false
---
<!DOCTYPE html>
<html>
    <body>
        <!-- LearningResource Schema for Gaussian Processes -->
        <script type="application/ld+json">
        {
        "@context": "https://schema.org",
        "@type": "LearningResource",
        "name": "Gaussian Processes",
        "description": "Learn about Gaussian processes as distributions over functions, kernel functions, GP regression, and applications in Bayesian optimization and uncertainty quantification",
        "learningResourceType": "lesson",
        "educationalUse": "instruction",
        "educationalLevel": "university",
        "interactivityType": "active",
        "author": {
            "@type": "Person",
            "name": "Yusuke Yokota",
            "jobTitle": "Mathematics & Computer Science Educator"
        },
        "publisher": {
            "@type": "Organization",
            "name": "MATH-CS COMPASS",
            "url": "https://yusukey01.github.io"
        },
        "about": [
            { "@type": "Thing", "name": "Gaussian Processes" },
            { "@type": "Thing", "name": "GP Regression" },
            { "@type": "Thing", "name": "Kernel Functions" },
            { "@type": "Thing", "name": "RBF Kernel" },
            { "@type": "Thing", "name": "Matern Kernel" },
            { "@type": "Thing", "name": "Bayesian Optimization" },
            { "@type": "Thing", "name": "Uncertainty Quantification" },
            { "@type": "Thing", "name": "Sparse GPs" },
            { "@type": "Thing", "name": "Inducing Points" },
            { "@type": "Thing", "name": "AutoML" }
        ],
        "teaches": [
            "Understanding GPs as distributions over functions",
            "Kernel function selection and design",
            "GP regression and prediction",
            "Computational efficiency techniques",
            "Bayesian optimization applications",
            "Uncertainty quantification in ML"
        ],
        "isPartOf": {
            "@type": "Course",
            "name": "Probability & Statistics",
            "description": "Explore fundamental concepts of probability and statistics essential for machine learning",
            "provider": {
            "@type": "Organization",
            "name": "MATH-CS COMPASS",
            "url": "https://yusukey01.github.io"
            },
            "instructor": {
            "@type": "Person",
            "name": "Yusuke Yokota",
            "jobTitle": "Mathematics & Computer Science Educator"
            },
            "courseCode": "III",
            "hasCourseInstance": {
            "@type": "CourseInstance",
            "courseMode": "online",
            "courseWorkload": "PT3H",
            "instructor": {
                "@type": "Person",
                "name": "Yusuke Yokota"
            }
            },
            "offers": {
            "@type": "Offer",
            "price": "0",
            "priceCurrency": "USD",
            "availability": "https://schema.org/InStock",
            "category": "free"
            }
        }
        }
        </script>
        
        <div class="hero-section">
            <h1 class="webpage-name">Gaussian Processes</h1>
        </div>

        {% include section_navigation.html %}

        <div class="topic-nav">
            <a href="#intro">Introduction</a>
            <a href="#kernel">Mercer Kernels</a>
            <a href="#regression">GP Regression</a>
        </div> 

        <div class="container">
            
            <section id="intro" class="section-content">
                <h2>Introduction</h2>

                <p>
                    So far, we have discussed <strong>parametric models</strong>. For example, in  <a href="../Machine_learning/deep_nn.html"><strong>deep neural networks</strong></a>, 
                    we use flexible function approximators of the form \(f(\boldsymbol{x}; \boldsymbol{\theta})\), where the number of parameters \(\dim(\boldsymbol{\theta})\) 
                    is fixed and independent of the size of the training set \(N\). Recall that such models can <em>overfit</em> when \(N\) is too small or <em>underfit</em> 
                    when \(N\) is too large. In contrast, we now consider models whose effective dimensionality adapts to the amount of data \(N\); 
                    these are called <strong>nonparametric models</strong>. 
                    From an AI systems perspective, most modern architectures are <em>hybrid</em>: they combine a parametric core — 
                    such as a Transformer — with nonparametric components used for memory, retrieval, or adaptive layers.
                 
                </p>

                <p>
                    Here, we focus on the <strong>Gaussian Process (GP)</strong>, which is one of the 
                    <a href="bayesian.html"><strong>Bayesian approaches</strong></a>. In this framework, we represent 
                    uncertainty about the mapping \(f\) by placing a prior distribution over functions, \(p(f)\), and 
                    then update it after observing data to obtain the posterior distribution \(p(f \mid \mathcal{D})\).
                </p>

                <p>
                    Consider a Gaussian random vector \(\boldsymbol{f} = [f_1, f_2, \ldots, f_N]^\top\), 
                    which is characterized by its <strong>mean</strong> 
                    \(\boldsymbol{\mu} = \mathbb{E}[\boldsymbol{f}]\) and <strong>covariance</strong> 
                    \(\boldsymbol{\Sigma} = \mathrm{Cov}[\boldsymbol{f}]\). 
                </p>

                <p>
                    Now, let \(f : \mathcal{X} \rightarrow \mathbb{R}\) be a function evaluated at a set of input points 
                    \(\boldsymbol{X} = \{\boldsymbol{x}_n \in \mathcal{X}\}_{n=1}^N\), and define 
                    \(\boldsymbol{f}_X = [f(\boldsymbol{x}_1), f(\boldsymbol{x}_2), \ldots, f(\boldsymbol{x}_N)]^\top\). 
                    If \(\boldsymbol{f}_X\) follows a joint Gaussian distribution for any finite set of inputs 
                    \(N \ge 1\), then 
                    \[
                        f : \mathcal{X} \rightarrow \mathbb{R}
                    \]
                    is said to follow a <strong>Gaussian Process (GP)</strong>, 
                    defined by its mean function \(m(\boldsymbol{x}) = \mathbb{E}[f(\boldsymbol{x})]\) 
                    and covariance function \(\mathcal{K}(\boldsymbol{x}, \boldsymbol{x}') = 
                    \mathrm{Cov}[f(\boldsymbol{x}), f(\boldsymbol{x}')]\), 
                    where \(\mathcal{K}\) is any positive-definite <strong>Mercer kernel</strong>. 
                    A common example is the <a href="../Machine_learning/intro_classification.html"><strong>RBF kernel</strong></a>:
                    \[
                        \mathcal{K}(\boldsymbol{x}, \boldsymbol{x}') 
                        \propto \exp(-\|\boldsymbol{x} - \boldsymbol{x}'\|^2).
                    \]
                </p>

                <p>
                    The corresponding GP prior is denoted as
                    \[
                        f(\boldsymbol{x}) \sim \mathcal{GP}(m(\boldsymbol{x}), \mathcal{K}(\boldsymbol{x}, \boldsymbol{x}')).
                    \]
                    Here, \(m(\boldsymbol{x}) = \mathbb{E}[f(\boldsymbol{x})]\) and 
                    \(\mathcal{K}(\boldsymbol{x}, \boldsymbol{x}') = 
                    \mathbb{E}[(f(\boldsymbol{x}) - m(\boldsymbol{x}))(f(\boldsymbol{x}') - m(\boldsymbol{x}'))]\).
                </p>

                <p>
                    For any finite collection of input points 
                    \(\boldsymbol{X} = \{\boldsymbol{x}_1, \boldsymbol{x}_2, \ldots, \boldsymbol{x}_N\}\), 
                    the corresponding function values follow a multivariate normal distribution:
                    \[
                        p(\boldsymbol{f}_X \mid \boldsymbol{X}) 
                        = \mathcal{N}(\boldsymbol{f}_X \mid \boldsymbol{\mu}_X, \boldsymbol{K}_{X,X}),
                    \]
                    where \(\boldsymbol{\mu}_X = [m(\boldsymbol{x}_1), m(\boldsymbol{x}_2), \ldots, m(\boldsymbol{x}_N)]^\top\) 
                    and \(\boldsymbol{K}_{X,X}(i, j) = \mathcal{K}(\boldsymbol{x}_i, \boldsymbol{x}_j)\).
                </p>
            </section>

            <section id="kernel" class="section-content">
                <h2>Mercer Kernels</h2>

                <p>
                    The <strong>kernel function</strong> defines the notion of <strong>similarity</strong> between function values 
                    at different input points. In a Gaussian process (GP), the choice of kernel is crucial since it 
                    determines the smoothness, periodicity, and other structural properties of the functions drawn 
                    from the GP prior.
                </p>

                <p>
                    While the kernel function intuitively measures similarity between inputs, not every function can serve 
                    as a valid covariance function in a Gaussian process.  
                    To ensure that the resulting covariance matrix is positive definite for any finite set of inputs — 
                    a requirement for the GP to define a proper multivariate Gaussian distribution — 
                    we restrict our attention to <strong>Mercer kernels</strong>.
                </p>

                 <div class="theorem">
                    <span class="theorem-title">Mercer Kernels:</span> 
                    <strong>Mercer kernel</strong> is also called a <strong>positive definite</strong> 
                    kernel, which is any symmetric function:
                    \[
                    \mathcal{K}: \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}
                    \]
                    such that for all \(N \in \mathbb{N}\) and all \(\boldsymbol{x}_i \in \mathcal{X}\)
                    \[
                    \sum_{i=1}^N \sum_{j=1}^N c_i c_j \mathcal{K}(\boldsymbol{x}_i, \boldsymbol{x}_j) \geq 0
                    \]
                    for any choice of numbers  \(c_i \in \mathbb{R}\). The equality allowed when \(\forall i, c_i =0\). 
                    <br>
                    This means that the corresponding Gram matrix 
                    \( \boldsymbol{K} = [\mathcal{K}(\boldsymbol{x}_i, \boldsymbol{x}_j)]_{i,j=1}^N \)
                    is <strong>positive semidefinite</strong>. 
                    Note that this condition does NOT require each individual kernel value 
                    \(\mathcal{K}(\boldsymbol{x}_i, \boldsymbol{x}_j)\) to be nonnegative.
                </div>

                 <p>
                    We often focus on <strong>stationary kernels</strong>, which depend only on the difference between inputs,
                    \(\boldsymbol{r} = \boldsymbol{x} - \boldsymbol{x}'\), and not on their absolute locations. 
                    In many cases, only the Euclidean distance between inputs matters:
                    \[
                    r = \|\boldsymbol{r}\|_2 = \|\boldsymbol{x} - \boldsymbol{x}'\|.
                    \]
                </p>

                <p>
                    As we have seen, one of the most widely used stationary kernel in practice is 
                    a <strong>radial basis function (RBF) kernel</strong>:
                    \[
                    \begin{align*}
                    \mathcal{K}_\text{RBF}(r ; \ell) &= \exp \left( -\frac{r^2}{2\ell^2}\right) \\\\
                                          &= \exp \left( -\frac{\| \boldsymbol{x} - \boldsymbol{x}'\|^2}{2\ell^2}\right)
                    \end{align*}
                    \]          
                    where \(\ell > 0\) is the <strong>length-scale</strong> parameter controlling how quickly the correlation 
                    between points decays with distance. This kernel is also known as the  <strong>Gaussian kernel</strong>.
                </p>
                
                <p>
                    Furthermore, by replacing Euclidean distance with Mahalanobis distance, we obtain the generalized RBF kernel: 
                    \[
                    \mathcal{K}(\boldsymbol{r}; \boldsymbol{\Sigma}, \sigma^2)
                    = \sigma^2 \exp \left( - \frac{1}{2} \boldsymbol{r}^\top \boldsymbol{\Sigma}^{-1} \boldsymbol{r}\right).
                    \]
                    In the case \(\boldsymbol{\Sigma}\) is diagonal, i.e. \(\boldsymbol{\Sigma} = \mathrm{diag}(\ell_1^2, \ldots, \ell_D^2)\),
                    we obtain the <strong>Automatic Relevance Determination (ARD) kernel</strong>:
                    \[
                    \mathcal{K}_\text{ARD}(\boldsymbol{r}; \boldsymbol{\Sigma}, \sigma^2)
                    =  \sigma^2 \exp \left( - \frac{1}{2} \sum_{d=1}^D \frac{1}{\ell_d^2 }r_d^2 \right)
                    \]
                    where \(\sigma^2\) is the overall variance and \(\ell_d >0\) is the characteristic length scale of dimension \(d\), 
                    controlling the sensitivity of the function to that input. If a dimension \(d\) is irrelevant, we can 
                    set \(\ell_d = \infty\), effectively ignoring that input. 
                </p>

                <p>
                    While RBF and ARD kernels are infinitely smooth and often used as default choices, there are 
                    situations where we may want more flexible control over the smoothness of the functions.  
                    For example, in Bayesian optimisation or when modelling rougher processes, it is useful to have kernels 
                    whose sample paths are only once or twice differentiable.
                </p>

                <p>
                    This motivates the use of <strong>Matérn kernels</strong>, which form a family of stationary kernels 
                    that include the RBF kernel as a limiting case (\(\nu \to \infty\)), while introducing an explicit 
                    smoothness parameter \(\nu > 0\) to control the differentiability of the functions.  
                    By adjusting \(\nu\), we can interpolate between very rough kernels (like the exponential kernel) and 
                    infinitely smooth kernels (like the RBF), giving us fine-grained control over function smoothness.
                </p>

                <p>
                    For parameters \(\nu>0\), \(\ell>0\), the Matérn kernel is
                    \[
                    \mathcal{K}_\text{Matérn}(r; \nu, \ell ) = \frac{2^{1-\nu}}{\Gamma(\nu)}\left(\frac{\sqrt{2\nu}r}{\ell}\right)^\nu 
                    K_\nu\left(\frac{\sqrt{2\nu}r}{\ell}\right)
                    \]
                <p>
                    where \(K_\nu\) denotes the modified Bessel function of the second kind.
                    The parameter \(\nu\) controls smoothness: sample paths of the GP are
                    \(\lfloor \nu - \tfrac{1}{2}\rfloor\)-times mean-square differentiable. Common choices
                    (up to the variance factor \(\sigma^2\)) admit closed forms:
                </p>

                <ul style="padding-left:40px;">
                    <li>Exponential kernel (non-differentiable):</li>
                    \[
                    \mathcal{K}_\text{Matérn}(r; \frac{1}{2}, \ell )  = \exp \left(- \frac{r}{\ell}\right)
                    \]
                    <li>Once differentiable:</li>
                    \[
                    \mathcal{K}_\text{Matérn}(r; \frac{3}{2}, \ell )  = \left(1 + \frac{\sqrt{3}r}{\ell}\right)\exp \left(- \frac{\sqrt{3}r}{\ell}\right)
                    \]
                    <li>Twice differentiable:</li>
                    \[
                    \mathcal{K}_\text{Matérn}(r; \frac{5}{2}, \ell )  = \left(1 + \frac{\sqrt{5}r}{\ell} + \frac{5r^2}{3\ell^2}\right)\exp \left(- \frac{\sqrt{5}r}{\ell}\right)
                    \]
                </ul>
                <br>
                <p>    
                    Note: In Gaussian process regression, the kernel can be multiplied by a positive amplitude 
                    parameter \(\sigma^2 > 0\) to scale the variance of the function values:
                    \[
                    \mathcal{K}(r;\nu,\ell) \;\longrightarrow\; \sigma^2 \mathcal{K}(r;\nu,\ell).
                    \]
                    This scaling changes the magnitude of function fluctuations but does not affect the smoothness or 
                    correlation structure of the GP.
                </p>

                <p>
                    Some functions exhibit repeating or periodic behavior that is not captured by standard RBF or Matérn kernels.  
                    To model such patterns, we can use a <strong>periodic kernel</strong>, which explicitly encodes periodicity into 
                    the covariance function.
                </p>

                <p>
                    A common choice for a one-dimensional periodic kernel is:
                </p>

                \[
                \mathcal{K}_\text{per}(r; \ell, p) = \exp \Bigg( - \frac{2 \, \sin^2\big(\pi r / p\big)}{\ell^2} \Bigg),
                \]

                <p>
                    where \(p > 0\) is the period of the function and \(\ell > 0\) is the length-scale controlling how quickly 
                    correlations decay away from exact multiples of the period. 
                </p>

                <p>
                    Periodic kernels are particularly useful in Gaussian process regression for modeling phenomena that repeat regularly over time or space, 
                    such as seasonal time series, cyclic patterns, or physical systems with inherent periodicity.
                </p>

            </section>

             <section id="regression" class="section-content">
                <h2>GP Regression</h2>
                <p>
                    Suppose we observe a training data set \(\mathcal{D} = \{(\boldsymbol{x}_n, y_n)\}_{n=1}^N\), where 
                    \(y_n = f(\boldsymbol{x}_n) + \epsilon_n\) with \(\epsilon_n \sim \mathcal{N}(0, \sigma_y^2)\).
                    Then we obtain the covariance of the observed noised responses:
                    \[
                    \begin{align*}
                    \text{Cov }[y_i, y_j] 
                    &= \text{Cov }[f_i, f_j] + \text{Cov }[\epsilon_i, \epsilon_j] \\\\
                    &= \mathcal{K}(\boldsymbol{x}_i, \boldsymbol{x}_j) + \sigma_y^2 \mathbb{I}(i \neq j).
                    \end{align*}
                    \]
                    So, 
                    \[
                     \text{Cov }[\boldsymbol{y} \mid \boldsymbol{X}] = \boldsymbol{K}_{X, X} + \sigma_y^2 \boldsymbol{I}_N.
                    \]
                    The joint density of the observed data and the latent, noise-free function on the test points is given by: 
                    \[
                    \begin{bmatrix} \boldsymbol{y} \\ \boldsymbol{f}_* \end{bmatrix} 
                    \sim 
                    \mathcal{N} \Bigg(
                        \begin{bmatrix} \boldsymbol{\mu}_X \\ \boldsymbol{\mu}_* \end{bmatrix},
                        \begin{bmatrix} 
                            \boldsymbol{K}_{X,X} + \sigma_y^2 \boldsymbol{I} & \boldsymbol{K}_{X,*} \\
                            \boldsymbol{K}_{X,*} & \boldsymbol{K}_{*, *}
                        \end{bmatrix}
                    \Bigg),
                    \]
                    The postrior predictive density at a set of test points \(\boldsymbol{X}_*\) is given by
                    \[
                    
                    \]
                </p>
               
             </section>


            <section id="regression" class="section-content">
                <h2>GP Regression</h2>

                <p>
                    Given training data 
                    \(\mathcal{D} = \{(x_i, y_i)\}_{i=1}^n\), 
                    where \(y_i = f(x_i) + \epsilon_i\) and \(\epsilon_i \sim \mathcal{N}(0, \sigma_n^2)\) represents Gaussian observation noise, 
                    the goal of Gaussian process (GP) regression is to predict function values at a set of test points \(X_* = \{x_*^{(j)}\}_{j=1}^{n_*}\).
                </p>

                <h3>Joint Distribution</h3>
                <p>
                    By the definition of a GP, the joint distribution of training outputs \(\mathbf{y}\) and test function values \(\mathbf{f}_*\) is multivariate Gaussian:
                </p>
                \[
                    \begin{bmatrix} \mathbf{y} \\ \mathbf{f}_* \end{bmatrix} \sim 
                    \mathcal{N} \Bigg(
                        \begin{bmatrix} \mathbf{m} \\ \mathbf{m}_* \end{bmatrix},
                        \begin{bmatrix} 
                            K(X,X) + \sigma_n^2 I & K(X,X_*) \\
                            K(X_*,X) & K(X_*,X_*)
                        \end{bmatrix}
                    \Bigg),
                \]
                <p>
                    where:
                    <ul style="padding-left:40px;">
                        <li>\(K(X,X)_{ij} = k(x_i, x_j)\) is the covariance between training points,</li>
                        <li>\(K(X,X_*)_{ij} = k(x_i, x_*^{(j)})\) is the covariance between training and test points,</li>
                        <li>\(K(X_*,X_*)_{ij} = k(x_*^{(i)}, x_*^{(j)})\) is the covariance between test points,</li>
                        <li>\(\mathbf{m}\) and \(\mathbf{m}_*\) are the mean functions evaluated at training and test points.</li>
                    </ul>
                </p>

                <h3>Posterior Predictive Distribution</h3>
                <p>
                    Conditioning on the observed data \(\mathbf{y}\), the posterior distribution for \(\mathbf{f}_*\) is:
                </p>
                \[
                    \mathbf{f}_* \mid X, \mathbf{y}, X_* \sim \mathcal{N}(\bar{\mathbf{f}}_*, \mathrm{Cov}[\mathbf{f}_*]),
                \]
                with
                \[
                    \bar{\mathbf{f}}_* = \mathbf{m}_* + K(X_*,X) \big(K(X,X) + \sigma_n^2 I\big)^{-1} (\mathbf{y} - \mathbf{m}),
                \]
                \[
                    \mathrm{Cov}[\mathbf{f}_*] = K(X_*,X_*) - K(X_*,X) \big(K(X,X) + \sigma_n^2 I\big)^{-1} K(X,X_*).
                \]

                <h3>Noise and Hyperparameters</h3>
                <p>
                    The noise variance \(\sigma_n^2\) (sometimes called the "nugget") serves multiple purposes:
                    <ul style="padding-left:40px;">
                        <li>Models observation noise in the training data,</li>
                        <li>Provides numerical stability by regularizing the kernel matrix,</li>
                        <li>Can be learned from data by maximizing the marginal likelihood (also called the log-evidence).</li>
                    </ul>
                </p>

                <p>
                    Kernel hyperparameters (e.g., length-scale \(\ell\), smoothness \(\nu\), amplitude \(\sigma^2\)) can also be optimized using the marginal likelihood:
                    \[
                        \log p(\mathbf{y} \mid X, \theta) = -\frac{1}{2} (\mathbf{y} - \mathbf{m})^\top (K(X,X)+\sigma_n^2 I)^{-1} (\mathbf{y} - \mathbf{m})
                        - \frac{1}{2} \log |K(X,X)+\sigma_n^2 I| - \frac{n}{2}\log 2\pi.
                    \]
                </p>

                <div class="theorem">
                    <span class="theorem-title">Computational Complexity</span>
                    Exact GP inference requires inverting the \(n \times n\) matrix \(K(X,X) + \sigma_n^2 I\), which costs \(O(n^3)\) time and \(O(n^2)\) memory.  
                    For large datasets (\(n \gtrsim 10,000\)), approximate methods such as sparse GPs or inducing-point methods are typically used.
                </div>
             </section>
       
        </div>
        
        <script src="/js/main.js"></script>
    </body>
</html>