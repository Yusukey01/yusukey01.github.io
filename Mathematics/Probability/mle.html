<!DOCTYPE html>
<html>
    <head> 
        <title>MLE</title>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <link rel="stylesheet" href="../styles.css">
        <meta name="viewport" content="width=device-width, initial-scale=1">
    </head>
    <body> 
        <h1>Point Estimators</h1>
        <blockquote>
            In practice, it is hard or impossible to find a population parameter \(\theta\). Instead, we estimate the unknown parameter by 
            statistic computations from sample data. The estimated parameter from the sample data is called a <strong>point estimator</strong> denoted 
            by \(\hat{\theta}\). 
            <br>For example, to estimate the population mean \(\mu = \theta\), we compute a sample mean
            \[
            \bar{X} = \frac{1}{n}\sum_{i = 1}^n  X_i = \hat{\theta}
            \]
            The point estimator is also a radom variable, and then a <strong>function</strong> of sampled random variables \(X_1, X_2, \cdots, X_n\). 
            <br><br>
            Once we obtained the estimator, it is natural that we want to know how much it is close to the true parameter. There are two 
            factors we have to be concerned about:
            <ol>
                <li>\(\text{Bias }(\hat{\theta}) = \mathbb{E}(\hat{\theta}) - \theta\)</li>
                The <strong>bias</strong> measures the average accuracy of the estimator. 
                <li>\(\text{Var }(\hat{\theta}) = \mathbb{E}[\hat{\theta} - \mathbb{E}(\hat{\theta})]^2\)</li>
                The <strong>variance</strong> measures the reliability(precision) of the estimator. 
            </ol>
            If both bias and variance are low enough, the estimator must be acceptable as an approximation of the population parameter. 
            <div class="theorem">
                <span class="theorem-title">Mean square error(MSE):</span>
                \[
                \begin{align*}
                \text{MSE }(\hat{\theta}) &= \mathbb{E }(\hat{\theta} - \theta)^2 \\\\
                                          &= \text{Var }(\hat{\theta}) + [\text{Bias }(\hat{\theta})]^2
                \end{align*}
                \] 
            </div>
            <div class="proof">
                \[
                \begin{align*}
                \text{MSE }(\hat{\theta}) &= \mathbb{E }(\hat{\theta} - \theta)^2 \\\\
                                          &= \mathbb{E }[\hat{\theta} - \theta + \mathbb{E }(\hat{\theta}) - \mathbb{E }(\hat{\theta})]^2 \\\\
                                          &= \mathbb{E }[\hat{\theta} - \mathbb{E }(\hat{\theta})]^2 
                                             + 2 \mathbb{E }[\hat{\theta} - \mathbb{E }(\hat{\theta})][\mathbb{E }(\hat{\theta})- \theta]
                                             + [\mathbb{E }(\hat{\theta})- \theta]^2 \\\\
                                         &= \mathbb{E }[\hat{\theta} - \mathbb{E }(\hat{\theta})]^2 
                                             + 2 [\mathbb{E }(\hat{\theta}) - \mathbb{E }(\hat{\theta})][\mathbb{E }(\hat{\theta})- \theta]
                                            + [\mathbb{E }(\hat{\theta})- \theta]^2 \\\\   
                                         &=  \text{Var }(\hat{\theta}) + 0 + [\text{Bias }(\hat{\theta})]^2
                \end{align*}
                \]
                Note: <strong>The population parameter is a fixed value.</strong> Thus, 
                \(\mathbb{E }\{[\mathbb{E }(\hat{\theta})- \theta]\} = [\mathbb{E }(\hat{\theta})- \theta] \) because 
                \(\mathbb{E }(constant) = constant\).
            </div>
            The MSE serves as a criterion for comparing estimators, enabling us to identify the most suitable one. Once an estimator 
            is selected, its precision in approximating the population parameter is typically assessed using the <strong>standard error (SE)</strong>, which 
            represents the <strong>standard deviation</strong> of the estimator's sampling distribution.
            <br><br>
            For example, since \(\text{Var }(\bar{X}) = \frac{\sigma^2}{n}\), the standard error of the mean (SEM) is given by
            \[
            \text{SE }(\bar{X}) = \sqrt{\text{Var }(\bar{X})} = \frac{\sigma}{\sqrt{n}}.
            \]
        </blockquote>

        <h1>Likelihood Functions</h1>
        <blockquote>
        Suppose observations \(X_1, X_2, \cdots, X_n\) are i.i.d. random variables. The "observed" values of these ramdom variables are 
        denoted by \(x_1, x_2, \cdots, x_n\) respectively. Then the joint p.d.f. or p.m.f. of \(X_1, X_2, \cdots, X_n\) is given by 
        \[
        f(x_1, x_2, \cdots, x_n | \theta) = \prod_{i = 1}^n f(x_i|\theta) 
        \]
        where \(\theta\) is some unknown parameter.
        <br><br>
        we call this <strong>likelihood function</strong> of \(\theta\) for observed  \(x_1, x_2, \cdots, x_n\) and denote it by 
        \(L(\theta | x_1, x_2, \cdots, x_n)\), or simply \(L(\theta)\): 
        \[
        \underbrace{L(\theta | x_1, x_2, \cdots, x_n)}_{\text{After sampling}}  = \underbrace{\prod_{i = 1}^n f(x_i|\theta)}_{\text{Before sampling}}
        \]
        </blockquote>

        <h1>Maximum likelihood Estimation</h1>
        <blockquote>
            In machine learning, model fitting (or training) is the process of estimating unknown 
            parameters \(\pmb{\theta} = (\theta_1, \theta_2, \cdots, \theta_k) \) from sample data 
            \(\mathcal{D} = \{\mathbf{x_1}, \mathbf{x_2}, \cdots, \mathbf{x_n}\}\), which can be represented by an optimization problem of the form 
            \[
            \pmb{\hat{\theta}} = \arg \min_{\pmb{\theta}} \mathcal{L} (\pmb{\theta})
            \]
            where \(\mathcal{L} (\pmb{\theta})\) is a loss function(or objective function).
            <br><br>
            The most common approach for the optimization problem is <strong>maximum likelihood estimation (MLE)</strong>:
            \[
            \pmb{\hat{\theta}_{MLE}} = \arg \max_{(\pmb{\theta})} L(\pmb{\theta})
            \]
            where \(L(\pmb{\theta}) \) is a likelihood function of \(\pmb{\theta}\) for sample data \(\mathcal{D}\).
            If \(L(\pmb{\theta})\) is differentiable function of \(\pmb{\theta}\), then \(\pmb{\hat{\theta}_{MLE}}\) can be calculated by solving the 
            following equation: 
            \[
            \begin{align*}
            &\nabla_{\pmb{\theta}} \ln L(\pmb{\theta}) = \nabla_{\pmb{\theta}} \ln \prod_{i = 1}^n f(\mathbf{x_i}|\pmb{\theta}) = 0 \\\\
            &\Longrightarrow \nabla_{\pmb{\theta}} \ln L(\pmb{\theta}) = \sum_{i=1}^ n \nabla_{\pmb{\theta}} \ln  f(\mathbf{x_i}|\pmb{\theta}) = 0
            \end{align*}
            \]
            Note: In practice, it is efficient to work with the <strong>log-likelihood function</strong> because we can 
            compute it additions instead of multiplications. 
        </blockquote>

        <h1>Example 1: Binomial Distribution \(X \sim b(n, p) \)</h1>
        <blockquote>
            Consider flipping a coin \(n\) times and we got \(k\) Heads. We assume \(P(Head) = \theta\) and 
            \(P(Tail) = (1- \theta)\), where \(\theta \in [0, 1]\). Then 
            \[
            P(\mathcal{D} | \theta) = \theta^k (1-\theta)^{n-k}.
            \]
            Then to obtain \(\hat{\theta}_{MLE}\), we can solve: 
            \[
            \frac{d}{d\theta}[\ln \theta^k (1-\theta)^{n-k}] = 0
            \]
            \[
            \begin{align*}
            &\Longrightarrow \frac{d}{d\theta}[ k\ln (\theta) + (n-k)\ln (1-\theta)] = 0 \\\\
            &\Longrightarrow \frac{k}{\theta} - \frac{n-k}{1-\theta} = 0 \\\\
            &\Longrightarrow k(1 - \theta) - (n-k)\theta = 0 
            \end{align*}
            \]
            Therefore, 
            \[
            \hat{\theta}_{MLE} = \frac{k}{n}.
            \]
            This is called the <strong>sample proportion</strong> \(\hat{p} = \hat{\theta}_{MLE}\), which is used as the point estimator for 
            the population proportion \(p = \theta\). 
            <br>
            Note: 
            \[\mathbb{E}[\hat{p}] = \frac{1}{n}\mathbb{E}[X] = \frac{1}{n}np = p\]
            \[\text{Var }(\hat{p}) = \frac{1}{n^2}\text{Var }[X] = \frac{1}{n^2}np(1-p) = \frac{p(1-p)}{n}\]

        </blockquote>
        <br><a href="../../index.html">Back to Home </a>
        <br> <a href="ml.html">Back to Probability </a>   
    </body>
</html>