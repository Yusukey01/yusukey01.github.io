<!DOCTYPE html>
<html>
    <head> 
        <title>Maximum likelihood Estimation</title>
        <link rel="stylesheet" href="../styles.css">
        <script id="MathJax-script" defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <meta name="viewport" content="width=device-width, initial-scale=1">
    </head>
    <body>
        <nav class="navbar">
            <div class="logo">
                <h1>Section III - Probability & Statistics</h1>
            </div>
            <ul class="nav-links">
                <li><a href="../../index.html">Home</a></li>
                <li><a href="../Linear_algebra/linear_algebra.html">Linear Algebra</a></li>
                <li><a href="../Calculus/calculus.html">Calculus to Optimization & Analysis</a></li>
                <li><a href="probability.html">Probability & Statistics</a></li>
                <li><a href="../Discrete/discrete_math.html">Discrete Mathematics & Algorithms</a></li>
            </ul>
            <div class="menu-toggle">&#9776;</div>
        </nav>

        <div class="hero-section">
            <h1 class="webpage-name">>Maximum Likelihood Estimation
            </h1>
        </div>

        <div class="topic-nav">
            <a href="#point">Point Estimators</a>
            <a href="#lf">Likelihood Functions</a>
            <a href="#mle">Maximum Likelihood Estimation</a>
            <a href="#ex1">Example 1: Binomial Distribution \(X \sim b(n, p) \)</a>
            <a href="#ex2">Example 2: Normal Distribution</a>
        </div> 

        <div class="container">  
           
            <section id="point" class="section-content">
            <h2>Point Estimators</h2>
            <p>
            In practice, it is hard or impossible to find a population parameter \(\theta\). Instead, we estimate the unknown parameter by 
            statistic computations from sample data. The estimated parameter from the sample data is called a <strong>point estimator</strong> denoted 
            by \(\hat{\theta}\). 
            <br>For example, to estimate the population mean \(\mu = \theta\), we compute a sample mean
            \[
            \bar{X} = \frac{1}{n}\sum_{i = 1}^n  X_i = \hat{\theta}
            \]
            The point estimator is also a radom variable, and then a <strong>function</strong> of sampled random variables \(X_1, X_2, \cdots, X_n\). 
            <br><br>
            Once we obtained the estimator, it is natural that we want to know how much it is close to the true parameter. There are two 
            factors we have to be concerned about:
            <ol style="padding-left: 40px;">
                <li>\(\text{Bias }(\hat{\theta}) = \mathbb{E}(\hat{\theta}) - \theta\)</li>
                The <strong>bias</strong> measures the average accuracy of the estimator. 
                <li>\(\text{Var }(\hat{\theta}) = \mathbb{E}[\hat{\theta} - \mathbb{E}(\hat{\theta})]^2\)</li>
                The <strong>variance</strong> measures the reliability(precision) of the estimator. 
            </ol>
            <br>
            If both bias and variance are low enough, the estimator must be acceptable as an approximation of the population parameter. 
            <div class="theorem">
                <span class="theorem-title">Mean square error(MSE):</span>
                \[
                \begin{align*}
                \text{MSE }(\hat{\theta}) &= \mathbb{E }(\hat{\theta} - \theta)^2 \\\\
                                          &= \text{Var }(\hat{\theta}) + [\text{Bias }(\hat{\theta})]^2
                \end{align*}
                \] 
            </div>
            <div class="proof">
                \[
                \begin{align*}
                \text{MSE }(\hat{\theta}) &= \mathbb{E }(\hat{\theta} - \theta)^2 \\\\
                                          &= \mathbb{E }[\hat{\theta} - \theta + \mathbb{E }(\hat{\theta}) - \mathbb{E }(\hat{\theta})]^2 \\\\
                                          &= \mathbb{E }[\hat{\theta} - \mathbb{E }(\hat{\theta})]^2 
                                             + 2 \mathbb{E }[\hat{\theta} - \mathbb{E }(\hat{\theta})][\mathbb{E }(\hat{\theta})- \theta]
                                             + [\mathbb{E }(\hat{\theta})- \theta]^2 \\\\
                                         &= \mathbb{E }[\hat{\theta} - \mathbb{E }(\hat{\theta})]^2 
                                             + 2 [\mathbb{E }(\hat{\theta}) - \mathbb{E }(\hat{\theta})][\mathbb{E }(\hat{\theta})- \theta]
                                            + [\mathbb{E }(\hat{\theta})- \theta]^2 \\\\   
                                         &=  \text{Var }(\hat{\theta}) + 0 + [\text{Bias }(\hat{\theta})]^2
                \end{align*}
                \]
                Note: <strong>The population parameter is a fixed value.</strong> Thus, 
                \(\mathbb{E }\{[\mathbb{E }(\hat{\theta})- \theta]\} = [\mathbb{E }(\hat{\theta})- \theta] \) because 
                \(\mathbb{E }(constant) = constant\).
            </div>
            The MSE serves as a criterion for comparing estimators, enabling us to identify the most suitable one. Once an estimator 
            is selected, its precision in approximating the population parameter is typically assessed using the <strong>standard error (SE)</strong>, which 
            represents the <strong>standard deviation</strong> of the estimator's sampling distribution.
            <br><br>
            For example, since \(\text{Var }(\bar{X}) = \frac{\sigma^2}{n}\), the standard error of the mean (SEM) is given by
            \[
            \text{SE }(\bar{X}) = \sqrt{\text{Var }(\bar{X})} = \frac{\sigma}{\sqrt{n}}.
            \]
            </p>
            </section>

            <section id="lf" class="section-content">
            <h2>Likelihood Functions</h2>
            <p>
            Suppose observations \(X_1, X_2, \cdots, X_n\) are i.i.d. random variables. The "observed" values of these ramdom variables are 
            denoted by \(x_1, x_2, \cdots, x_n\) respectively. Then the joint p.d.f. or p.m.f. of \(X_1, X_2, \cdots, X_n\) is given by 
            \[
            f(x_1, x_2, \cdots, x_n | \theta) = \prod_{i = 1}^n f(x_i|\theta) 
            \]
            where \(\theta\) is some unknown parameter.
            <br><br>
            we call this <strong>likelihood function</strong> of \(\theta\) for observed  \(x_1, x_2, \cdots, x_n\) and denote it by 
            \(L(\theta | x_1, x_2, \cdots, x_n)\), or simply \(L(\theta)\): 
            \[
            \underbrace{L(\theta | x_1, x_2, \cdots, x_n)}_{\text{After sampling}}  = \underbrace{\prod_{i = 1}^n f(x_i|\theta)}_{\text{Before sampling}}
            \]
            </p>
            </section>

            <section id="mle" class="section-content">
            <h2>Maximum Likelihood Estimation</h2>
            <p>
                In machine learning, model fitting (or training) is the process of estimating unknown 
                parameters \(\pmb{\theta} = (\theta_1, \theta_2, \cdots, \theta_k) \) from sample data 
                \(\mathcal{D} = \{\mathbf{x_1}, \mathbf{x_2}, \cdots, \mathbf{x_n}\}\), which can be represented by an optimization problem of the form 
                \[
                \pmb{\hat{\theta}} = \arg \min_{\pmb{\theta}} \mathcal{L} (\pmb{\theta})
                \]
                where \(\mathcal{L} (\pmb{\theta})\) is a loss function(or objective function).
                <br><br>
                The most common approach for the optimization problem is <strong>maximum likelihood estimation (MLE)</strong>:
                \[
                \pmb{\hat{\theta}_{MLE}} = \arg \max_{(\pmb{\theta})} L(\pmb{\theta})
                \]
                where \(L(\pmb{\theta}) \) is a likelihood function of \(\pmb{\theta}\) for sample data \(\mathcal{D}\).
                If \(L(\pmb{\theta})\) is differentiable function of \(\pmb{\theta}\), then \(\pmb{\hat{\theta}_{MLE}}\) can be calculated by solving the 
                following equation: 
                \[
                \begin{align*}
                &\nabla_{\pmb{\theta}} \ln L(\pmb{\theta}) = \nabla_{\pmb{\theta}} \ln \prod_{i = 1}^n f(\mathbf{x_i}|\pmb{\theta}) = 0 \\\\
                &\Longrightarrow \nabla_{\pmb{\theta}} \ln L(\pmb{\theta}) = \sum_{i=1}^ n \nabla_{\pmb{\theta}} \ln  f(\mathbf{x_i}|\pmb{\theta}) = 0
                \end{align*}
                \]
                Note: In practice, it is efficient to work with the <strong>log-likelihood function</strong> because we can 
                compute it additions instead of multiplications. 
            </p>
            </section>

            <section id="ex1" class="section-content">
                <h2>Example 1: Binomial Distribution \(X \sim b(n, p) \)</h2>
                <p>
                Consider flipping a coin \(n\) times and we got \(k\) Heads. We assume \(P(Head) = \theta\) and 
                \(P(Tail) = (1- \theta)\), where \(\theta \in [0, 1]\). Then 
                \[
                P(\mathcal{D} | \theta) = \theta^k (1-\theta)^{n-k}.
                \]
                To obtain \(\hat{\theta}_{MLE}\), we can solve: 
                \[
                \frac{d}{d\theta}[\ln \theta^k (1-\theta)^{n-k}] = 0
                \]
                \[
                \begin{align*}
                &\Longrightarrow \frac{d}{d\theta}[ k\ln (\theta) + (n-k)\ln (1-\theta)] = 0 \\\\
                &\Longrightarrow \frac{k}{\theta} - \frac{n-k}{1-\theta} = 0 \\\\
                &\Longrightarrow k(1 - \theta) - (n-k)\theta = 0 
                \end{align*}
                \]
                Therefore, 
                \[
                \hat{\theta}_{MLE} = \frac{k}{n}.
                \]
                This is equivalent to  the <strong>sample proportion</strong> \(\hat{p} = \frac{X}{n}\), which is used as the point estimator for 
                the population proportion \(p = \theta\). 
                <br><br>
                Note:
                \[
                \begin{align*}
                \mathbb{E}[\hat{p}] = \frac{1}{n}\mathbb{E}[X] = \frac{1}{n}np = p \\\\
                \text{Var }(\hat{p}) = \frac{1}{n^2}\text{Var }[X] = \frac{1}{n^2}np(1-p) = \frac{p(1-p)}{n}
                \end{align*}
                \]
                </p>
            </section>

            <section id="ex2" class="section-content">
                <h2>Example 2: Normal Distribution</h2>
                <p>
                Suppose \(\mathcal{D} = {x_1, x_2, \cdots, x_n}\) is from a normal distributuion with p.d.f 
                \[
                f(x | \mu, \sigma^2) = \frac{1}{\sigma \sqrt{2\pi}}\exp \Big\{- \frac{(x - \mu)^2}{2\sigma^2}\Big\}.
                \]
                And its likelihood fuction is given by
                \[
                \begin{align*}
                L(\mu, \sigma^2) &= \prod_{i=1}^n [ f(x_i | \mu, \sigma^2)] \\\\
                                &= \Big(\frac{1}{\sigma \sqrt{2\pi}}\Big)^n \exp \Big\{-\frac{1}{2\sigma^2} \sum_{i=1}^n (x_i - \mu)^2 \Big\}
                \end{align*}
                \]
                The log-likelihood function is given by 
                \[
                \begin{align*}
                \ln L(\mu, \sigma^2) &= n \ln \Big(\frac{1}{\sigma \sqrt{2\pi}}\Big) -\frac{1}{2\sigma^2} \sum_{i=1}^n (x_i - \mu)^2 \\\\
                                    &= -n \ln (\sigma) - n \ln (\sqrt{2\pi})  -\frac{1}{2\sigma^2} \sum_{i=1}^n (x_i - \mu)^2  \\\\
                                    &= -\frac{n}{2} \ln (\sigma^2) - \frac{n}{2}\ln (2\pi)  -\frac{1}{2\sigma^2} \sum_{i=1}^n (x_i - \mu)^2 \tag{1} \\\\ 
                \end{align*}
                \]
                Setting the partialderivative of (1) with respect to \(\mu\) equal to zero: 
                \[
                \frac{\partial \ln L(\mu, \sigma^2) }{\partial \mu} = \frac{1}{\sigma^2} \sum_{i=1}^n (x_i - \mu) = 0
                \]
                \[
                \Longrightarrow \sum_{i=1}^n (x_i) - n\mu = 0
                \]
                Thus, 
                \[
                \hat{\mu}_{MLE} = \frac{1}{n} \sum_{i=1}^n x_i = \bar{x} \tag{2}
                \]
                Similarly, setting the partialderivative of (1) with respect to \(\sigma^2\) equal to zero and substituting (2) in the equation:
                \[
                \frac{\partial \ln L(\mu, \sigma^2) }{\partial \sigma^2} = -\frac{n}{2\sigma^2}+ \frac{1}{2\sigma^4}\sum_{i=1}^n (x_i - \bar{x})^2  = 0
                \]
                \[
                \Longrightarrow -n \sigma^2 + \sum_{i=1}^n (x_i - \bar{x})^2 = 0
                \]
                Thus, 
                \[
                \hat{\sigma^2}_{MLE} = \frac{1}{n}\sum_{i=1}^n (x_i - \bar{x})^2
                \]
                Recall that the variance is biased, so we use \(s^2 = \frac{1}{n-1} \sum_{i=1}^n (x_i - \bar{x})^2\) for the sample variance.
                </p>
            </section>
        </div>
        <!-- Footer -->
        <footer>
            <div class="footer-content">
                <div class="footer-links">
                    <h3>Quick Links</h3>
                    <ul>
                        <li><a href="../../index.html">Home</a></li>
                        <li> <a href="probability.html">Section III - Probability & Statistics</a> </li>
                        <li><a href="basic.html">Basic Probability Ideas</a></li>   
                        <li><a href="random_variables.html">Random Variables</a></li> 
                        <li><a href="gamma.html">Gamma & Beta Distribution</a></li>
                        <li><a href="gaussian.html">Normal (Gaussian) Distribution</a></li> 
                        <li><a href="student.html">Student's \(t\)-Distribution</a></li>
                        <li><a href="covariance.html">Covariance</a></li>   
                        <li><a href="correlation.html">Correlation</a></li> 
                        <li><a href="mvn.html">Multivariate Distributions</a></li>
                        <li><a href="mle.html">Maximum Likelihood Estimate</a></li> 
                        <li><a href="hypothesis_testing.html">Statistical Inference & Hypothesis Testing</a></li>
                        <li><a href="linear_regression.html">Linear Regression</a></li>   
                        <li><a href="entropy.html">Entropy</a></li> 
                        <li><a href="convergence.html">Convergence</a></li>
                        <li><a href="bayesian.html">Intro to Bayesian Statistics</a></li> 
                        <li><a href="expfamily.html">The Exponential Family</a></li>
                        <li><a href="fisher_info.html">Fisher Information Matrix</a></li>
                        <li><a href="decision_theory.html">Bayesian Decision Theory</a></li>
                        <li><a href="markov.html">Markov Chains</a></li>          
                    </ul>
                </div>
            </div>
            
            <div class="footer-bottom">
                <p>&copy; 2025 Math-CS Compass. All rights reserved.</p>
            </div>
        </footer>    
    </body>
</html>