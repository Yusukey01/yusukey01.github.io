<!DOCTYPE html>
<html>
    <head> 
        <title>Bayesian Statistics</title>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <link rel="stylesheet" href="../styles.css">
        <meta name="viewport" content="width=device-width, initial-scale=1">
    </head>
    <body> 
        <h1>Bayesian inference</h1>
        <blockquote>
            The core of statistics is inferring the unknown parameters \(\theta\) given observed data \(\mathcal{D} = \{x_1, x_2, \cdots, x_n\}\), 
            which are random samples drawn from a probability distribution. This process is the inverse of probability theory. There are two main 
            approaches. One is called <strong>frequentist statistics</strong> which many people are familiar with, treats parameters as fixed and data as 
            random. The other is called <strong>Bayesian statistics</strong> which is the foundation for machine learning algorithms.
            <br><br>
            In Bayesian statistics, we assume that prior knowledge about the unknown parameters can be explained by a probability 
            distribution, which is called <strong>prior distribution</strong>, \(p(\theta)\). This means that the unknown parameters \(\theta\) are 
            treated as <strong>random variables</strong>, while the data \(\mathcal{D}\) are considered fixed. This approach is opposite 
            to that of frequentist statistics, where parameters are fixed and data are considered random. 
            <br><br>
            Bayesian inference updates our belief(= prior distribution) by using the observed data and
            expresses our updated belief about the parameters as the <strong>posterior distribution</strong>, \(p(\theta | \mathcal{D})\) 
            using Bayes' rule:
            \[
            \begin{align*}
            p(\theta | \mathcal{D}) &= \frac{p(\theta)p(\mathcal{D}| \theta)}{p(\mathcal{D})} \\\\
                                    &= \frac{p(\theta)p(\mathcal{D}| \theta)}{\int p(\theta ')p(\mathcal{D} | \theta ') \, d\theta '}
            \end{align*}
            \]
            The components are defined as follows:
            <ol>
                <li><strong>Prior distribution</strong> \(p(\theta)\):</li> 
                Represents our belief about the parameters \(\theta\) before observing any data. 
                It encodes prior knowledge, assumptions, or uncertainty about \(\theta\).
                <li><strong>Likelihood</strong> \(p(\mathcal{D}| \theta)\):</li>
                Describes how likely the observed data \(\mathcal{D}\) is, given a specific value of \(\theta\). 
                This reflects the model of the data-generating process.
                <li><strong>Marginal likelihood</strong>(or <strong>evidence</strong>) \(p(\mathcal{D})\): </li>
                A normalization constant that ensures the posterior is a valid probability distribution. 
                It is computed by integrating over all possible values of \(\theta\):  
                \[
                p(\mathcal{D}) = \int p(\theta')p(\mathcal{D} | \theta') \, d\theta'.
                \]
                Note: In the integral for the marginal likelihood, we use \(\theta'\) instead of \(\theta\) to clarify that 
                the integration is over the entire parameter space, not a specific parameter. 
            </ol>
            
        </blockquote>

        <h1>Conjugate Prior</h1>
        <blockquote>
            In general, specifying a prior is a bottleneck of the Bayesian inference. Here, we introduce some special case of priors.
            <br><br>
            A prior \(p(\theta) \in \mathcal{F}\) is saied to be <strong>conjugate prior</strong> for a likelihood function \(p(\mathcal{D} | \theta)\) if 
            the posterior is in the same parameterized family as the prior: \(p(\mathcal{D} | \theta) \in \mathcal{F}\). 
        </blockquote>

        <h1></h1>
        <blockquote>
        </blockquote>

        <br><a href="../../index.html">Back to Home </a>
        <br> <a href="probability.html">Back to Probability </a>   
    </body>
</html>