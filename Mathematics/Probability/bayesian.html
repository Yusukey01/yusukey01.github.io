<!DOCTYPE html>
<html>
    <head> 
        <title>Bayesian Statistics</title>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <link rel="stylesheet" href="../styles.css">
        <meta name="viewport" content="width=device-width, initial-scale=1">
    </head>
    <body> 
        <h1>Bayesian inference</h1>
        <blockquote>
            The core of statistics is inferring the unknown parameters \(\theta\) given observed data \(mathcal{D}\) by computing \(p(\theta | \mathcal{D})\), which is 
            exactly the inverse of probability theory. There are two approaches; <strong>frequentist statistics</strong> and  <strong>Bayesian statistics</strong>. Actually, 
            machine learning heavily relies on Bayesian statistics. 
            <br><br>
            In <strong>Bayesian statistics</strong>, the parameters \(\theta\) are treated as unknown, or <strong>random variable</strong> and the 
            data \(\mathcal{D}\) are fixed. This is the opposite of the frequentist statistics. After observing data, we represent our inference 
            about the parameters by computing the <strong>posterior distribution</strong>: \(p(\theta | \mathcal{D})\) using Bayes' rule as follows:
            \[
            \begin{align*}
            p(\theta | \mathcal{D}) &= \frac{p(\theta)p(\mathcal{D}| \theta)}{p(\mathcal{D})} \\\\
                                    &= \frac{p(\theta)p(\mathcal{D}| \theta)}{\int p(\theta ')p(\mathcal{D}| \theta ')d\theta '}
            \end{align*}
            \]
            where 
            <ol>
                <li>\(p(\theta)\): <strong>Prior distribution</strong></li> 
                    represents our guess based on before observing the data. 
                <li>\(p(\mathcal{D}| \theta)\): <strong>Likelifood</strong></li>
                represents our guess what data we expect to observe for each setting of the parameters.
                <li> \(p(\mathcal{D})\): <strong>Marginal likelifood</strong>(or <strong>evidence</strong>)</li>
                a normalization constant. 
            </ol>
            
        </blockquote>

        <h1></h1>
        <blockquote>
        </blockquote>

        <h1></h1>
        <blockquote>
        </blockquote>

        <br><a href="../../index.html">Back to Home </a>
        <br> <a href="probability.html">Back to Probability </a>   
    </body>
</html>