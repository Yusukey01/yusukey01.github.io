<!DOCTYPE html>
<html>
    <head> 
        <title>Bayesian Statistics</title>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <link rel="stylesheet" href="../styles.css">
        <meta name="viewport" content="width=device-width, initial-scale=1">
    </head>
    <body> 
        <h1>Bayesian inference</h1>
        <blockquote>
            The core of statistics is inferring the unknown parameters \(\theta\) given observed data \(mathcal{D}\) by computing \(p(\theta | \mathcal{D})\), which is 
            exactly the inverse of probability theory. There are two approaches; <strong>frequentist statistics</strong> that usually people are familiar with, and  
            <strong>Bayesian statistics</strong> that machine learning heavily relies on Bayesian statistics. 
            <br><br>
            In <strong>Bayesian statistics</strong>, the parameters \(\theta\) are treated as <strong>random variables</strong>, while the 
            data \(\mathcal{D}\) are considered fixed. This approach is opposite to that of frequentist statistics, where parameters are fixed 
            and data is random. After observing data, Bayesian inference expresses our updated belief about the parameters as the <strong>posterior distribution</strong>: 
            \(p(\theta | \mathcal{D})\) using Bayes' rule:
            \[
            \begin{align*}
            p(\theta | \mathcal{D}) &= \frac{p(\theta)p(\mathcal{D}| \theta)}{p(\mathcal{D})} \\\\
                                    &= \frac{p(\theta)p(\mathcal{D}| \theta)}{\int p(\theta ')p(\mathcal{D} | \theta ') \, d\theta '}
            \end{align*}
            \]
            Here, the terms are defined as follows:
            <ol>
                <li><strong>Prior distribution</strong> \(p(\theta)\):</li> 
                Represents our belief about \(\theta\) before observing any data. It encodes prior knowledge or assumptions.
                <li><strong>Likelihood</strong> \(p(\mathcal{D}| \theta)\):</li>
                Describes how likely the observed data \(\mathcal{D}\) is, given a specific value of \(\theta\). 
                This reflects the model of the data-generating process.
                <li><strong>Marginal likelihood</strong>(or <strong>evidence</strong>) \(p(\mathcal{D})\): </li>
                A normalization constant that ensures the posterior is a valid probability distribution. 
                It is computed by integrating over all possible values of \(\theta\):  
                \[
                p(\mathcal{D}) = \int p(\theta')p(\mathcal{D} | \theta') \, d\theta'.
                \]
                Note: In the integral of the denominator, we use \(\theta'\) instead of \(theta\) to clarify that 
                the integration is over the entire parameter space. 
            </ol>
            
        </blockquote>

        <h1></h1>
        <blockquote>
        </blockquote>

        <h1></h1>
        <blockquote>
        </blockquote>

        <br><a href="../../index.html">Back to Home </a>
        <br> <a href="probability.html">Back to Probability </a>   
    </body>
</html>