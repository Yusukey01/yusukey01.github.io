<!DOCTYPE html>
<html>
    <head> 
        <title>Statistical Inference & Hypothesis Testing</title>
        <link rel="stylesheet" href="../styles.css">
        <script id="MathJax-script" defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <script src="https://cdn.jsdelivr.net/pyodide/v0.23.3/full/pyodide.js"></script>
        <meta name="viewport" content="width=device-width, initial-scale=1">
    </head>
    <body> 
        <div class="toc-container">
            <h2>Contents</h2>
            <ul>
                <li><a href="#NHST">Null Hypothesis Significance Test</a></li>
                <li><a href="#"></a></li>
                <li><a href="#"></a></li>
                <li><a href="#"></a></li>
            </ul>
        </div>

        <h1 id="NHST">Null Hypothesis Significance Test</h1>
        <blockquote>
            Once we have a statistical model (or <strong>hypothesis</strong>), we need to assess whether it's plausible given our data \(\mathcal{D}\). 
            In this section, we introduce <strong>frequentist statistical inference</strong> and <strong>hypothesis testing</strong>. Although 
            <a href="bayesian.html"><strong>Bayesian inference</strong></a> can replace many frequentist techniques and is especially popular in 
            modern machine learning, frequentist methods remain valuable tools â€” they're often simpler to compute, more standardized, and provide 
            complementary insights.
            <br><br>
            Here, we discuss the <strong>null hypothesis significance test(NHST)</strong>.
            <br>
            Suppose we have two competing hypotheses:
            <ul>
                <li><strong>Null Hypothesis</strong> \(H_0\): This is the default assumption.</li>

                <li><strong>Alternative Hypothesis</strong> \(H_1\): This represents the claim we wish to support.</li>
            </ul>
            (So, hypothesis testing is a kind of <strong>binary classification</strong> problem.)
            <br><br>
            Our goal is to decide which hypothesis is more plausible. Usually, our reasoning is that if \(H_0\) is very unlikely, 
            then we conclude that \(H_1\) would be true (i.e., we <strong>reject the null hypothesis</strong>). 
            <br><br>
            Rejecting \(H_0\) does NOT mean \(H_1\) is absolutely true. Conversely, failing to reject \(H_0\) only means 
            that the evidence is insufficient to support \(H_1\). Thus, our conclusion can be wrong: 
            <ul>
                <li><strong>Type I error</strong>(or <strong>false negative</strong>) : Accidentally rejecting the null \(H_0\) when it is true.</li>
                <li><strong>Type II error</strong>(or <strong>false positive</strong>): Accidentally accepting \(H_0\) when the alternative \(H_1\) is true.</li>
            </ul>
            The type I error rate \(\alpha\) is called <strong>significance</strong> of the hypothesis test. It represents the probability of mistakenly rejecting 
            \(H_0\) when it is true (typically, set 0.05 to 0.01 in practice.)
            <br><br>
            To decide whether to reject \(H_0\), we define a function of the data \(\mathcal{D}\) that summarizes the evidence 
            against \(H_0\). This is called the <strong>test statistic</strong>, denoted as \(test(\mathcal{D})\).
            To evaluate the significance of our observed test statistic, we compare it to what we would expect under the null 
            hypothesis. That is, we sample the hypothetical dataset \(\tilde{\mathcal{D}}\) assuming \(H_0\) is true and compute 
            their test statistics, \(test(\tilde{\mathcal{D}})\). 
            <br><br>
            The <strong>p-value</strong> is defined as the probability, under \(H_0\), 
            of obtaining a test statistic at least as extreme as the one we observed:
            \[
            pval = P(test(\tilde{\mathcal{D}}) \geq test(\mathcal{D}) | \tilde{\mathcal{D}} \sim H_0).
            \]
            A "small" p-value (typically, \(< \alpha\)) indicates that the observed result is unlikely under \(H_0\), 
            leading us to reject the null hypothesis in favor of \(H_1\).
            <br>
            Hypothesis testing provides a systematic way to evaluate claims, but it has limitations:
            <ul>
                <li>Statistical significance does not imply practical significance.</li>
                <li>Even if \(H_0\) is rejected, the actual effect size may be too small to be meaningful.</li>
                <li>p-values depend on sample size. With very large datasets, even tiny, practically irrelevant differences may yield small p-values.</li>
                <li>Frequentist methods rely on fixed significance thresholds. Bayesian approaches offer an alternative framework by directly computing the probability of hypotheses given the data.</li>
            </ul>
        </blockquote>

        <h1 id=""></h1>
        <blockquote>
        </blockquote>

        <h1 id=""></h1>
        <blockquote>
        </blockquote>

        <h1 id=""></h1>
        <blockquote>
        </blockquote>


        
        <br><a href="../../index.html">Back to Home </a>
        <br> <a href="probability.html">Back to Probability </a> 

        <script src="../runPythonCode.js"></script>
        <script src="../collapsible.js"></script>  
    </body>
</html>