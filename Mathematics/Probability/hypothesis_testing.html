<!DOCTYPE html>
<html>
    <head> 
        <title>Statistical Inference & Hypothesis Testing</title>
        <link rel="stylesheet" href="../styles.css">
        <script id="MathJax-script" defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <script src="https://cdn.jsdelivr.net/pyodide/v0.23.3/full/pyodide.js"></script>
        <meta name="viewport" content="width=device-width, initial-scale=1">
    </head>
    <body> 
        <div class="toc-container">
            <h2>Contents</h2>
            <ul>
                <li><a href="#NHST">Null Hypothesis Significance Test</a></li>
                <li><a href="#test">Example: t-test</a></li>
                <li><a href="#"></a></li>
                <li><a href="#"></a></li>
            </ul>
        </div>

        <h1 id="NHST">Null Hypothesis Significance Test</h1>
        <blockquote>
            Once we have a statistical model (or <strong>hypothesis</strong>), we need to assess whether it's plausible given our data \(\mathcal{D}\). 
            In this section, we introduce <strong>frequentist statistical inference</strong> and <strong>hypothesis testing</strong>. Although 
            <a href="bayesian.html"><strong>Bayesian inference</strong></a> can replace many frequentist techniques and is especially popular in 
            modern machine learning, frequentist methods remain valuable tools â€” they're often simpler to compute, more standardized, and provide 
            complementary insights.
            <br><br>
            Here, we discuss the <strong>null hypothesis significance test(NHST)</strong>.
            <br>
            Suppose we have two competing hypotheses:
            <ul>
                <li><strong>Null Hypothesis</strong> \(H_0\): This is the default assumption.</li>

                <li><strong>Alternative Hypothesis</strong> \(H_1\): This represents the claim we wish to support.</li>
            </ul>
            (So, hypothesis testing is a kind of <strong>binary classification</strong> problem.)
            <br><br>
            Our goal is to decide which hypothesis is more plausible. Usually, our reasoning is that if \(H_0\) is very unlikely, 
            then we conclude that \(H_1\) would be true (i.e., we <strong>reject the null hypothesis</strong>). 
            <br><br>
            Rejecting \(H_0\) does NOT mean \(H_1\) is absolutely true. Conversely, failing to reject \(H_0\) only means 
            that the evidence is insufficient to support \(H_1\). Thus, our conclusion can be wrong: 
            <ul>
                <li><strong>Type I error</strong>(or <strong>false negative</strong>) : Accidentally rejecting the null \(H_0\) when it is true.</li>
                <li><strong>Type II error</strong>(or <strong>false positive</strong>): Accidentally accepting \(H_0\) when the alternative \(H_1\) is true.</li>
            </ul>
            The type I error rate \(\alpha\) is called <strong>significance</strong> of the test. It represents the probability of mistakenly rejecting 
            \(H_0\) when it is true (typically, set 0.05 to 0.01 in practice.)
            <br><br>
            To decide whether to reject \(H_0\), we define a function of the data \(\mathcal{D}\) that summarizes the evidence 
            against \(H_0\). This is called the <strong>test statistic</strong>, denoted as \(test(\mathcal{D})\).
            To evaluate the significance of our observed test statistic, we compare it to what we would expect under the null 
            hypothesis. That is, we sample the hypothetical dataset \(\tilde{\mathcal{D}}\) assuming \(H_0\) is true and compute 
            their test statistics, \(test(\tilde{\mathcal{D}})\). 
            <br><br>
            The <strong>p-value</strong> is defined as the probability, under \(H_0\), 
            of obtaining a test statistic at least as extreme as the one we observed:
            \[
            p = P(test(\tilde{\mathcal{D}}) \geq test(\mathcal{D}) | \tilde{\mathcal{D}} \sim H_0).
            \]
            A "small" p-value (typically, \(< \alpha\)) indicates that the observed result is unlikely under \(H_0\), 
            leading us to reject the null hypothesis in favor of \(H_1\). Traditionally we reject the null hypothesis 
            if the p-value is less than \(\alpha = 0.05\), which is called the <strong>significance level </strong> of the 
            test. Note that a p-value of 0.05 does NOT mean that the alternative hypothesis \(H_1\) is true with probability
            0.95. Indeed, even many scientists misinterpret p-values.
            <br><br>
            NHST provides a systematic way to evaluate claims, but it has limitations:
            <ul>
                <li>Statistical significance does not imply practical significance. Even if \(H_0\) is rejected, the actual effect size may be too small to be meaningful.</li>
                <li>p-values depend on sample size. With very large datasets, even tiny, practically irrelevant differences may yield small p-values.</li>
                <li>Frequentist methods rely on fixed significance thresholds. Bayesian approaches offer an alternative framework by directly computing the probability of hypotheses given the data.</li>
            </ul>
        </blockquote>

        <h1 id="test">t-test</h1>
        <blockquote>
            <div class="proof">
                <span class="proof-title">Example:</span>
                Suppose we are analyzing the test scores of students in a school. Historically, the average test score is 70. A researcher 
                believes that a new teaching method has improved scores. To test this, we collect a sample of 30 students' scores after 
                using the new method.
                <ul>
                    <li>\(H_0\): The new method has no effect, meaning the true mean is still 70.</li>
                    <li>\(H_1\): The new method increases the average score, meaning the mean is greater than 70.</li>
                </ul>
                We collected a sample of (\(n = 30\)) students with the following observed statistics:
                <ul>
                    <li> Sample mean: \(\bar{x} = 75.20\). </li>
                    <li> Sample standard deviation \(s = 9.00\)</li>
                </ul>
                where 
                \[
                s = \sqrt{\frac{1}{n-1}\sum_{i=1}^n (x_i - \bar{x})^2}.
                \]
                Since the population standard deviation \(\sigma\) is unknown, we use <strong>one-sample t-test</strong>.
                The test statistic is computed by: 
                \[
                t = \frac{\bar{x} - \mu_0}{s / \sqrt{n}} = \frac{75.12 - 70}{9.00 / \sqrt{30}} \approx 3.12.
                \]
                The p-value is the probability of observing a test statistic as extreme as (or more extreme than) the calculated 
                t-value under the null hypothesis. This follows a Student's t-distribution with \(n -1 = 29 \) degrees of freedom.
                Then we have \(p \approx 0.0021\) via some numerical computation. Set the significance level \(\alpha = 0.05\). 
                Since \(p < 0.05\), we reject \(H_0\). Therefore, there is strong statistical evidence that the new teaching method 
                increases students' test scores.
                <br>
                We can only say that the data we observed(test scores) are very unlikely under the assumption that the true mean is still 70. 
                <br>
                It does <strong>NOT</strong> mean that..
                <ul>
                    <li>the new teaching method definitely increases test scores.</li>
                    <li>the probability that \(H_0\) is true is 0.0021.</li>
                    <li>the effect is practically significant.</li>
                </ul>
                
        </blockquote>

        <h1 id=""></h1>
        <blockquote>
        </blockquote>

        <h1 id=""></h1>
        <blockquote>
        </blockquote>


        
        <br><a href="../../index.html">Back to Home </a>
        <br> <a href="probability.html">Back to Probability </a> 

        <script src="../runPythonCode.js"></script>
        <script src="../collapsible.js"></script>  
    </body>
</html>