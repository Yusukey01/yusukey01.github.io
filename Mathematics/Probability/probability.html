<!DOCTYPE html>
<html>
    <head> 
        <title>Probability & Statistics</title>
        <link rel="stylesheet" href="../styles.css">
        <script id="MathJax-script" defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="description" content= "Explore fundamental concepts of probability and statistics essential for machine learning, 
         including probability theory, random variables, distributions, Bayesian inference, entropy, and the Fisher Information Matrix.">
    </head>
    <body> 
        <div class="container">
            <!-- Hero Section -->
            <div class="hero-section">
                <h1 class="webpage-name">Probability & Statistics
                    <span class="subheading">Foundations for Machine Learning</span>
                </h1>
            </div>

            <!-- Introduction -->
            <div class="homepage-introduction">
                <p>
                    To truly understand machine learning, a strong grasp of <strong>probability theory</strong> and <strong>statistics</strong> 
                    is essential because machine learning is an elegant combination of statistics and algorithms. In Section I: Linear Algebra, we 
                    intentionally avoided applications related to statistics, focusing instead on foundational concepts. Now, it's time to build 
                    upon the probabilistic basis of machine learning. This section introduces the essential concepts of probability, providing 
                    the tools and insights necessary to understand and apply machine learning techniques. At its core, statistics involves inferring 
                    unknown parameters from outcomes. This process is the inverse of probability theory. Two main approaches dominate statistical 
                    inference: <strong>frequentist statistics</strong>, which treats parameters as fixed and data as random, and in contrast, 
                    <strong>Bayesian statistics</strong>, which treats data as fixed and parameters as random. In particular, Bayesian statistics 
                    forms the foundation of many machine learning algorithms.
                </p>
            </div>

            <!-- Main Content - Topic Cards -->
            <section>
                <div class="topic-cards">

                    <div class="card">
                        <div class="card-icon">p</div>
                        <h3><a href="basic.html">Part 1: Basic Probability Ideas</a></h3>
                        <div class="keywords">
                            <span>Probability</span>
                            <span>Sample Space</span>
                            <span>Events</span>
                            <span>Mutually Exclusive</span>
                            <span>Permutation</span>
                            <span>Combinations</span>
                            <span>Conditional Probability</span>
                            <span>Independent Events</span>
                            <span>Law of Total Probability</span>
                            <span>Bayes' Theorem</span>
                        </div>
                    </div>

                    <div class="card">
                        <div class="card-icon">\(X\)</div>
                        <h3><a href="random_variables.html">Part 2: Random Variables</a></h3>
                        <div class="keywords">
                            <span>Discrete Random Variables</span>
                            <span>Continuous Random Variables</span>
                            <span>Probability Mass Function (p.m.f.)</span>
                            <span>Probability Density Function (p.d.f.)</span>
                            <span>Cumulative Distribution Function(c.d.f.)</span>
                            <span>Expected Value</span>
                            <span>Variance</span>
                            <span>Standard Deviation</span>
                        </div>
                    </div>

                    <div class="card">
                        <div class="card-icon">\(\Gamma\)</div>
                        <h3><a href="gamma.html">Part 3: Gamma & Beta Distribution</a></h3>
                        <div class="keywords">
                            <span>Gamma Distribution</span>
                            <span>Gamma Function</span>
                            <span>Exponential Distribution</span>
                            <span>Beta Function</span>
                            <span>Beta Distribution</span>
                            <span>Uniform Distribution</span>
                        </div>
                    </div>

                    <div class="card">
                        <div class="card-icon">\(\mathcal{N}\)</div>
                        <h3><a href="gaussian.html">Part 4: Normal (Gaussian) Distribution</a></h3>
                        <div class="keywords">
                            <span>Gaussian Function</span>
                            <span>Error Function</span>
                            <span>Gaussian Integral</span>
                            <span>Normal(Gaussian) Distribution</span>
                            <span>Standard Normal Distribution</span>
                            <span>Independent and Identically Distributed(i.i.d.)</span>
                            <span>Random Sample</span>
                            <span>Sample Mean</span>
                            <span>Sample Variance</span>
                            <span>Central Limit Theorem</span>
                        </div>
                    </div>

                    <div class="card">
                        <div class="card-icon">t</div>
                        <h3><a href="student.html">Part 5: Student's \(t\)-Distribution</a></h3>
                        <div class="keywords">
                            <span>Student's \(t\)-Distribution</span>
                            <span>Degrees of Freedom</span>
                            <span>Cauchy Distribution</span>
                            <span>Half Cauchy Distribution</span>
                            <span>Laplace Distribution</span>
                            <span>Double Sided Exponential Distribution</span>
                        </div>
                    </div>

                    <div class="card">
                        <div class="card-icon">Cov</div>
                        <h3><a href="covariance.html">Covariance</a></h3>
                        <div class="keywords">
                            <span class="code-tag">Code Included</span>
                            <span>Covariance</span>
                            <span>Covariance Matrix</span>
                            <span>Total Variance</span>
                            <span>Principal Component</span>
                            <span>Principal Component Analysis(PCA)</span>
                        </div>
                    </div>

                    <div class="card">
                        <div class="card-icon">Corr</div>
                        <h3><a href="correlation.html">Part 7: Correlation</a></h3>
                        <div class="keywords">
                            <span>Cross-Covariance Matrix</span>
                            <span>Auto-Covariance Matrix</span>
                            <span>Correlation Coefficient</span>
                            <span>Correlation Matrix</span>
                        </div>
                    </div>

                    <div class="card">
                        <div class="card-icon">\(\Sigma\)</div>
                        <h3><a href="mvn.html">Part 8: Multivariate Distributions</a></h3>
                        <div class="keywords">
                            <span>Multivariate Normal Distribution (MVN)</span>
                            <span>Mahalanobis Distance</span>
                            <span>Bivariate Normal Distribution</span>
                            <span>Dirichlet Distribution</span>
                            <span>Probability Simplex</span>
                            <span>Wishart Distribution</span>
                            <span>Inverse Wishart Distribution</span>
                        </div>
                    </div>

                    <div class="card">
                        <div class="card-icon">\(\mathcal{L}\)</div>
                        <h3><a href="mle.html">Part 9: Maximum Likelihood Estimation</a></h3>
                        <div class="keywords">
                            <span>Point Estimator</span>
                            <span>Mean Square Error(MSE)</span>
                            <span>Standard Error (SE)</span>
                            <span>Likelihood Function</span>
                            <span>Log-likelihood Function</span>
                            <span>Maximum Likelihood Estimation(MLE)</span>
                            <span>Binomial Distribution</span>
                            <span>Sample Proportion</span>
                        </div>
                    </div>

                    <div class="card">
                        <div class="card-icon">\(H_0 \, H_1\)</div>
                        <h3><a href="hypothesis_testing.html">Part 10: Statistical Inference & Hypothesis Testing</a></h3>
                        <div class="keywords">
                            <span>Null Hypothesis</span>
                            <span>Alternative Hypothesis</span>
                            <span>Type I Error (False Negative)</span>
                            <span>Type II Error (False Positive)</span>
                            <span>Significance Level</span>
                            <span>Test Statistic</span>
                            <span>Null Hypothesis Significance Test(NHST)</span>
                            <span>One Sample t-Tests</span>
                            <span>Confidence intervals</span>
                            <span>Critical Values</span>
                            <span>z-scores</span>
                            <span>Credible Intervals</span>
                            <span>Bootstrap</span>
                        </div>
                    </div>

                    <div class="card">
                        <div class="card-icon">LS</div>
                        <h3><a href="linear_regression.html">Part 11: Linear Regression</a></h3>
                        <div class="keywords">
                            <span>Linear Regression</span>
                            <span>Least-Squares Estimation</span>
                        </div>
                    </div>

                    <div class="card">
                        <div class="card-icon">\(\mathbb{H}\)</div>
                        <h3><a href="entropy.html">Part 12: Entropy</a></h3>
                        <div class="keywords">
                            <span>Information Content</span>
                            <span>Entropy</span>
                            <span>Joint Entropy</span>
                            <span>Conditional Entropy</span>
                            <span>Cross Entropy</span>
                            <span>KL Divergence(Relative Entropy, Information Gain)</span>
                            <span>Gibbs' Inequality</span>
                            <span>Log Sum Inequality</span>
                            <span>Jensen's Inequality</span>
                            <span>Mutual Information (MI)</span>
                        </div>
                    </div>

                    <!-- Part 13 -->
                    <div class="card">
                        <div class="card-icon">\(\lim_{n \to \infty }\)</div>
                        <h3><a href="convergence.html">Part 13: Convergence</a></h3>
                        <div class="keywords">
                            <span>Convergence in Probability</span>
                            <span>Convergence in Distribution</span>
                            <span>Asymptotic(limiting) Distribution</span>
                            <span>Moment Generating Function(m.g.f.)</span>
                            <span>Central Limit Theorem(CLT)</span>
                        </div>
                    </div>

                    <!-- Part 14 -->
                    <div class="card">
                        <div class="card-icon">\(p(\theta | \mathcal{D})\)</div>
                        <h3><a href="bayesian.html">Part 14: Intro to Bayesian Statistics</a></h3>
                        <div class="keywords">
                            <span>Bayesian Inference</span>
                            <span>Prior Distribution</span>
                            <span>Posterior Distribution</span>
                            <span>Marginal Likelihood</span>
                            <span>Conjugate Prior</span>
                            <span>Posterior Predictive Distribution</span>
                            <span>Beta-Binomial Model</span>
                            <span>Normal Distribution Model with known Variance \(\sigma^2\)</span>
                            <span>Normal Distribution Model with known Mean \(\mu\)</span>
                        </div>
                    </div>

                    <div class="card">
                        <div class="card-icon">\(\nu\)</div>
                        <h3><a href="expfamily.html">Part 15: The Exponential Family</a></h3>
                        <div class="keywords">
                            <span>Exponential Family</span>
                            <span>Natural Parameters(Canonical Parameters)</span>
                            <span>Base Measure</span>
                            <span>Sufficient Statistics</span>
                            <span>Partition Function</span>
                            <span>Minimal Representation</span>
                            <span>Natural Exponential Family(NEF)</span>
                            <span>Moment Parameters</span>
                            <span>Precision Matrix</span>
                            <span>Information Form</span>
                            <span>Moment Matching</span>
                            <span>Cumulants</span>
                        </div>
                    </div>

                    <div class="card">
                        <div class="card-icon">\(F(\theta)\)</div>
                        <h3><a href="fisher_info.html">Part 16: Fisher Information Matrix</a></h3>
                        <div class="keywords">
                            <span>Fisher Information Matrix(FIM)</span>
                            <span>Score Function</span>
                            <span>Covariance</span>
                            <span>Negative Log Likelihood</span>
                            <span>Log Partition Function</span>
                            <span>Approximated KL Divergence</span>
                            <span>Natural Gradient</span>
                            <span>Jeffreys Prior</span>
                            <span>Uninformative Prior</span>
                            <span>Reference Prior</span>
                            <span>Mutual Information</span>
                        </div>
                    </div>

                    <div class="card">
                        <div class="card-icon">\(\pi\)</div>
                        <h3><a href="decision_theory.html"> Part 17: Bayesian Decision Theory</a></h3>
                        <div class="keywords">
                            <span>Decision Theory</span>
                            <span>Optimal Policy(Bayes estimator)</span>
                            <span>Zero-One Loss</span>
                            <span>Maximum A Posteriori (MAP) Estimate</span>
                            <span>Reject Option</span>
                            <span>Confusion Matrix</span>
                            <span>False Positive (FP, Type I error)</span>
                            <span>False Negative (FN, Type II error)</span>
                            <span>Receiver Operating Characteristic (ROC) Curve</span>
                            <span>Equal Error Rate (EER)</span>
                            <span>Precision-Recall (PR) Curve</span>
                            <span>Interpolated Precision</span>
                            <span>Average Precision (AP)</span>
                        </div>
                    </div>

                    <div class="card">
                        <div class="card-icon">\(\prod\)</div>
                        <h3><a href="markov.html">Part 18: Markov Chains</a></h3>
                        <div class="keywords">
                            <span class="code-tag">Code Included</span>
                            <span>Markov Chains</span>
                            <span>Language Modeling</span>
                            <span>n-gram</span>
                            <span>Transition Function(Kernel)</span>
                            <span>Stochastic Matrix(Transition Matrix)</span>
                            <span>Maximum likelihood estimation(MLE) in Markov models</span>
                            <span>Sparse Data Problem</span>
                            <span>Add-One Smoothing</span>
                            <span>Dirichlet Prior</span>
                        </div>
                    </div>
                </div>
            </section>

            <!-- Back to Home Link -->
            <div class="contact-section">
                <a href="../../index.html">Back to Home</a>
            </div>
        </div>

        <!-- Footer -->
        <footer>
            <div class="footer-content">
                <div class="footer-about">
                    <h3>About</h3>
                    <p>This page provides resources for understanding probability and statistics concepts essential for machine learning applications.</p>
                </div>
                <div class="footer-links">
                    <h3>Quick Links</h3>
                    <ul>
                        <li><a href="../../index.html">Home</a></li>
                        <li><a href="../Linear/linear_algebra.html">Linear Algebra</a></li>
                        <li><a href="../Calculus/calculus.html">Calculus</a></li>
                        <li><a href="../Discrete/discrete_math.html">Discrete Math</a></li>
                    </ul>
                </div>
            </div>
            <div class="footer-bottom">
                <p>&copy; 2025 Math-CS Compass. All rights reserved.</p>
            </div>
        </footer>
    </body>
</html>