<!DOCTYPE html>
<html>
    <head> 
        <title>Probability & Statistics</title>
        <link rel="stylesheet" href="../styles.css">
        <script id="MathJax-script" defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="description" content= "Explore fundamental concepts of probability and statistics essential for machine learning, 
         including probability theory, random variables, distributions, Bayesian inference, entropy, and the Fisher Information Matrix.">
    </head>
    <body> 
        <div class="container">
            <!-- Hero Section -->
            <div class="hero-section">
                <h1 class="webpage-name">Probability & Statistics
                    <span class="subheading">Foundations for Machine Learning</span>
                </h1>
            </div>

            <!-- Introduction -->
            <div class="homepage-introduction">
                <p>
                    To truly understand machine learning, a strong grasp of <strong>probability theory</strong> and <strong>statistics</strong> 
                    is essential because machine learning is an elegant combination of statistics and algorithms. In Section I: Linear Algebra, we 
                    intentionally avoided applications related to statistics, focusing instead on foundational concepts. Now, it's time to build 
                    upon the probabilistic basis of machine learning. This section introduces the essential concepts of probability, providing 
                    the tools and insights necessary to understand and apply machine learning techniques. At its core, statistics involves inferring 
                    unknown parameters from outcomes. This process is the inverse of probability theory. Two main approaches dominate statistical 
                    inference: <strong>frequentist statistics</strong>, which treats parameters as fixed and data as random, and in contrast, 
                    <strong>Bayesian statistics</strong>, which treats data as fixed and parameters as random. In particular, Bayesian statistics 
                    forms the foundation of many machine learning algorithms.
                </p>
            </div>

            <!-- Main Content - Topic Cards -->
            <section>
                <div class="topic-cards">
                    <!-- Part 1 -->
                    <div class="card">
                        <div class="card-icon">1</div>
                        <h3><a href="basic.html">Basic Probability Ideas</a></h3>
                        <p>Key concepts: Probability, Sample Space, Events, Mutually Exclusive, Permutation, Combinations, Conditional Probability, Independent Events, Law of Total Probability, Bayes' Theorem</p>
                    </div>

                    <!-- Part 2 -->
                    <div class="card">
                        <div class="card-icon">2</div>
                        <h3><a href="random_variables.html">Random Variables</a></h3>
                        <p>Key concepts: Discrete Random Variables, Continuous Random Variables, Probability Mass Function (p.m.f.), Probability Density Function (p.d.f.), Cumulative Distribution Function(c.d.f.), Expected Value, Variance, Standard Deviation</p>
                    </div>

                    <!-- Part 3 -->
                    <div class="card">
                        <div class="card-icon">3</div>
                        <h3><a href="gamma.html">Gamma & Beta Distribution</a></h3>
                        <p>Key concepts: Gamma Distribution, Gamma Function, Exponential Distribution, Beta Function, Beta Distribution, Uniform Distribution</p>
                    </div>

                    <!-- Part 4 -->
                    <div class="card">
                        <div class="card-icon">4</div>
                        <h3><a href="gaussian.html">Normal (Gaussian) Distribution</a></h3>
                        <p>Key concepts: Gaussian Function, Error Function, Gaussian Integral, Normal (Gaussian) Distribution, Standard Normal Distribution, Independent and Identically Distributed (i.i.d.), Random Sample, Sample Mean, Sample Variance, Central Limit Theorem</p>
                    </div>

                    <!-- Part 5 -->
                    <div class="card">
                        <div class="card-icon">5</div>
                        <h3><a href="student.html">Student's \(t\)-Distribution</a></h3>
                        <p>Key concepts: Student's \(t\)-Distribution, Degrees of Freedom, Cauchy Distribution, Half Cauchy Distribution, Laplace Distribution (Double Sided Exponential Distribution)</p>
                    </div>

                    <!-- Part 6 -->
                    <div class="card">
                        <div class="card-icon">6</div>
                        <h3><a href="covariance.html">Covariance</a></h3>
                        <p>Key concepts: <span class="code-tag">Code Included</span>, Covariance, Covariance Matrix, Total Variance, Principal Component, Principal Component Analysis (PCA)</p>
                    </div>

                    <!-- Part 7 -->
                    <div class="card">
                        <div class="card-icon">7</div>
                        <h3><a href="correlation.html">Correlation</a></h3>
                        <p>Key concepts: Cross-Covariance Matrix, Auto-Covariance Matrix, Correlation Coefficient, Correlation Matrix</p>
                    </div>

                    <!-- Part 8 -->
                    <div class="card">
                        <div class="card-icon">8</div>
                        <h3><a href="mvn.html">Multivariate Distributions</a></h3>
                        <p>Key concepts: Multivariate Normal Distribution (MVN), Mahalanobis Distance, Bivariate Normal Distribution, Dirichlet Distribution, Probability Simplex, Wishart Distribution, Inverse Wishart Distribution</p>
                    </div>

                    <!-- Part 9 -->
                    <div class="card">
                        <div class="card-icon">9</div>
                        <h3><a href="mle.html">Maximum Likelihood Estimate</a></h3>
                        <p>Key concepts: Point Estimator, Mean Square Error (MSE), Standard Error (SE), Likelihood Function, Log-likelihood Function, Maximum Likelihood Estimate (MLE), Binomial Distribution, Sample Proportion</p>
                    </div>

                    <!-- Part 10 -->
                    <div class="card">
                        <div class="card-icon">10</div>
                        <h3><a href="hypothesis_testing.html">Statistical Inference & Hypothesis Testing</a></h3>
                        <p>Key concepts: Null Hypothesis, Alternative Hypothesis, Type I Error (False Negative), Type II Error (False Positive), Significance Level, Test Statistic, Null Hypothesis Significance Test (NHST), One Sample t-Tests, Confidence Intervals, Critical Values, z-scores, Credible Intervals, Bootstrap</p>
                    </div>

                    <!-- Part 11 -->
                    <div class="card">
                        <div class="card-icon">11</div>
                        <h3><a href="linear_regression.html">Linear Regression</a></h3>
                        <p>Key concepts: Linear Regression, Least-Squares Estimation</p>
                    </div>

                    <!-- Part 12 -->
                    <div class="card">
                        <div class="card-icon">12</div>
                        <h3><a href="entropy.html">Entropy</a></h3>
                        <p>Key concepts: Information Content, Entropy, Joint Entropy, Conditional Entropy, Cross Entropy, KL Divergence (Relative Entropy, Information Gain), Gibbs' Inequality, Log Sum Inequality, Jensen's Inequality, Mutual Information (MI)</p>
                    </div>

                    <!-- Part 13 -->
                    <div class="card">
                        <div class="card-icon">13</div>
                        <h3><a href="convergence.html">Convergence</a></h3>
                        <p>Key concepts: Convergence in Probability, Convergence in Distribution, Asymptotic (Limiting) Distribution, Moment Generating Function (m.g.f.), Central Limit Theorem (CLT)</p>
                    </div>

                    <!-- Part 14 -->
                    <div class="card">
                        <div class="card-icon">14</div>
                        <h3><a href="bayesian.html">Intro to Bayesian Statistics</a></h3>
                        <p>Key concepts: Bayesian Inference, Prior Distribution, Posterior Distribution, Marginal Likelihood, Conjugate Prior, Posterior Predictive Distribution, Beta-Binomial Model, Normal Distribution Model with known Variance \(\sigma^2\), Normal Distribution Model with known Mean \(\mu\)</p>
                    </div>

                    <!-- Part 15 -->
                    <div class="card">
                        <div class="card-icon">15</div>
                        <h3><a href="expfamily.html">The Exponential Family</a></h3>
                        <p>Key concepts: Exponential Family, Natural Parameters (Canonical Parameters), Base Measure, Sufficient Statistics, Partition Function, Minimal Representation, Natural Exponential Family (NEF), Moment Parameters, Precision Matrix, Information Form, Moment Matching, Cumulants</p>
                    </div>

                    <!-- Part 16 -->
                    <div class="card">
                        <div class="card-icon">16</div>
                        <h3><a href="fisher_info.html">Fisher Information Matrix</a></h3>
                        <p>Key concepts: Fisher Information Matrix (FIM), Score Function, Covariance, Negative Log Likelihood, Log Partition Function, Approximated KL Divergence, Natural Gradient, Jeffreys Prior, Uninformative Prior, Reference Prior, Mutual Information</p>
                    </div>

                    <!-- Part 17 -->
                    <div class="card">
                        <div class="card-icon">17</div>
                        <h3><a href="decision_theory.html">Bayesian Decision Theory</a></h3>
                        <p>Key concepts: Decision Theory, Optimal Policy (Bayes estimator), Zero-One Loss, Maximum A Posteriori (MAP) Estimate, Reject Option, Confusion Matrix, False Positive (FP, Type I error), False Negative (FN, Type II error), Receiver Operating Characteristic (ROC) Curve, Equal Error Rate (EER), Precision-Recall (PR) Curve, Interpolated Precision, Average Precision (AP)</p>
                    </div>

                    <!-- Part 18 -->
                    <div class="card">
                        <div class="card-icon">18</div>
                        <h3><a href="markov.html">Markov Chains</a></h3>
                        <p>Key concepts: <span class="code-tag">Code Included</span>, Markov Chains, Language Modeling, n-gram, Transition Function (Kernel), Stochastic Matrix (Transition Matrix), Maximum Likelihood Estimation (MLE) in Markov models, Sparse Data Problem, Add-One Smoothing, Dirichlet Prior</p>
                    </div>
                </div>
            </section>

            <!-- Back to Home Link -->
            <div class="contact-section">
                <a href="../../index.html">Back to Home</a>
            </div>
        </div>

        <!-- Footer -->
        <footer>
            <div class="footer-content">
                <div class="footer-about">
                    <h3>About</h3>
                    <p>This page provides resources for understanding probability and statistics concepts essential for machine learning applications.</p>
                </div>
                <div class="footer-links">
                    <h3>Quick Links</h3>
                    <ul>
                        <li><a href="../../index.html">Home</a></li>
                        <li><a href="../Linear/linear_algebra.html">Linear Algebra</a></li>
                        <li><a href="../Calculus/calculus.html">Calculus</a></li>
                        <li><a href="../Discrete/discrete_math.html">Discrete Math</a></li>                  
                    </ul>
                </div>
            </div>
            <div class="footer-bottom">
                <p>&copy; 2025 Mathematics Resource Site. All rights reserved.</p>
            </div>
        </footer>
    </body>
</html>