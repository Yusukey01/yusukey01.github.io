---
layout: default
title: Probability & Statistics
level: section
description: Explore fundamental concepts of probability and statistics essential for machine learning, including probability theory, random variables, distributions, Bayesian inference, entropy, and the Fisher Information Matrix
uses_math: true
uses_python: false
---
<!DOCTYPE html>
<html>
    <body> 
        <!-- Course Schema for Probability & Statistics Section -->
        <script type="application/ld+json">
        {
        "@context": "https://schema.org",
        "@type": "Course",
        "name": "Probability & Statistics",
        "description": "Explore fundamental concepts of probability and statistics essential for machine learning, including probability theory, random variables, distributions, Bayesian inference, entropy, and the Fisher Information Matrix",
        "provider": {
            "@type": "Organization",
            "name": "MATH-CS COMPASS",
            "url": "https://math-cs-compass.com"
        },
        "instructor": {
            "@type": "Person",
            "name": "Yusuke Yokota",
            "jobTitle": "Mathematics & Computer Science Educator"
        },
        "courseCode": "III",
        "educationalLevel": "university",
        "about": [
            { "@type": "Thing", "name": "Probability Theory" },
            { "@type": "Thing", "name": "Statistics" },
            { "@type": "Thing", "name": "Bayesian Inference" },
            { "@type": "Thing", "name": "Random Variables" },
            { "@type": "Thing", "name": "Statistical Distributions" }
        ],
        "teaches": [
            "Fundamental probability concepts",
            "Random variables and distributions",
            "Bayesian statistics and inference",
            "Maximum likelihood estimation",
            "Hypothesis testing and confidence intervals",
            "Entropy and information theory",
            "Markov chains and Monte Carlo methods"
        ],
        "hasCourseInstance": {
            "@type": "CourseInstance",
            "courseMode": "online",
            "courseWorkload": "PT2H30M",
            "instructor": {
            "@type": "Person",
            "name": "Yusuke Yokota"
            }
        },
        "offers": {
            "@type": "Offer",
            "price": "0",
            "priceCurrency": "USD",
            "availability": "https://schema.org/InStock",
            "category": "free"
        },
        "isPartOf": {
            "@type": "EducationalOrganization",
            "name": "MATH-CS COMPASS"
        }
        }
        </script>
        <div class="hero-section">
            <h1 class="webpage-name">III - Probability & Statistics
            </h1>
        </div>

        {% include section_navigation.html %}

        <div class="container">

            <div class="homepage-introduction">
                <h3>The Mathematics of Uncertainty and Inference</h3>

                <p>
                    <strong>Probability</strong> and <strong>Statistics</strong> are essential to truly understand machine learning, because 
                    machine learning is an elegant combination of statistics and algorithms. In <a href="../Linear_algebra/linear_algebra.html"><strong>Linear Algebra</strong></a>, 
                    we intentionally avoided applications related to statistics, focusing instead on foundational concepts. Now, it's time to build upon the 
                    probabilistic basis of machine learning. This section introduces the essential concepts of probability, providing the tools 
                    and insights necessary to understand and apply machine learning techniques. At its core, statistics involves inferring unknown 
                    parameters from outcomes - a process that is essentially the inverse of probability theory. Two main approaches dominate 
                    statistical inference: <strong>frequentist statistics</strong>, which treats parameters as fixed and data as random, and in contrast, 
                    <strong>Bayesian statistics</strong>, which treats data as fixed and parameters as random. In particular, Bayesian statistics forms the foundation 
                    of many machine learning algorithms.
                </p>

                <p>
                    Probability serves as the critical bridge between the exactness of algebraic structures and the unpredictability of the real 
                    world. In the "Compass" ecosystem, this section acts as a vital junction between the <a href="../Discrete/discrete_math.html"><strong>Discrete World (Section IV)</strong></a> 
                    and the <a href="../Calculus/calculus.html"><strong>Continuous World (Section II)</strong></a>. 
                    We explore how discrete combinatorics and information entropy provide the security for Section IV, while continuous Gaussian 
                    distributions and density functions provide the analytical engine for the optimization in Section II. From the foundational 
                    axioms to advanced Gaussian Processes, this section provides the statistical rigor required to validate models and interpret 
                    the hidden, stochastic patterns within the high-dimensional data of <a href="../Machine_learning/ml.html"><strong>Machine Learning (Section V)</strong></a>.
                </p>
            </div>

            <section class="container">
                <div id="topic-cards-container">
                    <!-- Cards will be injected here -->
                </div>
            </section>      

        </div>

        <script src="/js/sectionCards.js"></script>
        <script>
            SectionCards.init('III');
        </script>
        <script src="/js/main.js"></script>
        <script src="/js/search.js"></script>
    </body>
</html>