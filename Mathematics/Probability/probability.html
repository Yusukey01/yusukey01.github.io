---
layout: default
title: Probability & Statistics
level: section
description: Explore fundamental concepts of probability and statistics essential for machine learning, including probability theory, random variables, distributions, Bayesian inference, entropy, and the Fisher Information Matrix
uses_math: true
uses_python: false
---
<!DOCTYPE html>
<html>
    <body> 
        <!-- Course Schema for Probability & Statistics Section -->
        <script type="application/ld+json">
        {
        "@context": "https://schema.org",
        "@type": "Course",
        "name": "Probability & Statistics",
        "description": "Explore fundamental concepts of probability and statistics essential for machine learning, including probability theory, random variables, distributions, Bayesian inference, entropy, and the Fisher Information Matrix",
        "provider": {
            "@type": "Organization",
            "name": "MATH-CS COMPASS",
            "url": "https://yusukey01.github.io"
        },
        "instructor": {
            "@type": "Person",
            "name": "Yusuke Yokota",
            "jobTitle": "Mathematics & Computer Science Educator"
        },
        "courseCode": "III",
        "educationalLevel": "university",
        "about": [
            { "@type": "Thing", "name": "Probability Theory" },
            { "@type": "Thing", "name": "Statistics" },
            { "@type": "Thing", "name": "Bayesian Inference" },
            { "@type": "Thing", "name": "Random Variables" },
            { "@type": "Thing", "name": "Statistical Distributions" }
        ],
        "teaches": [
            "Fundamental probability concepts",
            "Random variables and distributions",
            "Bayesian statistics and inference",
            "Maximum likelihood estimation",
            "Hypothesis testing and confidence intervals",
            "Entropy and information theory",
            "Markov chains and Monte Carlo methods"
        ],
        "hasCourseInstance": {
            "@type": "CourseInstance",
            "courseMode": "online",
            "courseWorkload": "PT2H30M",
            "instructor": {
            "@type": "Person",
            "name": "Yusuke Yokota"
            }
        },
        "offers": {
            "@type": "Offer",
            "price": "0",
            "priceCurrency": "USD",
            "availability": "https://schema.org/InStock",
            "category": "free"
        },
        "isPartOf": {
            "@type": "EducationalOrganization",
            "name": "MATH-CS COMPASS"
        }
        }
        </script>
        <div class="hero-section">
            <h1 class="webpage-name">III - Probability & Statistics
            </h1>
        </div>

        {% include section_navigation.html %}

        <div class="container">

            <div class="homepage-introduction">
                <p>
                    To truly understand machine learning, a strong grasp of <strong>probability theory</strong> and <strong>statistics</strong> 
                    is essential because machine learning is an elegant combination of statistics and algorithms. In Section I: Linear Algebra, we 
                    intentionally avoided applications related to statistics, focusing instead on foundational concepts. Now, it's time to build 
                    upon the probabilistic basis of machine learning. This section introduces the essential concepts of probability, providing 
                    the tools and insights necessary to understand and apply machine learning techniques. At its core, statistics involves inferring 
                    unknown parameters from outcomes. This process is the inverse of probability theory. Two main approaches dominate statistical 
                    inference: <strong>frequentist statistics</strong>, which treats parameters as fixed and data as random, and in contrast, 
                    <strong>Bayesian statistics</strong>, which treats data as fixed and parameters as random. In particular, Bayesian statistics 
                    forms the foundation of many machine learning algorithms.
                </p>
            </div>

            <section>
                <div class="topic-cards">

                    <div class="card">
                        <div class="card-icon">p</div>
                        <h3><a href="basic.html">Part 1: Basic Probability Ideas</a></h3>
                        <div class="keywords">
                            <span>Probability</span>
                            <span>Sample Space</span>
                            <span>Events</span>
                            <span>Mutually Exclusive</span>
                            <span>Permutation</span>
                            <span>Combinations</span>
                            <span>Conditional Probability</span>
                            <span>Independent Events</span>
                            <span>Law of Total Probability</span>
                            <span>Bayes' Theorem</span>
                        </div>
                    </div>

                    <div class="card">
                        <div class="card-icon">\(X\)</div>
                        <h3><a href="random_variables.html">Part 2: Random Variables</a></h3>
                        <div class="keywords">
                            <span>Discrete Random Variables</span>
                            <span>Continuous Random Variables</span>
                            <span>Probability Mass Function (p.m.f.)</span>
                            <span>Probability Density Function (p.d.f.)</span>
                            <span>Cumulative Distribution Function(c.d.f.)</span>
                            <span>Expected Value</span>
                            <span>Variance</span>
                            <span>Standard Deviation</span>
                        </div>
                    </div>

                    <div class="card">
                        <div class="card-icon">\(\Gamma\)</div>
                        <h3><a href="gamma.html">Part 3: Gamma & Beta Distribution</a></h3>
                        <div class="keywords">
                            <span><strong>Interactive Demo</strong></span>
                            <span>Gamma Distribution</span>
                            <span>Gamma Function</span>
                            <span>Exponential Distribution</span>
                            <span>Beta Function</span>
                            <span>Beta Distribution</span>
                            <span>Uniform Distribution</span>
                        </div>
                    </div>

                    <div class="card">
                        <div class="card-icon">\(\mathcal{N}\)</div>
                        <h3><a href="gaussian.html">Part 4: Normal (Gaussian) Distribution</a></h3>
                        <div class="keywords">
                            <span>Gaussian Function</span>
                            <span>Error Function</span>
                            <span>Gaussian Integral</span>
                            <span>Normal(Gaussian) Distribution</span>
                            <span>Standard Normal Distribution</span>
                            <span>Independent and Identically Distributed(i.i.d.)</span>
                            <span>Random Sample</span>
                            <span>Sample Mean</span>
                            <span>Sample Variance</span>
                            <span>Central Limit Theorem</span>
                        </div>
                    </div>

                    <div class="card">
                        <div class="card-icon">t</div>
                        <h3><a href="student.html">Part 5: Student's \(t\)-Distribution</a></h3>
                        <div class="keywords">
                            <span>Student's \(t\)-Distribution</span>
                            <span>Degrees of Freedom</span>
                            <span>Cauchy Distribution</span>
                            <span>Half Cauchy Distribution</span>
                            <span>Laplace Distribution</span>
                            <span>Double Sided Exponential Distribution</span>
                        </div>
                    </div>

                    <div class="card">
                        <div class="card-icon">Cov</div>
                        <h3><a href="covariance.html">Covariance</a></h3>
                        <div class="keywords">
                            <span><strong>Code Included</strong></span>
                            <span>Covariance</span>
                            <span>Covariance Matrix</span>
                            <span>Total Variance</span>
                            <span>Principal Component</span>
                            <span>Principal Component Analysis(PCA)</span>
                        </div>
                    </div>

                    <div class="card">
                        <div class="card-icon">r</div>
                        <h3><a href="correlation.html">Part 7: Correlation</a></h3>
                        <div class="keywords">
                            <span>Cross-Covariance Matrix</span>
                            <span>Auto-Covariance Matrix</span>
                            <span>Correlation Coefficient</span>
                            <span>Correlation Matrix</span>
                        </div>
                    </div>

                    <div class="card">
                        <div class="card-icon">\(\Sigma\)</div>
                        <h3><a href="mvn.html">Part 8: Multivariate Distributions</a></h3>
                        <div class="keywords">
                            <span>Multivariate Normal Distribution (MVN)</span>
                            <span>Mahalanobis Distance</span>
                            <span>Bivariate Normal Distribution</span>
                            <span>Dirichlet Distribution</span>
                            <span>Probability Simplex</span>
                            <span>Wishart Distribution</span>
                            <span>Inverse Wishart Distribution</span>
                        </div>
                    </div>

                    <div class="card">
                        <div class="card-icon">\(\mathcal{L}\)</div>
                        <h3><a href="mle.html">Part 9: Maximum Likelihood Estimation</a></h3>
                        <div class="keywords">
                            <span>Point Estimator</span>
                            <span>Mean Square Error(MSE)</span>
                            <span>Standard Error (SE)</span>
                            <span>Likelihood Function</span>
                            <span>Log-likelihood Function</span>
                            <span>Maximum Likelihood Estimation(MLE)</span>
                            <span>Binomial Distribution</span>
                            <span>Sample Proportion</span>
                        </div>
                    </div>

                    <div class="card">
                        <div class="card-icon">\(H_0\) vs \(H_1\)</div>
                        <h3><a href="hypothesis_testing.html">Part 10: Statistical Inference & Hypothesis Testing</a></h3>
                        <div class="keywords">
                            <span>Null Hypothesis</span>
                            <span>Alternative Hypothesis</span>
                            <span>Type I Error (False Negative)</span>
                            <span>Type II Error (False Positive)</span>
                            <span>Significance Level</span>
                            <span>Test Statistic</span>
                            <span>Null Hypothesis Significance Test(NHST)</span>
                            <span>One Sample t-Tests</span>
                            <span>Confidence intervals</span>
                            <span>Critical Values</span>
                            <span>z-scores</span>
                            <span>Credible Intervals</span>
                            <span>Bootstrap</span>
                        </div>
                    </div>

                    <div class="card">
                        <div class="card-icon">LS</div>
                        <h3><a href="linear_regression.html">Part 11: Linear Regression</a></h3>
                        <div class="keywords">
                            <span><strong>Interactive Demo</strong></span>
                            <span>Linear Regression</span>
                            <span>Least-Squares Estimation</span>
                        </div>
                    </div>

                    <div class="card">
                        <div class="card-icon">\(\mathbb{H}\)</div>
                        <h3><a href="entropy.html">Part 12: Entropy</a></h3>
                        <div class="keywords">
                            <span>Information Content</span>
                            <span>Entropy</span>
                            <span>Joint Entropy</span>
                            <span>Conditional Entropy</span>
                            <span>Cross Entropy</span>
                            <span>KL Divergence(Relative Entropy, Information Gain)</span>
                            <span>Gibbs' Inequality</span>
                            <span>Log Sum Inequality</span>
                            <span>Jensen's Inequality</span>
                            <span>Mutual Information (MI)</span>
                        </div>
                    </div>

                    <div class="card">
                        <div class="card-icon">\(n \to \infty\)</div>
                        <h3><a href="convergence.html">Part 13: Convergence</a></h3>
                        <div class="keywords">
                            <span>Convergence in Probability</span>
                            <span>Convergence in Distribution</span>
                            <span>Asymptotic(limiting) Distribution</span>
                            <span>Moment Generating Function(m.g.f.)</span>
                            <span>Central Limit Theorem(CLT)</span>
                        </div>
                    </div>

                    <div class="card">
                        <div class="card-icon">\(p(\theta | \mathcal{D})\)</div>
                        <h3><a href="bayesian.html">Part 14: Intro to Bayesian Statistics</a></h3>
                        <div class="keywords">
                            <span>Bayesian Inference</span>
                            <span>Prior Distribution</span>
                            <span>Posterior Distribution</span>
                            <span>Marginal Likelihood</span>
                            <span>Conjugate Prior</span>
                            <span>Posterior Predictive Distribution</span>
                            <span>Beta-Binomial Model</span>
                            <span>Normal Distribution Model with known Variance \(\sigma^2\)</span>
                            <span>Normal Distribution Model with known Mean \(\mu\)</span>
                        </div>
                    </div>

                    <div class="card">
                        <div class="card-icon">\(\eta\)</div>
                        <h3><a href="expfamily.html">Part 15: The Exponential Family</a></h3>
                        <div class="keywords">
                            <span>Exponential Family</span>
                            <span>Natural Parameters(Canonical Parameters)</span>
                            <span>Base Measure</span>
                            <span>Sufficient Statistics</span>
                            <span>Partition Function</span>
                            <span>Minimal Representation</span>
                            <span>Natural Exponential Family(NEF)</span>
                            <span>Moment Parameters</span>
                            <span>Precision Matrix</span>
                            <span>Information Form</span>
                            <span>Moment Matching</span>
                            <span>Cumulants</span>
                        </div>
                    </div>

                    <div class="card">
                        <div class="card-icon">\(F(\theta)\)</div>
                        <h3><a href="fisher_info.html">Part 16: Fisher Information Matrix</a></h3>
                        <div class="keywords">
                            <span>Fisher Information Matrix(FIM)</span>
                            <span>Score Function</span>
                            <span>Covariance</span>
                            <span>Negative Log Likelihood</span>
                            <span>Log Partition Function</span>
                            <span>Approximated KL Divergence</span>
                            <span>Natural Gradient</span>
                            <span>Jeffreys Prior</span>
                            <span>Uninformative Prior</span>
                            <span>Reference Prior</span>
                            <span>Mutual Information</span>
                        </div>
                    </div>

                    <div class="card">
                        <div class="card-icon">\(\pi\)</div>
                        <h3><a href="decision_theory.html"> Part 17: Bayesian Decision Theory</a></h3>
                        <div class="keywords">
                            <span>Decision Theory</span>
                            <span>Optimal Policy(Bayes estimator)</span>
                            <span>Zero-One Loss</span>
                            <span>Maximum A Posteriori (MAP) Estimate</span>
                            <span>Reject Option</span>
                            <span>Confusion Matrix</span>
                            <span>False Positive (FP, Type I error)</span>
                            <span>False Negative (FN, Type II error)</span>
                            <span>Receiver Operating Characteristic (ROC) Curve</span>
                            <span>Equal Error Rate (EER)</span>
                            <span>Precision-Recall (PR) Curve</span>
                            <span>Interpolated Precision</span>
                            <span>Average Precision (AP)</span>
                        </div>
                    </div>

                    <div class="card">
                        <div class="card-icon">\(\prod\)</div>
                        <h3><a href="markov.html">Part 18: Markov Chains</a></h3>
                        <div class="keywords">
                            <span><strong>Code Included</strong></span>
                            <span>Probabilistic Graphical Models(PGMs)</span>
                            <span>Bayesian Networks</span>
                            <span>Markov Chains</span>
                            <span>Language Modeling</span>
                            <span>n-gram</span>
                            <span>Transition Function(Kernel)</span>
                            <span>Stochastic Matrix(Transition Matrix)</span>
                            <span>Maximum likelihood estimation(MLE) in Markov models</span>
                            <span>Sparse Data Problem</span>
                            <span>Add-One Smoothing</span>
                            <span>Dirichlet Prior</span>
                        </div>
                    </div>

                    <div class="card">
                        <div class="card-icon">\(p^*(\theta)\)</div>
                        <h3><a href="monte_carlo.html">Part 19: Monte Carlo Methods</a></h3>
                        <div class="keywords">
                            <span><strong>Interactive Demo</strong></span>
                            <span>Credible Intervals</span>
                            <span>Central Credible Intervals</span>
                            <span>Monte Carlo Approximation</span>
                            <span>Highest Posterior Density (HPD)</span>
                            <span>Markov Chain Monte Carlo (MCMC)</span>
                        </div>
                    </div>

                    <div class="card">
                        <div class="card-icon">\(\varphi\)</div>
                        <h3><a href="">Part 20: Importance Sampling</a></h3>
                        <div class="keywords">
                            <span>Importance Sampling</span>
                            <span>Importance Weights</span>
                            <span>Direct Importance Sampling</span>
                            <span>Effective Sample Size (ESS)</span>
                            <span>Self-Normalized Importance Sampling (SNIS)</span>
                            <span>Annealed Importance Sampling (AIS)</span>
                            <span>Annealing Schedule</span>
                        </div>
                    </div>
                    
                </div>
            </section>

            <section id="diagnostics" class="section-content">
                <h2>Diagnostics and Debugging</h2>
                
                <p>
                    Importance sampling can fail silently, producing estimates with high bias or variance. 
                    Here are essential diagnostics to monitor:
                </p>
                
                <h3>1. Weight Diagnostics</h3>
                <div class="theorem">
                    <span class="theorem-title">Key Diagnostic Metrics</span>
                    <ul>
                        <li><strong>Effective Sample Size (ESS)</strong>: Should be \(> N_s/2\) for reliable estimates</li>
                        <li><strong>Maximum weight ratio</strong>: \(\max_i W_i\) should be \(< 0.1\) to avoid single-sample domination</li>
                        <li><strong>Coefficient of variation</strong>: \(\text{CV} = \frac{\text{std}(\tilde{w})}{\text{mean}(\tilde{w})}\) should be \(< 1\) for stable estimation</li>
                        <li><strong>Weight entropy</strong>: \(H(W) = -\sum_i W_i \log W_i\) measures weight uniformity (higher is better)</li>
                    </ul>
                </div>
                
                <h3>2. Common Failure Modes and Solutions</h3>
                <table style="width: 100%; border-collapse: collapse; margin: 20px 0;">
                    <tr style="background: #f0f0f0;">
                        <th style="padding: 10px; text-align: left; border: 1px solid #ddd;">Symptom</th>
                        <th style="padding: 10px; text-align: left; border: 1px solid #ddd;">Likely Cause</th>
                        <th style="padding: 10px; text-align: left; border: 1px solid #ddd;">Solution</th>
                    </tr>
                    <tr>
                        <td style="padding: 10px; border: 1px solid #ddd;">ESS ≪ N_s</td>
                        <td style="padding: 10px; border: 1px solid #ddd;">Poor proposal choice</td>
                        <td style="padding: 10px; border: 1px solid #ddd;">Use heavier-tailed proposal or AIS</td>
                    </tr>
                    <tr>
                        <td style="padding: 10px; border: 1px solid #ddd;">High variance across runs</td>
                        <td style="padding: 10px; border: 1px solid #ddd;">Weight degeneracy</td>
                        <td style="padding: 10px; border: 1px solid #ddd;">Increase samples or improve proposal</td>
                    </tr>
                    <tr>
                        <td style="padding: 10px; border: 1px solid #ddd;">Biased estimates</td>
                        <td style="padding: 10px; border: 1px solid #ddd;">Violated support condition</td>
                        <td style="padding: 10px; border: 1px solid #ddd;">Ensure q(x) > 0 wherever π(x) > 0</td>
                    </tr>
                    <tr>
                        <td style="padding: 10px; border: 1px solid #ddd;">Numerical instability</td>
                        <td style="padding: 10px; border: 1px solid #ddd;">Extreme weight ratios</td>
                        <td style="padding: 10px; border: 1px solid #ddd;">Work in log-space; use log-sum-exp trick</td>
                    </tr>
                </table>
                
                <h3>3. Validation Techniques</h3>
                <p>
                    To verify your importance sampling implementation:
                </p>
                <ul style="padding-left: 40px;">
                    <li><strong>Known ground truth</strong>: Test on problems with analytical solutions</li>
                    <li><strong>Convergence check</strong>: Estimate should stabilize as N_s increases</li>
                    <li><strong>Multiple proposals</strong>: Different valid proposals should give consistent estimates</li>
                    <li><strong>Bootstrap confidence intervals</strong>: Resample weights to assess uncertainty</li>
                </ul>
            </section>
        </div>
        <script src="/js/main.js"></script>
        <script src="/js/search.js"></script>
    </body>
</html>