---
layout: default
title: Importance Sampling
level: detail
description: Introduction to importance sampling for efficient Monte Carlo estimation with applications to modern AI and machine learning.
uses_math: true
uses_python: false
---

<!DOCTYPE html>
<html>
    <body>
        <!-- Meta script for importance_sampling.html -->
        <script type="application/ld+json">
        {
        "@context": "https://schema.org",
        "@type": "LearningResource",
        "name": "{{ page.title }}",
        "description": "{{ page.description }}",
        "learningResourceType": "lesson",
        "educationalUse": "instruction",
        "educationalLevel": "university",
        "interactivityType": "{% if page.content contains 'Interactive Demo' or page.content contains 'demo' %}active{% else %}expositive{% endif %}",
        "author": {
            "@type": "Person",
            "name": "Yusuke Yokota",
            "jobTitle": "Mathematics & Computer Science Educator"
        },
        "publisher": {
            "@type": "Organization",
            "name": "MATH-CS COMPASS",
            "url": "https://yusukey01.github.io"
        },
        "about": [
            { "@type": "Thing", "name": "Importance Sampling" },
            { "@type": "Thing", "name": "Direct Importance Sampling" },
            { "@type": "Thing", "name": "Self-Normalized Importance Sampling" },
            { "@type": "Thing", "name": "Annealed Importance Sampling" },
            { "@type": "Thing", "name": "AIS" },
            { "@type": "Thing", "name": "Importance Weights" },
            { "@type": "Thing", "name": "Proposal Distribution" },
            { "@type": "Thing", "name": "Target Distribution" },
            { "@type": "Thing", "name": "Monte Carlo Methods" },
            { "@type": "Thing", "name": "Variance Reduction" },
            { "@type": "Thing", "name": "RLHF" },
            { "@type": "Thing", "name": "Off-Policy Learning" },
            { "@type": "Thing", "name": "Variational Inference" },
            { "@type": "Thing", "name": "Bayesian Inference" },
            { "@type": "Thing", "name": "Normalizing Constant" },
            { "@type": "Thing", "name": "Partition Function" }
        ],
        "teaches": [
            "Importance sampling fundamentals",
            "Direct and self-normalized importance sampling",
            "Annealed importance sampling",
            "Applications to modern AI"
        ],
        "isPartOf": {
            "@type": "Course",
            "name": "Probability & Statistics",
            "description": "Explore fundamental concepts of probability and statistics essential for machine learning",
            "provider": {
            "@type": "Organization",
            "name": "MATH-CS COMPASS",
            "url": "https://yusukey01.github.io"
            },
            "instructor": {
            "@type": "Person",
            "name": "Yusuke Yokota",
            "jobTitle": "Mathematics & Computer Science Educator"
            },
            "courseCode": "III",
            "hasCourseInstance": {
            "@type": "CourseInstance",
            "courseMode": "online",
            "courseWorkload": "PT2H30M",
            "instructor": {
                "@type": "Person",
                "name": "Yusuke Yokota"
            }
            },
            "offers": {
            "@type": "Offer",
            "price": "0",
            "priceCurrency": "USD",
            "availability": "https://schema.org/InStock",
            "category": "free"
            }
        }
        }
        </script>
        <!-- WebApplication Schema for Interactive Tools -->
        <script type="application/ld+json">
        {
        "@context": "https://schema.org",
        "@type": "WebApplication",
        "name": "{{ page.title }} Interactive Tool",
        "description": "Interactive tool for exploring {{ page.title | downcase }} concepts with real-time visualizations and computational examples",
        "applicationCategory": "EducationalApplication",
        "operatingSystem": "Web Browser",
        "url": "https://yusukey01.github.io{{ page.url }}",
        "author": {
            "@type": "Person",
            "name": "Yusuke Yokota"
        },
        "applicationSubCategory": "Mathematical Computation Tool",
        "featureList": [
            "Interactive mathematical visualization",
            "Real-time parameter adjustment",
            "Educational demonstrations",
            "Statistical computation",
            "Importance sampling simulation",
            "Variance comparison",
            "AIS demonstration"
        ],
        "isAccessibleForFree": true,
        "educationalUse": "instruction",
        "educationalLevel": "university"
        }
        </script>
        <div class="hero-section">
            <h1 class="webpage-name">Importance Sampling
            </h1>
        </div>

        {% include section_navigation.html %}

        <div class="topic-nav">
            <a href="#intro">Introduction</a>
            <a href="#direct">Direct Importance Sampling</a>
            <a href="#selfnorm">Self-Normalized IS</a>
            <a href="#ais">Annealed Importance Sampling</a>
        </div> 

        <div class="container">  
            <section id="intro" class="section-content">
                <h2>Introduction</h2>
                
                <p>
                    <strong>Importance sampling</strong> is a fundamental Monte Carlo technique for estimating expectations under one probability 
                    distribution by sampling from a different, more convenient distribution. This approach addresses two critical challenges in 
                    computational statistics and machine learning when:
                    <ul style="padding-left: 40px;">
                        <li>direct sampling from the target distribution is difficult or impossible.</li>
                        <li>we want to focus computational resources on the most "important" regions of the distribution.</li>
                    </ul>
                </p>

                <p>
                    Suppose we want to compute the expectation of a <strong>target function</strong> \(\varphi(\mathbf{x})\) under a 
                    <strong>target distribution</strong> \(\pi(\mathbf{x})\):
                    \[
                    I = \mathbb{E}_{\pi}[\varphi(\mathbf{x})] = \int \varphi(\mathbf{x}) \pi(\mathbf{x}) \, d\mathbf{x}
                    \]
                </p>
                <p>
                    Standard <a href="monte_carlo.html"><strong>Monte Carlo estimation</strong></a> would draw independent samples 
                    \(x^{(1)}, \ldots, x^{(S)} \sim \pi(x)\) and approximate:
                    \[
                    I \approx \hat{I}_{\text{MC}} = \frac{1}{S} \sum_{s=1}^S \varphi(x^{(s)}).
                    \]
                </p>
                <p>
                    However, this approach fails when:
                    <ul style="padding-left: 40px;">
                        <li>Sampling from \(\pi(\mathbf{x})\) is computationally expensive or infeasible</li>
                        <li>The normalizing constant of \(\pi(\mathbf{x})\) is unknown (common in Bayesian inference)</li>
                        <li>\(\varphi(\mathbf{x})\) has large values only in regions where \(\pi(\mathbf{x})\) has low probability (rare events)</li>
                    </ul>
                </p>

                <p>
                    <strong>Importance sampling</strong> solves these problems by introducing a <strong>proposal distribution</strong> 
                    (or <strong>importance distribution</strong>) \(q(\mathbf{x})\) from which we can easily sample, then reweighting samples 
                    to account for the distribution mismatch.
                </p>

                <p>
                    Importance sampling has become essential in modern AI, particularly in the following domains:
                </p>

                <h4><strong>1. Reinforcement Learning from Human Feedback (RLHF)</strong></h4>
                <p>
                    Modern large language models(LLMs) like ChatGPT, are fine-tuned using RLHF, which fundamentally relies on 
                    importance sampling. The <strong>Proximal Policy Optimization (PPO)</strong> algorithm uses importance sampling to learn 
                    from data collected under an old policy while optimizing a new policy:
                    \[
                    L(\theta) = \mathbb{E}_{(s,a) \sim \mu}\left[\frac{\pi_\theta(a|s)}{\mu(a|s)} A(s,a)\right]
                    \]
                    where \(\mu\) is the behavior policy (old data) and \(\pi_\theta\) is the target policy being optimized. Without importance 
                    sampling, we would need to collect entirely new data after each policy update, making RLHF expensive.
                </p>

                <h4><strong>2. Off-Policy Reinforcement Learning</strong></h4>
                <p>
                    In robotics, autonomous systems, and game AI, collecting on-policy data can be dangerous or expensive. Importance sampling 
                    enables learning from historical data, human demonstrations, or safer exploration policies. This is crucial for algorithms 
                    like Soft Actor-Critic (SAC), Conservative Q-Learning (CQL), and offline RL methods.
                </p>

                <h4><strong>3. Variational Inference and Deep Generative Models</strong></h4>
                <p>
                    Importance-weighted autoencoders (IWAE) use importance sampling to obtain tighter bounds on the evidence lower bound (ELBO), 
                    leading to better generative models and more accurate uncertainty estimates in Bayesian deep learning.
                </p>

                <h4><strong>4. Imbalanced Data and Rare Events</strong></h4>
                <p>
                    In applications like fraud detection, medical diagnosis of rare diseases, and AI safety, the events we care most about 
                    are extremely rare. Importance sampling allows us to oversample these critical cases and reweight appropriately, ensuring 
                    models do not ignore rare but important scenarios.
                </p>

                <h4><strong>5. Computing Normalizing Constants</strong></h4>
                <p>
                    Many probabilistic models in AI (Boltzmann machines, energy-based models, certain Bayesian posteriors) have intractable 
                    normalizing constants. Importance sampling, particularly <strong>annealed importance sampling</strong>, provides practical methods to 
                    estimate these constants, which is essential for model comparison and learning.
                </p>

                <p>
                    The critical insight is that importance sampling enables <strong>sample efficiency</strong>: in an era where data collection 
                    and computation are expensive, the ability to reuse and reweight existing data rather than collecting new samples is invaluable. 
                    This makes importance sampling one of the foundational techniques enabling practical, scalable AI systems.
                </p>
            </section>

            <section id="direct" class="section-content">
                <h2>Direct Importance Sampling</h2>
                
                <p>
                    The key mathematical insight behind importance sampling is the change of measure. We can rewrite the expectation under 
                    the target distribution \(\pi(\mathbf{x})\) as an expectation under some proposal distribution \(q(\mathbf{x})\):
                    \[
                    \begin{align*}
                    I &= \int \varphi(\mathbf{x}) \pi(\mathbf{x}) \, d\mathbf{x} \\\\
                      &= \int \varphi(\mathbf{x}) \frac{\pi(\mathbf{x})}{q(\mathbf{x})} q(\mathbf{x}) \, d\mathbf{x} \\\\
                      &= \mathbb{E}_{q}\left[\varphi(\mathbf{x}) \frac{\pi(\mathbf{x})}{q(\mathbf{x})}\right]
                    \end{align*}
                    \]
                </p>
                <p>
                    This identity holds for any \(q(\mathbf{x})\) that satisfies the support condition: 
                    \(q(\mathbf{x}) > 0\) whenever \(\pi(\mathbf{x}) \neq 0\).
                </p>
                <p>
                    The ratio
                    \[
                    \tilde{w}_n = \frac{\pi(\mathbf{x})}{q(\mathbf{x})}
                    \]
                    is called the <strong>importance weights</strong>. It measures how much more (or less) likely \(\mathbf{x}\) is under the target 
                    distribution \(\pi\) compared to the proposal distribution \(q\). Note that while \(\pi\) and \(q\) are normalized 
                    distributions, the weights \(\tilde{w}_n(\mathbf{x}_n)\) do not sum to any particular value.
                </p>
                <p>
                    Assume that the normalized \(\pi (\mathbf{x})\) can be evaluated, but we are not able to sample from it. 
                    If we draw \(N_s\) samples, \(\mathbf{x}_n \sim q(\mathbf{x})\), the <strong>direct importance sampling estimator</strong> is:
                    \[
                    \hat{I}_{\text{IS}} = \frac{1}{N_s} \sum_{n=1}^{N_s} \tilde{w}_n \varphi(x_n).
                    \]
                </p>
                <p>
                    This estimator is <strong>unbiased</strong>:
                    \[
                    \begin{align*}
                    \mathbb{E}_q[\hat{I}_{\text{IS}}] &= \mathbb{E}_q\left[\frac{1}{N_s}\sum_{n=1}^{N_s} \tilde{w}_n(\mathbf{x}_n)\varphi(\mathbf{x}_n)\right] \\\\
                                                      &= \mathbb{E}_q[\tilde{w}_n(\mathbf{x})\varphi(\mathbf{x})] \\\\
                                                      &= I
                    \end{align*}
                    \]
                </p>

                <p>
                    While the estimator is unbiased, its variance depends critically on the choice of \(q\):
                    \[
                    \text{Var}[\hat{I}_{\text{IS}}] = \frac{1}{N_s}\left(\mathbb{E}_q[\tilde{w}_n^2(\mathbf{x})\varphi^2(\mathbf{x})] - I^2\right)
                    \]
                </p>
                <p>
                    If the importance weights vary dramatically, a few samples will dominate the estimate, leading to high variance. 
                    This is quantified by the <strong>effective sample size (ESS)</strong>, which can be computed from the 
                    unnormalized weights as:
                    \[
                    \text{ESS} = \frac{\left(\sum_{n=1}^{N_s} \tilde{w}_n\right)^2}{\sum_{n=1}^{N_s} (\tilde{w}_n)^2}.
                    \]
                    The ESS ranges from 1 (maximum degeneracy, all weight on one sample) to \(N_s\) (all weights equal, no degeneracy). 
                    A good rule of thumb is that if ESS < \(N_s /2\), the proposal distribution should be reconsidered.
                </p>

                <div class="theorem">
                    <span class="theorem-title">Requirements for Direct Importance Sampling</span>
                    For direct importance sampling to be valid and practical:
                     <ul>
                        <li><strong>Support condition</strong>: \(q(\mathbf{x}) > 0\) whenever \(\pi(\mathbf{x}) > 0\)</li>
                        <li><strong>Normalization</strong>: Both \(\pi(\mathbf{x})\) and \(q(\mathbf{x})\) must be properly normalized probability distributions</li>
                        <li><strong>Evaluability</strong>: We must be able to evaluate \(\pi(\mathbf{x})\) and \(q(\mathbf{x})\) for any \(\mathbf{x}\)</li>
                        <li><strong>Sampling</strong>: We must be able to draw samples from \(q(\mathbf{x})\) efficiently</li>
                    </ul>
                </div>

            </section>

            <section id="selfnorm" class="section-content">
                <h2>Self-Normalized Importance Sampling</h2>
                
                <p>
                    In direct importance sampling, we need to evaluate the normalized target distribution \(\pi\), but it is simpler to 
                    evaluate the <strong>unnormalized target distribution</strong>:
                    \[
                    \tilde{\gamma}(\mathbf{x}) = Z \pi(\mathbf{x})
                    \]
                    where 
                    \[
                    Z = \int \tilde{\gamma}(\mathbf{x})d\mathbf{z}
                    \]
                    is the normalization constant, which is also approximated. In the end, <strong>self-normalized importance sampling(SNIS)</strong> evaluates a 
                    ratio of two estimates: 
                    \[
                    \begin{align*}
                    \mathbb{E}[\varphi(\mathbf{x})] &= \int \varphi(\mathbf{x}) \pi(\mathbf{x})d\mathbf{x} \\\\
                                                    &= \frac{\int \varphi(\mathbf{x})\tilde{\gamma}(\mathbf{x})d\mathbf{x}}
                                                            {\int \tilde{\gamma}(\mathbf{x})d\mathbf{x}} \\\\
                                                    &=\frac{\int [\frac{\tilde{\gamma}(\mathbf{x})}{q(\mathbf{x})}\varphi(\mathbf{x})]q(\mathbf{x})d\mathbf{x}}
                                                           {\int [\frac{\tilde{\gamma}(\mathbf{x})}{q(\mathbf{x})}]q(\mathbf{x})d\mathbf{x}} \\\\
                                                    &\approx \frac{\frac{1}{N_s} \sum_{n=1}^{N_s} \tilde{w_n}\varphi{\mathbf{x}_n}}
                                                                    {\frac{1}{N_s}\sum_{n=1}^{N_s}\tilde{w}_n} \\\\
                                                    &=\sum_{n=1}^{N_s} W_n \varphi(\mathbf{x}_n)
                    \end{align*}
                    \]
                    where 
                    \[
                    \tilde{w}_n = \frac{\tilde{\gamma}(\mathbf{x}_n)}{q(\mathbf{x}_n)}
                    \]
                    is the <strong>unnormalized weights</strong> and 
                    \[
                    W_n = \frac{\tilde{w}_n}{\sum_{n' =1}^{N_s}\tilde{w}_{n'}}
                    \]
                    is the <strong>normalized weights</strong>.
                </p>

                <p>
                    This estimator is <strong>biased</strong>, but as \(N_s\) approaches \(\infty\), the bias becomes negligible.
                </p>
            </section>

            <section id="ais" class="section-content">
                <h2>Annealed Importance Sampling (AIS)</h2>
                
                <h3>Motivation: The Bridge Distribution Approach</h3>
                <p>
                    When the target distribution \(\pi(\mathbf{x})\) and proposal distribution \(q(\mathbf{x})\) are very different, direct importance sampling 
                    often fails because the importance weights have extremely high variance. Most samples from \(q\) will have near-zero weight 
                    under \(\pi\), while a few rare samples will have enormous weights, leading to unreliable estimates.
                </p>
                <p>
                    <strong>Annealed importance sampling (AIS)</strong>, introduced by Neal (2001), solves this problem by introducing a sequence 
                    of intermediate distributions that gradually "anneal" from the proposal to the target distribution. This creates a smoother 
                    path, dramatically reducing variance.
                </p>

                <h3>The Annealing Schedule</h3>
                <p>
                    Define a sequence of intermediate distributions using an <strong>annealing schedule</strong> \(0 = \beta_0 < \beta_1 < \cdots < \beta_n = 1\):
                    \[
                    \pi_k(x) = \frac{\tilde{\pi}_k(x)}{Z_k}, \quad \text{where } \tilde{\pi}_k(x) = \tilde{q}(x)^{1-\beta_k} \tilde{\pi}(x)^{\beta_k}
                    \]
                </p>
                <p>
                    Notice that:
                </p>
                <ul style="padding-left: 40px;">
                    <li>At \(k=0\): \(\pi_0(\mathbf{x}) = q(\mathbf{x})\) (the proposal distribution)</li>
                    <li>At \(k=n\): \(\pi_n(\mathbf{x}) = \pi(\mathbf{x})\) (the target distribution)</li>
                    <li>For intermediate \(k\): \(\pi_k(\mathbf{x})\) is a geometric mixture between \(q\) and \(\pi\)</li>
                </ul>
                <p>
                    Common choices for the annealing schedule include:
                </p>
                <ul style="padding-left: 40px;">
                    <li><strong>Linear</strong>: \(\beta_k = \frac{k}{n}\)</li>
                    <li><strong>Geometric</strong>: \(\beta_k = \left(\frac{k}{n}\right)^\alpha\) for some \(\alpha > 0\)</li>
                    <li><strong>Adaptive</strong>: chosen to maintain roughly constant ESS between steps</li>
                </ul>

                <h3>Sequential Importance Sampling</h3>
                <p>
                    AIS uses a sequence of importance sampling steps. Starting with a sample \(\mathbf{x}^{(0)} \sim q(\mathbf{x})\), we:
                </p>
                <ol style="padding-left: 40px;">
                    <li>Compute the initial weight: \(w_0 = 1\)</li>
                    <li>For each intermediate distribution \(k = 1, \ldots, n-1\):
                        <ul>
                            <li>Update the sample: \(\mathbf{x}^{(k)} \sim T_k(\mathbf{x}'|\mathbf{x}^{(k-1)})\) using a Markov transition kernel that leaves \(\pi_k\) invariant</li>
                            <li>Update the weight: \(w_k = w_{k-1} \cdot \frac{\tilde{\pi}_k(\mathbf{x}^{(k)})}{\tilde{\pi}_{k-1}(\mathbf{x}^{(k-1)})}\)</li>
                        </ul>
                    </li>
                    <li>The final importance weight is: \(w = w_{n-1} \cdot \frac{\tilde{\pi}(\mathbf{x}^{(n)})}{\tilde{\pi}_{n-1}(\mathbf{x}^{(n-1)})}\)</li>
                </ol>

                <h3>The AIS Weight Formula</h3>
                <p>
                    The final importance weight can be written as a telescoping product:
                    \[
                    w = \frac{\tilde{\pi}(\mathbf{x}^{(n)})}{\tilde{q}(\mathbf{x}^{(0)})} \cdot \prod_{k=1}^{n-1} \frac{\tilde{\pi}_k(\mathbf{x}^{(k)})}{\tilde{\pi}_{k+1}(\mathbf{x}^{(k)})}
                    \]
                </p>
                <p>
                    More explicitly:
                    \[
                    w = \frac{\tilde{\pi}_1(\mathbf{x}^{(1)})}{\tilde{q}(\mathbf{x}^{(0)})} \cdot \frac{\tilde{\pi}_2(\mathbf{x}^{(2)})}{\tilde{\pi}_1(\mathbf{x}^{(1)})} \cdot \frac{\tilde{\pi}_3(\mathbf{x}^{(3)})}{\tilde{\pi}_2(\mathbf{x}^{(2)})} \cdots \frac{\tilde{\pi}(\mathbf{x}^{(n)})}{\tilde{\pi}_{n-1}(\mathbf{x}^{(n-1)})}
                    \]
                </p>
                <p>
                    Each ratio \(\frac{\tilde{\pi}_{k+1}(\mathbf{x}^{(k)})}{\tilde{\pi}_k(\mathbf{x}^{(k)})}\) is typically close to 1 (unlike the direct ratio 
                    \(\frac{\tilde{\pi}(\mathbf{x})}{\tilde{q}(\mathbf{x})}\)), which keeps the weights stable and reduces variance.
                </p>

                <h3>Estimating Partition Functions</h3>
                <p>
                    AIS is particularly powerful for estimating normalizing constants (partition functions). Using self-normalized importance 
                    sampling with AIS weights, we can estimate the ratio of partition functions:
                    \[
                    \frac{Z_\pi}{Z_q} \approx \frac{1}{S} \sum_{s=1}^S w^{(s)}
                    \]
                </p>
                <p>
                    If \(Z_q\) is known (e.g., \(q\) is a simple distribution like a Gaussian), we can estimate \(Z_\pi\). This is crucial for:
                </p>
                <ul style="padding-left: 40px;">
                    <li>Model comparison in Bayesian inference (estimating marginal likelihoods)</li>
                    <li>Training energy-based models and Boltzmann machines</li>
                    <li>Evaluating the evidence in variational inference</li>
                </ul>

                <h3>Why AIS Works</h3>
                <p>
                    The key insight is that by breaking the difficult transition from \(q\) to \(\pi\) into many small steps, we ensure that:
                </p>
                <ul style="padding-left: 40px;">
                    <li>Each individual importance weight ratio is close to 1</li>
                    <li>The product of many near-1 ratios has much lower variance than a single large ratio</li>
                    <li>The Markov transitions allow samples to explore the support of each intermediate distribution</li>
                </ul>
                <p>
                    The variance of AIS decreases as the number of intermediate distributions \(n\) increases, though computational cost increases 
                    proportionally. In practice, choosing \(n = 100\) to \(n = 10000\) is common depending on how different \(q\) and \(\pi\) are.
                </p>

                <div class="theorem">
                    <span class="theorem-title">Applications of AIS</span>
                    Annealed importance sampling is used in:
                    <ul>
                        <li><strong>Deep learning</strong>: Estimating partition functions of restricted Boltzmann machines (RBMs) and 
                        deep belief networks</li>
                        <li><strong>Bayesian inference</strong>: Computing marginal likelihoods for model selection</li>
                        <li><strong>Statistical physics</strong>: Estimating free energies in simulations (where it was originally developed)</li>
                        <li><strong>Variational inference</strong>: Improving ELBO estimates and comparing variational approximations</li>
                    </ul>
                </div>
            </section>
        </div>

        <script src="/js/main.js"></script>

    </body>
</html>