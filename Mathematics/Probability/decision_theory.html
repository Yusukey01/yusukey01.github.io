<!DOCTYPE html>
<html>
    <head> 
        <title>Bayesian Decision Theory</title>
        <link rel="stylesheet" href="../styles.css">
        <script id="MathJax-script" defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <meta name="viewport" content="width=device-width, initial-scale=1">
    </head>
    <body> 
        <div class="toc-container">
            <h2>Contents</h2>
            <ul>
                <li><a href="#intro">Introduction</a></li>
                <li><a href="#class">Classification (zero-one loss)</a></li>
                <li><a href="#"></a></li>
                <li><a href="#"></a></li>
            </ul>
        </div>
        <h1 id="intro">Introduction</h1>
        <blockquote>
            In decision theory, we assume an <strong>agent</strong> has a set of possible actions to choose from. Every <strong>action</strong> \(a \in \mathcal{A}\) 
            has some information such as costs and benefits, which depend on the <strong>state of nature</strong> \(h \in \mathcal{H}\). These information will be 
            encoded into a <strong>loss function</strong> \(l(h, a)\). 
            <br><br>
            <strong>Bayesian decision theory</strong> is a probabilistic approach to decision-making under uncertainty. It provides a framework for making 
            optimal decisions based on prior knowledge and observed data. The fundamental idea is to minimize the expected risk (or loss) by 
            leveraging <a href="bayesian.html"><strong>Bayesian inference</strong></a>. This approach is widely used in probabilistic machine 
            learning models such as classification, inference, and prediction.
            <br><br>
            For each action \(a\) given all evidence, which may be single datum \(\boldsymbol{x}\) or an data set \(\mathcal{D}\), we compute the 
            <strong>posterior expected loss</strong>: 
            \[
            \rho(a | \boldsymbol{x}) = \mathbb{E}_{p(h| \boldsymbol{x})} [l(h, a)] = \sum_{h \in \mathcal{H}} l(h, a)p(h |\boldsymbol{x}).
            \]
            The <strong>Bayes estimator</strong>(or Bayes decision rule, optimal policy) is given by:
            \[
            \pi^* (\boldsymbol{x}) = \arg \min_{a \in \mathcal{A}} \mathbb{E}_{p(h|\boldsymbol{x})}[l(h, a)].
            \]
            Equivalently,
            \[
            \pi^* (\boldsymbol{x}) = \arg \max_{a \in \mathcal{A}} \mathbb{E}_{h}[U(h, a)],
            \]
            where \(U(h, a) = - l(h, a)\) is a <strong>utility function</strong> that is the desirability of each action in each possible state.
        </blockquote> 

        <h1 id="class">Classification (Zero-One Loss)</h1>
        <blockquote>
            Suppose the states of nature correspond to class labels:
            \[
            \mathcal{H} = \mathcal{Y} = \{1, \cdots, C\}.
            \]
            Furthermore, suppose actions also corresponds to class labels:
            \[
            \mathcal{A} = \mathcal{Y}.
            \]
            We define <strong>zero-one loss</strong>:
            \[
            l_{01} (y^*, \hat{y}) = \mathbb{I}(y^* \neq \hat{y}).
            \]
            The posterior expected loss will be
            \[
            \rho(\hat{y}| \boldsymbol{x}) = p(\hat{y} \neq y^* | \boldsymbol{x}) = 1 - p (y^* = \hat{y}| \boldsymbol{x}).
            \]
            Hence, the action that minimizes the expected loss is to choose the most probable label:
            \[
            \pi (\boldsymbol{x}) = \arg \max_{y \in \mathcal{Y}} p( y | boldsymbol{x} ).
            \]
            In other words, this is the <strong>mode</strong> of posterior distribution which is known as the 
            <strong>maximum a posteriori</strong> or <strong>MAP estimate</strong>.
        </blockquote>

        <h1 id=""></h1>
        <blockquote>
        </blockquote>

        <h1 id=""></h1>
        <blockquote>
        </blockquote>

        <br><a href="../../index.html">Back to Home </a>
        <br> <a href="probability.html">Back to Probability </a> 

    </body>
</html>