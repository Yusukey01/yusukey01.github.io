<!DOCTYPE html>
<html>
    <head> 
        <title>Bayesian Decision Theory</title>
        <link rel="stylesheet" href="../styles.css">
        <script id="MathJax-script" defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <meta name="viewport" content="width=device-width, initial-scale=1">
    </head>
    <body> 
        <div class="toc-container">
            <h2>Contents</h2>
            <ul>
                <li><a href="#intro">Introduction</a></li>
                <li><a href="#class">Classification (zero-one loss)</a></li>
                <li><a href="#CM">Confusion Matrix</a></li>
                <li><a href="#"></a></li>
            </ul>
        </div>
        <h1 id="intro">Introduction</h1>
        <blockquote>
            In decision theory, an <strong>agent</strong> has a set of possible actions to choose from. Each <strong>action</strong> \(a \in \mathcal{A}\) 
            is associated with costs and benefits that depend on the <strong>state of nature</strong> \(h \in \mathcal{H}\). 
            This relationship is encoded into a <strong>loss function</strong> \(l(h, a)\). 
            <br><br>
            <strong>Bayesian decision theory</strong> is a probabilistic approach to decision-making under uncertainty. It provides a framework for making 
            optimal decisions based on prior knowledge and observed data. The fundamental idea is to minimize the expected risk (or loss) by 
            leveraging <a href="bayesian.html"><strong>Bayesian inference</strong></a>. This approach is widely used in probabilistic machine 
            learning models such as classification, inference, and prediction.
            <br><br>
            For any action \(a\) given evidence, which can be single observation \(\boldsymbol{x}\) or a dataset \(\mathcal{D}\), we compute the 
            <strong>posterior expected loss</strong>: 
            \[
            \rho(a | \boldsymbol{x}) = \mathbb{E}_{p(h| \boldsymbol{x})} [l(h, a)] = \sum_{h \in \mathcal{H}} l(h, a)p(h |\boldsymbol{x}).
            \]
            The <strong>Bayes estimator</strong>(or Bayes decision rule / optimal policy) specifies what action to take when presented with 
            evidence \(boldsymbol{x}\) so as to minimize the risk(loss):
            \[
            \pi^* (\boldsymbol{x}) = \arg \min_{a \in \mathcal{A}} \mathbb{E}_{p(h|\boldsymbol{x})}[l(h, a)].
            \]
            Equivalently,
            \[
            \pi^* (\boldsymbol{x}) = \arg \max_{a \in \mathcal{A}} \mathbb{E}_{h}[U(h, a)],
            \]
            where \(U(h, a) = - l(h, a)\) is a <strong>utility function</strong> that is the desirability of each action in each possible state. 
            This formulation is useful when working in utility-based decision-making, where the focus is on maximizing expected rewards rather 
            than minimizing losses. (e.g., economics, game theory, and reinforcement learning)
        </blockquote> 

        <h1 id="class">Classification (Zero-One Loss)</h1>
        <blockquote>
            A common application of Bayesian decision theory is <strong>classification</strong>. 
            In this context, we aim to select the optimal class label for a given input \(\boldsymbol{x} \in \mathcal{X}\).
            <br><br>
            Suppose that the states of nature correspond to labels:
            \[
            \mathcal{H} = \mathcal{Y} = \{1, \cdots, C\}.
            \]
            and that the possible actions are also the class labels:
            \[
            \mathcal{A} = \mathcal{Y}.
            \]
            In this case, a typical loss function is the <strong>zero-one loss</strong>:
            \[
            l_{01} (y^*, \hat{y}) = \mathbb{I}(y^* \neq \hat{y}), 
            \]
            where \(y^*\) is the true label, and \(\hat{y}\) is the predicted label.
            <br><br>
            Under the zero-one loss, the posterior expected loss for choosing label \(\hat{y}\) becomes will be
            \[
            \rho(\hat{y}| \boldsymbol{x}) = p(\hat{y} \neq y^* | \boldsymbol{x}) = 1 - p (y^* = \hat{y}| \boldsymbol{x}).
            \] 
            Thus, minimizing the expected loss is equivalent to maximizing the posterior probability:
            \[
            \pi (\boldsymbol{x}) = \arg \max_{y \in \mathcal{Y}} p( y | \boldsymbol{x} ).
            \]
            In other words, the optimal decision is to select the <strong>mode</strong> of the posterior distribution, which 
            is the <strong>maximum a posteriori (MAP) estimate</strong>.
            <br><br>
            In some scenarios, especially when the risks associated with making an incorrect decision are high, it is prudent 
            to allow the system to express uncertainty. This is called the <strong>reject option</strong>. Under this approach, 
            the set of available actions is expanded to include the reject action:
            \[
            \mathcal{A} = \mathcal{Y} \cup \{0\},
            \]
            where action \(0\) represents the reject option (i.e., saying "I'm not sure").
            <br><br>
            The loss function can be defined as:
            \[
            l(y^*, a)=
                \begin{cases}
                0 &\text{if \(y^* = a\) and \(a \in \{1, \cdots, C\}\)} \\
                \lambda_r &\text{if \(a = 0\)} \\
                \lambda_e &\text{otherwise} \\
                \end{cases}
            \]
            where \(\lambda_r\) is the cost of reject action, and \(\lambda_e\) is the cost of a classification error.
            <br><br>
            Under this framework, instead of always choosing the label with the highest posterior probability, the optimal policy is to 
            choose a label only when the classifier is sufficiently confident: 
            \[
            a^* = 
                \begin{cases} 
                y^*  & \text{if \(p^* > \lambda*\)} \\
                \text{reject} & \text{otherwise}
                \end{cases}
            \]
            where 
            \[
            \begin{align*}
            &p^* = p(y^* | x) = \max_{y \in \{1, \cdots, C\}} p(y | x) \\
            &\lambda^* = 1 - \frac{\lambda_r}{\lambda_e}
            \end{align*}
            \]
            <div class="proof">
                <span class="proof-title">\(\lambda^* = 1 - \frac{\lambda_r}{\lambda_e}\):</span>
                The optimal decision is to choose the class label \(y\) if and only if its risk, \(R(y)\) is lower than the risk of rejecting, \(R(\text{reject})\). 
                In other words, we choose \(y\) if 
                \[
                \begin{align*}
                &R(y) < R(\text{reject}) \\\\
                &\Longrightarrow \lambda_e \Bigl[\sum_{y^* \neq y} p(y^* | x)\Bigr]  < \lambda_r \\\\
                &\Longrightarrow \lambda_e [1 - p(y|x)]  < \lambda_r \\\\
                &\Longrightarrow p(y|x) > 1 - \frac{\lambda_r}{\lambda_e} \\\\
                \end{align*}
                \]
                Thus, if the maximum posterior probability \(p^*\) exceeds the threshold \(\lambda^* = 1 - \frac{\lambda_r}{\lambda_e}\), 
                then the classifier should choose the corresponding label. Otherwise, it should reject.
            </div>
        </blockquote>

        <h1 id="CM">Confusion Matrix</h1>
        <blockquote>
            In classification tasks, it's crucial to evaluate how well the model performs. A common tool for this is the 
            <strong>confusion matrix</strong>, which summarizes the outcomes of classification decisions. The confusion matrix 
            captures the following:
            <ul>
                <li>True Positives (TP):</li>
                    The number of instances correctly classified as positive.
                <li>True Negatives (TN):</li>
                    The number of instances correctly classified as negative.
                <li><strong>False Positives (FP)</strong>:</li>
                    The number of instances incorrectly classified as positive (<strong>Type I error</strong>).
                <li><strong>False Negatives (FN)</strong>:</li>
                    The number of instances incorrectly classified as negative (<strong>Type II error</strong>).
            </ul>
            For binary classification, the confusion matrix typically looks like this:
            <br>
            <table border="1">
                <thead>
                  <tr>
                    <th></th>
                    <th>Predicted Positive</th>
                    <th>Predicted Negative</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td><strong>Actual Positive</strong></td>
                    <td>TP</td>
                    <td>FN</td>
                  </tr>
                  <tr>
                    <td><strong>Actual Negative</strong></td>
                    <td>FP</td>
                    <td>TN</td>
                  </tr>
                </tbody>
              </table>
              <br>
              In our context, the confusion matrix quantifies the outcomes of decisions made by the Bayes estimator, which is designed to 
              minimize the zero-one loss. It provides an empirical measure of the performance of that optimal policy by counting how often 
              the decisions (predicted labels) match or mismatch the true labels.
              <br><br>
              Note: Understanding a false positive (FP) and false negative (FN) is crucial because the costs associated with each error type may differ significantly depending on 
              the application. In safety-critical systems, a FN (missing a dangerous condition) might be far more 
              costly than a FP (raising an unnecessary alarm). For example, in a medical test, a FP would mean 
              diagnosing a healthy patient as having a disease, whereas a FN would mean missing a disease in a patient who 
              actually has it, which is more critical in mdedical fields. 
        </blockquote>

        <h1 id=""></h1>
        <blockquote>
        </blockquote>

        <br><a href="../../index.html">Back to Home </a>
        <br> <a href="probability.html">Back to Probability </a> 

    </body>
</html>