<!DOCTYPE html>
<html>
    <head> 
        <title>Bayesian Decision Theory</title>
        <link rel="stylesheet" href="../styles.css">
        <script id="MathJax-script" defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <meta name="viewport" content="width=device-width, initial-scale=1">
    </head>
    <body> 
        <div class="toc-container">
            <h2>Contents</h2>
            <ul>
                <li><a href="#intro">Introduction</a></li>
                <li><a href="#class">Classification (zero-one loss)</a></li>
                <li><a href="#"></a></li>
                <li><a href="#"></a></li>
            </ul>
        </div>
        <h1 id="intro">Introduction</h1>
        <blockquote>
            In decision theory, an <strong>agent</strong> has a set of possible actions to choose from. Each <strong>action</strong> \(a \in \mathcal{A}\) 
            is associated with costs and benefits that depend on the <strong>state of nature</strong> \(h \in \mathcal{H}\). 
            This relationship is encoded into a <strong>loss function</strong> \(l(h, a)\). 
            <br><br>
            <strong>Bayesian decision theory</strong> is a probabilistic approach to decision-making under uncertainty. It provides a framework for making 
            optimal decisions based on prior knowledge and observed data. The fundamental idea is to minimize the expected risk (or loss) by 
            leveraging <a href="bayesian.html"><strong>Bayesian inference</strong></a>. This approach is widely used in probabilistic machine 
            learning models such as classification, inference, and prediction.
            <br><br>
            For any action \(a\) given evidence, which can be single observation \(\boldsymbol{x}\) or a dataset \(\mathcal{D}\), we compute the 
            <strong>posterior expected loss</strong>: 
            \[
            \rho(a | \boldsymbol{x}) = \mathbb{E}_{p(h| \boldsymbol{x})} [l(h, a)] = \sum_{h \in \mathcal{H}} l(h, a)p(h |\boldsymbol{x}).
            \]
            The <strong>Bayes estimator</strong>(or Bayes decision rule / optimal policy) specifies what action to take when presented with 
            evidence \(boldsymbol{x}\) so as to minimize the risk(loss):
            \[
            \pi^* (\boldsymbol{x}) = \arg \min_{a \in \mathcal{A}} \mathbb{E}_{p(h|\boldsymbol{x})}[l(h, a)].
            \]
            Equivalently,
            \[
            \pi^* (\boldsymbol{x}) = \arg \max_{a \in \mathcal{A}} \mathbb{E}_{h}[U(h, a)],
            \]
            where \(U(h, a) = - l(h, a)\) is a <strong>utility function</strong> that is the desirability of each action in each possible state. 
            This formulation is useful when working in utility-based decision-making, where the focus is on maximizing expected rewards rather 
            than minimizing losses. (e.g., economics, game theory, and reinforcement learning)
        </blockquote> 

        <h1 id="class">Classification (Zero-One Loss)</h1>
        <blockquote>
            A common application of Bayesian decision theory is <strong>classification</strong>. 
            In this context, we aim to select the optimal class label for a given input \(\boldsymbol{x} \in \mathcal{X}\).
            <br><br>
            Suppose that the states of nature correspond to labels:
            \[
            \mathcal{H} = \mathcal{Y} = \{1, \cdots, C\}.
            \]
            and that the possible actions are also the class labels:
            \[
            \mathcal{A} = \mathcal{Y}.
            \]
            In this case, a typical loss function is the <strong>zero-one loss</strong>:
            \[
            l_{01} (y^*, \hat{y}) = \mathbb{I}(y^* \neq \hat{y}), 
            \]
            where \(y^*\) is the true label, and \(\hat{y}\) is the predicted label.
            <br><br>
            Under the zero-one loss, the posterior expected loss for choosing label \(\hat{y}\) becomes will be
            \[
            \rho(\hat{y}| \boldsymbol{x}) = p(\hat{y} \neq y^* | \boldsymbol{x}) = 1 - p (y^* = \hat{y}| \boldsymbol{x}).
            \] 
            Thus, minimizing the expected loss is equivalent to maximizing the posterior probability:
            \[
            \pi (\boldsymbol{x}) = \arg \max_{y \in \mathcal{Y}} p( y | \boldsymbol{x} ).
            \]
            In other words, the optimal decision is to select the <strong>mode</strong> of the posterior distribution, which 
            is the <strong>maximum a posteriori (MAP) estimate</strong>.
            <br><br>
            Furthermore, oftern we need to say "I'm not sure" when we have critical risks. This is called the <strong>reject option</strong>. 
            In this case, the actions will be:
            \[
            \mathcal{A} = \mathcal{Y} \cup \{0\},
            \]
            where action \(0\) represents the reject action.
            <br><br>
            The loss function can be defined as:
            \[
            l(y^*, a)=
                \begin{cases}
                0 &\text{if \(y^* = a\) and \(a \in \{1, \cdots, C\}\)} \\
                \lambda_r &\text{if \(a = 0\)} \\
                \lambda_e &\text{otherwise} \\
                \end{cases}
            \]
            where \(\lambda_r\) is the cost of reject action, and \(\lambda_e\) is the cost of a classification error.
            <br><br>
            The optimal policy is 
            \[
            a^* = 
                \begin{cases} 
                y^*  &\text{if \(p^* > \lambda*\)}
                \text{reject} & \text{otherwise}
                \end{cases}
            \]
            where 
            \[
            \begin{align*}
            &p^* = p(y^* | x) = \max_{y \in \{1, \cdots, C\}} p(y | x) \\\\
            &\lambda^* = 1 - \frac{\lambda_r}{\lambda_e}
            \end{align*}
            \]
        </blockquote>

        <h1 id=""></h1>
        <blockquote>
        </blockquote>

        <h1 id=""></h1>
        <blockquote>
        </blockquote>

        <br><a href="../../index.html">Back to Home </a>
        <br> <a href="probability.html">Back to Probability </a> 

    </body>
</html>