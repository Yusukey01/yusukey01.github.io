<!DOCTYPE html>
<html>
    <head> 
        <title>Convergence</title>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <link rel="stylesheet" href="../styles.css">
        <meta name="viewport" content="width=device-width, initial-scale=1">
    </head>
    <body> 
        <h1>Convergence in Probability</h1>
        <blockquote>
            Even in computer science, we often accept the convergence of probabilities and distributions based on experimental results. 
            However, to gain a deeper understanding of statistics, we introduce formal definitions to clarify what convergence means in 
            statistical terms. 
            <br><br>
            Let \(\{X_n\}\) be a sequence of random variables and \(X\) be a random variable defined on a sample space. 
            \(X_n\) <strong>converges in probability</strong> to \(X\) denoted by
            \[
            X_n \xrightarrow{P} X,
            \]
             if \(\quad \forall \epsilon > 0\), 
            \[
            \lim_{n \to \infty} P [| X_n - X | \geq \epsilon ] = 0 
            \]
            or equivalently,
            \[
            \lim_{n \to \infty} P [| X_n - X | < \epsilon ] = 1.
            \]
            <br>
            <div class="theorem">
                <span class="theorem-title">Theorem 1:</span>
                Suppose \(X_n \xrightarrow{P} a\) where \(a\) is a constant, and the real function \(f\) is continuous at \(a\). Then 
                \[
                f(X_n) \xrightarrow{P} f(a).
                \]
            </div>

            <div class="proof">
                <span class="proof-title">Proof:</span>
                Let \(\epsilon > 0\). Since \(f\) is continuous at \(a\), \(\, \exists \delta > 0 \) such that if 
                \[
                |x - a | < \delta \Longrightarrow |f(x) - f(a)| < \epsilon. 
                \]
                Thus, 
                \[
                |f(x) - f(a)| \geq \epsilon \Longrightarrow  |x - a | \geq \delta
                \]
                Substituting \(X_n\) for \(x\), we obtain 
                \[
                P[|f(X_n) - f(a)| \geq \epsilon] \leq P [| X_n - a| \geq \delta ].
                \]
                As \(n \to \infty\), we have \(f(X_n) \xrightarrow{P} f(a)\).
            </div>
            <br>
            In general, if \(X_n \xrightarrow{P} X,\) and \(f\) is a continuous function, then 
            \[
            f(X_n) \xrightarrow{P} f(X).
            \]
        </blockquote>

        <h1>Convergence in distribution</h1>
        <blockquote>
            Let \(\{X_n\}\) be a sequence of random variables and let \(X\) be a random variable. Let \(F_{X_n}\) and 
            \(F_X\) be the cdfs of \(X_n\) and \(X\)  respectively. 
            <br>
            Let \(C(F_X)\) denote the set of all points where \(F_X\) is continuous. 
            <br>
            \(X_n\) <strong>converges in distribution</strong> to \(X\) denoted by 
            \[
            X_n \xrightarrow{D} X,
            \]
            if \(\quad \forall x \in C(F_{X})\), 
            \[
            \lim_{n \to \infty} F_{X_n} (x) = F_X (x).
            \]
            Often the distribution of \(X\) is called the <strong>asymptotic distribution</strong> of the sequence of random 
            variables of \(\{X_n\}\).
            <br>
            In this case, \(X_n\) does NOT always get close to \(X\) in probability. However, follwing theorem gives us a connection 
            between the two concepts.
            <div class="theorem">
                <span class="theorem-title">Theorem 2:</span>
                If \(X_n\) converges to \(X\) in probability, then \(X_n\) converges to \(X\) in distribution. 
            </div>
            <div class="proof">
                <span class="proof-title">Proof:</span>
                Let \(x\) be a point of continuity of \(F_{x_n}(x)\).  
                <br>
                \(\forall \, \epsilon > 0\),
                \[
                \begin{align*}
                F_{X_n}(x) &= P[X_n \leq x] \\\\
                           &= P[\{X_n \leq x\} \cap \{|X_n - X| < \epsilon\}] + P [\{X_n \leq x\} \cap \{|X_n - X| \geq \epsilon\}] \\\\
                           &\leq P[X_n \leq x + \epsilon] + P[|X_n - X| \geq \epsilon]
                \end{align*}
                \]
                Since \(X_n \xrightarrow{P} X\), we get a upper bound 
                \[
                \lim_{n \to \infty} \sup F_{X_n}(x) \leq F_{X}(x + \epsilon) \tag{1}.
                \]
                Similarly, we can get a lower bound:
                \[
                P[X_n  > x ] \leq P[X \geq x - \epsilon] + P[|X_n - X| \geq \epsilon]
                \]
                \[
                \Longrightarrow  \lim_{n \to \infty} \inf F_{X_n}(x) \geq F_X (x - \epsilon) \tag{2}.
                \]
                Combaining (1) and (2), we obtain 
                \[
                F_X(x - \epsilon) \leq \lim_{n \to \infty} \inf F_{X_n}(x) \leq \lim_{n \to \infty} \sup F_{X_n}(x) \leq F_X (x + \epsilon).
                \]
                Here, as \(\epsilon \to 0\), we conclude
                \[
                \lim_{n \to \infty} F_{X_n}(x) = F_X (x).
                \]
            </div>
        </blockquote>

        <br><a href="../../index.html">Back to Home </a>
        <br> <a href="probability.html">Back to Probability </a>   
    </body>
</html>