<!DOCTYPE html>
<html>
    <head> 
        <title>Numerical Example</title>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <script src="https://cdn.jsdelivr.net/pyodide/v0.23.3/full/pyodide.js"></script>
        <script defer src="https://pyscript.net/alpha/pyscript.js"></script>
        <link rel="stylesheet" href="../styles.css">
        <meta name="viewport" content="width=device-width, initial-scale=1">
    </head>
    <body> 
        <h1>Numerical Computation Example</h1>
        <blockquote>
            <div class="container">
                <h2>\(df = X(dx)+(dX)X\) for \(f(X) = X^2\)</h2>
                In this example, we numerically compute the derivative of the matrix equation \(f(X) = X^2\). Defining 
                differentiation rules, such as the general chain rule and vector-Jacobian product can significantly enhance 
                computational performance.
                <br>
                To validate your differentiation rules, comparing them with the <strong>finite-difference approximation</strong> 
                is a practical approach. Two common methods for finite-difference approximation are:
                \[
                f(x+dx) - f(x) \text{ (Forward Difference)}
                \]
                \[
                f(x) - f(x-dx) \text{ (Fackward Difference)}
                \]
                When evaluating the accuracy of your results, it is essential to use <strong>relative error</strong> as a metric. 
                A small simple difference between two quantities does not always imply that the error is negligible. Instead, relative 
                error is calculated as:
                \[
                Relative Error = \frac{\| True - Approximation \|}{\|True \|}
                \]
                Theoritically, as \(dx \to 0\), the approximation of \(df\) becomes more accurate. However, this is not always true in 
                practice due to the limitations of floating-point arithmetic. When \(dx\) becomes too small, the computed difference 
                \(f(x+dx) - f(x) \) may be rounded off to zero by the computer, leading to a loss of accuracy and an increase in the error.
                <br><br>
                In the example, we set dX = np.random.randn(2, 2) * <strong>1e-8</strong>. This is small enough to approximate the derivative 
                but not so small that numerical rounding errors dominate the computation. (Check <strong>machine epsilon</strong>)
                <br><br>
                Finally, to ensure the reliability of your code, avoid choosing arbitrarily "nice" numbers. Instead, generate random 
                <strong>non-integer</strong> values to test the robustness of your implementation and minimize bias in your results.

                <pre class="python-code">
                    import numpy as np
                    # Matrix X 
                    X = np.random.randn(2, 2)

                    # Differential matrix dX
                    dX = np.random.randn(2, 2) * 1e-8

                    # Define f(X) = X^2
                    f_X = X @ X  

                    # Forward Difference Approximation: f(X + dX) - f(X) 
                    f_approx = (X + dX) @ (X + dX) - f_X

                    # Backward Difference Approximation: f(X) - f(X - dX)
                    b_approx = f_X - (X - dX) @ (X - dX) 

                    # Exact: X dX + dX X
                    exact = X @ dX + dX @ X

                    # Relative errors (by Frobenius norm)
                    f_relative_error =  np.linalg.norm(f_approx - exact, 'fro')  / np.linalg.norm(exact, 'fro')
                    b_relative_error =  np.linalg.norm(b_approx - exact, 'fro')  / np.linalg.norm(exact, 'fro')

                    # Print the results
                    print(f"Input Matrix X:\n {X} \n")
                    print(f"Differential dX:\n {dX}\n")
                    print(f"Foward Difference (f(X + dX) - f(X)):\n {f_approx}\n")
                    print(f"Backward Difference (f(X) - f(X - dX)):\n {b_approx}\n")
                    print(f"Exact (X dX + dX X):\n {exact}\n")
                    print(f"\nRelative Error(Forward): {f_relative_error:.2e}")
                    print(f"\nRelative Error(Backward): {b_relative_error:.2e}")
                </pre>
                <button class="run-button" onclick="runPythonCode()">Run Code</button>
                <div class="python-output" id="output"></div>
            </div>
            
        </blockquote>
             <a href="../index.html">Back to Home </a>
        <br> <a href="calculus.html">Back to Calculus </a>

        <script> 
            let pyodide; 
            async function loadPyodideAndPackages() { 
                pyodide = await loadPyodide(); 
                await pyodide.loadPackage("numpy");
                console.log("Pyodide and numpy loaded successfully."); 
            } 
            async function runPythonCode() { 
                const pythonCode = document.querySelector('.python-code').textContent; 
                const outputContainer = document.getElementById('output'); 
                outputContainer.innerHTML = ""; 
                try { 
                    let capturedOutput = []; 
                    pyodide.globals.set("print", function(...args) { 
                        capturedOutput.push(args.join(" ")); 
                    }); 
                    await pyodide.runPythonAsync(pythonCode); 
                    outputContainer.textContent = capturedOutput.join("\n"); 
                } catch (error) { 
                    outputContainer.textContent = `Error: ${error.message}`; 
                    console.error("Execution error:", error); } 
                } 
                loadPyodideAndPackages(); 
        </script>
        
    </body>
</html>