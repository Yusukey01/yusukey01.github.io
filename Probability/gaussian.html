<!DOCTYPE html>
<html>
    <head> 
        <title>Gaussian Distribution</title>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <link rel="stylesheet" href="../styles.css">
        <meta name="viewport" content="width=device-width, initial-scale=1">
    </head>
    <body> 
        <h1>Gaussian Function</h1>
        <blockquote>
            a <strong>Gaussian function</strong> is defined as:
            \[
            f(x) = e^{-x^2}
            \]
            and often is parametrized as 
            \[
            f(x) = a e^{-\frac{(x-b)^2}{2c^2}} \tag{1}
            \]
            where \(a, b , c \in \mathbb{R}, \text{ and } c \neq 0\).
            <br>
            The family of Gaussian functions does not have elementary antiderivatives. To represent the integral of Gaussian functions, 
            we use the special function known as the <strong>error function</strong>: 
            \[
            \text{erf }(z) = \frac{2}{\sqrt{\pi}} \int_{0}^z e^{-t^2}dt. \qquad \text{erf}: \mathbb{C} \to \mathbb{C}
            \]
            Then, 
            \[
            \int e^{-x^2} dx = \frac{\sqrt{\pi}}{2}\text{erf }(x) + C. 
            \]
            <br>On the other hand, their improper integrals over \(\mathbb{R}\) can be evaluated exactly using <strong>Gaussian integral</strong>:
            <div class="theorem">
                <span class="theorem-title">Theorem 1: Gaussian Integral</span>
            \[
            \int_{-\infty} ^\infty  e^{-x^2} dx = \sqrt{\pi}. 
            \]
            Note: The generalized Gaussian integral is given by 
            \[
            \int_{-\infty} ^\infty  e^{-ax^2} dx = \sqrt{\frac{\pi}{a}} \qquad a > 0 \tag{3}
            \]
            </div>
            <div class="proof">
                <span class="proof-title">Proof:</span>
                Let \(I =  \int_{-\infty} ^\infty  e^{-x^2} dx\). Then
                \[
                \begin{align*}
                I^2 &= \int_{-\infty} ^\infty  \int_{-\infty} ^\infty   e^{-u^2}  e^{-v^2} du dv \\\\
                    &= \int_{-\infty} ^\infty  \int_{-\infty} ^\infty  e^{-(u+v)^2} dudv.
                \end{align*}
                \]
                Using the polar coordinates, let \(u = r\cos \theta\), and \(v = r\sin \theta\) and then 
                \(u^2 + v^2 = r^2\) and \(dudv = r dr d\theta\).
                <br>
                Thus, 
                \[
                \begin{align*}
                I^2 &= \int_{0} ^{2\pi}  \int_{0} ^\infty  e^{-r^2} r dr d\theta \\\\
                    &= \int_{0} ^{2\pi} 1d\theta \int_{0} ^\infty  e^{-r^2} r dr. \\\\\
                \end{align*}
                \]
                Here, let \(w = -r^2\) and \(dw = -2rdr\). Then
                \[
                \begin{align*}
                I^2 &= (2\pi) (-\frac{1}{2}\int_{0} ^{-\infty}  e^{w} dw) \\\\\
                    &= (2\pi) (\frac{1}{2}\int_{-\infty} ^0  e^{w} dw)\\\\\
                    &= (2\pi)(\frac{1}{2}\cdot 1)
                \end{align*}
                \]
                Therefore, 
                \[
                I = \sqrt{\pi}.
                \]
            </div>
            <br>
            Here, we use the parametrized Gaussian function (1).
            <br>
            Let \(u = \frac{x-b}{c}\), which implies \(x = cu + b \) and \(dx = cdu\). Then
            \[
            \begin{align*}
            \int_{-\infty} ^\infty  a e^{-\frac{(x-b)^2}{2c^2}} dx 
                        &= \int_{-\infty} ^\infty  a e^{-\frac{(cu)^2}{2c^2}}cdu \\\\
                        &= ac \int_{-\infty} ^\infty e^{-\frac{u^2}{2}}du
            \end{align*} 
            \]
            Using the Gaussian integral (3), 
            \[
            \begin{align*}
            \int_{-\infty} ^\infty  a e^{-\frac{(x-b)^2}{2c^2}} dx
                         &=  ac \int_{-\infty} ^\infty e^{-\frac{u^2}{2}}du \\\\
                         &= ac\sqrt{2\pi}.
            \end{align*}
            \]
        </blockquote>

        <h1>Normal(Gaussian) Distribution</h1>
        <blockquote>
            A random variable \(X\) has a <strong>normal(Gaussian) distribution</strong> with parameters \(\mu \) and 
            \(\sigma^2\) if its p.d.f. is given by 
            \[
            f(x) = \frac{1}{\sigma \sqrt{2\pi}}e^{\frac{-(x - \mu)^2}{2\sigma^2}}  \qquad x \in \mathbb{R}
            \]
            where \(\mu \in \mathbb{R}\) and \(\sigma > 0\) and it is denoted as \(X \sim N(\mu,  \sigma^2)\).
            <br><br>
            <div style="text-align: center;">
                <img src="normal_dist_pdf.png" alt="Normal Distribution PDF" style="width:600px;">
            </div>
            <br>
            The above figure shows the normal p.d.f. curve. \(\mu\) is the center, and \(\sigma\) is the distance from 
            the center to the inflection point of the curve. The one reason the normal distribution is widely used in statistics 
            and machine learning as well, the parameters capture its mean and variance, which are essential properties of the distribution. 
            \[
            \mathbb{E }[X] = \mu \qquad \text{Ver }(X) = \sigma^2.
            \]
            So, in the normal distribution, measures of central tendency(mean, median, and mode) are the same. 
            <br><br>
            The simplest and useful normal distribution is the one with zero mean and unit variance. We call it 
            the <strong>standard normal distribution</strong> denoted by
            \[
            Z \sim N(0, 1).
            \]
            Any normally distributed random variable can be "transformed" to a standard normal random variable. If 
            \(X \sim N(\mu,  \sigma^2)\), then 
            \[
            Z = \frac{X- \mu}{\sigma}\sim N(0, 1).
            \]
            This process is called <strong>standardization</strong>. This is just a special case of the linear transformation:
            \[
            Y = aX + b \sim N(a\mu+b, a^2\sigma^2)
            \]
            Here, \(a = \frac{1}{\sigma} \) and \(b = \frac{-\mu}{\sigma}\) and then you 
            get \(\mathbb{E}[\frac{1}{\sigma}x -\frac{\mu}{\sigma}] = 0 \) and \(\text{Var }(\frac{1}{\sigma}x -\frac{\mu}{\sigma}) =1\).
            <br><br>
            The p.d.f. of \(Z\) is denoted by 
            \[
            \phi (z) = f(x) = \frac{1}{\sqrt{2\pi}}e^{\frac{-z^2}{2}} \qquad z \in \mathbb{R}
            \]
            and c.d.f. of \(Z\) is denoted by 
            \[
            \Phi (z) = P(Z \leq z) = \int_{-\infty} ^z \phi (u)du.
            \]
        </blockquote>

        <h1>Central Limit Theorem</h1>
        <blockquote>
            Now, we consider a set of random variables \(\{X_1, X_2, \cdots, X_n\}\). In <strong>random sampling</strong>, we assume that each 
            \(X_i\) has the same distribution and is independent of each other. We call it <strong>independent and identically distributed(i.i.d.)</strong>. 
            In this case, a single <strong>sample mean</strong> \(\bar{X} = \frac{1}{n}\sum_{i=1} ^n X_i\) is not exactly equivalent to the <strong>population mean</strong> \(\mu\) 
            because there are many different sampling possibilities. However, the "average" of sample means is equal to \(\mu\):
            \[
            \begin{align*}
            \mathbb{E}[\bar{X}] &=  \mathbb{E}[\frac{1}{n}(X_1 + X_2 + \cdots +X_n)] \\\\
                                &= \frac{1}{n}[\mathbb{E}[X_1] + \mathbb{E}[X_2] + \cdots + \mathbb{E}[X_n]] \\\\
                                &= \frac{n\mu}{n} \\\\
                                &= \mu
            \end{align*}
            \]
            Thus, the sample mean is an <strong>unbiased estimator</strong> of the population mean \(\mu\). On the other hand, 
            the <strong>sample variance</strong> \(s^2\) is a <strong>biased estimator</strong> of the population variance \(\sigma^2\):
            \[
            \begin{align*}
            \text{Var }(\bar{X}) &= \text{Var }[\frac{1}{n}(X_1 + X_2 + \cdots +X_n)] \\\\
                                 &= \frac{1}{n^2}[\text{Var }(X_1) + \text{Var }(X_2) + \cdots + \text{Var }(X_n)] \\\\
                                 &= \frac{n \sigma^2}{n^2} \\\\
                                 &= \frac{\sigma^2}{n}
            \end{align*}
            \]
            To make the estimator unbiased, we adjust the denominator:  
            \[
            s^2 = \frac{1}{n-1} \sum_{i=1}^n (X_i - \bar{X})^2.
            \]
            <br>
            At this point, you might wonder how we can make inferences about the unknown distribution of random variables in practice. 
            The <strong>Central Limit Theorem (CLT)</strong> provides a result: regardless of the underlying distribution of a population, 
            the distribution of the sample mean approaches a normal distribution as the sample size \(n\) becomes large enough. This remarkable 
            property is one of the reasons the normal distribution is widely used in statistics and forms the foundation of many statistical methods 
            and theories.
            
            <div class="theorem">
                <span class="theorem-title">Theorem 1: Central Limit Thorem</span>
                Let \(X_1, X_2, \cdots, X_n\) be i.i.d. random variables with mean \(\mu\) and variance \(\sigma^2\). Then the distribution of 
                \[
                Z = \frac{\bar{X} - \mu}{\frac{\sigma}{\sqrt{n}}}
                \] 
                converges to the standard normal distribution as the smaple size \(n \to \infty\).
                <br>
                Note: The SD of \(\bar{X}\) is \(\sqrt{\frac{\sigma^2}{n}} = \frac{\sigma}{\sqrt{n}}\).
            </div>
            If the smaple size \(n\) is large enough,
            \[
            \bar{X} \sim N(\mu, \frac{\sigma^2}{n})
            \]
            and also, 
            \[
            \sum_{i =1} ^n  X_i \sim N(n\mu , n\sigma^2).
            \]
            <br>
            Note: For example, highly skewed distributions such as the exponential distribution requires 
            around \(n \geq 100\) for applying CLT. 
        </blockquote>
        <a href="../index.html">Back to Home </a>
    <br> <a href="probability.html">Back to Probability </a>   
</body>
</html>