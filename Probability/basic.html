<!DOCTYPE html>
<html>
    <head> 
        <title>Basic Ideas</title>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <link rel="stylesheet" href="../styles.css">
        <meta name="viewport" content="width=device-width, initial-scale=1">
    </head>
    <body> 
        <h1>Probability</h1>
        <blockquote>
            The collection of every possible outcome of an experiment is called a <strong>sample space</strong> denoted as \(S\).
            It can be discrete or continuous. An <strong>event</strong> \(A\) is a set of outcomes of an experiment, or a 
            subset of the sample space \(A \subseteq S\). 
            <br><br>
            The <strong>probability</strong> of \(A\) denoted as \(P(A)\) satisfies the following axioms:
            <ol>
                <li>\( 0 \leq P(A) \leq 1\)</li>
                <li>\(P(S) = 1\) and \(P(\emptyset) = 0\) </li>
                <li>If the events \(A\) and \(B\) are <strong>mutually exclusive</strong>, \(P(A \cup B) = P(A) + P(B)\)</li>
            </ol>
            and can be calculated as: 
            \[
            P(A) = \frac{\text{number of outcomes in } A}{\text{number of outocomes in } S}.
            \]
            <br>
            Using event algebra, the follwing basic facts can be derived: 
            <ol>
                <li><strong>Complement</strong>: \(P(\bar{A}) = 1 - P(A)\)</li>
                Note: \(1 = P(S) = P(A \cup \bar{A}) = P(A) + P(\bar{A})\)
                <br><br>
                <li><strong>Addition</strong>:\(P(A \cup B) = P(A) + P(B) - P(A \cap B)\)</li>
                Note: If \(A\) and \(B\) are mutually exclusive , \(P(A \cap B) = P(\emptyset) = 0\).
                <br><br>
                <li><strong>Inclusion</strong>: If \(B \subset A\), then \(A \cap B = B\), and so \(P(A) - P(B) = P(A \cap \bar{B})\)</li>
                <br>
                <li><strong>de Morgan's laws</strong>: <br>
                    \(P(\overline{A \cup B}) = P( \bar{A} \cap \bar{B})\) <br>
                    \(P(\overline{A \cap B}) = P( \bar{A} \cup \bar{B})\)</li>
            </ol>
            There are some counting rules to find all possible outcomes in \(S\) quickly.
            <ol>
                <li><strong>Multiplication principle</strong>:</li>
                If an experiment consists of a sequence of operations \(O_1, O_2, \cdots, O_r\) 
                with \(n_1, n_2, \cdots, n_r\) outcomes respectively, then the total mumber of possible outcomes 
                is the product \(n_1n_2\cdots n_r\).
                <br><br>
                <li><strong>Permutation</strong>:</li>
                Sampling without replacement where the oder of sampling matters. 
                \[
                {}_n P_r = n(n-1)(n-2)\cdots (n-1+1) = \frac{n!}{(n-r)!}
                \] 
                (Multiplying the number of possible outcomes at each step until \(r\)th one is made.)
                <br><br>
                <li><strong>Combinations</strong>:</li>
                Sampling without replacement where the oder of sampling does not matter. 
                \[
                {}_n C_r = \binom{n}{r} = \frac{n!}{r!(n-r)!} = \frac{ {}_n P_r }{r!}
                \] 
                Note: This is the <strong>Binomial coefficient</strong> for the binomial expansion. 
                <br><br>
                We can generalize the binomial coefficient in \(k \geq 2\) groups so that there are \(r_i\) in 
                group \(i \quad (1 \leq i \leq k)\) and \(r_1 + r_2 + \cdots + r_k = n\):
                \[
                \frac{n!}{r_1 ! r_2 ! \cdots r_k !}
                \]
                This is called the <strong>multinomial coefficient</strong>. 
                <br>For example, if we deal 52 cards evenly among 4 players, giving each player 13 cards, the total 
                number of different hands within the 4 players is caluculated by  
                \[
                \frac{52!}{13!13!13!13!}
                \]  
            </ol>
        </blockquote>

        <h1>Conditional Probability</h1>
        <blockquote>
            The <strong>conditional probability</strong> of an event \(A\) given an event \(B\) is defined as 
            \[
            P(A \mid B) = \frac{P(A \cap B)}{P(B)}. \tag{1}
            \]
            which means 
            \[
            P(A \cap B) = P(A \mid B) P(B).  \tag{2}
            \]
            Also, from Equation (1),
            \[
            P(B \mid A) = \frac{P(B \cap A)}{P(A)} = \frac{P(A \cap B)}{P(A)}.
            \]
            Using Equation (2), 
            \[
            P(B \mid A) = \frac{P(A \mid B) P(B)}{P(A)}.
            \]
            <br>
            If \(A\) and \(B\)  are mutually <strong>independent</strong>, 
            \[
            P(A \mid B) = P(A), \text{ and } P(B \mid A) = P(B).
            \]
            Thus 
            \[
            P(A \cap B) = P(A) P(B).
            \]
            In addition, in this case, 
            \[
            \begin{align*}
            P(A)P(\bar{B}) &= P(A)[1 - P(B)] \\\\
                           &= P(A) - P(A)P(B) \\\\
                           &= P(A) - P(A \cap B) \\\\
                           &= P(A \cap \bar{B})
            \end{align*}
            \]
            Similarly, \( P(\bar{A})P(B) = P(\bar{A} \cap B)\) and \( P(\bar{A})P(\bar{B}) = P(\bar{A} \cap \bar{B})\).
        </blockquote>

        <h1>Law of Total Probability</h1>
        <blockquote>
            <div class="theorem">
                <span class="theorem-title">Theorem 1: Law of Total Probability</span> 
                Let the sample space \(S\) be decomposed into \(k\) mutually exclusive events \(B_1, B_2, \cdots B_k\).
                Then for any event \(A\), 
                \[
                \begin{align*}
                P(A) &= P(A \mid B_1)P(B_1) + P(A \mid B_2)P(B_2) + \cdots +  P(A \mid B_k)P(B_k) \\\\
                     &= \sum_{i =1} ^k P(A \mid B_i)P(B_i)
                \end{align*}
                \]
            </div>
            <div class="proof">
                <span class="proof-title">Proof:</span>
                \begin{align*}
                P(A) &= P[(A \cap B_1) \cup (A \cap B_2) \cup \cdots \cup (A \cap B_k)] \\\\
                     &= P(A \cap B_1) +  P(A \cap B_2) + \cdots +  P(A \cap B_k) \\\\
                     &= P(A \mid B_1)P(B_1) + P(A \mid B_2)P(B_2) + \cdots +  P(A \mid B_k)P(B_k) \\\\
                     &= \sum_{i =1} ^k P(A \mid B_i)P(B_i)
                \end{align*}
            </div>
        </blockquote>

        <h1>Bayes' Theorem</h1>
        <blockquote>
            <div class="theorem">
                <span class="theorem-title">Theorem 2: Bayes' Theorem</span>
                Let mutually exclusive events \(B_1, B_2, \cdots B_k\) be partitions of the sample space \(S\) with the 
                condition that \(P(B_i) > 0\) for \(i = 1, 2, \cdots, k\). Then for \(j = 1, 2, \cdots, k\),
                \[
                \begin{align*}
                P(B_j \mid A) &= \frac{P(A \mid B_j)P(B_j)}{\sum_{i=1}^k P(A \mid B_i)P(B_i)} \\\\
                              &= \frac{P(B_j \cap A)}{P(A)}
                \end{align*}
                \]
            </div>
        </blockquote>


        <a href="../index.html">Back to Home </a>
        <br> <a href="probability.html">Back to Probability </a>   
    </body>
</html>