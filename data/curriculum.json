{
  "meta": {
    "version": "1.2.1",
    "lastUpdated": "2026-02-02",
    "author": "Yusuke Yokota",
    "description": "Centralized curriculum data for MATH-CS COMPASS - single source of truth for section pages and compass map"
  },
  "sectionColors": {
    "I": "#1565c0", "II": "#2e7d32", "III": "#00838f", "IV": "#6a1b9a", "V": "#ef6c00"
  },
  "sections": {
    "I": {
      "id": "linear-algebra",
      "title": "Linear Algebra to Algebraic Foundations",
      "shortTitle": "Algebraic Structures",
      "description": "Master the transition from computational Linear Algebra to the rigorous world of Abstract Algebra. This section covers linear systems, vector spaces, and eigenvalues, before generalizing these concepts into the fundamental algebraic structures of Groups, Rings, and Fields.",
      "tagline": "From Vector Spaces to Universal Algebraic Structures",
      "icon": "fa-vector-square",
      "indexUrl": "Mathematics/Linear_algebra/linear_algebra.html",
      "baseUrl": "Mathematics/Linear_algebra/",
      "parts": [
        {
          "id": "linalg-1",
          "part": 1,
          "title": "Linear Equations",
          "url": "linear_equations.html",
          "icon": "Ax=b",
          "keywords": [
            "System of linear equations", "Reduced row echelon form", "Linear combination", "Span", "Matrix equation",
            "Homogeneous & nonhomogeneous system", "Linear independence & dependence", "Parametric vector form"
          ],
          "badges": [],
          "prereqs": [],
          "mapCoords": {
            "q": -1, "r": 1
          },
          "topicGroup": "core-linalg",
          "tesseraMessage": "Where every journey begins! Ax = b is the foundation of so much in CS.",

          "headline": "Linear Equations: The Foundation of Linear Algebra",
          "description": "Learn about systems of linear equations, row reduction, linear combinations, span, and linear independence.",
          "abstract": "Systems of linear equations form the foundation of linear algebra and computational mathematics. We develop the systematic approach of row reduction to reduced row echelon form, introduce linear combinations and span, and distinguish between homogeneous and nonhomogeneous systems. Linear independence characterizes when solutions are unique.",
          "datePublished": "2024-10-21",
          "dateModified": "2026-01-31",

          "teaches": [
            "Systems of linear equations in matrix form Ax = b",
            "Row reduction and reduced row echelon form (RREF)",
            "Linear combinations of vectors",
            "Span of a set of vectors",
            "Matrix equations and their solutions",
            "Homogeneous vs nonhomogeneous systems",
            "Linear independence and dependence",
            "Parametric vector form of solutions"
          ],

          "competencyRequired": [
            "Basic algebra", "Vector notation"
          ],

          "sections": [
            {"id": "intro", "name": "Introduction"},
            {"id": "systems", "name": "Systems of Linear Equations"},
            {"id": "rref", "name": "Row Reduction and RREF"},
            {"id": "combination", "name": "Linear Combinations"},
            {"id": "span", "name": "Span"},
            {"id": "independence", "name": "Linear Independence"}
          ],

          "definitions": [
            {
              "term": "Linear Combination",
              "definition": "c₁v₁ + c₂v₂ + ... + cₙvₙ where cᵢ are scalars"
            },
            {
              "term": "Span",
              "definition": "Span{v₁,...,vₙ} = set of all linear combinations of v₁,...,vₙ"
            },
            {
              "term": "Linear Independence",
              "definition": "Vectors v₁,...,vₙ are independent if c₁v₁+...+cₙvₙ=0 implies all cᵢ=0"
            },
            {
              "term": "RREF",
              "definition": "Reduced Row Echelon Form: leading 1s, zeros above/below, staircase pattern"
            },
            {
              "term": "Homogeneous System",
              "definition": "Ax = 0, always has trivial solution x = 0"
            }
          ],

          "about": [
            {"name": "Linear Equations", "description": "Foundation of linear algebra"},
            {"name": "Row Reduction", "description": "Systematic method for solving linear systems"},
            {"name": "Linear Independence", "description": "Characterizes unique solutions"},
            {"name": "Span", "description": "Set of all reachable vectors via linear combinations"}
          ]
        },
        {
          "id": "linalg-2",
          "part": 2,
          "title": "Linear Transformation",
          "url": "linear_transformation.html",
          "icon": "T",
          "keywords": [
            "Linear transformation",
            "Linearity",
            "Onto",
            "One-to-one",
            "Matrix multiplication"
          ],
          "badges": [
            "interactive"
          ],
          "prereqs": [
            "linalg-1"
          ],
          "mapCoords": {
            "q": -2,
            "r": 2
          },
          "topicGroup": "core-linalg",
          "tesseraMessage": "Transformations are how we move and reshape data. Graphics, ML, everywhere!",

          "headline": "Linear Transformations: Functions That Preserve Structure",
          "description": "Learn about linear transformations, their properties, and the connection to matrix multiplication.",
          "abstract": "Linear transformations are functions between vector spaces that preserve addition and scalar multiplication. Every linear transformation can be represented by a matrix, and matrix multiplication computes the transformation. Properties like onto (surjective) and one-to-one (injective) characterize when transformations are invertible.",
          "datePublished": "2024-10-21",
          "dateModified": "2026-01-31",

          "teaches": [
            "Definition of linear transformation",
            "Linearity properties: T(u+v) = T(u)+T(v), T(cu) = cT(u)",
            "Standard matrix of a linear transformation",
            "Matrix multiplication as composition",
            "Onto (surjective) transformations",
            "One-to-one (injective) transformations",
            "Geometric interpretations: rotation, reflection, scaling"
          ],

          "competencyRequired": [
            "Linear equations and solutions",
            "Matrix-vector multiplication",
            "Function concepts"
          ],

          "sections": [
            {"id": "intro", "name": "Introduction"},
            {"id": "definition", "name": "Linear Transformations"},
            {"id": "matrix", "name": "Matrix of a Transformation"},
            {"id": "onto", "name": "Onto Transformations"},
            {"id": "onetoone", "name": "One-to-One Transformations"},
            {"id": "demo", "name": "Interactive Demo"}
          ],

          "definitions": [
            {
              "term": "Linear Transformation",
              "definition": "T: V → W satisfying T(u+v) = T(u)+T(v) and T(cu) = cT(u)"
            },
            {
              "term": "Standard Matrix",
              "definition": "A = [T(e₁) T(e₂) ... T(eₙ)] where eᵢ are standard basis vectors"
            },
            {
              "term": "Onto",
              "definition": "T is onto if for every w ∈ W, there exists v ∈ V with T(v) = w"
            },
            {
              "term": "One-to-One",
              "definition": "T is one-to-one if T(u) = T(v) implies u = v"
            }
          ],

          "about": [
            {"name": "Linear Transformations", "description": "Structure-preserving maps between vector spaces"},
            {"name": "Matrix Representation", "description": "Every linear transformation has a matrix"},
            {"name": "Injectivity and Surjectivity", "description": "Properties determining invertibility"}
          ]
        },
        {
          "id": "linalg-3",
          "part": 3,
          "title": "Matrix Algebra",
          "url": "matrix_algebra.html",
          "icon": "A",
          "keywords": [
            "Diagonal matrix",
            "Identity matrix",
            "Transpose of a matrix",
            "Invertible matrix",
            "Singular matrix",
            "Elementary matrix",
            "Partitioned Matrix",
            "LU Factorization"
          ],
          "badges": [],
          "prereqs": [
            "linalg-1"
          ],
          "mapCoords": {
            "q": -2,
            "r": 1
          },
          "topicGroup": "core-linalg",
          "tesseraMessage": "Matrices are the language computers speak. Master this well!",

          "headline": "Matrix Algebra: Operations and Factorizations",
          "description": "Learn about matrix operations, special matrices, invertibility, and LU factorization.",
          "abstract": "Matrix algebra provides the computational framework for linear algebra. We study special matrices (diagonal, identity, elementary), matrix operations (transpose, inverse), and the important LU factorization that decomposes a matrix into lower and upper triangular factors for efficient equation solving.",
          "datePublished": "2024-10-21",
          "dateModified": "2026-01-31",

          "teaches": [
            "Matrix addition and scalar multiplication",
            "Matrix multiplication and its properties",
            "Diagonal and identity matrices",
            "Matrix transpose and its properties",
            "Invertible vs singular matrices",
            "Computing matrix inverses",
            "Elementary matrices and row operations",
            "LU factorization"
          ],

          "competencyRequired": [
            "Linear equations",
            "Basic matrix notation"
          ],

          "sections": [
            {"id": "intro", "name": "Introduction"},
            {"id": "operations", "name": "Matrix Operations"},
            {"id": "special", "name": "Special Matrices"},
            {"id": "transpose", "name": "Transpose"},
            {"id": "inverse", "name": "Invertible Matrices"},
            {"id": "elementary", "name": "Elementary Matrices"},
            {"id": "lu", "name": "LU Factorization"}
          ],

          "definitions": [
            {
              "term": "Identity Matrix",
              "definition": "I with 1s on diagonal, 0s elsewhere; AI = IA = A"
            },
            {
              "term": "Invertible Matrix",
              "definition": "A is invertible if there exists A⁻¹ with AA⁻¹ = A⁻¹A = I"
            },
            {
              "term": "Transpose",
              "definition": "(Aᵀ)ᵢⱼ = Aⱼᵢ, rows become columns"
            },
            {
              "term": "LU Factorization",
              "definition": "A = LU where L is lower triangular, U is upper triangular"
            },
            {
              "term": "Elementary Matrix",
              "definition": "Matrix obtained by applying one row operation to identity"
            }
          ],

          "about": [
            {"name": "Matrix Operations", "description": "Arithmetic with matrices"},
            {"name": "Matrix Inverse", "description": "The multiplicative inverse of a matrix"},
            {"name": "LU Factorization", "description": "Decomposition for efficient linear system solving"}
          ]
        },
        {
          "id": "linalg-4",
          "part": 4,
          "title": "Determinants",
          "url": "determinants.html",
          "icon": "|A|",
          "keywords": [
            "Determinant",
            "Cofactor expansion",
            "Cramer's rule",
            "Adjugate",
            "Inverse formula",
            "Invertible matrix"
          ],
          "badges": [],
          "prereqs": [
            "linalg-3"
          ],
          "mapCoords": {
            "q": -3,
            "r": 2
          },
          "topicGroup": "core-linalg",
          "tesseraMessage": "Determinants tell you if a matrix is invertible — one number, so much meaning.",

          "headline": "Determinants: A Single Number Encoding Matrix Properties",
          "description": "Learn about determinants, cofactor expansion, Cramer's rule, and the adjugate matrix.",
          "abstract": "The determinant is a scalar value that encodes fundamental properties of a square matrix. A matrix is invertible if and only if its determinant is nonzero. We develop cofactor expansion for computation, derive the adjugate formula for inverses, and apply Cramer's rule to solve linear systems.",
          "datePublished": "2024-10-21",
          "dateModified": "2026-01-31",

          "teaches": [
            "Determinant definition for 2×2 and 3×3 matrices",
            "Cofactor expansion along any row or column",
            "Properties: det(AB) = det(A)det(B)",
            "Determinant and invertibility",
            "Adjugate (classical adjoint) matrix",
            "Inverse formula: A⁻¹ = adj(A)/det(A)",
            "Cramer's rule for solving Ax = b"
          ],

          "competencyRequired": [
            "Matrix operations",
            "Invertible matrices"
          ],

          "sections": [
            {"id": "intro", "name": "Introduction"},
            {"id": "definition", "name": "Determinant Definition"},
            {"id": "cofactor", "name": "Cofactor Expansion"},
            {"id": "properties", "name": "Properties of Determinants"},
            {"id": "adjugate", "name": "Adjugate Matrix"},
            {"id": "cramer", "name": "Cramer's Rule"}
          ],

          "definitions": [
            {
              "term": "Determinant",
              "definition": "det(A) or |A|, scalar encoding invertibility and volume scaling"
            },
            {
              "term": "Cofactor",
              "definition": "Cᵢⱼ = (-1)^(i+j) Mᵢⱼ where Mᵢⱼ is the (i,j) minor"
            },
            {
              "term": "Adjugate",
              "definition": "adj(A) = Cᵀ, transpose of cofactor matrix"
            },
            {
              "term": "Cramer's Rule",
              "definition": "xᵢ = det(Aᵢ)/det(A) where Aᵢ has column i replaced by b"
            }
          ],

          "about": [
            {"name": "Determinant", "description": "Scalar characterizing matrix invertibility"},
            {"name": "Cofactor Expansion", "description": "Recursive method for computing determinants"},
            {"name": "Cramer's Rule", "description": "Explicit formula for linear system solutions"}
          ]
        },
        {
          "id": "linalg-5",
          "part": 5,
          "title": "Vector Spaces",
          "url": "vectorspaces.html",
          "icon": "V",
          "keywords": [
            "Vector space",
            "Subspace",
            "Null space",
            "Kernel",
            "Column space",
            "Row space",
            "Basis",
            "Spanning set",
            "Coordinate systems",
            "Dimension",
            "Rank"
          ],
          "badges": [],
          "prereqs": [
            "linalg-2"
          ],
          "mapCoords": {
            "q": -3,
            "r": 3
          },
          "topicGroup": "core-linalg",
          "tesseraMessage": "Vector spaces are where linear algebra becomes truly abstract and powerful.",

          "headline": "Vector Spaces: The Abstract Framework of Linear Algebra",
          "description": "Learn about vector spaces, subspaces, basis, dimension, and the fundamental subspaces of a matrix.",
          "abstract": "Vector spaces abstract the properties of ℝⁿ to general settings. Subspaces are vector spaces within vector spaces. Every matrix has four fundamental subspaces: column space, row space, null space, and left null space. Basis and dimension provide the tools to characterize these spaces precisely.",
          "datePublished": "2024-10-21",
          "dateModified": "2026-01-31",

          "teaches": [
            "Vector space axioms",
            "Subspace definition and subspace test",
            "Null space (kernel) of a matrix",
            "Column space (range) of a matrix",
            "Row space of a matrix",
            "Basis and spanning sets",
            "Dimension of a vector space",
            "Rank of a matrix",
            "Rank-nullity theorem"
          ],

          "competencyRequired": [
            "Linear transformations",
            "Linear independence",
            "Span"
          ],

          "sections": [
            {"id": "intro", "name": "Introduction"},
            {"id": "axioms", "name": "Vector Space Axioms"},
            {"id": "subspace", "name": "Subspaces"},
            {"id": "null", "name": "Null Space"},
            {"id": "column", "name": "Column Space"},
            {"id": "basis", "name": "Basis and Dimension"},
            {"id": "rank", "name": "Rank"}
          ],

          "definitions": [
            {
              "term": "Vector Space",
              "definition": "Set V with addition and scalar multiplication satisfying 10 axioms"
            },
            {
              "term": "Null Space",
              "definition": "Nul(A) = {x : Ax = 0}, kernel of linear transformation"
            },
            {
              "term": "Column Space",
              "definition": "Col(A) = span of columns of A = range of T(x) = Ax"
            },
            {
              "term": "Basis",
              "definition": "Linearly independent set that spans the space"
            },
            {
              "term": "Rank",
              "definition": "rank(A) = dim(Col(A)) = number of pivot columns"
            }
          ],

          "about": [
            {"name": "Vector Spaces", "description": "Abstract algebraic structure for linear algebra"},
            {"name": "Fundamental Subspaces", "description": "Four subspaces characterizing a matrix"},
            {"name": "Basis and Dimension", "description": "Minimal spanning sets and size of spaces"}
          ]
        },
        {
          "id": "linalg-6",
          "part": 6,
          "title": "Eigenvalues & Eigenvectors",
          "url": "eigenvectors.html",
          "icon": "λ",
          "keywords": [
            "Eigenvalues",
            "Eigenvectors",
            "Eigenspace",
            "Characteristic equation",
            "Similarity",
            "Diagonalization",
            "Complex eigenvalues & eigenvectors"
          ],
          "badges": [],
          "prereqs": [
            "linalg-5"
          ],
          "mapCoords": {
            "q": -4,
            "r": 4
          },
          "topicGroup": "core-linalg",
          "tesseraMessage": "Eigenvalues appear everywhere — from Google's PageRank to quantum mechanics!",

          "headline": "Eigenvalues and Eigenvectors: The Natural Modes of a Matrix",
          "description": "Learn about eigenvalues, eigenvectors, the characteristic equation, and matrix diagonalization.",
          "abstract": "Eigenvalues and eigenvectors reveal the natural modes of a linear transformation—directions that only get scaled. The characteristic equation det(A - λI) = 0 finds eigenvalues. Diagonalization A = PDP⁻¹ simplifies matrix powers and exponentials, with applications from vibration analysis to machine learning.",
          "datePublished": "2024-10-21",
          "dateModified": "2026-01-31",

          "teaches": [
            "Eigenvalue and eigenvector definition: Av = λv",
            "Eigenspace as null space of (A - λI)",
            "Characteristic polynomial and equation",
            "Finding eigenvalues and eigenvectors",
            "Similar matrices and their properties",
            "Diagonalization: A = PDP⁻¹",
            "Conditions for diagonalizability",
            "Complex eigenvalues and eigenvectors"
          ],

          "competencyRequired": [
            "Determinants",
            "Null space",
            "Linear independence"
          ],

          "sections": [
            {"id": "intro", "name": "Introduction"},
            {"id": "definition", "name": "Eigenvalues and Eigenvectors"},
            {"id": "characteristic", "name": "Characteristic Equation"},
            {"id": "eigenspace", "name": "Eigenspaces"},
            {"id": "similarity", "name": "Similar Matrices"},
            {"id": "diagonalization", "name": "Diagonalization"},
            {"id": "complex", "name": "Complex Eigenvalues"}
          ],

          "definitions": [
            {
              "term": "Eigenvalue",
              "definition": "Scalar λ such that Av = λv for some nonzero v"
            },
            {
              "term": "Eigenvector",
              "definition": "Nonzero vector v satisfying Av = λv"
            },
            {
              "term": "Characteristic Equation",
              "definition": "det(A - λI) = 0, polynomial equation for eigenvalues"
            },
            {
              "term": "Diagonalization",
              "definition": "A = PDP⁻¹ where D is diagonal of eigenvalues, P has eigenvectors"
            },
            {
              "term": "Similar Matrices",
              "definition": "A ~ B if B = P⁻¹AP for some invertible P"
            }
          ],

          "about": [
            {"name": "Eigenvalues", "description": "Scaling factors of eigenvectors under transformation"},
            {"name": "Diagonalization", "description": "Decomposition simplifying matrix computations"},
            {"name": "Characteristic Polynomial", "description": "Polynomial whose roots are eigenvalues"}
          ]
        },
        {
          "id": "linalg-7",
          "part": 7,
          "title": "Orthogonality",
          "url": "orthogonality.html",
          "icon": "⊥",
          "keywords": [
            "Inner product",
            "Euclidean norm",
            "Orthogonality",
            "Orthogonal complement",
            "Orthogonal & Orthonormal set",
            "Orthogonal projection",
            "Orthogonal matrix",
            "Gram-Schmidt algorithm",
            "QR factorization"
          ],
          "badges": [
            "interactive"
          ],
          "prereqs": [
            "linalg-5"
          ],
          "mapCoords": {
            "q": -3,
            "r": 4
          },
          "topicGroup": "core-linalg",
          "tesseraMessage": "Orthogonality keeps things clean and independent. QR is essential!",

          "headline": "Orthogonality: Perpendicularity in Higher Dimensions",
          "description": "Learn about inner products, orthogonal projections, Gram-Schmidt orthogonalization, and QR factorization.",
          "abstract": "Orthogonality generalizes perpendicularity to higher dimensions using inner products. Orthogonal projections find the closest point in a subspace. The Gram-Schmidt algorithm converts any basis to an orthonormal basis, leading to QR factorization—essential for numerical stability in solving least squares problems.",
          "datePublished": "2024-10-21",
          "dateModified": "2026-01-31",

          "teaches": [
            "Inner product and its properties",
            "Euclidean norm from inner product",
            "Orthogonal vectors: u·v = 0",
            "Orthogonal complement of a subspace",
            "Orthogonal and orthonormal sets",
            "Orthogonal projection onto subspaces",
            "Orthogonal matrices: QᵀQ = I",
            "Gram-Schmidt orthogonalization",
            "QR factorization"
          ],

          "competencyRequired": [
            "Vector spaces and subspaces",
            "Basis and dimension"
          ],

          "sections": [
            {"id": "intro", "name": "Introduction"},
            {"id": "inner", "name": "Inner Product"},
            {"id": "orthogonal", "name": "Orthogonal Vectors"},
            {"id": "complement", "name": "Orthogonal Complement"},
            {"id": "projection", "name": "Orthogonal Projection"},
            {"id": "gramschmidt", "name": "Gram-Schmidt Algorithm"},
            {"id": "qr", "name": "QR Factorization"},
            {"id": "demo", "name": "Interactive Demo"}
          ],

          "definitions": [
            {
              "term": "Inner Product",
              "definition": "u·v = uᵀv = Σuᵢvᵢ, generalized dot product"
            },
            {
              "term": "Orthogonal Complement",
              "definition": "W⊥ = {v : v·w = 0 for all w ∈ W}"
            },
            {
              "term": "Orthonormal Set",
              "definition": "Orthogonal set with all vectors having unit length"
            },
            {
              "term": "Orthogonal Matrix",
              "definition": "Q with QᵀQ = QQᵀ = I, preserves lengths and angles"
            },
            {
              "term": "QR Factorization",
              "definition": "A = QR where Q is orthogonal, R is upper triangular"
            }
          ],

          "about": [
            {"name": "Orthogonality", "description": "Perpendicularity generalized to vector spaces"},
            {"name": "Gram-Schmidt", "description": "Algorithm for orthonormalizing a basis"},
            {"name": "QR Factorization", "description": "Numerically stable matrix decomposition"}
          ]
        },
        {
          "id": "linalg-8",
          "part": 8,
          "title": "Least-Squares Problems",
          "url": "leastsquares.html",
          "icon": "min",
          "keywords": [
            "Least-squares solution",
            "Normal equation",
            "Least-squares error",
            "Linear regression",
            "Moore-Penrose pseudo-inverse"
          ],
          "badges": [
            "interactive"
          ],
          "prereqs": [
            "linalg-7"
          ],
          "mapCoords": {
            "q": -3,
            "r": 5
          },
          "topicGroup": "core-linalg",
          "tesseraMessage": "When exact solutions don't exist, least squares finds the best approximation.",

          "headline": "Least-Squares Problems: Best Approximations When Exact Solutions Don't Exist",
          "description": "Learn about least-squares solutions, normal equations, and the Moore-Penrose pseudo-inverse.",
          "abstract": "When Ax = b has no exact solution, the least-squares solution minimizes ||Ax - b||². The normal equations AᵀAx̂ = Aᵀb characterize this solution. Using QR factorization improves numerical stability. The Moore-Penrose pseudo-inverse generalizes matrix inverse to non-square and singular matrices.",
          "datePublished": "2024-10-21",
          "dateModified": "2026-01-31",

          "teaches": [
            "Overdetermined systems and no exact solution",
            "Least-squares problem formulation",
            "Normal equations derivation",
            "Geometric interpretation via projection",
            "Solving via QR factorization",
            "Least-squares error computation",
            "Linear regression as least squares",
            "Moore-Penrose pseudo-inverse"
          ],

          "competencyRequired": [
            "Orthogonal projection",
            "QR factorization",
            "Matrix transpose"
          ],

          "sections": [
            {"id": "intro", "name": "Introduction"},
            {"id": "problem", "name": "Least-Squares Problem"},
            {"id": "normal", "name": "Normal Equations"},
            {"id": "qr", "name": "QR Solution Method"},
            {"id": "regression", "name": "Linear Regression"},
            {"id": "pseudoinverse", "name": "Moore-Penrose Pseudo-inverse"},
            {"id": "demo", "name": "Interactive Demo"}
          ],

          "definitions": [
            {
              "term": "Least-Squares Solution",
              "definition": "x̂ minimizing ||Ax - b||², the closest approximation"
            },
            {
              "term": "Normal Equations",
              "definition": "AᵀAx̂ = Aᵀb, necessary condition for least-squares"
            },
            {
              "term": "Least-Squares Error",
              "definition": "||b - Ax̂||, residual of the approximation"
            },
            {
              "term": "Pseudo-inverse",
              "definition": "A⁺ = (AᵀA)⁻¹Aᵀ or via SVD: A⁺ = VΣ⁺Uᵀ"
            }
          ],

          "about": [
            {"name": "Least-Squares", "description": "Optimal approximation minimizing squared error"},
            {"name": "Normal Equations", "description": "Linear system characterizing least-squares solution"},
            {"name": "Pseudo-inverse", "description": "Generalized inverse for non-invertible matrices"}
          ]
        },
        {
          "id": "linalg-9",
          "part": 9,
          "title": "Symmetry",
          "url": "symmetry.html",
          "icon": "S",
          "keywords": [
            "Symmetric matrix",
            "Orthogonally diagonalizable matrix",
            "Spectrum",
            "Quadratic form",
            "Positive definite",
            "Positive semi-definite",
            "Singular value decomposition(SVD)",
            "Condition number",
            "Moore-Penrose pseudo-inverse"
          ],
          "badges": [
            "interactive"
          ],
          "prereqs": [
            "linalg-7"
          ],
          "mapCoords": {
            "q": -4,
            "r": 5
          },
          "topicGroup": "core-linalg",
          "tesseraMessage": "SVD is one of the most useful decompositions in all of applied mathematics!",

          "headline": "Symmetric Matrices and SVD: Structure and Decomposition",
          "description": "Learn about symmetric matrices, quadratic forms, positive definiteness, and singular value decomposition.",
          "abstract": "Symmetric matrices have real eigenvalues and orthogonal eigenvectors, enabling orthogonal diagonalization A = QΛQᵀ. Quadratic forms xᵀAx are classified by eigenvalue signs into positive definite, semi-definite, or indefinite. The SVD A = UΣVᵀ extends these ideas to any matrix, revealing its fundamental structure.",
          "datePublished": "2024-10-21",
          "dateModified": "2026-01-31",

          "teaches": [
            "Symmetric matrices: A = Aᵀ",
            "Spectral theorem for symmetric matrices",
            "Orthogonal diagonalization: A = QΛQᵀ",
            "Quadratic forms and their classification",
            "Positive definite and semi-definite matrices",
            "Principal axis theorem",
            "Singular Value Decomposition (SVD)",
            "Condition number and numerical stability"
          ],

          "competencyRequired": [
            "Eigenvalues and eigenvectors",
            "Orthogonal matrices",
            "Diagonalization"
          ],

          "sections": [
            {"id": "intro", "name": "Introduction"},
            {"id": "symmetric", "name": "Symmetric Matrices"},
            {"id": "spectral", "name": "Spectral Theorem"},
            {"id": "quadratic", "name": "Quadratic Forms"},
            {"id": "definite", "name": "Positive Definiteness"},
            {"id": "svd", "name": "Singular Value Decomposition"},
            {"id": "condition", "name": "Condition Number"},
            {"id": "demo", "name": "Interactive Demo"}
          ],

          "definitions": [
            {
              "term": "Symmetric Matrix",
              "definition": "A = Aᵀ, equal to its own transpose"
            },
            {
              "term": "Spectral Theorem",
              "definition": "Symmetric A = QΛQᵀ with orthogonal Q, diagonal Λ"
            },
            {
              "term": "Positive Definite",
              "definition": "xᵀAx > 0 for all nonzero x, equivalently all eigenvalues > 0"
            },
            {
              "term": "SVD",
              "definition": "A = UΣVᵀ where U, V orthogonal, Σ diagonal with singular values"
            },
            {
              "term": "Condition Number",
              "definition": "κ(A) = σ_max/σ_min, measures sensitivity to perturbations"
            }
          ],

          "about": [
            {"name": "Symmetric Matrices", "description": "Matrices with special spectral properties"},
            {"name": "Positive Definiteness", "description": "Classification of quadratic forms"},
            {"name": "SVD", "description": "Universal matrix decomposition revealing structure"}
          ]
        },
        {
          "id": "linalg-10",
          "part": 10,
          "title": "Trace and Norms",
          "url": "trace.html",
          "icon": "Tr",
          "keywords": [
            "Trace",
            "Frobenius norm",
            "Nuclear norm",
            "Induced norm",
            "Spectral norm",
            "p-norm",
            "Manhattan norm",
            "Maximum norm",
            "Normalization",
            "Regularization",
            "Metric space",
            "Normed vector space",
            "Inner product space",
            "Euclidean space"
          ],
          "badges": [],
          "prereqs": [
            "linalg-9"
          ],
          "mapCoords": {
            "q": -5,
            "r": 5
          },
          "topicGroup": "core-linalg",
          "tesseraMessage": "Norms measure size, and different norms give different perspectives.",

          "headline": "Trace and Norms: Measuring Matrices and Vectors",
          "description": "Learn about the trace operator, various matrix and vector norms, and their applications in ML.",
          "abstract": "The trace is the sum of diagonal elements with useful properties like cyclic invariance. Norms measure vector and matrix sizes: p-norms for vectors, Frobenius and spectral norms for matrices. These concepts define metric spaces and normed vector spaces, fundamental for optimization and regularization in machine learning.",
          "datePublished": "2024-10-21",
          "dateModified": "2026-01-31",

          "teaches": [
            "Trace definition and properties",
            "Cyclic property: tr(ABC) = tr(CAB)",
            "Trace and eigenvalues: tr(A) = Σλᵢ",
            "Vector p-norms: L1, L2, L∞",
            "Frobenius norm for matrices",
            "Spectral (operator) norm",
            "Nuclear (trace) norm",
            "Normed vector spaces and metric spaces"
          ],

          "competencyRequired": [
            "Eigenvalues",
            "SVD",
            "Inner products"
          ],

          "sections": [
            {"id": "intro", "name": "Introduction"},
            {"id": "trace", "name": "Trace"},
            {"id": "vector-norms", "name": "Vector Norms"},
            {"id": "matrix-norms", "name": "Matrix Norms"},
            {"id": "spaces", "name": "Normed Vector Spaces"},
            {"id": "applications", "name": "Applications in ML"}
          ],

          "definitions": [
            {
              "term": "Trace",
              "definition": "tr(A) = Σaᵢᵢ = Σλᵢ, sum of diagonal elements or eigenvalues"
            },
            {
              "term": "p-Norm",
              "definition": "||x||_p = (Σ|xᵢ|^p)^(1/p) for vectors"
            },
            {
              "term": "Frobenius Norm",
              "definition": "||A||_F = √(Σaᵢⱼ²) = √tr(AᵀA)"
            },
            {
              "term": "Spectral Norm",
              "definition": "||A||_2 = σ_max(A), largest singular value"
            },
            {
              "term": "Nuclear Norm",
              "definition": "||A||_* = Σσᵢ, sum of singular values"
            }
          ],

          "about": [
            {"name": "Trace", "description": "Sum of diagonal elements with cyclic invariance"},
            {"name": "Matrix Norms", "description": "Measures of matrix size for different purposes"},
            {"name": "Regularization", "description": "Using norms to control model complexity"}
          ]
        },
        {
          "id": "linalg-11",
          "part": 11,
          "title": "Kronecker Product & Tensor",
          "url": "kronecker.html",
          "icon": "⊗",
          "keywords": [
            "Vectorization",
            "Kronecker product",
            "Tensor",
            "Tensor Product"
          ],
          "badges": [],
          "prereqs": [
            "linalg-10"
          ],
          "mapCoords": {
            "q": -5,
            "r": 4
          },
          "topicGroup": "core-linalg",
          "tesseraMessage": "Tensors power modern deep learning. This is where things get multidimensional!",

          "headline": "Kronecker Product and Tensors: Higher-Dimensional Linear Algebra",
          "description": "Learn about vectorization, the Kronecker product, and tensors as generalizations of matrices.",
          "abstract": "The Kronecker product A⊗B builds larger matrices from smaller ones with elegant algebraic properties. Vectorization reshapes matrices into vectors for unified treatment. Tensors generalize matrices to higher dimensions—scalars, vectors, matrices, and beyond—essential for representing images, videos, and deep learning architectures.",
          "datePublished": "2025-01-27",
          "dateModified": "2026-01-31",

          "teaches": [
            "Vectorization: stacking columns into a vector",
            "Row-major vs column-major ordering",
            "Kronecker product definition and computation",
            "Mixed-product property: (A⊗B)(C⊗D) = AC⊗BD",
            "Kronecker product eigenvalues",
            "Tensors as higher-order arrays",
            "Tensor rank and tensor product spaces",
            "Applications in deep learning"
          ],

          "competencyRequired": [
            "Matrix operations",
            "Eigenvalues",
            "Trace and determinant"
          ],

          "sections": [
            {"id": "vec", "name": "Vectorization"},
            {"id": "k_pro", "name": "Kronecker Product"},
            {"id": "tensor", "name": "Tensor"}
          ],

          "definitions": [
            {
              "term": "Vectorization",
              "definition": "vec(A) stacks columns of A into a single column vector"
            },
            {
              "term": "Kronecker Product",
              "definition": "A⊗B replaces each aᵢⱼ with aᵢⱼB, resulting in (mp)×(nq) matrix"
            },
            {
              "term": "Tensor",
              "definition": "Element of tensor product V₁⊗...⊗Vᵣ, generalizing matrices to r dimensions"
            },
            {
              "term": "Mixed-Product Property",
              "definition": "(A⊗B)(C⊗D) = (AC)⊗(BD)"
            }
          ],

          "about": [
            {"name": "Kronecker Product", "description": "Matrix operation creating block structures"},
            {"name": "Tensors", "description": "Higher-dimensional generalizations of matrices"},
            {"name": "Deep Learning", "description": "Tensors as fundamental data structures in neural networks"}
          ]
        },
        {
          "id": "linalg-12",
          "part": 12,
          "title": "Woodbury Matrix Identity",
          "url": "woodbury.html",
          "icon": "W",
          "keywords": [
            "Woodbury matrix identity",
            "Sherman-Morrison formula"
          ],
          "badges": [],
          "prereqs": [
            "linalg-3"
          ],
          "mapCoords": {
            "q": -3,
            "r": 1
          },
          "topicGroup": "core-linalg",
          "tesseraMessage": "A clever identity that makes updating inverses efficient. Very practical!",

          "headline": "Woodbury Matrix Identity: Efficient Inverse Updates",
          "description": "Learn about the Woodbury matrix identity and Sherman-Morrison formula for efficient matrix inverse updates.",
          "abstract": "The Woodbury matrix identity provides an efficient way to compute the inverse of a matrix after a low-rank update. When you already have A⁻¹ and need (A + UBV)⁻¹, Woodbury avoids recomputing from scratch. The Sherman-Morrison formula is the rank-1 special case, essential for online learning and recursive algorithms.",
          "datePublished": "2024-01-17",
          "dateModified": "2026-01-31",

          "teaches": [
            "Woodbury matrix identity statement and proof",
            "Sherman-Morrison formula as rank-1 special case",
            "Efficient computation of updated inverses",
            "Applications in recursive least squares",
            "Online learning algorithms",
            "Low-rank matrix updates"
          ],

          "competencyRequired": [
            "Matrix inverse",
            "Matrix multiplication",
            "Rank of a matrix"
          ],

          "sections": [
            {"id": "wmi", "name": "Woodbury Matrix Identity"}
          ],

          "definitions": [
            {
              "term": "Woodbury Identity",
              "definition": "(A + UBV)⁻¹ = A⁻¹ - A⁻¹U(B⁻¹ + VA⁻¹U)⁻¹VA⁻¹"
            },
            {
              "term": "Sherman-Morrison Formula",
              "definition": "(A + uvᵀ)⁻¹ = A⁻¹ - A⁻¹uvᵀA⁻¹/(1 + vᵀA⁻¹u) for rank-1 updates"
            },
            {
              "term": "Low-Rank Update",
              "definition": "Modification A → A + UBV where U, V have few columns"
            }
          ],

          "about": [
            {"name": "Woodbury Identity", "description": "Formula for inverse of low-rank updated matrix"},
            {"name": "Sherman-Morrison", "description": "Rank-1 special case of Woodbury"},
            {"name": "Computational Efficiency", "description": "Avoiding full matrix inversion for updates"}
          ]
        },
        {
          "id": "linalg-13",
          "part": 13,
          "title": "Stochastic Matrix",
          "url": "stochastic.html",
          "icon": "P",
          "keywords": [
            "Stochastic matrix",
            "Column-stochastic matrix",
            "Row-stochastic matrix",
            "Probability vector",
            "Markov chain",
            "Steady-state vector",
            "Spectral radius",
            "Doubly stochastic matrix"
          ],
          "badges": [],
          "prereqs": [
            "linalg-6"
          ],
          "mapCoords": {
            "q": -4,
            "r": 3
          },
          "topicGroup": "core-linalg",
          "tesseraMessage": "Markov chains are everywhere — from web search to speech recognition.",

          "headline": "Stochastic Matrices: Linear Algebra Meets Probability",
          "description": "Learn about stochastic matrices, Markov chains, steady-state distributions, and doubly stochastic matrices.",
          "abstract": "Stochastic matrices have nonnegative entries with rows or columns summing to 1, representing transition probabilities in Markov chains. The steady-state vector is an eigenvector for eigenvalue 1. Google's PageRank algorithm is a famous application. Doubly stochastic matrices preserve both row and column sums.",
          "datePublished": "2025-02-11",
          "dateModified": "2026-01-31",

          "teaches": [
            "Row-stochastic and column-stochastic matrices",
            "Probability vectors",
            "Markov chains and state transitions",
            "Steady-state (stationary) distribution",
            "Eigenvalue 1 and the Perron-Frobenius theorem",
            "Spectral radius of stochastic matrices",
            "Doubly stochastic matrices",
            "PageRank as Markov chain"
          ],

          "competencyRequired": [
            "Eigenvalues and eigenvectors",
            "Matrix powers",
            "Basic probability"
          ],

          "sections": [
            {"id": "intro", "name": "Introduction"},
            {"id": "definition", "name": "Stochastic Matrices"},
            {"id": "markov", "name": "Markov Chains"},
            {"id": "steady", "name": "Steady-State Vector"},
            {"id": "doubly", "name": "Doubly Stochastic Matrices"}
          ],

          "definitions": [
            {
              "term": "Row-Stochastic Matrix",
              "definition": "Nonnegative matrix with each row summing to 1"
            },
            {
              "term": "Probability Vector",
              "definition": "Vector with nonnegative entries summing to 1"
            },
            {
              "term": "Steady-State Vector",
              "definition": "Probability vector π with Pπ = π (eigenvector for λ = 1)"
            },
            {
              "term": "Doubly Stochastic",
              "definition": "Both rows and columns sum to 1"
            },
            {
              "term": "Spectral Radius",
              "definition": "ρ(A) = max|λᵢ|, for stochastic matrices ρ(P) = 1"
            }
          ],

          "about": [
            {"name": "Stochastic Matrices", "description": "Matrices representing probability transitions"},
            {"name": "Markov Chains", "description": "Probabilistic state transition systems"},
            {"name": "PageRank", "description": "Web search ranking via Markov chains"}
          ]
        },
        {
          "id": "linalg-14",
          "part": 14,
          "title": "Graph Laplacians and Spectral Methods",
          "url": "graph_laplacian.html",
          "icon": "L",
          "keywords": [
            "Graph Laplacian",
            "Dirichlet energy",
            "Normalized Laplacian",
            "Fiedler vector",
            "Algebraic connectivity",
            "Cheeger's inequality",
            "Graph Fourier Transform (GFT)",
            "Heat diffusion on graphs"
          ],
          "badges": [],
          "prereqs": [
            "linalg-6",
            "disc-1"
          ],
          "mapCoords": {
            "q": -1,
            "r": 2
          },
          "topicGroup": "core-linalg",
          "tesseraMessage": "Graph Laplacians connect linear algebra to network analysis. Beautiful!",

          "headline": "Graph Laplacians: Spectral Analysis of Networks",
          "description": "Learn about graph Laplacians, spectral clustering, the Fiedler vector, and the Graph Fourier Transform.",
          "abstract": "The graph Laplacian L = D - A encodes network structure in a matrix. Its eigenvalues reveal connectivity: the second-smallest eigenvalue (algebraic connectivity) and its eigenvector (Fiedler vector) enable spectral clustering. The Graph Fourier Transform extends signal processing to graphs, powering graph neural networks.",
          "datePublished": "2025-11-05",
          "dateModified": "2026-01-31",

          "teaches": [
            "Graph Laplacian L = D - A construction",
            "Laplacian eigenvalues and graph connectivity",
            "Dirichlet energy and smoothness on graphs",
            "Normalized Laplacian for varying degrees",
            "Fiedler vector for graph partitioning",
            "Algebraic connectivity and Cheeger's inequality",
            "Graph Fourier Transform",
            "Heat diffusion and random walks on graphs"
          ],

          "competencyRequired": [
            "Eigenvalues and eigenvectors",
            "Graph theory basics",
            "Quadratic forms"
          ],

          "sections": [
            {"id": "intro", "name": "Introduction"},
            {"id": "laplacian", "name": "Graph Laplacian"},
            {"id": "eigenvalues", "name": "Laplacian Eigenvalues"},
            {"id": "fiedler", "name": "Fiedler Vector and Partitioning"},
            {"id": "normalized", "name": "Normalized Laplacian"},
            {"id": "gft", "name": "Graph Fourier Transform"}
          ],

          "definitions": [
            {
              "term": "Graph Laplacian",
              "definition": "L = D - A where D is degree matrix, A is adjacency matrix"
            },
            {
              "term": "Algebraic Connectivity",
              "definition": "λ₂(L), second-smallest eigenvalue measuring graph connectivity"
            },
            {
              "term": "Fiedler Vector",
              "definition": "Eigenvector for λ₂, used for spectral graph partitioning"
            },
            {
              "term": "Normalized Laplacian",
              "definition": "L_norm = D⁻¹/²LD⁻¹/² = I - D⁻¹/²AD⁻¹/²"
            },
            {
              "term": "Graph Fourier Transform",
              "definition": "f̂ = Uᵀf where U contains Laplacian eigenvectors"
            }
          ],

          "about": [
            {"name": "Graph Laplacian", "description": "Matrix encoding graph structure for spectral analysis"},
            {"name": "Spectral Clustering", "description": "Clustering using Laplacian eigenvectors"},
            {"name": "Graph Signal Processing", "description": "Extending Fourier analysis to graphs"}
          ]
        },
        {
          "id": "linalg-15",
          "part": 15,
          "title": "Intro to Abstract Algebra",
          "url": "intro_groups.html",
          "icon": "G",
          "keywords": [
            "Abstract Algebra",
            "Groups",
            "Binary operation",
            "Closure",
            "Abelian",
            "Non-Abelian",
            "General linear group, GL(n, F)",
            "Modular arithmetic",
            "Units Modulo n, U(n)",
            "Additive vs Multiplicative groups",
            "Order of a group",
            "Order of an element",
            "Subgroups",
            "One-step subgroup test",
            "Cyclic",
            "Generator",
            "Center of a group",
            "Centralizer"
          ],
          "badges": [],
          "prereqs": [
            "linalg-10"
          ],
          "mapCoords": {
            "q": -6,
            "r": 5
          },
          "topicGroup": "abstract-algebra",
          "tesseraMessage": "Welcome to abstract algebra! Groups reveal the hidden symmetry in mathematics.",

          "headline": "Introduction to Groups: The Algebra of Symmetry",
          "description": "Learn about groups, subgroups, cyclic groups, and the foundations of abstract algebra.",
          "abstract": "Groups formalize the concept of symmetry with a binary operation satisfying closure, associativity, identity, and inverse. Examples include integers under addition, invertible matrices GL(n,F), and units modulo n. Subgroups, cyclic groups, and generators provide structure. The center and centralizer measure commutativity.",
          "datePublished": "2025-12-03",
          "dateModified": "2026-01-31",

          "teaches": [
            "Group axioms: closure, associativity, identity, inverse",
            "Abelian vs non-Abelian groups",
            "General linear group GL(n, F)",
            "Modular arithmetic and U(n)",
            "Order of a group and order of elements",
            "Subgroups and the one-step subgroup test",
            "Cyclic groups and generators",
            "Center and centralizer of a group"
          ],

          "competencyRequired": [
            "Matrix operations",
            "Modular arithmetic basics",
            "Set theory"
          ],

          "sections": [
            {"id": "intro", "name": "Introduction to Abstract Algebra"},
            {"id": "groups", "name": "Groups"},
            {"id": "examples", "name": "Examples of Groups"},
            {"id": "order", "name": "Order"},
            {"id": "subgroups", "name": "Subgroups"},
            {"id": "cyclic", "name": "Cyclic Groups"},
            {"id": "center", "name": "Center and Centralizer"}
          ],

          "definitions": [
            {
              "term": "Group",
              "definition": "Set G with operation * satisfying closure, associativity, identity, and inverse"
            },
            {
              "term": "Abelian Group",
              "definition": "Group where a * b = b * a for all elements (commutative)"
            },
            {
              "term": "Order of Element",
              "definition": "|a| = smallest positive n with aⁿ = e"
            },
            {
              "term": "Cyclic Group",
              "definition": "Group G = ⟨a⟩ generated by a single element"
            },
            {
              "term": "Center",
              "definition": "Z(G) = {a ∈ G : ax = xa for all x ∈ G}"
            }
          ],

          "about": [
            {"name": "Group Theory", "description": "Study of algebraic structures with one operation"},
            {"name": "Symmetry", "description": "Groups formalize symmetry operations"},
            {"name": "Cyclic Groups", "description": "Groups generated by a single element"}
          ]
        },
        {
          "id": "linalg-16",
          "part": 16,
          "title": "More Finite Groups",
          "url": "cyclic_groups.html",
          "icon": "Sₙ",
          "keywords": [
            "Group of Integers Modulo n",
            "Fundamental Theorem of Cyclic Groups",
            "Euler Phi Function",
            "Permutation Groups",
            "Symmetric Groups",
            "Cycle Notation",
            "Products of Disjoint Cycles",
            "Product of 2-Cycles",
            "The Parity Theorem",
            "Alternating Groups"
          ],
          "badges": [],
          "prereqs": [
            "linalg-15"
          ],
          "mapCoords": {
            "q": -7,
            "r": 5
          },
          "topicGroup": "abstract-algebra",
          "tesseraMessage": "Permutations and cyclic structures — the building blocks of cryptography.",

          "headline": "Finite Groups: Cyclic Groups and Permutations",
          "description": "Learn about cyclic groups, the Euler phi function, permutation groups, and alternating groups.",
          "abstract": "Cyclic groups ℤₙ are completely understood via the Fundamental Theorem of Cyclic Groups. The Euler phi function counts generators. Permutation groups Sₙ consist of all bijections on n elements, written in cycle notation. The parity theorem distinguishes even and odd permutations, defining the alternating group Aₙ.",
          "datePublished": "2025-12-18",
          "dateModified": "2026-01-31",

          "teaches": [
            "Structure of ℤₙ under addition",
            "Fundamental Theorem of Cyclic Groups",
            "Euler phi function φ(n)",
            "Permutation groups and symmetric group Sₙ",
            "Cycle notation for permutations",
            "Disjoint cycle decomposition",
            "Transpositions (2-cycles)",
            "Even and odd permutations",
            "Alternating group Aₙ"
          ],

          "competencyRequired": [
            "Group basics",
            "Modular arithmetic",
            "Function composition"
          ],

          "sections": [
            {"id": "cyclic", "name": "Cyclic Groups ℤₙ"},
            {"id": "fundamental", "name": "Fundamental Theorem of Cyclic Groups"},
            {"id": "euler", "name": "Euler Phi Function"},
            {"id": "permutation", "name": "Permutation Groups"},
            {"id": "cycle", "name": "Cycle Notation"},
            {"id": "parity", "name": "Parity and Alternating Groups"}
          ],

          "definitions": [
            {
              "term": "Euler Phi Function",
              "definition": "φ(n) = number of integers 1 ≤ k ≤ n with gcd(k,n) = 1"
            },
            {
              "term": "Symmetric Group",
              "definition": "Sₙ = group of all permutations on {1,2,...,n}"
            },
            {
              "term": "Cycle Notation",
              "definition": "(a₁ a₂ ... aₖ) maps aᵢ → aᵢ₊₁ cyclically"
            },
            {
              "term": "Transposition",
              "definition": "2-cycle (a b) that swaps two elements"
            },
            {
              "term": "Alternating Group",
              "definition": "Aₙ = even permutations in Sₙ, |Aₙ| = n!/2"
            }
          ],

          "about": [
            {"name": "Cyclic Groups", "description": "Groups generated by one element"},
            {"name": "Permutation Groups", "description": "Groups of bijections"},
            {"name": "Alternating Groups", "description": "Even permutations forming a subgroup"}
          ]
        },
        {
          "id": "linalg-17",
          "part": 17,
          "title": "Structural Group Theory",
          "url": "group_isomorphism.html",
          "icon": "≅",
          "keywords": [
            "Cosets",
            "Lagrange's Theorem",
            "Fermat's Little Theorem",
            "Pohlig-Hellman algorithm",
            "Normal Subgroups",
            "Factor Groups",
            "Group Homomorphisms",
            "Kernel",
            "Group Isomorphisms",
            "Group Automorphisms",
            "Inner Automorphisms",
            "Cayley's Theorem",
            "First Isomorphism Theorem"
          ],
          "badges": [],
          "prereqs": [
            "linalg-16"
          ],
          "mapCoords": {
            "q": -8,
            "r": 5
          },
          "topicGroup": "abstract-algebra",
          "tesseraMessage": "Isomorphisms show when different-looking structures are secretly the same.",

          "headline": "Structural Group Theory: Homomorphisms and Isomorphisms",
          "description": "Learn about cosets, Lagrange's theorem, normal subgroups, factor groups, and the isomorphism theorems.",
          "abstract": "Cosets partition groups, leading to Lagrange's theorem: subgroup order divides group order. Normal subgroups enable factor groups G/N. Homomorphisms preserve structure; their kernels are normal subgroups. The First Isomorphism Theorem connects these: G/ker(φ) ≅ im(φ). Cayley's theorem embeds every group in a symmetric group.",
          "datePublished": "2025-12-26",
          "dateModified": "2026-01-31",

          "teaches": [
            "Left and right cosets",
            "Lagrange's theorem and its consequences",
            "Fermat's Little Theorem from group theory",
            "Normal subgroups and their characterization",
            "Factor (quotient) groups G/N",
            "Group homomorphisms and their properties",
            "Kernel and image of homomorphisms",
            "First Isomorphism Theorem",
            "Cayley's theorem"
          ],

          "competencyRequired": [
            "Subgroups",
            "Cyclic groups",
            "Permutations"
          ],

          "sections": [
            {"id": "cosets", "name": "Cosets"},
            {"id": "lagrange", "name": "Lagrange's Theorem"},
            {"id": "normal", "name": "Normal Subgroups"},
            {"id": "factor", "name": "Factor Groups"},
            {"id": "homomorphism", "name": "Group Homomorphisms"},
            {"id": "isomorphism", "name": "Isomorphism Theorems"},
            {"id": "cayley", "name": "Cayley's Theorem"}
          ],

          "definitions": [
            {
              "term": "Coset",
              "definition": "aH = {ah : h ∈ H}, left coset of H by a"
            },
            {
              "term": "Lagrange's Theorem",
              "definition": "|H| divides |G| for subgroup H of finite group G"
            },
            {
              "term": "Normal Subgroup",
              "definition": "N ⊴ G if gNg⁻¹ = N for all g ∈ G"
            },
            {
              "term": "Kernel",
              "definition": "ker(φ) = {g ∈ G : φ(g) = e}, always a normal subgroup"
            },
            {
              "term": "First Isomorphism Theorem",
              "definition": "G/ker(φ) ≅ im(φ) for homomorphism φ"
            }
          ],

          "about": [
            {"name": "Lagrange's Theorem", "description": "Subgroup orders divide group order"},
            {"name": "Normal Subgroups", "description": "Subgroups allowing quotient construction"},
            {"name": "Isomorphism Theorems", "description": "Fundamental structure theorems for groups"}
          ]
        },
        {
          "id": "linalg-18",
          "part": 18,
          "title": "Classification of Finite Abelian Groups",
          "url": "group_classification.html",
          "icon": "⊕",
          "keywords": [
            "External Direct Products",
            "Internal Direct Products",
            "Fundamental Theorem of Finite Abelian Groups"
          ],
          "badges": [],
          "prereqs": [
            "linalg-17"
          ],
          "mapCoords": {
            "q": -7,
            "r": 4
          },
          "topicGroup": "abstract-algebra",
          "tesseraMessage": "Every finite abelian group can be broken down — a beautiful classification!",

          "headline": "Classification of Finite Abelian Groups",
          "description": "Learn about direct products and the Fundamental Theorem of Finite Abelian Groups.",
          "abstract": "The Fundamental Theorem of Finite Abelian Groups provides a complete classification: every finite abelian group decomposes uniquely (up to order) as a direct product of cyclic groups of prime power order. External and internal direct products provide equivalent viewpoints. This theorem is foundational for cryptography and coding theory.",
          "datePublished": "2025-12-30",
          "dateModified": "2026-01-31",

          "teaches": [
            "External direct products G × H",
            "Properties of direct products",
            "Internal direct products",
            "When G ≅ H × K internally",
            "Fundamental Theorem of Finite Abelian Groups",
            "Primary decomposition",
            "Invariant factor decomposition"
          ],

          "competencyRequired": [
            "Cyclic groups",
            "Group isomorphisms",
            "Prime factorization"
          ],

          "sections": [
            {"id": "external", "name": "External Direct Products"},
            {"id": "internal", "name": "Internal Direct Products"},
            {"id": "fundamental", "name": "Fundamental Theorem of Finite Abelian Groups"}
          ],

          "definitions": [
            {
              "term": "External Direct Product",
              "definition": "G × H = {(g,h) : g ∈ G, h ∈ H} with componentwise operation"
            },
            {
              "term": "Internal Direct Product",
              "definition": "G = HK where H ∩ K = {e} and elements commute"
            },
            {
              "term": "Fundamental Theorem",
              "definition": "Every finite abelian group ≅ ℤ_{p₁^{k₁}} × ... × ℤ_{pₙ^{kₙ}}"
            }
          ],

          "about": [
            {"name": "Direct Products", "description": "Building groups from smaller groups"},
            {"name": "Classification Theorem", "description": "Complete description of finite abelian groups"},
            {"name": "Prime Power Decomposition", "description": "Breaking groups into cyclic prime power pieces"}
          ]
        },
        {
          "id": "linalg-19",
          "part": 19,
          "title": "The Architecture of Rings and Fields",
          "url": "intro_rings.html",
          "icon": "R",
          "keywords": [
            "Rings",
            "Unity",
            "Unit",
            "Subrings",
            "Subring Test",
            "Integral Domains",
            "Zero-Divisors",
            "Cancellation Law",
            "Fields",
            "Characteristic of a Ring"
          ],
          "badges": [],
          "prereqs": [
            "linalg-18"
          ],
          "mapCoords": {
            "q": -8,
            "r": 4
          },
          "topicGroup": "abstract-algebra",
          "tesseraMessage": "Rings and fields — the algebraic structures behind polynomials and numbers.",

          "headline": "Rings and Fields: Two Operations Working Together",
          "description": "Learn about rings, integral domains, fields, and the characteristic of a ring.",
          "abstract": "Rings have two operations (addition and multiplication) with addition forming an abelian group. Integral domains have no zero-divisors, enabling cancellation. Fields are integral domains where every nonzero element has a multiplicative inverse. The characteristic measures how addition relates to the integers.",
          "datePublished": "2026-01-10",
          "dateModified": "2026-01-31",

          "teaches": [
            "Ring axioms and examples",
            "Unity (multiplicative identity) and units",
            "Subrings and the subring test",
            "Zero-divisors and their problems",
            "Integral domains and cancellation",
            "Fields as division rings",
            "Characteristic of a ring",
            "Finite fields introduction"
          ],

          "competencyRequired": [
            "Group theory basics",
            "Modular arithmetic",
            "Polynomial arithmetic"
          ],

          "sections": [
            {"id": "intro", "name": "Introduction"},
            {"id": "rings", "name": "Rings"},
            {"id": "subrings", "name": "Subrings"},
            {"id": "domains", "name": "Integral Domains"},
            {"id": "fields", "name": "Fields"},
            {"id": "characteristic", "name": "Characteristic"}
          ],

          "definitions": [
            {
              "term": "Ring",
              "definition": "Set R with +, × where (R,+) is abelian group, × is associative, distributive"
            },
            {
              "term": "Zero-Divisor",
              "definition": "Nonzero a with ab = 0 for some nonzero b"
            },
            {
              "term": "Integral Domain",
              "definition": "Commutative ring with unity and no zero-divisors"
            },
            {
              "term": "Field",
              "definition": "Commutative ring where every nonzero element has multiplicative inverse"
            },
            {
              "term": "Characteristic",
              "definition": "char(R) = smallest positive n with n·1 = 0, or 0 if none exists"
            }
          ],

          "about": [
            {"name": "Ring Theory", "description": "Study of algebraic structures with two operations"},
            {"name": "Integral Domains", "description": "Rings without zero-divisors"},
            {"name": "Fields", "description": "Rings where division is always possible"}
          ]
        },
        {
          "id": "linalg-20",
          "part": 20,
          "title": "Ideals and Factor Rings",
          "url": "ideals.html",
          "icon": "R/A",
          "keywords": [
            "Ideals",
            "Ideal Test",
            "Principal Ideals",
            "Factor Rings",
            "Prime Ideals",
            "Maximal Ideals",
            "Ring Homomorphisms",
            "Fundamental Theorem of Ring Homomorphisms"
          ],
          "badges": [],
          "prereqs": [
            "linalg-19"
          ],
          "mapCoords": {
            "q": -7,
            "r": 3
          },
          "topicGroup": "abstract-algebra",
          "tesseraMessage": "Ideals are to rings what normal subgroups are to groups. Deep connections!",

          "headline": "Ideals and Factor Rings: Quotient Structures in Ring Theory",
          "description": "Learn about ideals, principal ideals, prime and maximal ideals, factor rings, and ring homomorphisms.",
          "abstract": "Ideals are subrings closed under multiplication by any ring element—the ring analogue of normal subgroups. Principal ideals are generated by single elements. Factor rings R/I generalize modular arithmetic. Prime ideals correspond to integral domain quotients; maximal ideals yield fields. The Fundamental Theorem connects homomorphisms to quotients.",
          "datePublished": "2026-01-13",
          "dateModified": "2026-01-31",

          "teaches": [
            "Ideal definition and ideal test",
            "Principal ideals ⟨a⟩",
            "Factor (quotient) rings R/I",
            "Prime ideals and their characterization",
            "Maximal ideals and field quotients",
            "Ring homomorphisms",
            "Kernel as an ideal",
            "Fundamental Theorem of Ring Homomorphisms"
          ],

          "competencyRequired": [
            "Rings and integral domains",
            "Normal subgroups and factor groups",
            "Modular arithmetic"
          ],

          "sections": [
            {"id": "ideals", "name": "Ideals"},
            {"id": "principal", "name": "Principal Ideals"},
            {"id": "factor", "name": "Factor Rings"},
            {"id": "prime", "name": "Prime Ideals"},
            {"id": "maximal", "name": "Maximal Ideals"},
            {"id": "homomorphism", "name": "Ring Homomorphisms"}
          ],

          "definitions": [
            {
              "term": "Ideal",
              "definition": "Subring A ⊆ R with ra ∈ A and ar ∈ A for all r ∈ R, a ∈ A"
            },
            {
              "term": "Principal Ideal",
              "definition": "⟨a⟩ = {ra : r ∈ R}, ideal generated by single element"
            },
            {
              "term": "Prime Ideal",
              "definition": "P with ab ∈ P implies a ∈ P or b ∈ P; R/P is integral domain"
            },
            {
              "term": "Maximal Ideal",
              "definition": "M with no ideal strictly between M and R; R/M is a field"
            },
            {
              "term": "Ring Homomorphism",
              "definition": "φ: R → S with φ(a+b) = φ(a)+φ(b) and φ(ab) = φ(a)φ(b)"
            }
          ],

          "about": [
            {"name": "Ideals", "description": "Special subrings enabling quotient construction"},
            {"name": "Factor Rings", "description": "Quotient structures in ring theory"},
            {"name": "Prime and Maximal Ideals", "description": "Ideals characterizing domain and field quotients"}
          ]
        },
        {
          "id": "linalg-21",
          "part": 21,
          "title": "Polynomial Rings",
          "url": "polynomial_rings.html",
          "icon": "R[x]",
          "keywords": [
            "Polynomial Rings",
            "Division Algorithm",
            "Irreducible Polynomials",
            "Eisenstein's Criterion",
            "Principal Ideal Domain",
            "Gauss's Lemma",
            "Unique Factorization",
            "AES Cryptography",
            "Post-Quantum Cryptography"
          ],
          "badges": [],
          "prereqs": [
            "linalg-20"
          ],
          "mapCoords": {
            "q": -8,
            "r": 3
          },
          "topicGroup": "abstract-algebra",
          "tesseraMessage": "Polynomial rings power modern cryptography — from AES to post-quantum!",

          "headline": "Polynomial Rings: Algebra Meets Cryptography",
          "description": "Learn about polynomial rings, irreducibility, the division algorithm, and applications to cryptography.",
          "abstract": "Polynomial rings R[x] extend coefficient rings with an indeterminate. Over fields, F[x] is a principal ideal domain with division algorithm. Irreducible polynomials are the 'primes' enabling unique factorization. Eisenstein's criterion tests irreducibility. These structures underpin AES encryption and post-quantum cryptographic schemes.",
          "datePublished": "2026-01-15",
          "dateModified": "2026-01-31",

          "teaches": [
            "Polynomial ring construction R[x]",
            "Division algorithm in F[x]",
            "F[x] as principal ideal domain",
            "Irreducible polynomials",
            "Eisenstein's criterion for irreducibility",
            "Gauss's Lemma and ℤ[x]",
            "Unique factorization in polynomial rings",
            "Applications to AES cryptography",
            "Polynomial rings in post-quantum crypto"
          ],

          "competencyRequired": [
            "Ideals and factor rings",
            "Principal ideal domains",
            "Modular arithmetic"
          ],

          "sections": [
            {"id": "intro", "name": "Introduction"},
            {"id": "construction", "name": "Polynomial Rings R[x]"},
            {"id": "division", "name": "Division Algorithm"},
            {"id": "irreducible", "name": "Irreducible Polynomials"},
            {"id": "eisenstein", "name": "Eisenstein's Criterion"},
            {"id": "factorization", "name": "Unique Factorization"},
            {"id": "crypto", "name": "Cryptographic Applications"}
          ],

          "definitions": [
            {
              "term": "Polynomial Ring",
              "definition": "R[x] = {aₙxⁿ + ... + a₁x + a₀ : aᵢ ∈ R}"
            },
            {
              "term": "Irreducible Polynomial",
              "definition": "p(x) that cannot be factored into lower-degree polynomials"
            },
            {
              "term": "Eisenstein's Criterion",
              "definition": "p|aᵢ for i<n, p∤aₙ, p²∤a₀ implies irreducible over ℚ"
            },
            {
              "term": "Gauss's Lemma",
              "definition": "Primitive polynomial irreducible over ℤ iff irreducible over ℚ"
            },
            {
              "term": "PID",
              "definition": "Principal Ideal Domain: every ideal is principal; F[x] is a PID"
            }
          ],

          "about": [
            {"name": "Polynomial Rings", "description": "Ring of polynomials over a base ring"},
            {"name": "Irreducibility", "description": "Polynomial analogue of primality"},
            {"name": "Cryptographic Applications", "description": "AES and post-quantum schemes use polynomial arithmetic"}
          ]
        },
        {
          "id": "linalg-22",
          "part": 22,
          "title": "Hierarchies of Integrity",
          "url": "integral_domains.html",
          "icon": "UFD",
          "keywords": [
            "Euclidean Domains",
            "Principal Ideal Domains",
            "Unique Factorization Domains",
            "Prime Elements",
            "Irreducible Elements",
            "Divisibility",
            "Norm",
            "Gauss's Lemma"
          ],
          "badges": [],
          "prereqs": [
            "linalg-21"
          ],
          "mapCoords": {
            "q": -7,
            "r": 2
          },
          "topicGroup": "abstract-algebra",
          "tesseraMessage": "Understanding where factorization holds — and where it breaks — is essential for designing robust computational systems.",

          "headline": "Hierarchies of Integrity: Euclidean Domains, PIDs, and UFDs",
          "description": "Deep dive into the hierarchy of integral domains, focusing on the uniqueness of factorization and its implications in computer science.",
          "abstract": "We classify integral domains into a nesting hierarchy: $\\text{ED} \\subset \\text{PID} \\subset \\text{UFD} \\subset \\text{Integral Domains}$. While primes and irreducibles are identical in PIDs, they diverge in general rings, leading to the collapse of unique factorization. Understanding these structures is vital for algorithms like the Euclidean Algorithm and cryptographic reliability.",
          "datePublished": "2026-02-03",
          "dateModified": "2026-02-03",

          "teaches": [
            "Distinction between irreducible and prime elements",
            "The hierarchy: ED ⊂ PID ⊂ UFD ⊂ Integral Domains",
            "The role of the Division Algorithm in defining Euclidean Domains",
            "Why every Principal Ideal Domain is a Unique Factorization Domain",
            "Multiplicative norms as factorization validators",
            "Gauss's Lemma and inherited integrity in ℤ[x]",
            "Computational significance in Extended Euclidean Algorithm and RSA"
          ],

          "competencyRequired": [
            "Rings and Integral Domains",
            "Polynomial Rings",
            "Ideal theory basics",
            "Modular arithmetic"
          ],

          "sections": [
            {"id": "intro", "name": "Introduction"},
            {"id": "irreducibles", "name": "Irreducibles vs. Primes"},
            {"id": "hierarchy", "name": "Hierarchy of Domains"},
            {"id": "application", "name": "Computational Significance"},
            {"id": "conclusion", "name": "Conclusion"}
          ],

          "definitions": [
            {
              "term": "Irreducible",
              "definition": "Non-unit a where a = bc implies b or c is a unit"
            },
            {
              "term": "Prime",
              "definition": "Non-unit a where a | bc implies a | b or a | c"
            },
            {
              "term": "Euclidean Domain",
              "definition": "Domain with a measure function d allowing for a division algorithm"
            },
            {
              "term": "Principal Ideal Domain",
              "definition": "Integral domain where every ideal is generated by a single element"
            },
            {
              "term": "Unique Factorization Domain",
              "definition": "Domain where every non-unit factors uniquely into irreducibles"
            }
          ],

          "about": [
            {"name": "Domain Hierarchies", "description": "Classification of rings based on their divisibility rules"},
            {"name": "Factorization Integrity", "description": "Study of when unique prime decomposition is guaranteed"},
            {"name": "Computational Algebra", "description": "Algorithms like GCD and inversion based on Euclidean structures"}
          ]
        },
        {
          "id": "linalg-23",
          "part": 23,
          "title": "Extension Fields",
          "url": "extension_field.html",
          "icon": "F(a)",
          "keywords": [
            "Extension Field",
            "Fundamental Theorem of Field Theory",
            "Splitting Field",
            "Simple Extension",
            "Irreducible Polynomial",
            "Minimal Polynomial",
            "Algebraic Element",
            "Field Isomorphism"
          ],
          "badges": [],
          "prereqs": [
            "linalg-22"
          ],
          "mapCoords": {
            "q": -8,
            "r": 2
          },
          "topicGroup": "abstract-algebra",
          "tesseraMessage": "When a field can't solve an equation, we build a bigger universe that can — this is the foundation of both cryptography and geometric deep learning.",

          "headline": "Extension Fields: Expanding Algebraic Universes",
          "description": "Learn how to construct larger fields that contain solutions to polynomial equations, the foundation for finite fields and algebraic geometry.",
          "abstract": "Extension fields expand a base field F to a larger field E containing roots of polynomials unsolvable in F. The Fundamental Theorem guarantees such extensions always exist. Splitting fields provide minimal environments where polynomials factor completely, while the isomorphism F(a) ≅ F[x]/⟨p(x)⟩ gives a concrete construction. This shared foundation branches into algebraic extensions (toward Lie groups) and finite fields (toward cryptography).",
          "datePublished": "2026-02-05",
          "dateModified": "2026-02-05",

          "teaches": [
            "Definition of extension fields E/F",
            "Fundamental Theorem of Field Theory: roots always exist in some extension",
            "Splitting fields as minimal complete environments",
            "Simple extensions F(a) and their structure",
            "Isomorphism theorem: F(a) ≅ F[x]/⟨p(x)⟩",
            "Elements of F(a) as polynomials of degree < n",
            "Uniqueness of splitting fields up to isomorphism",
            "The two paths: algebraic extensions vs finite fields"
          ],

          "competencyRequired": [
            "Polynomial rings F[x]",
            "Irreducible polynomials",
            "Factor rings and quotient structures",
            "Field basics from intro_rings"
          ],

          "sections": [
            {"id": "intro", "name": "Introduction"},
            {"id": "FTF", "name": "Fundamental Theorem of Field Theory"},
            {"id": "splitting", "name": "Splitting Fields"},
            {"id": "crossroads", "name": "Two Paths Forward"}
          ],

          "definitions": [
            {
              "term": "Extension Field",
              "definition": "E is an extension of F if F ⊆ E and F's operations are E's operations restricted to F"
            },
            {
              "term": "Splitting Field",
              "definition": "Minimal extension E where f(x) = a(x-a₁)(x-a₂)⋯(x-aₙ) with E = F(a₁,...,aₙ)"
            },
            {
              "term": "Simple Extension",
              "definition": "F(a) = smallest field containing F and element a"
            },
            {
              "term": "Fundamental Theorem",
              "definition": "For any nonconstant f(x) ∈ F[x], there exists an extension E where f(x) has a zero"
            }
          ],

          "about": [
            {"name": "Extension Fields", "description": "Enlarging fields to solve polynomial equations"},
            {"name": "Splitting Fields", "description": "Minimal environments for complete factorization"},
            {"name": "Simple Extensions", "description": "Adjoining a single algebraic element"},
            {"name": "Field Construction", "description": "Building new fields via quotient rings"}
          ]
        },
        {
          "id": "linalg-24",
          "part": 24,
          "title": "Geometry of Symmetry",
          "url": "dihedral_group.html",
          "icon": "Dₙ",
          "keywords": [
            "Dihedral Group",
            "Rotation",
            "Reflection",
            "SO(3)",
            "SE(3)",
            "Special Orthogonal Group",
            "Special Euclidean Group",
            "Lie Group",
            "Rigid Body Motion",
            "Manifold",
            "Degrees of Freedom"
          ],
          "badges": [
            "interactive"
          ],
          "prereqs": [
            "linalg-17",
            "linalg-23"
          ],
          "mapCoords": {
            "q": 8,
            "r": 1
          },
          "topicGroup": "abstract-algebra",
          "tesseraMessage": "From discrete clicks to continuous flow — this is where algebra meets geometry and robotics.",

          "headline": "Geometry of Symmetry: From Dihedral Groups to Lie Groups",
          "description": "Explore the visual geometry of symmetry groups, from discrete dihedral groups to continuous Lie groups SO(3) and SE(3).",
          "abstract": "Dihedral groups Dₙ capture discrete symmetries of regular polygons through rotations and reflections. Taking n → ∞ transitions us from discrete 'clicks' to continuous 'flows' — the realm of Lie groups. SO(3) describes pure rotations (3D manifold), while SE(3) combines rotation with translation for rigid body motion (6D manifold). This page bridges abstract algebra with differential geometry and robotics.",
          "datePublished": "2026-02-05",
          "dateModified": "2026-02-05",

          "teaches": [
            "Dihedral group Dₙ as symmetries of regular n-gons",
            "Generators and relations: ⟨r, s | rⁿ = 1, s² = 1, srs = r⁻¹⟩",
            "Non-commutativity: rs ≠ sr",
            "Transition from discrete to continuous groups",
            "SO(3) as the group of 3D rotations",
            "SE(3) as rigid body motion (rotation + translation)",
            "Degrees of freedom equal manifold dimension",
            "Preview of Lie algebras and calculus on manifolds"
          ],

          "competencyRequired": [
            "Group theory basics",
            "Matrix representations",
            "Determinants",
            "Basic geometric intuition"
          ],

          "sections": [
            {"id": "intro", "name": "Introduction"},
            {"id": "Dihedral", "name": "Dihedral Groups"},
            {"id": "SOSE", "name": "Special Orthogonal Group & Special Euclidean Group"},
            {"id": "ahead", "name": "Toward Manifolds and Analysis"}
          ],

          "definitions": [
            {
              "term": "Dihedral Group",
              "definition": "Dₙ = ⟨r, s | rⁿ = 1, s² = 1, srs = r⁻¹⟩, symmetries of regular n-gon, order 2n"
            },
            {
              "term": "SO(3)",
              "definition": "Special Orthogonal Group: 3×3 orthogonal matrices with det = 1; all 3D rotations"
            },
            {
              "term": "SE(3)",
              "definition": "Special Euclidean Group: rigid body motions combining SO(3) rotation with ℝ³ translation"
            },
            {
              "term": "Lie Group",
              "definition": "A group that is also a smooth manifold, where group operations are differentiable"
            },
            {
              "term": "Degrees of Freedom",
              "definition": "Number of independent parameters; equals dimension of the manifold"
            }
          ],

          "about": [
            {"name": "Dihedral Groups", "description": "Discrete symmetries of polygons"},
            {"name": "Lie Groups", "description": "Continuous groups with smooth structure"},
            {"name": "SO(3)", "description": "Group of 3D rotations, 3-dimensional manifold"},
            {"name": "SE(3)", "description": "Rigid body motion group, 6-dimensional manifold"},
            {"name": "Robotics Applications", "description": "Configuration spaces and motion planning"}
          ]
        }
      ],
      "reservedSlots": [
        {
          "q": -9,
          "r": 2
        },
        {
          "q": -7,
          "r": 0
        }
      ]
    },
    "II": {
      "id": "calculus",
      "title": "Calculus to Optimization & Analysis",
      "shortTitle": "Calculus",
      "description": "Master the transition from computational calculus to rigorous mathematical analysis. This section bridges multivariable calculus and optimization with the analytical blueprints of ML, including Taylor's theorem, convergence theory in metric spaces, and the measure-theoretic foundations of Lebesgue integration.",
      "tagline": "The Mathematics of Change and Convergence",
      "icon": "fa-chart-line",
      "indexUrl": "Mathematics/Calculus/calculus.html",
      "baseUrl": "Mathematics/Calculus/",
      "parts": [
        {
        "id": "calc-1",
        "part": 1,
        "title": "The Derivative of f:ℝⁿ→ℝ",
        "url": "linear_approximation.html",
        "icon": "∇",
        "keywords": [
          "Linear approximation",
          "Linearization",
          "Differentials",
          "Product rule",
          "Gradient",
          "Quadratic form",
          "L₂ norm",
          "Euclidean norm",
          "Tangent line",
          "Linear operator"
        ],
        "badges": [],
        "prereqs": ["linalg-1"],
        "mapCoords": {
          "q": -1, 
          "r": 0
        },
        "topicGroup": "derivatives",
        "tesseraMessage": "The gradient points uphill. In ML, we usually go the opposite way!",

        "headline": "Linear Approximations: From Scalar Calculus to Gradients",
        "description": "Learn about differentials and linearization, intro to matrix calculus.",
        "abstract": "Linear approximation extends single-variable calculus to vectors via the gradient. The differential notation df = f'(x)dx generalizes naturally to vector spaces, where f' becomes a linear operator mapping input changes to output changes—the foundation for all optimization in ML.",
        "datePublished": "2024-10-21",
        "dateModified": "2026-01-31",

        "teaches": [
          "Linear approximation of functions near a point",
          "Linearization and tangent line interpretation",
          "Differential notation: df = f'(x)dx",
          "Extending differentials to vector spaces",
          "Computing gradients of scalar-valued functions",
          "Product rule in differential notation",
          "Derivative of quadratic forms xᵀAx",
          "Derivative of L₂ norm"
        ],

        "competencyRequired": [
          "Basic calculus (limits, derivatives)",
          "Vectors and dot products",
          "Matrix-vector multiplication"
        ],

        "sections": [
          {"id": "lapp", "name": "Linear Approximations"},
          {"id": "diff", "name": "Differentials"},
          {"id": "ex1", "name": "Example 1: f(x) = xᵀx"},
          {"id": "ex2", "name": "Example 2: f(x) = xᵀAx"},
          {"id": "ex3", "name": "Example 3: f(x) = ||x||₂"}
        ],

        "definitions": [
          {
            "term": "Linear Approximation",
            "definition": "L(x) = f(x₀) + f'(x₀)(x - x₀) ≈ f(x) near x₀"
          },
          {
            "term": "Differential",
            "definition": "df = f'(x)dx where f'(x) is a linear operator mapping dx to df"
          },
          {
            "term": "Gradient",
            "definition": "∇f = column vector of partial derivatives; for f:ℝⁿ→ℝ, ∇f ∈ ℝⁿ"
          },
          {
            "term": "Quadratic Form",
            "definition": "f(x) = xᵀAx; gradient is (A + Aᵀ)x, or 2Ax when A is symmetric"
          }
        ],

        "about": [
          {"name": "Linear Approximation", "description": "Local linear model of a function"},
          {"name": "Gradient", "description": "Direction of steepest ascent"},
          {"name": "Differential Notation", "description": "Foundation for matrix calculus"},
          {"name": "Quadratic Forms", "description": "Key structure in optimization"}
        ]
        },
        { 
          "id": "calc-2",
          "part": 2,
          "title": "The Derivative of f:ℝⁿ→ℝⁿ",
          "url": "jacobian.html",
          "icon": "J",
          "keywords": [
            "Jacobian matrix",
            "Chain rule",
            "Backpropagation",
            "Reverse mode automatic differentiation",
            "Forward mode automatic differentiation",
            "Vector-Jacobian product",
            "Neural networks",
            "Loss function"
          ],
          "badges": [],
          "prereqs": ["calc-1"],
          "mapCoords": {
            "q": -2, 
            "r": 0
          },
          "topicGroup": "derivatives",
          "tesseraMessage": "The Jacobian is the backbone of backpropagation. Chain rule magic!",

          "headline": "Jacobian Matrix: The Foundation of Backpropagation",
          "description": "Learn about Jacobian, chain rule, backpropagation.",
          "abstract": "The Jacobian matrix generalizes the derivative to vector-valued functions, with each entry Jᵢⱼ = ∂fᵢ/∂xⱼ. The chain rule becomes matrix multiplication, and backpropagation exploits this by computing vector-Jacobian products from output to input—the key insight behind training neural networks.",
          "datePublished": "2024-10-21",
          "dateModified": "2026-01-31",

          "teaches": [
            "Jacobian matrix for vector-valued functions",
            "Jacobian as linear operator mapping dx to df",
            "Chain rule as Jacobian matrix multiplication",
            "Non-commutativity of chain rule in high dimensions",
            "Backpropagation as reverse-mode autodiff",
            "Vector-Jacobian products for efficiency",
            "Forward vs reverse mode differentiation",
            "Computational efficiency for neural networks"
          ],

          "competencyRequired": [
            "Gradients of scalar functions",
            "Matrix multiplication",
            "Partial derivatives"
          ],

          "sections": [
            {"id": "jacobian", "name": "Jacobian"},
            {"id": "chain", "name": "Chain Rule"},
            {"id": "backp", "name": "Backpropagation"}
          ],

          "definitions": [
            {
              "term": "Jacobian Matrix",
              "definition": "m×n matrix J where Jᵢⱼ = ∂fᵢ/∂xⱼ for f:ℝⁿ→ℝᵐ"
            },
            {
              "term": "Chain Rule",
              "definition": "df = g'(h(x))·h'(x)·dx; Jacobians multiply in order"
            },
            {
              "term": "Backpropagation",
              "definition": "Reverse-mode autodiff computing gradients via left-to-right Jacobian products"
            },
            {
              "term": "Vector-Jacobian Product",
              "definition": "vᵀJ computed efficiently without forming full Jacobian; key to backprop"
            }
          ],

          "about": [
            {"name": "Jacobian Matrix", "description": "Derivative of vector-valued functions"},
            {"name": "Chain Rule", "description": "Composition rule via matrix multiplication"},
            {"name": "Backpropagation", "description": "Efficient gradient computation for neural networks"},
            {"name": "Automatic Differentiation", "description": "Forward vs reverse mode tradeoffs"}
          ]
        },
        {    
          "id": "calc-3",
          "part": 3,
          "title": "The Derivative of f:ℝⁿˣⁿ→ℝⁿˣⁿ",
          "url": "matrix_cal.html",
          "icon": "M",
          "keywords": [
            "Matrix calculus",
            "Powers of a matrix",
            "Inverse of a matrix",
            "LU decomposition",
            "Matrix-valued functions",
            "Product rule for matrices",
            "Non-commutative operations"
          ],
          "badges": [],
          "prereqs": ["calc-2", "linalg-3"],
          "mapCoords": {
            "q": -1, 
            "r": -1
          },
          "topicGroup": "derivatives",
          "tesseraMessage": "Matrix calculus — where derivatives get truly multidimensional.",

          "headline": "Matrix Calculus: Derivatives of Matrix-Valued Functions",
          "description": "Learn about derivative of matrix functions.",
          "abstract": "Matrix calculus extends differentiation to matrix-valued functions of matrices. Non-commutativity requires careful ordering: d(X³) = (dX)X² + X(dX)X + X²(dX). Key results include the derivative of matrix inverse and LU decomposition—essential for advanced optimization and numerical methods.",
          "datePublished": "2024-10-21",
          "dateModified": "2026-01-31",

          "teaches": [
            "Derivative of matrix-valued functions",
            "Product rule for matrix functions",
            "Non-commutativity in matrix derivatives",
            "Derivative of matrix powers: d(X³)",
            "Derivative of matrix inverse: d(X⁻¹) = -X⁻¹(dX)X⁻¹",
            "Derivative of LU decomposition",
            "Triangular matrix properties in derivatives",
            "Symbolic vs component-wise differentiation"
          ],

          "competencyRequired": [
            "Jacobian matrices",
            "Matrix multiplication and inverses",
            "LU decomposition basics"
          ],

          "sections": [
            {"id": "square", "name": "Derivative of the square matrix functions"},
            {"id": "lu", "name": "Derivative of the LU decomposition matrix"}
          ],

          "definitions": [
            {
              "term": "Matrix Derivative",
              "definition": "df = (∂f/∂X)dX where both input and output are matrices"
            },
            {
              "term": "Derivative of X⁻¹",
              "definition": "d(X⁻¹) = -X⁻¹(dX)X⁻¹ derived from d(X⁻¹X) = 0"
            },
            {
              "term": "LU Derivative",
              "definition": "d(X) = d(LU) = (dL)U + L(dU) preserving triangular structure"
            }
          ],

          "about": [
            {"name": "Matrix Calculus", "description": "Differentiation of matrix-valued functions"},
            {"name": "Matrix Powers", "description": "Non-commutative product rule"},
            {"name": "Matrix Inverse", "description": "Key derivative for optimization"},
            {"name": "LU Decomposition", "description": "Derivatives preserving triangular structure"}
          ]
        },
        { 
          "id": "calc-4",
          "part": 4,
          "title": "Intro to Numerical Computation",
          "url": "numerical_example1.html",
          "icon": "≈",
          "keywords": [
            "Finite-difference approximation",
            "Forward difference",
            "Backward difference",
            "Relative error",
            "Roundoff error",
            "Machine epsilon",
            "Numerical stability",
            "Floating-point arithmetic"
          ],
          "badges": ["code"],
          "prereqs": ["calc-1"],
          "mapCoords": {
            "q": -1, 
            "r": -2
          },
          "topicGroup": "numerical",
          "tesseraMessage": "Theory meets practice! Understanding numerical errors is crucial for real code.",

          "headline": "Numerical Computation: Validating Derivatives with Finite Differences",
          "description": "Introduction to numerical computation with coding.",
          "abstract": "Finite differences approximate derivatives numerically, enabling validation of analytical formulas. Choosing the right step size balances truncation error (too large) against roundoff error (too small). Relative error provides meaningful accuracy measures when comparing analytical and numerical results.",
          "datePublished": "2024-10-21",
          "dateModified": "2026-01-31",

          "teaches": [
            "Finite difference approximations",
            "Forward and backward difference methods",
            "Relative error as accuracy metric",
            "Floating-point arithmetic limitations",
            "Machine epsilon and precision limits",
            "Validating analytical derivatives numerically",
            "Choosing appropriate step sizes",
            "Random testing for robust validation"
          ],

          "competencyRequired": [
            "Differential calculus",
            "Basic Python/NumPy",
            "Matrix operations"
          ],

          "sections": [
            {"id": "ex1", "name": "Numerical Computation Example"}
          ],

          "definitions": [
            {
              "term": "Forward Difference",
              "definition": "f(x+dx) - f(x) approximates df"
            },
            {
              "term": "Backward Difference",
              "definition": "f(x) - f(x-dx) approximates df"
            },
            {
              "term": "Relative Error",
              "definition": "||True - Approximation|| / ||True||; meaningful accuracy measure"
            },
            {
              "term": "Machine Epsilon",
              "definition": "Smallest ε where 1 + ε ≠ 1 in floating-point; ~10⁻¹⁶ for double precision"
            }
          ],

          "about": [
            {"name": "Finite Differences", "description": "Numerical approximation of derivatives"},
            {"name": "Relative Error", "description": "Scale-independent accuracy metric"},
            {"name": "Numerical Stability", "description": "Balancing truncation and roundoff errors"},
            {"name": "Computational Validation", "description": "Testing analytical formulas with code"}
          ]
        },
        { 
          "id": "calc-5",
          "part": 5,
          "title": "The Derivative of Scalar Functions of Matrices",
          "url": "det.html",
          "icon": "det",
          "keywords": [
            "Frobenius inner product",
            "Frobenius norm",
            "Trace",
            "Determinant",
            "Cofactor",
            "Adjugate",
            "Characteristic polynomial",
            "Automatic differentiation"
          ],
          "badges": [],
          "prereqs": ["calc-3", "linalg-4"],
          "mapCoords": {
            "q": -2, 
            "r": -1
          },
          "topicGroup": "derivatives",
          "tesseraMessage": "Derivatives of determinants and traces — essential for advanced ML proofs.",

          "headline": "Scalar Functions of Matrices: Frobenius Norm and Determinant Derivatives",
          "description": "Learn about derivatives of Frobenius norm and determinants.",
          "abstract": "Scalar functions of matrices arise throughout ML: the Frobenius norm measures matrix magnitude, while determinants appear in Gaussian distributions and volume calculations. The trace trick expresses derivatives elegantly: df = tr((∇f)ᵀdX). Automatic differentiation often outperforms analytical formulas numerically.",
          "datePublished": "2024-10-21",
          "dateModified": "2026-01-31",

          "teaches": [
            "Derivative of Frobenius norm: ∇||X||_F = X/||X||_F",
            "Differential notation with trace: df = tr((∇f)ᵀdX)",
            "Frobenius inner product representation",
            "Derivative of determinant via cofactors",
            "Relationship: ∇(det A) = cofactor(A) = (det A)(A⁻¹)ᵀ",
            "Derivative of characteristic polynomial",
            "Analytical vs automatic differentiation comparison",
            "Numerical stability considerations"
          ],

          "competencyRequired": [
            "Matrix calculus fundamentals",
            "Determinants and cofactors",
            "Trace properties"
          ],

          "sections": [
            {"id": "frob", "name": "Derivative of the Frobenius norm"},
            {"id": "det", "name": "Derivative of the Determinant"}
          ],

          "definitions": [
            {
              "term": "Frobenius Norm",
              "definition": "||X||_F = √(tr(XᵀX)); gradient is X/||X||_F"
            },
            {
              "term": "Trace Trick",
              "definition": "df = tr((∇f)ᵀdX) expresses scalar derivatives of matrices"
            },
            {
              "term": "Determinant Derivative",
              "definition": "d(det A) = (det A)·tr(A⁻¹dA) = tr(adj(A)dA)"
            },
            {
              "term": "Cofactor/Adjugate Relation",
              "definition": "∇(det A) = cofactor(A) = adj(A)ᵀ = (det A)(A⁻¹)ᵀ"
            }
          ],

          "about": [
            {"name": "Frobenius Norm", "description": "Matrix magnitude via trace"},
            {"name": "Trace Trick", "description": "Elegant derivative representation"},
            {"name": "Determinant Derivative", "description": "Via cofactors or adjugate"},
            {"name": "Automatic Differentiation", "description": "Practical alternative to analytical formulas"}
          ]
        },
        { 
          "id": "calc-6",
          "part": 6,
          "title": "The Mean Value Theorem",
          "url": "mvt.html",
          "icon": "μ",
          "keywords": [
            "Rolle's Theorem",
            "Lagrange's Mean Value Theorem",
            "Cauchy's Mean Value Theorem",
            "Taylor's Theorem",
            "Taylor polynomial",
            "Taylor series",
            "Lagrange remainder",
            "little-o notation",
            "Higher-dimensional MVT",
            "Linearization"
          ],
          "badges": [],
          "prereqs": ["calc-1"],
          "mapCoords":{
            "q": -3, 
            "r": -2
          },
          "topicGroup": "optimization",
          "tesseraMessage": "Taylor's theorem lets us approximate any smooth function locally. Powerful!",

          "headline": "Mean Value Theorem: The Bridge Between Derivatives and Differences",
          "description": "Learn about Lagrange's & Cauchy's mean value theorem, and Higher-dimensional MVT.",
          "abstract": "The Mean Value Theorem connects point derivatives to average rates of change. Taylor's theorem extends this to polynomial approximations of any order. The higher-dimensional MVT provides the foundation for gradient descent in multivariable optimization.",
          "datePublished": "2024-10-21",
          "dateModified": "2026-01-31",

          "teaches": [
            "Lagrange's Mean Value Theorem and its proof",
            "Rolle's Theorem as a special case",
            "Taylor's Theorem and polynomial approximations",
            "Lagrange's form of the remainder",
            "Little-o notation in asymptotic analysis",
            "Taylor series and radius of convergence",
            "Cauchy's Mean Value Theorem",
            "Higher-dimensional MVT for gradient descent"
          ],

          "competencyRequired": [
            "Differentiation of single-variable functions",
            "Continuity and limits",
            "Basic proof techniques"
          ],

          "sections": [
            {"id": "l_mvt", "name": "Lagrange's Mean Value Theorem"},
            {"id": "taylor", "name": "Taylor's Theorem"},
            {"id": "c_mvt", "name": "Cauchy's Mean Value Theorem"},
            {"id": "h_dim", "name": "Higher-dimensional MVT"}
          ],

          "definitions": [
            {
              "term": "Lagrange's MVT",
              "definition": "If f is continuous on [a,b] and differentiable on (a,b), then ∃c ∈ (a,b) such that f'(c) = (f(b)-f(a))/(b-a)"
            },
            {
              "term": "Rolle's Theorem",
              "definition": "Special case of MVT: if f(a) = f(b), then ∃c ∈ (a,b) such that f'(c) = 0"
            },
            {
              "term": "Taylor Polynomial",
              "definition": "Tₙ(x) = Σₖ₌₀ⁿ f⁽ᵏ⁾(a)/k! · (x-a)ᵏ, the n-th order approximation of f near a"
            },
            {
              "term": "Lagrange Remainder",
              "definition": "Rₙ(x) = f⁽ⁿ⁾(c)/n! · (x-a)ⁿ where c ∈ (a,x)"
            },
            {
              "term": "Higher-dimensional MVT",
              "definition": "f(b) - f(a) = ∇f(c) · (b-a) for some c on the line segment [a,b]"
            }
          ],

          "about": [
            {"name": "Mean Value Theorem", "description": "Connects derivatives to differences over intervals"},
            {"name": "Taylor's Theorem", "description": "Polynomial approximation with quantified error"},
            {"name": "Linearization", "description": "First-order Taylor approximation"},
            {"name": "Higher-dimensional MVT", "description": "Foundation for gradient-based optimization"}
          ]
        },
        {   
        "id": "calc-7",
        "part": 7,
        "title": "Gradient Descent (First-order Method)",
        "url": "gradient.html",
        "icon": "↓",
        "keywords": [
          "Optimization problems",
          "Convexity",
          "Convex functions",
          "Gradient Descent (GD)",
          "Steepest Descent",
          "Stochastic Gradient Descent (SGD)",
          "Mini-batch SGD",
          "Sub-gradient",
          "Sub-differentiable",
          "Learning rate",
          "Local minimum",
          "Global minimum"
        ],
        "badges": ["code"],
        "prereqs": ["calc-6"],
        "mapCoords": {
          "q": -3, 
          "r": -1
        },
        "topicGroup": "optimization",
        "tesseraMessage": "Gradient descent is the workhorse of ML — simple yet powerful optimization.",

        "headline": "Gradient Descent: The Workhorse of Machine Learning Optimization",
        "description": "Learn about convexity, gradient descent, and stochastic gradient descent.",
        "abstract": "Gradient descent iteratively moves toward minima by following the negative gradient. This first-order method, with variants like SGD and mini-batch SGD, trains virtually every neural network. Understanding convexity reveals when gradient descent finds global optima.",
        "datePublished": "2024-12-25",
        "dateModified": "2026-01-31",

        "teaches": [
          "Optimization problem formulation in machine learning",
          "Convex functions and their properties",
          "Local vs global minima and the role of convexity",
          "Gradient descent algorithm and steepest descent",
          "Learning rate selection and convergence",
          "Stochastic gradient descent for large datasets",
          "Mini-batch SGD and epoch-based training",
          "Subgradient descent for non-smooth functions"
        ],

        "competencyRequired": [
          "Partial derivatives and gradients",
          "Mean Value Theorem",
          "Matrix-vector operations"
        ],

        "sections": [
          {"id": "intro", "name": "Introduction to Optimization"},
          {"id": "convexity", "name": "Convexity"},
          {"id": "gradient", "name": "Gradient Descent"},
          {"id": "sgd", "name": "Stochastic Gradient Descent"},
          {"id": "subgradient", "name": "Sub-gradient Descent"}
        ],

        "definitions": [
          {
            "term": "Convex Function",
            "definition": "f(λx + (1-λ)y) ≤ λf(x) + (1-λ)f(y) for all x,y and λ ∈ [0,1]"
          },
          {
            "term": "Gradient Descent",
            "definition": "θ⁽ᵏ⁺¹⁾ = θ⁽ᵏ⁾ - η∇f(θ⁽ᵏ⁾), iteratively moving opposite to gradient"
          },
          {
            "term": "Stochastic Gradient Descent",
            "definition": "Update using gradient of single random sample instead of full batch"
          },
          {
            "term": "Mini-batch SGD",
            "definition": "Gradient computed on small subset B of data, balancing noise and efficiency"
          },
          {
            "term": "Subgradient",
            "definition": "g such that f(z) ≥ f(x) + gᵀ(z-x) for all z; generalizes gradient to non-smooth functions"
          }
        ],

        "about": [
          {"name": "Convexity", "description": "Property guaranteeing global optimum from any local optimum"},
          {"name": "Gradient Descent", "description": "First-order iterative optimization algorithm"},
          {"name": "SGD", "description": "Scalable variant using random sampling"},
          {"name": "Subgradient", "description": "Extension to non-differentiable convex functions"}
        ]
        },
        {   
        "id": "calc-8",
        "part": 8,
        "title": "Newton's method (Second-order Method)",
        "url": "newton.html",
        "icon": "N",
        "keywords": [
          "Line search",
          "Armijo condition",
          "Curvature condition",
          "Wolfe conditions",
          "Newton's method",
          "Quasi-Newton methods",
          "BFGS",
          "L-BFGS",
          "Secant condition",
          "Inverse Hessian approximation",
          "Rosenbrock function",
          "Second-order optimization"
        ],
        "badges": ["code"],
        "prereqs": ["calc-7", "linalg-9"],
        "mapCoords": {
          "q": -3, 
          "r": 0
        },
        "topicGroup": "optimization",
        "tesseraMessage": "Newton's method: faster convergence using curvature. BFGS makes it practical!",

        "headline": "Newton's Method: Second-Order Optimization with Curvature",
        "description": "Learn about line search, Newton's method, and BFGS method.",
        "abstract": "Newton's method uses the Hessian matrix to achieve quadratic convergence near optima. BFGS and L-BFGS approximate the inverse Hessian efficiently, avoiding expensive matrix inversions while retaining fast convergence. Line search with Wolfe conditions ensures stable step sizes.",
        "datePublished": "2025-01-16",
        "dateModified": "2026-01-31",

        "teaches": [
          "Line search methods and step size selection",
          "Armijo condition for sufficient decrease",
          "Wolfe conditions: Armijo + curvature",
          "Newton's method using Hessian information",
          "Quasi-Newton methods avoiding explicit Hessian",
          "BFGS inverse Hessian approximation",
          "Secant condition and rank-2 updates",
          "L-BFGS for large-scale optimization",
          "Rosenbrock function as optimization benchmark"
        ],

        "competencyRequired": [
          "Gradient descent fundamentals",
          "Matrix inverses and positive definiteness",
          "Quadratic forms"
        ],

        "sections": [
          {"id": "LS", "name": "Line Search"},
          {"id": "NM", "name": "Newton's Method"},
          {"id": "BFGS", "name": "BFGS Method"}
        ],

        "definitions": [
          {
            "term": "Newton's Method",
            "definition": "θ⁽ᵏ⁺¹⁾ = θ⁽ᵏ⁾ - H⁻¹∇f(θ⁽ᵏ⁾), using Hessian for quadratic convergence"
          },
          {
            "term": "Armijo Condition",
            "definition": "f(θ + ηp) ≤ f(θ) + c₁η∇f(θ)ᵀp, ensuring sufficient decrease"
          },
          {
            "term": "Wolfe Conditions",
            "definition": "Armijo condition + curvature condition for proper step sizes"
          },
          {
            "term": "BFGS",
            "definition": "Quasi-Newton method approximating inverse Hessian via rank-2 updates"
          },
          {
            "term": "L-BFGS",
            "definition": "Limited-memory BFGS storing only m recent (s,y) pairs for large-scale problems"
          },
          {
            "term": "Secant Condition",
            "definition": "Bₖ₊₁sₖ = yₖ where sₖ = θₖ₊₁ - θₖ and yₖ = ∇f(θₖ₊₁) - ∇f(θₖ)"
          }
        ],

        "about": [
          {"name": "Newton's Method", "description": "Second-order optimization with Hessian"},
          {"name": "Line Search", "description": "Choosing step sizes satisfying Wolfe conditions"},
          {"name": "BFGS", "description": "Practical quasi-Newton without explicit Hessian"},
          {"name": "L-BFGS", "description": "Memory-efficient BFGS for high-dimensional problems"}
        ]
        },
        {
        
        "id": "calc-9",
        "part": 9,
        "title": "Constrained Optimization",
        "url": "constrained_opt.html",
        "icon": "λ",
        "keywords": [
          "Constrained optimization problems",
          "Penalty terms",
          "Lagrange Multipliers",
          "Lagrangian",
          "Karush-Kuhn-Tucker (KKT) conditions",
          "Active set",
          "Slack variables",
          "Complementary slackness",
          "Primal feasibility",
          "Dual feasibility"
        ],
        "badges": ["code"],
        "prereqs": ["calc-7"],
        "mapCoords": {
          "q": -4, 
          "r": 0
        },
        "topicGroup": "optimization",
        "tesseraMessage": "KKT conditions are the foundation of constrained optimization. Essential theory!",

        "headline": "Constrained Optimization: KKT Conditions and Lagrange Multipliers",
        "description": "Learn about constrained optimization ideas, KKT Conditions, and Lagrange multipliers.",
        "abstract": "Constrained optimization handles equality and inequality constraints via Lagrange multipliers and KKT conditions. These necessary conditions for optimality underpin support vector machines, portfolio optimization, and many ML algorithms requiring bounded or structured solutions.",
        "datePublished": "2025-01-28",
        "dateModified": "2026-01-31",

        "teaches": [
          "Formulation of constrained optimization problems",
          "Equality constraints and Lagrange multipliers",
          "Inequality constraints and KKT conditions",
          "Stationarity, primal feasibility, dual feasibility",
          "Complementary slackness conditions",
          "Active and inactive constraints",
          "Penalty method implementation",
          "Slack variables for numerical solvers"
        ],

        "competencyRequired": [
          "Gradient descent and unconstrained optimization",
          "Partial derivatives",
          "Systems of nonlinear equations"
        ],

        "sections": [
          {"id": "intro", "name": "Constrained Optimization Problem"},
          {"id": "lm", "name": "Lagrange Multipliers"},
          {"id": "kkt", "name": "The KKT Conditions"}
        ],

        "definitions": [
          {
            "term": "Lagrangian",
            "definition": "L(x,λ,μ) = f(x) + Σλᵢhᵢ(x) + Σμⱼgⱼ(x), combining objective with constraints"
          },
          {
            "term": "KKT Conditions",
            "definition": "Necessary conditions: stationarity, primal feasibility, dual feasibility, complementary slackness"
          },
          {
            "term": "Complementary Slackness",
            "definition": "μᵢgᵢ(x) = 0 for all i; either constraint is active or multiplier is zero"
          },
          {
            "term": "Lagrange Multiplier",
            "definition": "λ measuring sensitivity of objective to constraint; shadow price"
          },
          {
            "term": "Slack Variable",
            "definition": "Transforms gᵢ(x) ≤ 0 to gᵢ(x) + sᵢ = 0 with sᵢ ≥ 0"
          }
        ],

        "about": [
          {"name": "KKT Conditions", "description": "First-order necessary conditions for constrained optimality"},
          {"name": "Lagrangian", "description": "Augmented objective incorporating constraints"},
          {"name": "Complementary Slackness", "description": "Key condition linking constraints and multipliers"},
          {"name": "Penalty Method", "description": "Approximate constrained problems via unconstrained penalties"}
        ]
        },  
        {
        "id": "calc-10",
        "part": 10,
        "title": "Riemann Integration",
        "url": "riemann.html",
        "icon": "∫",
        "keywords": [
          "Riemann integral",
          "Riemann integrable",
          "Improper Riemann integration",
          "Partition",
          "Upper sum",
          "Lower sum",
          "Dirichlet function",
          "Dense sets",
          "Gaussian integral",
          "Cauchy distribution"
        ],
        "badges": [],
        "prereqs": ["calc-6"],
        "mapCoords": {
          "q": -3, 
          "r": -3
        },
        "topicGroup": "analysis",
        "tesseraMessage": "Riemann integration — the classical approach before Lebesgue changed everything.",

        "headline": "Riemann Integration: The Classical Approach to Integration",
        "description": "Learn about Riemann integration, and improper Riemann integration.",
        "abstract": "Riemann integration defines integrals via partitions and upper/lower sums. While sufficient for continuous functions, its limitations with highly discontinuous functions like the Dirichlet function motivate the more powerful Lebesgue integral needed in probability theory.",
        "datePublished": "2025-02-04",
        "dateModified": "2026-01-31",

        "teaches": [
          "Formal definition of Riemann integration",
          "Partitions, upper sums, and lower sums",
          "Conditions for Riemann integrability",
          "Improper integrals on unbounded intervals",
          "Handling singularities in integrals",
          "Limitations with dense discontinuities",
          "Connection to Gaussian and Cauchy integrals",
          "Motivation for Lebesgue integration"
        ],

        "competencyRequired": [
          "Limits and continuity",
          "Infimum and supremum",
          "Basic integration techniques"
        ],

        "sections": [
          {"id": "rieman", "name": "Riemann Integration"},
          {"id": "i_rieman", "name": "Improper Riemann Integration"},
          {"id": "limit", "name": "Limitation of the (Improper) Riemann integration"}
        ],

        "definitions": [
          {
            "term": "Riemann Integral",
            "definition": "∫ₐᵇf(x)dx = α when sup L(f,P) = inf U(f,P) = α over all partitions"
          },
          {
            "term": "Partition",
            "definition": "P = {x₀, x₁, ..., xₙ} with a = x₀ < x₁ < ... < xₙ = b"
          },
          {
            "term": "Upper/Lower Sum",
            "definition": "U(f,P) = Σ Mᵢ(xᵢ-xᵢ₋₁), L(f,P) = Σ mᵢ(xᵢ-xᵢ₋₁)"
          },
          {
            "term": "Improper Integral",
            "definition": "∫ₐ^∞ f(x)dx = lim_{b→∞} ∫ₐᵇ f(x)dx when the limit exists"
          },
          {
            "term": "Dense Set",
            "definition": "A ⊂ ℝ is dense if for any x < y, there exists a ∈ A with x < a < y"
          }
        ],

        "about": [
          {"name": "Riemann Integration", "description": "Classical partition-based integration"},
          {"name": "Improper Integrals", "description": "Extending integration to unbounded domains"},
          {"name": "Dirichlet Function", "description": "Example showing Riemann's limitations"},
          {"name": "Dense Discontinuities", "description": "Why Lebesgue integration is needed"}
        ]
        },
        {
        "id": "calc-11",
        "part": 11,
        "title": "Measure Theory with Probability",
        "url": "measure.html",
        "icon": "σ",
        "keywords": [
          "Sample space",
          "σ-algebra",
          "σ-field",
          "Measurable set",
          "Measurable space",
          "Measure",
          "Probability measure",
          "Probability space",
          "Countable additivity",
          "Borel σ-algebra",
          "Borel set",
          "Lebesgue measure",
          "Caratheodory Extension Theorem"
        ],
        "badges": [],
        "prereqs": ["calc-10", "prob-1"],
        "mapCoords": {
          "q": -2, 
          "r": -4
        },
        "topicGroup": "analysis",
        "tesseraMessage": "Measure theory makes probability rigorous. σ-algebras are everywhere!",

        "headline": "Measure Theory: The Rigorous Foundation of Probability",
        "description": "Learn about measure theory in terms of probability, and Lebesgue measure.",
        "abstract": "Measure theory provides the rigorous foundation for probability via σ-algebras and probability measures. The Carathéodory extension theorem constructs the Lebesgue measure, enabling integration of functions that defeat Riemann's approach.",
        "datePublished": "2025-02-08",
        "dateModified": "2026-01-31",

        "teaches": [
          "Formal structure of probability spaces (Ω, F, P)",
          "Sample spaces and elementary outcomes",
          "σ-algebras and closure properties",
          "Probability measures and axioms",
          "Countable vs finite additivity",
          "Carathéodory's extension theorem",
          "Construction of Lebesgue measure on ℝ",
          "Borel σ-algebra and measurable sets"
        ],

        "competencyRequired": [
          "Riemann integration and its limitations",
          "Basic probability concepts",
          "Set operations and notation"
        ],

        "sections": [
          {"id": "pspace", "name": "Probability Space"},
          {"id": "sample", "name": "Sample Space"},
          {"id": "sigma", "name": "σ-algebra (σ-field)"},
          {"id": "pm", "name": "Probability Measure"},
          {"id": "add", "name": "Finite Additivity"},
          {"id": "ext", "name": "Caratheodory's Extension Theorem"},
          {"id": "leb", "name": "Lebesgue measure"}
        ],

        "definitions": [
          {
            "term": "Probability Space",
            "definition": "Triple (Ω, F, P): sample space, σ-algebra, and probability measure"
          },
          {
            "term": "σ-algebra",
            "definition": "Collection F of subsets of Ω closed under complement and countable unions, containing Ω"
          },
          {
            "term": "Probability Measure",
            "definition": "Function P: F → [0,1] with P(Ω) = 1 and countable additivity"
          },
          {
            "term": "Borel σ-algebra",
            "definition": "Smallest σ-algebra containing all intervals; denoted B"
          },
          {
            "term": "Lebesgue Measure",
            "definition": "μ on (ℝ, B) with μ([a,b]) = b - a; extends 'length' to Borel sets"
          },
          {
            "term": "Carathéodory Extension",
            "definition": "Theorem extending finitely additive pre-measure to full σ-algebra"
          }
        ],

        "about": [
          {"name": "Probability Space", "description": "Formal triple (Ω, F, P) modeling random experiments"},
          {"name": "σ-algebra", "description": "Events we can assign probabilities to"},
          {"name": "Lebesgue Measure", "description": "Generalized notion of length/volume"},
          {"name": "Carathéodory", "description": "Foundational extension theorem"}
        ]
        },
        {
        "id": "calc-12",
        "part": 12,
        "title": "Intro to Lebesgue Integration",
        "url": "lebesgue.html",
        "icon": "a.e.",
        "keywords": [
          "Lebesgue integral",
          "Abstract integration",
          "Characteristic function",
          "Indicator function",
          "Simple function",
          "Almost everywhere (a.e.)",
          "Almost surely (a.s.)",
          "Measurable function",
          "Dirichlet function",
          "Dirichlet integral",
          "Sinc function"
        ],
        "badges": [],
        "prereqs": ["calc-11"],
        "mapCoords": {
          "q": -3, 
          "r": -4
        },
        "topicGroup": "analysis",
        "tesseraMessage": "Lebesgue integration handles functions Riemann cannot. 'Almost everywhere' matters!",

        "headline": "Lebesgue Integration: Beyond Riemann's Limitations",
        "description": "Learn about integration of measurable functions.",
        "abstract": "Lebesgue integration builds integrals from measure theory, handling highly discontinuous functions that defeat Riemann. The concept of 'almost everywhere' formalizes ignoring measure-zero exceptions—essential for probability theory where 'almost surely' governs convergence.",
        "datePublished": "2025-02-13",
        "dateModified": "2026-01-31",

        "teaches": [
          "Abstract integration in measure spaces",
          "Characteristic (indicator) functions",
          "Simple functions and their integrals",
          "Integration of nonnegative measurable functions",
          "Integration of general measurable functions via f⁺ and f⁻",
          "Concept of 'almost everywhere' (a.e.)",
          "'Almost surely' in probability contexts",
          "Comparison: Riemann vs Lebesgue integrability",
          "Conditionally convergent integrals (sinc function)"
        ],

        "competencyRequired": [
          "Measure spaces and σ-algebras",
          "Lebesgue measure",
          "Supremum and infimum"
        ],

        "sections": [
          {"id": "abs", "name": "Abstract Integration"},
          {"id": "c_f", "name": "Characteristic function"},
          {"id": "fnm", "name": "The Integral of Finite Nonnegative Measurable Functions"},
          {"id": "nm", "name": "The Integral of Nonnegative Measurable Functions"},
          {"id": "gm", "name": "The Integral of General Measurable Functions"}
        ],

        "definitions": [
          {
            "term": "Lebesgue Integral",
            "definition": "∫g dμ defined via simple function approximation on measure space (Ω, F, μ)"
          },
          {
            "term": "Characteristic Function",
            "definition": "χ_A(ω) = 1 if ω ∈ A, 0 otherwise; indicator of set A"
          },
          {
            "term": "Simple Function",
            "definition": "g = Σᵢ aᵢχ_{Aᵢ} with finitely many values; integral = Σᵢ aᵢμ(Aᵢ)"
          },
          {
            "term": "Almost Everywhere (a.e.)",
            "definition": "Property holds except on a set of measure zero"
          },
          {
            "term": "Almost Surely (a.s.)",
            "definition": "'Almost everywhere' in probability context; holds with probability 1"
          }
        ],

        "about": [
          {"name": "Lebesgue Integral", "description": "Measure-theoretic generalization of Riemann"},
          {"name": "Simple Functions", "description": "Building blocks for Lebesgue integration"},
          {"name": "Almost Everywhere", "description": "Ignoring measure-zero exceptions"},
          {"name": "Dirichlet Function", "description": "Classic example: Lebesgue integral = 0"}
        ]
        },
        {   
        "id": "calc-13",
        "part": 13,
        "title": "Duality in Optimization & Analysis",
        "url": "duality.html",
        "icon": "P⟷D",
        "keywords": [
          "Duality",
          "Primal problem",
          "Dual problem",
          "Weak duality",
          "Strong duality",
          "Duality gap",
          "Slater's condition",
          "Smoothness",
          "L-smooth",
          "Lipschitz continuity",
          "Contraction mapping",
          "Convergence rate",
          "Condition number"
        ],
        "badges": ["interactive"],
        "prereqs": ["calc-9"],
        "mapCoords": {
          "q": -4, 
          "r": -1
        },
        "topicGroup": "optimization",
        "tesseraMessage": "Duality gives us bounds and insights. Strong duality = same optimal value!",

        "headline": "Duality: Primal-Dual Relationships and Convergence Analysis",
        "description": "Learn about duality and Lipschitz Continuity.",
        "abstract": "Duality theory provides lower bounds on optimization problems and enables efficient algorithms. Lipschitz continuity and L-smoothness govern convergence rates—the condition number determines how fast gradient descent converges for quadratic problems.",
        "datePublished": "2025-03-11",
        "dateModified": "2026-01-31",

        "teaches": [
          "Primal and dual optimization problems",
          "Weak duality: dual provides lower bound",
          "Strong duality and zero duality gap",
          "Slater's condition for strong duality",
          "Lipschitz continuity and bounded rate of change",
          "L-smooth functions and gradient Lipschitz",
          "Contraction mappings and linear convergence",
          "Condition number and convergence rate analysis"
        ],

        "competencyRequired": [
          "Constrained optimization and KKT conditions",
          "Lagrangian formulation",
          "Eigenvalues of symmetric matrices"
        ],

        "sections": [
          {"id": "duality", "name": "Duality"},
          {"id": "Lip", "name": "Lipschitz Continuity"},
          {"id": "duality-visualization", "name": "Interactive Duality Visualization"}
        ],

        "definitions": [
          {
            "term": "Weak Duality",
            "definition": "d* ≤ p* always; dual optimal value bounds primal"
          },
          {
            "term": "Strong Duality",
            "definition": "d* = p*; primal and dual have same optimal value"
          },
          {
            "term": "Duality Gap",
            "definition": "p* - d* ≥ 0; zero gap means strong duality holds"
          },
          {
            "term": "L-smooth",
            "definition": "||∇f(x) - ∇f(y)|| ≤ L||x - y||; gradient is Lipschitz with constant L"
          },
          {
            "term": "Contraction Mapping",
            "definition": "d(f(x), f(y)) ≤ k·d(x,y) with k < 1; guarantees convergence"
          },
          {
            "term": "Condition Number",
            "definition": "κ = λ_max/λ_min; determines convergence rate μ = ((κ-1)/(κ+1))²"
          }
        ],

        "about": [
          {"name": "Duality", "description": "Relationship between primal and dual problems"},
          {"name": "Strong Duality", "description": "When primal and dual optima coincide"},
          {"name": "L-smoothness", "description": "Lipschitz gradient enabling convergence bounds"},
          {"name": "Condition Number", "description": "Key factor determining optimization speed"}
        ]
        },
        { 
        "id": "calc-14",
        "part": 14,
        "title": "Fourier Series",
        "url": "fourier_series.html",
        "icon": "∿",
        "keywords": [
          "Fourier series",
          "Fourier coefficients",
          "Orthogonality of Trigonometric Functions",
          "Complex exponential form",
          "Parseval's identity",
          "L² convergence",
          "Gibbs phenomenon",
          "Periodic functions",
          "Harmonic analysis",
          "Signal decomposition"
        ],
        "badges": [],
        "prereqs": ["calc-12", "linalg-7"],
        "mapCoords": {
          "q": -4, 
          "r": -4
        },
        "topicGroup": "fourier",
        "tesseraMessage": "Fourier series decompose signals into frequencies. Music, images, everything!",

        "headline": "Fourier Series: Decomposing Periodic Functions into Frequencies",
        "description": "Learn about Fourier series decomposition of periodic functions and their convergence properties.",
        "abstract": "Fourier series represent periodic functions as infinite sums of sines and cosines. This fundamental technique, developed by Joseph Fourier for studying heat conduction, underlies signal processing, data compression (JPEG, MP3), and modern machine learning architectures.",
        "datePublished": "2025-11-01",
        "dateModified": "2026-01-31",

        "teaches": [
          "Fourier series representation of periodic functions",
          "Computing Fourier coefficients via integration",
          "Orthogonality of trigonometric functions",
          "Complex exponential form of Fourier series",
          "Parseval's identity and energy conservation",
          "L² (mean square) convergence of Fourier series",
          "Pointwise convergence and bounded variation",
          "Gibbs phenomenon at discontinuities"
        ],

        "competencyRequired": [
          "Riemann integration",
          "Inner products and orthogonality",
          "Complex exponentials"
        ],

        "sections": [
          {"id": "intro", "name": "Introduction"},
          {"id": "ortho", "name": "Orthogonality of Trigonometric Functions"},
          {"id": "coeff", "name": "Fourier Coefficients"},
          {"id": "complex", "name": "Complex Exponential Form"},
          {"id": "parseval", "name": "Parseval's Identity"},
          {"id": "convergence", "name": "Convergence Properties"}
        ],

        "definitions": [
          {
            "term": "Fourier Series",
            "definition": "Representation of a periodic function f(x) as f(x) = a₀/2 + Σ(aₙcos(nπx/L) + bₙsin(nπx/L))"
          },
          {
            "term": "Fourier Coefficients",
            "definition": "The constants aₙ and bₙ computed by integrating f(x) against cos and sin basis functions"
          },
          {
            "term": "Parseval's Identity",
            "definition": "Energy conservation: (1/L)∫|f(x)|²dx = a₀²/2 + Σ(aₙ² + bₙ²)"
          },
          {
            "term": "Gibbs Phenomenon",
            "definition": "~9% overshoot in partial sums near jump discontinuities that persists as N→∞"
          }
        ],

        "about": [
          {"name": "Fourier Series", "description": "Infinite trigonometric series representing periodic functions"},
          {"name": "Parseval's Identity", "description": "Energy equivalence between time and frequency domains"},
          {"name": "Gibbs Phenomenon", "description": "Persistent oscillation artifact at discontinuities"},
          {"name": "L² Convergence", "description": "Mean square convergence in function space"}
        ]
        },
        {   
        "id": "calc-15",
        "part": 15,
        "title": "Fourier Transform",
        "url": "fourier_transform.html",
        "icon": "ℱ",
        "keywords": [
          "Fourier transform",
          "Inverse Fourier transform",
          "Plancherel's theorem",
          "Convolution theorem",
          "Discrete Fourier Transform (DFT)",
          "Fast Fourier Transform (FFT)",
          "Fourier Neural Operators (FNO)",
          "Frequency domain",
          "Signal processing",
          "Spectral analysis"
        ],
        "badges": ["interactive"],
        "prereqs": ["calc-14"],
        "mapCoords": {
          "q": -4, 
          "r": -5
        },
        "topicGroup": "fourier",
        "tesseraMessage": "FFT changed the world — fast signal processing makes modern tech possible.",

        "headline": "Fourier Transform: From Time Domain to Frequency Domain",
        "description": "Learn about the Fourier transform, its properties, and applications to signal processing and machine learning.",
        "abstract": "The Fourier transform extends Fourier series to non-periodic functions, decomposing signals into continuous frequency spectra. The FFT algorithm enables efficient computation, powering everything from audio processing to Fourier Neural Operators for solving PDEs.",
        "datePublished": "2025-11-14",
        "dateModified": "2026-01-31",

        "teaches": [
          "Continuous Fourier transform and inverse transform",
          "Properties: linearity, shifting, scaling, differentiation",
          "Convolution theorem: multiplication in frequency domain",
          "Plancherel's theorem for energy preservation",
          "Discrete Fourier Transform (DFT) for sampled signals",
          "Fast Fourier Transform (FFT) algorithm and O(N log N) complexity",
          "Applications: audio processing, image filtering, neural networks",
          "Fourier Neural Operators for solving PDEs"
        ],

        "competencyRequired": [
          "Fourier series and coefficients",
          "Complex exponentials",
          "Improper integrals"
        ],

        "sections": [
          {"id": "continuous", "name": "Continuous Fourier Transform"},
          {"id": "properties", "name": "Properties of Fourier Transform"},
          {"id": "convolution", "name": "Convolution Theorem"},
          {"id": "discrete", "name": "Discrete Fourier Transform"},
          {"id": "fft", "name": "Fast Fourier Transform"},
          {"id": "ml", "name": "Applications in Machine Learning"},
          {"id": "demos", "name": "Interactive Demos"}
        ],

        "definitions": [
          {
            "term": "Fourier Transform",
            "definition": "f̂(ξ) = ∫f(x)e^(ixξ)dx, mapping a function to its frequency spectrum"
          },
          {
            "term": "Inverse Fourier Transform",
            "definition": "f(x) = (1/2π)∫f̂(ξ)e^(-ixξ)dξ, recovering the original function"
          },
          {
            "term": "Convolution Theorem",
            "definition": "ℱ{f * g} = ℱ{f} · ℱ{g}, convolution becomes multiplication in frequency domain"
          },
          {
            "term": "Fast Fourier Transform (FFT)",
            "definition": "Divide-and-conquer algorithm computing DFT in O(N log N) instead of O(N²)"
          },
          {
            "term": "Fourier Neural Operator",
            "definition": "Neural network architecture that learns operators in Fourier space for solving PDEs"
          }
        ],

        "about": [
          {"name": "Fourier Transform", "description": "Integral transform mapping functions to frequency spectra"},
          {"name": "Convolution Theorem", "description": "Fundamental connection between convolution and multiplication"},
          {"name": "FFT", "description": "Efficient algorithm enabling real-time signal processing"},
          {"name": "Fourier Neural Operators", "description": "ML architecture for learning solution operators to PDEs"}
        ]
        },
        {
        "id": "calc-16",
        "part": 16,
        "title": "Foundations of Analysis: Metric Spaces",
        "url": "metric_space.html",
        "icon": "(X, d)",
        "keywords": [
          "Metric Space",
          "Distance",
          "Isolated Points",
          "Accumulation Points",
          "Nearest Points",
          "Boundary",
          "Interior",
          "Closure",
          "Open and Closed Sets",
          "Topology",
          "Completeness",
          "Open Balls",
          "Closed Balls",
          "Convexity"
        ],
        "badges": [],
        "prereqs": ["linalg-10", "calc-9", "ml-3"],
        "mapCoords": {
          "q": -4, 
          "r": -3
        },
        "topicGroup": "analysis",
        "tesseraMessage": "Metric spaces abstract the notion of distance. The gateway to topology!",

        "headline": "Metric Spaces: The Foundation for Analysis and Optimization",
        "description": "Formalize the structure of metric spaces, covering distance, boundary, open and closed sets, topology, and completeness—the foundations ensuring optimization algorithms converge to valid solutions.",
        "abstract": "Metric spaces provide the rigorous framework for understanding distance, convergence, and completeness. These concepts formalize exactly when iterative algorithms like gradient descent converge to valid solutions, extending intuition from Euclidean space to function spaces and beyond.",
        "datePublished": "2026-01-22",
        "dateModified": "2026-01-31",

        "teaches": [
          "Formal definition of metric spaces and the three metric axioms",
          "Distance from points to sets and between sets",
          "Isolated points, accumulation points, and nearest points",
          "Boundary, interior, closure, and exterior of sets",
          "Open and closed subsets of metric spaces",
          "Topology determined by a metric",
          "Complete metric spaces and their importance for optimization",
          "Open and closed balls as building blocks of topology",
          "Convex sets in normed linear spaces"
        ],

        "competencyRequired": [
          "Norms and normed vector spaces",
          "Basic set theory notation",
          "Infimum and supremum"
        ],

        "sections": [
          {"id": "intro", "name": "Introduction"},
          {"id": "metric", "name": "Metric Space"},
          {"id": "dist", "name": "Distance"},
          {"id": "bound", "name": "Boundary"},
          {"id": "opcl", "name": "Open & Closed Sets"},
          {"id": "ball", "name": "Balls"}
        ],

        "definitions": [
          {
            "term": "Metric Space",
            "definition": "A set X with distance function d: X×X → ℝ satisfying positivity, symmetry, and triangle inequality"
          },
          {
            "term": "Open Set",
            "definition": "A set equal to its interior; every point has a neighborhood contained in the set"
          },
          {
            "term": "Closed Set",
            "definition": "A set containing all its boundary points; complement of an open set"
          },
          {
            "term": "Complete Metric Space",
            "definition": "A space where X is closed in every metric superspace (equivalently, every Cauchy sequence converges)"
          },
          {
            "term": "Open Ball",
            "definition": "B[x; r) = {y ∈ X | d(x,y) < r}, all points within distance r of center x"
          },
          {
            "term": "Topology",
            "definition": "The collection of open subsets determined by the metric"
          }
        ],

        "about": [
          {"name": "Metric Spaces", "description": "Abstract spaces with well-defined notion of distance"},
          {"name": "Completeness", "description": "Property ensuring limits of Cauchy sequences exist in the space"},
          {"name": "Topology", "description": "Structure determined by which sets are open"},
          {"name": "Convexity", "description": "Sets containing all line segments between their points"}
        ]
        },
        {
          "id": "calc-17",
          "part": 17,
          "title": "Convergence & Boundedness",
          "url": "limit_convergence.html",
          "icon": "tailₘ(xₙ)",
          "keywords": [
            "Tails",
            "Convergence",
            "Limits",
            "Cauchy Sequences",
            "Boundedness",
            "Diameter",
            "Total Boundedness",
            "Completeness",
            "Sequences",
            "Metric Spaces",
            "Gradient Descent Convergence",
            "Iterative Algorithms"
          ],
          "badges": [],
          "prereqs": ["calc-16"],
          "mapCoords": {
            "q": -4, 
            "r": -2
          },
          "topicGroup": "analysis",
          "tesseraMessage": "Convergence is when the 'tail' of a sequence settles into an arbitrarily small ball. It's the structural anchor for every iterative process!",

          "headline": "Convergence & Boundedness in Metric Spaces",
          "description": "Formalize convergence of sequences in metric spaces, establish the relationship between Cauchy sequences and completeness, and explore boundedness—the theoretical foundation for analyzing iterative algorithms.",
          "abstract": "This chapter shifts from pointwise ε-N definitions to structural topology, defining convergence through the behavior of sequence tails settling into arbitrarily small neighborhoods. Essential for understanding why optimization algorithms like gradient descent converge.",
          "datePublished": "2026-01-26",
          "dateModified": "2026-01-31",

          "teaches": [
            "Formal definition of sequence convergence in metric spaces",
            "Tail-based approach to convergence vs pointwise ε-N definitions",
            "Uniqueness of limits in metric spaces",
            "Cauchy sequences and the Cauchy criterion",
            "Equivalence of completeness definitions",
            "Bounded sets, bounded sequences, and diameter",
            "Total boundedness and covering numbers",
            "Applications to gradient descent, weight clipping, and regularization"
          ],

          "competencyRequired": [
            "Metric spaces and distance functions",
            "Open balls and neighborhoods",
            "Basic sequence notation"
          ],

          "sections": [
            {"id": "intro", "name": "Introduction"},
            {"id": "convergence", "name": "Convergence of Sequences"},
            {"id": "cauchy", "name": "Cauchy Sequences"},
            {"id": "bounds", "name": "Boundedness"},
            {"id": "complete-revisit", "name": "Completeness Revisited"}
          ],

          "definitions": [
            {
              "term": "Tail of a Sequence",
              "definition": "For sequence {xₙ}, the tail from m is tail_m = {xₙ | n ≥ m}"
            },
            {
              "term": "Convergent Sequence",
              "definition": "A sequence converges to limit L if for every ε > 0, some tail is contained in the ε-ball around L"
            },
            {
              "term": "Cauchy Sequence",
              "definition": "A sequence where for every ε > 0, some tail has diameter less than ε"
            },
            {
              "term": "Complete Metric Space",
              "definition": "A space where every Cauchy sequence converges to a point within the space"
            },
            {
              "term": "Bounded Set",
              "definition": "A set contained in some ball of finite radius"
            },
            {
              "term": "Totally Bounded",
              "definition": "A set coverable by finitely many balls of any given radius ε"
            }
          ],

          "about": [
            {"name": "Convergence", "description": "The property of a sequence approaching a limit"},
            {"name": "Cauchy Sequences", "description": "Sequences where terms become arbitrarily close to each other"},
            {"name": "Completeness", "description": "A metric space where every Cauchy sequence converges"},
            {"name": "Boundedness", "description": "Sets contained within a ball of finite radius"},
            {"name": "Total Boundedness", "description": "Sets coverable by finitely many balls of any given radius"}
          ]
        },
        {
        "id": "calc-18",
        "part": 18,
        "title": "Continuity",
        "url": "continuity.html",
        "icon": "ε-δ",
        "keywords": [
          "Continuity",
          "Uniform Continuity",
          "Lipschitz Continuity",
          "Lipschitz Constant",
          "Contraction",
          "Strong Contraction",
          "Isometry",
          "Homeomorphism",
          "Metric Spaces"
        ],
        "badges": [],
        "prereqs": ["calc-17"],
        "mapCoords": {
          "q": -5, 
          "r": -2
        },
        "topicGroup": "analysis",
        "tesseraMessage": "Lipschitz constants are the guardrails of AI, ensuring our gradients don't explode and algorithms actually converge.",

        "headline": "Continuity: From ε-δ to Lipschitz Bounds",
        "description": "Formalize continuous and uniformly continuous functions between metric spaces, explore Lipschitz continuity, and understand why these properties ensure optimization algorithms behave predictably.",
        "abstract": "Continuity ensures small input changes produce small output changes. This chapter develops the hierarchy from basic continuity through uniform continuity to Lipschitz continuity—each providing stronger guarantees essential for proving convergence of gradient descent, GAN stability, and fixed-point iterations.",
        "datePublished": "2026-01-27",
        "dateModified": "2026-01-31",

        "teaches": [
          "Definition of continuity via open sets in metric spaces",
          "ε-δ characterization of continuity",
          "Sequential characterization of continuity",
          "Topological characterization via preimages of open sets",
          "Uniform continuity and global δ guarantees",
          "Lipschitz continuous functions and Lipschitz constants",
          "Contractions (L ≤ 1) and strong contractions (L < 1)",
          "Isometries as distance-preserving maps",
          "Hierarchy: Lipschitz ⟹ Uniformly Continuous ⟹ Continuous"
        ],

        "competencyRequired": [
          "Metric spaces and open sets",
          "Convergence of sequences",
          "Open balls and neighborhoods"
        ],

        "sections": [
          {"id": "intro", "name": "Introduction"},
          {"id": "continuity", "name": "Continuity"},
          {"id": "uniform", "name": "Uniform Continuity"},
          {"id": "lipschitz", "name": "Lipschitz Continuity"}
        ],

        "definitions": [
          {
            "term": "Continuous Function",
            "definition": "f: X → Y where for each open V ⊂ Y with f(z) ∈ V, there exists open U ⊂ X with z ∈ U and f(U) ⊆ V"
          },
          {
            "term": "Uniformly Continuous",
            "definition": "For every ε > 0, there exists a single δ > 0 such that d(x,z) < δ implies e(f(x),f(z)) < ε for ALL x,z"
          },
          {
            "term": "Lipschitz Continuous",
            "definition": "e(f(a),f(b)) ≤ L·d(a,b) for all a,b ∈ X, where L is the Lipschitz constant"
          },
          {
            "term": "Contraction",
            "definition": "A Lipschitz map with constant L ≤ 1; never increases distances"
          },
          {
            "term": "Strong Contraction",
            "definition": "A Lipschitz map with constant L < 1; strictly decreases distances"
          },
          {
            "term": "Isometry",
            "definition": "A map preserving distances exactly: e(φ(a),φ(b)) = d(a,b)"
          }
        ],

        "about": [
          {"name": "Continuity", "description": "Preserving nearness between domain and range"},
          {"name": "Uniform Continuity", "description": "Global stability with position-independent δ"},
          {"name": "Lipschitz Continuity", "description": "Bounded rate of change with explicit constant"},
          {"name": "Contractions", "description": "Maps fundamental for fixed-point theorems and RL convergence"}
        ]
        },
        {
          "id": "calc-19",
          "part": 19,
          "title": "Completeness",
          "url": "completeness.html",
          "icon": "X̂",
          "keywords": [
            "Completeness",
            "Complete Metric Space",
            "Cauchy Sequence",
            "Banach's Fixed-Point Theorem",
            "Contraction Mapping",
            "Completion of Metric Spaces",
            "Cantor's Intersection Theorem",
            "Convergence Rate"
          ],
          "badges": [],
          "prereqs": [
            "calc-18"
          ],
          "mapCoords": {
            "q": -6,
            "r": -1
          },
          "topicGroup": "analysis",
          "tesseraMessage": "Completeness ensures there are 'no holes' in our space—essential for knowing our algorithms will actually land somewhere!",
          "headline": "Completeness: The Theoretical Backbone of Convergence",
          "description": "Develop the full theory of complete metric spaces and Banach's Fixed-Point Theorem to guarantee the convergence of iterative algorithms.",
          "abstract": "Completeness is the property that ensures every Cauchy sequence converges within the space. We explore the construction of space completions, Cantor's Intersection Theorem, and the Banach Fixed-Point Theorem. This provides the rigorous foundation for why iterative methods like Newton's method and Value Iteration in RL are guaranteed to reach a solution.",
          "datePublished": "2026-01-31",
          "dateModified": "2026-01-31",
          "teaches": [
            "Complete metric spaces and the Cauchy criterion",
            "Completeness of R^n and other standard spaces",
            "Completion of an incomplete metric space",
            "Contraction mappings and the Banach's Fixed-Point Theorem",
            "Convergence rate estimates for iterative methods",
            "Cantor's Intersection Theorem",
            "Applications to optimization algorithms"
          ],
          "competencyRequired": [
            "Metric spaces and distance axioms",
            "Convergence and limits",
            "Basic set theory"
          ],
          "sections": [
            { "id": "intro", "name": "Introduction" },
            { "id": "complete", "name": "Complete Spaces" },
            { "id": "completion", "name": "Completion" },
            { "id": "cantor", "name": "Cantor's Theorem" },
            { "id": "banach", "name": "Banach's Fixed-Point" }
          ],
          "definitions": [
            {
              "term": "Complete Metric Space",
              "definition": "A space where every Cauchy sequence converges to an element in the space"
            },
            {
              "term": "Contraction Mapping",
              "definition": "A function f such that d(f(x), f(y)) ≤ kd(x, y) for some 0 ≤ k < 1"
            },
            {
              "term": "Banach Fixed-Point Theorem",
              "definition": "In a complete metric space, a contraction mapping has a unique fixed point x* such that f(x*) = x*"
            },
            {
              "term": "Cauchy Sequence",
              "definition": "A sequence where elements become arbitrarily close to each other as the index increases"
            },
            {
              "term": "Completion",
              "definition": "The smallest complete metric space containing a given metric space as a dense subset"
            }
          ],
          "about": [
            { "name": "Completeness", "description": "Property ensuring convergence of all Cauchy sequences" },
            { "name": "Banach's Fixed-Point Theorem", "description": "Foundational result for iterative algorithm convergence" },
            { "name": "Contraction Mappings", "description": "Functions that reduce distances between points" },
            { "name": "Completion of Metric Spaces", "description": "Filling the 'holes' in a metric space" },
            { "name": "Cantor's Intersection Theorem", "description": "Characterization of completeness via nested sets" }
          ]
        },
        {
          "id": "calc-20",
          "part": 20,
          "title": "Connectedness",
          "url": "connectedness.html",
          "icon": "↝",
          "keywords": [
            "Connectedness",
            "Path-Connectedness",
            "Connected Components",
            "Separation",
            "Intermediate Value Theorem",
            "DBSCAN",
            "Spectral Clustering",
            "Mode Connectivity",
            "Optimization",
            "Manifold"
          ],
          "badges": [],
          "prereqs": [
            "calc-19"
          ],
          "mapCoords": {
            "q": -5,
            "r": -1
          },
          "topicGroup": "analysis",
          "tesseraMessage": "If your feasible region isn't connected, you can't just 'walk' to the best solution. Topology is the ultimate map for our algorithms!",
          "headline": "Connectedness: The Topology of Feasible Paths",
          "description": "Distinguish between spaces that are 'in one piece' versus 'fragmented,' and understand why this property underpins the Intermediate Value Theorem and modern clustering algorithms.",
          "abstract": "Connectedness is the topological guarantee that a space cannot be split into isolated parts. This chapter explores the rigorous definitions of connected and path-connected spaces, establishing the mathematical foundation for the Intermediate Value Theorem. We further bridge these concepts to Computer Science by analyzing how connectedness defines clusters in DBSCAN and ensures parameter reachability in Neural Network Mode Connectivity.",
          "datePublished": "2026-02-01",
          "dateModified": "2026-02-01",
          "teaches": [
            "Definition of connected sets via separation",
            "Criteria for connectedness (Clopen sets, Boundaries)",
            "Path-connectedness and its relationship to connectedness",
            "Connected components and their role in global structure",
            "The Intermediate Value Theorem in metric spaces",
            "Preservation of connectedness under continuous maps",
            "Applications: DBSCAN, Spectral Clustering, and Mode Connectivity"
          ],
          "competencyRequired": [
            "Metric spaces and open sets",
            "Continuous functions",
            "Basic set theory"
          ],
          "sections": [
            { "id": "intro", "name": "Introduction" },
            { "id": "connected", "name": "Connected Sets" },
            { "id": "ivt", "name": "Intermediate Value Theorem" },
            { "id": "components", "name": "Connected Components" },
            { "id": "path", "name": "Path-Connectedness" }
          ],
          "definitions": [
            {
              "term": "Connected Metric Space",
              "definition": "A space X that cannot be expressed as the union of two disjoint non-empty open subsets (a separation)"
            },
            {
              "term": "Separation",
              "definition": "A pair of disjoint non-empty open sets U, V such that X = U ∪ V"
            },
            {
              "term": "Path",
              "definition": "A continuous function f: [0, 1] → X joining two points"
            },
            {
              "term": "Pathwise Connected",
              "definition": "A space where every pair of points can be joined by a continuous path"
            },
            {
              "term": "Connected Component",
              "definition": "A maximal connected subset of a metric space"
            }
          ],
          "about": [
            { "name": "Connectedness", "description": "The property of being 'all in one piece' without separation" },
            { "name": "Path-Connectedness", "description": "Reachability between points via continuous curves" },
            { "name": "IVT", "description": "Topological guarantee that continuous functions cannot skip values" },
            { "name": "Clustering", "description": "Identifying connected components in data manifolds (DBSCAN)" },
            { "name": "Mode Connectivity", "description": "Navigating low-loss paths between neural network solutions" }
          ]
        },
        {
          "id": "calc-21",
          "part": 21,
          "title": "Compactness",
          "url": "compactness.html",
          "icon": "C",
          "keywords": [
            "Compactness", "Open Cover", "Finite Subcover", "Heine-Borel Theorem", "Extreme Value Theorem",
            "Sequential Compactness", "Total Boundedness", "Uniform Continuity", "Bolzano-Weierstrass Theorem"
          ],
          "badges": [],
          "prereqs": ["calc-20"],
          "mapCoords": { 
            "q": -5,
            "r": 0
           },
          "topicGroup": "analysis",
          "tesseraMessage": "Compactness is like a 'guarantee of existence.' It ensures that if you're looking for an optimal solution in a bounded search space, it's actually there to be found!",
          "headline": "Compactness: The Topological Guarantee of Optima",
          "description": "Generalize the essential properties of closed and bounded intervals to metric spaces. Understand why compactness is the critical bridge between continuity and the existence of global maximums and minimums.",
          "abstract": "Compactness captures the idea of a 'finite-like' space within an infinite setting. By formalizing this through open covers and finite subcovers, we establish the Heine-Borel theorem and the Extreme Value Theorem (EVT). This chapter provides the theoretical bedrock for convergence in machine learning, ensuring that optimization algorithms operating on closed and bounded feasible regions possess well-defined optimal solutions.",
          "datePublished": "2026-02-02",
          "dateModified": "2026-02-02",
          "teaches": [
            "Formal definition of compactness via the Finite Subcover property",
            "The 10 equivalent criteria of compactness in metric spaces and their CS applications",
            "The operational significance of Sequential Compactness and Bolzano-Weierstrass",
            "Heine-Borel Theorem: Characterizing compactness in R^n as closed and bounded",
            "The Extreme Value Theorem (EVT): Guaranteeing the existence of global extrema",
            "Preservation of compactness under continuous maps",
            "The Heine-Cantor Theorem: Uniform continuity and its role in optimization stability"
          ],
          "competencyRequired": [
            "Metric spaces and open/closed sets",
            "Completeness and internal stability of sequences",
            "Continuity of real-valued functions"
          ],
          "sections": [
            { "id": "intro", "name": "Introduction" },
            { "id": "covers", "name": "Open Covers and Compactness" },
            { "id": "heine-borel", "name": "The Heine-Borel Theorem" },
            { "id": "functions", "name": "Continuous Functions on Compact Sets" }
          ]
        }
      ],
      "reservedSlots": [
        {
          "q": -5,
          "r": -4
        },
        {
          "q": -6,
          "r": 0
        },
        {
          "q": -7,
          "r": 1
        },
        {
          "q": -6,
          "r": 1
        }
      ]
    },
    "III": {
      "id": "probability",
      "title": "Probability & Statistics",
      "shortTitle": "Probability",
      "description": "Explore fundamental concepts of probability and statistics essential for machine learning, including probability theory, random variables, distributions, Bayesian inference, entropy, and the Fisher Information Matrix.",
      "tagline": "The Mathematics of Uncertainty and Inference",
      "icon": "fa-dice",
      "indexUrl": "Mathematics/Probability/probability.html",
      "baseUrl": "Mathematics/Probability/",
      "parts": [
        {
          "id": "prob-1",
          "part": 1,
          "title": "Basic Probability Ideas",
          "url": "basic.html",
          "icon": "p",
          "keywords": [
            "Probability",
            "Sample Space",
            "Events",
            "Mutually Exclusive",
            "Permutation",
            "Combinations",
            "Conditional Probability",
            "Independent Events",
            "Law of Total Probability",
            "Bayes' Theorem"
          ],
          "badges": [],
          "prereqs": [],
          "mapCoords": {
            "q": 0,
            "r": -1
          },
          "topicGroup": "foundations",
          "tesseraMessage": "Probability is the language of uncertainty. Bayes' theorem is your new best friend!",

          "headline": "Basic Probability Ideas: The Foundation of Statistical Reasoning",
          "description": "Learn about basic probability ideas such as conditional probability and Bayes' theorem.",
          "abstract": "Probability theory provides the mathematical framework for quantifying uncertainty. Starting from sample spaces and events, we build up to conditional probability, independence, and the powerful Bayes' theorem. These concepts form the foundation for all statistical inference and machine learning.",
          "datePublished": "2024-11-25",
          "dateModified": "2026-01-31",

          "teaches": [
            "Sample spaces and events as subsets",
            "Probability axioms and basic properties",
            "Counting principles: permutations and combinations",
            "Conditional probability and its interpretation",
            "Independence of events",
            "Law of Total Probability for partitioned sample spaces",
            "Bayes' theorem and inverse probability",
            "Applications to classification and diagnosis"
          ],

          "competencyRequired": [
            "Basic set theory notation",
            "Combinatorics fundamentals",
            "Algebraic manipulation"
          ],

          "sections": [
            {"id": "pro", "name": "Probability"},
            {"id": "conditional", "name": "Conditional Probability"},
            {"id": "total", "name": "Law of Total Probability"},
            {"id": "bayes", "name": "Bayes' Theorem"}
          ],

          "definitions": [
            {
              "term": "Sample Space",
              "definition": "The collection S of every possible outcome of an experiment"
            },
            {
              "term": "Event",
              "definition": "A subset A ⊆ S of outcomes from the sample space"
            },
            {
              "term": "Conditional Probability",
              "definition": "P(A|B) = P(A ∩ B)/P(B), the probability of A given B has occurred"
            },
            {
              "term": "Independence",
              "definition": "Events A and B are independent if P(A ∩ B) = P(A)P(B)"
            },
            {
              "term": "Bayes' Theorem",
              "definition": "P(Bⱼ|A) = P(A|Bⱼ)P(Bⱼ) / Σᵢ P(A|Bᵢ)P(Bᵢ)"
            }
          ],

          "about": [
            {"name": "Probability Theory", "description": "Mathematical framework for quantifying uncertainty"},
            {"name": "Conditional Probability", "description": "Updating probabilities given new information"},
            {"name": "Bayes' Theorem", "description": "Foundation of Bayesian inference and classification"},
            {"name": "Combinatorics", "description": "Counting techniques for computing probabilities"}
          ]
        },
        {
          "id": "prob-2",
          "part": 2,
          "title": "Random Variables",
          "url": "random_variables.html",
          "icon": "X",
          "keywords": [
            "Discrete Random Variables",
            "Continuous Random Variables",
            "Probability Mass Function (p.m.f.)",
            "Probability Density Function (p.d.f.)",
            "Cumulative Distribution Function(c.d.f.)",
            "Expected Value",
            "Variance",
            "Standard Deviation"
          ],
          "badges": [],
          "prereqs": [
            "prob-1"
          ],
          "mapCoords": {
            "q": 0,
            "r": -2
          },
          "topicGroup": "foundations",
          "tesseraMessage": "Random variables turn randomness into mathematics we can compute with.",

          "headline": "Random Variables: Bridging Probability and Numerical Analysis",
          "description": "Learn about random variables, expected values, variance.",
          "abstract": "Random variables map outcomes to numerical values, enabling mathematical analysis of uncertainty. We distinguish discrete variables (with probability mass functions) from continuous variables (with probability density functions), and introduce the fundamental concepts of expected value and variance that characterize distributions.",
          "datePublished": "2024-11-25",
          "dateModified": "2026-01-31",

          "teaches": [
            "Random variables as functions from sample spaces to real numbers",
            "Discrete random variables and probability mass functions",
            "Continuous random variables and probability density functions",
            "Cumulative distribution functions and their properties",
            "Expected value as weighted average",
            "Variance and standard deviation",
            "Linear transformations of random variables",
            "Fundamental Theorem of Calculus connection to CDFs"
          ],

          "competencyRequired": [
            "Basic probability concepts",
            "Integration and summation",
            "Function concepts"
          ],

          "sections": [
            {"id": "rv", "name": "Random Variables"},
            {"id": "exp", "name": "Expected Value"},
            {"id": "var", "name": "Variance"}
          ],

          "definitions": [
            {
              "term": "Random Variable",
              "definition": "A function mapping each outcome in sample space to a numerical value"
            },
            {
              "term": "Probability Mass Function",
              "definition": "f(x) = P(X = x) for discrete random variables"
            },
            {
              "term": "Probability Density Function",
              "definition": "f(x) where P(a ≤ X ≤ b) = ∫ₐᵇ f(x)dx for continuous variables"
            },
            {
              "term": "Expected Value",
              "definition": "E[X] = μ = Σₓ x·f(x) (discrete) or ∫ x·f(x)dx (continuous)"
            },
            {
              "term": "Variance",
              "definition": "Var(X) = E[(X - μ)²] = E[X²] - μ²"
            }
          ],

          "about": [
            {"name": "Random Variables", "description": "Numerical representation of random outcomes"},
            {"name": "Distribution Functions", "description": "PMF, PDF, and CDF characterizing random variables"},
            {"name": "Expected Value", "description": "Center of mass of a probability distribution"},
            {"name": "Variance", "description": "Measure of spread around the mean"}
          ]
        },
        {
          "id": "prob-3",
          "part": 3,
          "title": "Gamma & Beta Distribution",
          "url": "gamma.html",
          "icon": "Γ",
          "keywords": [
            "Gamma Distribution",
            "Gamma Function",
            "Exponential Distribution",
            "Beta Function",
            "Beta Distribution",
            "Uniform Distribution"
          ],
          "badges": [
            "interactive"
          ],
          "prereqs": [
            "prob-2"
          ],
          "mapCoords": {
            "q": 0,
            "r": -3
          },
          "topicGroup": "distributions",
          "tesseraMessage": "Gamma and Beta distributions model waiting times and proportions beautifully.",

          "headline": "Gamma & Beta Distributions: Special Functions Meet Probability",
          "description": "Learn about Gamma and Beta distribution.",
          "abstract": "The Gamma and Beta distributions are fundamental continuous distributions built on special functions. The Gamma distribution generalizes the exponential distribution and models waiting times, while the Beta distribution models probabilities and proportions on [0,1]. Both play crucial roles as conjugate priors in Bayesian inference.",
          "datePublished": "2024-11-25",
          "dateModified": "2026-01-31",

          "teaches": [
            "Gamma function as generalized factorial",
            "Gamma distribution parameterization and properties",
            "Exponential distribution as special case",
            "Beta function and its relationship to Gamma",
            "Beta distribution for modeling proportions",
            "Uniform distribution as Beta(1,1)",
            "Shape and rate parameters interpretation",
            "Integration by parts for Gamma function"
          ],

          "competencyRequired": [
            "Random variables and PDFs",
            "Integration techniques",
            "Special integrals"
          ],

          "sections": [
            {"id": "gamma_f", "name": "Gamma Function"},
            {"id": "gamma_d", "name": "Gamma Distribution"},
            {"id": "beta_f", "name": "Beta Function"},
            {"id": "beta_d", "name": "Beta Distribution"},
            {"id": "demo", "name": "Interactive Visualization"}
          ],

          "definitions": [
            {
              "term": "Gamma Function",
              "definition": "Γ(z) = ∫₀^∞ t^(z-1)e^(-t)dt, with Γ(n+1) = n! for positive integers"
            },
            {
              "term": "Gamma Distribution",
              "definition": "f(x|α,β) ∝ x^(α-1)e^(-βx) for x > 0 with shape α and rate β"
            },
            {
              "term": "Beta Function",
              "definition": "B(a,b) = Γ(a)Γ(b)/Γ(a+b) = ∫₀¹ t^(a-1)(1-t)^(b-1)dt"
            },
            {
              "term": "Beta Distribution",
              "definition": "f(x|a,b) = x^(a-1)(1-x)^(b-1)/B(a,b) for x ∈ [0,1]"
            },
            {
              "term": "Exponential Distribution",
              "definition": "Gamma(1, λ), models time between events in Poisson process"
            }
          ],

          "about": [
            {"name": "Gamma Function", "description": "Extension of factorial to real and complex numbers"},
            {"name": "Gamma Distribution", "description": "Models waiting times and sums of exponentials"},
            {"name": "Beta Distribution", "description": "Natural distribution for probabilities on [0,1]"},
            {"name": "Special Functions", "description": "Mathematical functions with important properties"}
          ]
        },
        {
          "id": "prob-4",
          "part": 4,
          "title": "Normal (Gaussian) Distribution",
          "url": "gaussian.html",
          "icon": "𝒩",
          "keywords": [
            "Gaussian Function",
            "Error Function",
            "Gaussian Integral",
            "Normal(Gaussian) Distribution",
            "Standard Normal Distribution",
            "Independent and Identically Distributed(i.i.d.)",
            "Random Sample",
            "Sample Mean",
            "Sample Variance",
            "Central Limit Theorem"
          ],
          "badges": [],
          "prereqs": [
            "prob-2"
          ],
          "mapCoords": {
            "q": 1,
            "r": -4
          },
          "topicGroup": "distributions",
          "tesseraMessage": "The Gaussian is everywhere! Central Limit Theorem explains why.",

          "headline": "Normal Distribution: The Bell Curve at the Heart of Statistics",
          "description": "Learn about normal distribution and central limit theorem.",
          "abstract": "The Normal (Gaussian) distribution is the most important probability distribution in statistics and machine learning. Starting from the Gaussian function and its integral, we derive the standard normal distribution and explore why it appears everywhere through the Central Limit Theorem.",
          "datePublished": "2024-11-25",
          "dateModified": "2026-01-31",

          "teaches": [
            "Gaussian function and its parameterization",
            "Gaussian integral evaluation technique",
            "Error function for Gaussian CDF",
            "Normal distribution properties",
            "Standardization and z-scores",
            "Sum of independent normal variables",
            "Sample mean and sample variance",
            "Central Limit Theorem statement and implications"
          ],

          "competencyRequired": [
            "Random variables and distributions",
            "Integration techniques",
            "Exponential functions"
          ],

          "sections": [
            {"id": "gaussian_f", "name": "Gaussian Function"},
            {"id": "normal", "name": "Normal(Gaussian) Distribution"},
            {"id": "clt", "name": "Central Limit Theorem"}
          ],

          "definitions": [
            {
              "term": "Gaussian Function",
              "definition": "f(x) = ae^(-(x-b)²/2c²) with parameters controlling height, center, and width"
            },
            {
              "term": "Normal Distribution",
              "definition": "X ~ N(μ,σ²) with PDF f(x) = (1/σ√(2π))exp(-(x-μ)²/2σ²)"
            },
            {
              "term": "Standard Normal",
              "definition": "Z ~ N(0,1), the normal distribution with mean 0 and variance 1"
            },
            {
              "term": "Error Function",
              "definition": "erf(z) = (2/√π)∫₀^z e^(-t²)dt, used to express Gaussian CDF"
            },
            {
              "term": "Central Limit Theorem",
              "definition": "Sample mean of n iid variables approaches N(μ, σ²/n) as n → ∞"
            }
          ],

          "about": [
            {"name": "Normal Distribution", "description": "The ubiquitous bell curve in statistics"},
            {"name": "Gaussian Integral", "description": "∫₋∞^∞ e^(-x²)dx = √π, fundamental improper integral"},
            {"name": "Central Limit Theorem", "description": "Explains universality of normal distribution"},
            {"name": "Standardization", "description": "Converting any normal to standard normal"}
          ]
        },
        {
          "id": "prob-5",
          "part": 5,
          "title": "Student's t-Distribution",
          "url": "student.html",
          "icon": "t",
          "keywords": [
            "Student's t-Distribution",
            "Degrees of Freedom",
            "Cauchy Distribution",
            "Half Cauchy Distribution",
            "Laplace Distribution",
            "Double Sided Exponential Distribution"
          ],
          "badges": [],
          "prereqs": [
            "prob-4"
          ],
          "mapCoords": {
            "q": 0,
            "r": -4
          },
          "topicGroup": "distributions",
          "tesseraMessage": "Student's t handles small samples with grace. Heavy tails, robust inference.",

          "headline": "Student's t-Distribution: Robust Statistics with Heavy Tails",
          "description": "Learn about Student's t-distribution, Cauchy distribution, and Laplace distribution.",
          "abstract": "The Student's t-distribution provides a robust alternative to the normal distribution with heavier tails that better handle outliers. We explore its properties, special cases like the Cauchy distribution, and related distributions like the Laplace that are essential for robust statistical modeling.",
          "datePublished": "2024-11-25",
          "dateModified": "2026-01-31",

          "teaches": [
            "Student's t-distribution and degrees of freedom",
            "Heavy tails and robustness to outliers",
            "Cauchy distribution as t with ν=1",
            "Non-existence of mean for Cauchy",
            "Half Cauchy distribution for positive parameters",
            "Laplace (double-sided exponential) distribution",
            "Applications in robust regression",
            "Convergence to normal as ν increases"
          ],

          "competencyRequired": [
            "Normal distribution",
            "PDFs and moments",
            "Improper integrals"
          ],

          "sections": [
            {"id": "student", "name": "Student's t-Distribution"},
            {"id": "cauchy", "name": "Cauchy Distribution"},
            {"id": "laplace", "name": "Laplace Distribution"}
          ],

          "definitions": [
            {
              "term": "Student's t-Distribution",
              "definition": "f(y|μ,σ²,ν) ∝ [1 + (1/ν)((y-μ)/σ)²]^(-(ν+1)/2) with ν degrees of freedom"
            },
            {
              "term": "Degrees of Freedom",
              "definition": "Parameter ν controlling tail heaviness; larger ν approaches normal"
            },
            {
              "term": "Cauchy Distribution",
              "definition": "t-distribution with ν=1; f(x|μ,γ) = 1/(γπ[1+((x-μ)/γ)²])"
            },
            {
              "term": "Laplace Distribution",
              "definition": "f(y|μ,b) = (1/2b)exp(-|y-μ|/b), double-sided exponential"
            },
            {
              "term": "Heavy Tails",
              "definition": "More probability mass in tails than normal distribution"
            }
          ],

          "about": [
            {"name": "Student's t-Distribution", "description": "Robust alternative to normal for small samples"},
            {"name": "Cauchy Distribution", "description": "Extreme heavy-tailed distribution without finite mean"},
            {"name": "Laplace Distribution", "description": "Double exponential used in robust regression"},
            {"name": "Robustness", "description": "Resistance to outlier influence"}
          ]
        },
        {
          "id": "prob-6",
          "part": 6,
          "title": "Covariance",
          "url": "covariance.html",
          "icon": "Cov",
          "keywords": [
            "Covariance",
            "Covariance Matrix",
            "Total Variance",
            "Principal Component",
            "Principal Component Analysis(PCA)"
          ],
          "badges": [
            "code"
          ],
          "prereqs": [
            "prob-4",
            "linalg-6"
          ],
          "mapCoords": {
            "q": 2,
            "r": -4
          },
          "topicGroup": "multivariate",
          "tesseraMessage": "Covariance reveals how variables move together. PCA lives here!",

          "headline": "Covariance Matrix: Understanding Multivariate Relationships",
          "description": "Learn about covariance matrix and principal component analysis(PCA).",
          "abstract": "Moving to multivariate statistics, the covariance matrix captures how multiple variables vary together. We explore its properties, connection to total variance, and its central role in Principal Component Analysis (PCA), the foundational dimensionality reduction technique.",
          "datePublished": "2024-11-25",
          "dateModified": "2026-01-31",

          "teaches": [
            "Covariance between two random variables",
            "Covariance matrix for random vectors",
            "Population vs sample covariance matrix",
            "Properties of positive semi-definite matrices",
            "Total variance and trace",
            "Principal Component Analysis basics",
            "SVD connection to PCA",
            "Eigendecomposition of covariance matrix"
          ],

          "competencyRequired": [
            "Random variables and variance",
            "Matrix operations",
            "Eigenvalues and eigenvectors"
          ],

          "sections": [
            {"id": "covariance", "name": "Covariance Matrix"},
            {"id": "pca", "name": "Principal Component Analysis (PCA)"},
            {"id": "svd", "name": "PCA with Singular Value Decomposition(SVD)"}
          ],

          "definitions": [
            {
              "term": "Covariance",
              "definition": "Cov[X,Y] = E[(X - E[X])(Y - E[Y])], measures joint variation"
            },
            {
              "term": "Covariance Matrix",
              "definition": "Σ = E[(x - E[x])(x - E[x])ᵀ], symmetric positive semi-definite"
            },
            {
              "term": "Sample Covariance Matrix",
              "definition": "S = (1/(n-1))(X - X̄)ᵀ(X - X̄), unbiased estimator of Σ"
            },
            {
              "term": "Principal Components",
              "definition": "Eigenvectors of covariance matrix, directions of maximum variance"
            },
            {
              "term": "Total Variance",
              "definition": "tr(Σ) = Σᵢ Var(Xᵢ), sum of all diagonal elements"
            }
          ],

          "about": [
            {"name": "Covariance Matrix", "description": "Captures pairwise relationships between variables"},
            {"name": "PCA", "description": "Dimensionality reduction via variance maximization"},
            {"name": "SVD", "description": "Computational approach to PCA"},
            {"name": "Multivariate Statistics", "description": "Analysis of vector-valued random variables"}
          ]
        },
        {
          "id": "prob-7",
          "part": 7,
          "title": "Correlation",
          "url": "correlation.html",
          "icon": "r",
          "keywords": [
            "Cross-Covariance Matrix",
            "Auto-Covariance Matrix",
            "Correlation Coefficient",
            "Correlation Matrix"
          ],
          "badges": [],
          "prereqs": [
            "prob-6"
          ],
          "mapCoords": {
            "q": 3,
            "r": -5
          },
          "topicGroup": "multivariate",
          "tesseraMessage": "Correlation normalizes covariance — easier to interpret, same insight.",

          "headline": "Correlation: Standardized Measures of Linear Relationship",
          "description": "Learn about correlation.",
          "abstract": "Correlation standardizes covariance to produce scale-free measures of linear relationship between -1 and 1. We cover cross-covariance for different datasets, the correlation coefficient, and the correlation matrix that enables fair comparison across features with different scales.",
          "datePublished": "2024-11-25",
          "dateModified": "2026-01-31",

          "teaches": [
            "Cross-covariance between different datasets",
            "Auto-covariance as same-dataset covariance",
            "Population correlation coefficient",
            "Sample correlation coefficient",
            "Boundedness proof using Cauchy-Schwarz",
            "Correlation matrix construction",
            "Standardization from covariance to correlation",
            "Applications in feature scaling for PCA"
          ],

          "competencyRequired": [
            "Covariance and covariance matrix",
            "Standard deviation",
            "Cauchy-Schwarz inequality"
          ],

          "sections": [
            {"id": "cross", "name": "Cross-Covariance"},
            {"id": "corr", "name": "Correlation"}
          ],

          "definitions": [
            {
              "term": "Cross-Covariance Matrix",
              "definition": "K_AB = (1/(m-1))(A-Ā)ᵀ(B-B̄) for datasets A and B"
            },
            {
              "term": "Auto-Covariance Matrix",
              "definition": "K_AA, the cross-covariance of dataset with itself (= covariance matrix)"
            },
            {
              "term": "Correlation Coefficient",
              "definition": "ρ_XY = Cov[X,Y]/(σ_X σ_Y), standardized covariance in [-1,1]"
            },
            {
              "term": "Correlation Matrix",
              "definition": "R = (diag(K))^(-1/2) K (diag(K))^(-1/2), standardized covariance matrix"
            },
            {
              "term": "Sample Correlation",
              "definition": "r_xy = Σ(xᵢ-x̄)(yᵢ-ȳ) / [(n-1)s_x s_y]"
            }
          ],

          "about": [
            {"name": "Correlation Coefficient", "description": "Scale-free measure of linear relationship"},
            {"name": "Cross-Covariance", "description": "Relationships between different variable sets"},
            {"name": "Correlation Matrix", "description": "Normalized covariance for fair feature comparison"},
            {"name": "Standardization", "description": "Removing scale effects from covariance"}
          ]
        },
        {
          "id": "prob-8",
          "part": 8,
          "title": "Multivariate Distributions",
          "url": "mvn.html",
          "icon": "Σ",
          "keywords": [
            "Multivariate Normal Distribution (MVN)",
            "Mahalanobis Distance",
            "Bivariate Normal Distribution",
            "Cholesky Decomposition",
            "Dirichlet Distribution",
            "Probability Simplex",
            "Wishart Distribution",
            "Inverse Wishart Distribution"
          ],
          "badges": [],
          "prereqs": [
            "prob-6",
            "linalg-9"
          ],
          "mapCoords": {
            "q": 2,
            "r": -5
          },
          "topicGroup": "multivariate",
          "tesseraMessage": "Multivariate Gaussians are the foundation of Gaussian processes and more.",

          "headline": "Multivariate Distributions: Joint Probability in Higher Dimensions",
          "description": "Learn about multivariate normal distribution, Dirichlet distribution, and Wishart distribution.",
          "abstract": "Multivariate distributions extend probability theory to random vectors. The multivariate normal is the cornerstone of multivariate statistics, while the Dirichlet distribution models probability vectors and the Wishart distribution models covariance matrices—both essential as conjugate priors in Bayesian inference.",
          "datePublished": "2024-11-25",
          "dateModified": "2026-01-31",

          "teaches": [
            "Multivariate normal distribution PDF",
            "Mahalanobis distance and elliptical contours",
            "Bivariate normal special case",
            "Sampling via Cholesky decomposition",
            "Dirichlet distribution on probability simplex",
            "Dirichlet as conjugate prior for multinomial",
            "Wishart distribution for covariance matrices",
            "Inverse Wishart as conjugate prior"
          ],

          "competencyRequired": [
            "Covariance matrices",
            "Matrix decompositions",
            "Normal distribution"
          ],

          "sections": [
            {"id": "mn", "name": "Multivariate Normal Distribution"},
            {"id": "cholesky", "name": "Cholesky Decomposition"},
            {"id": "dirichlet", "name": "Dirichlet Distribution"},
            {"id": "wishart", "name": "Wishart Distribution"}
          ],

          "definitions": [
            {
              "term": "Multivariate Normal",
              "definition": "X ~ N(μ,Σ) with PDF ∝ exp(-½(x-μ)ᵀΣ⁻¹(x-μ))"
            },
            {
              "term": "Mahalanobis Distance",
              "definition": "d_M(x) = √((x-μ)ᵀΣ⁻¹(x-μ)), scale-invariant distance"
            },
            {
              "term": "Dirichlet Distribution",
              "definition": "Dir(α) on simplex {x: xᵢ≥0, Σxᵢ=1}, conjugate prior for multinomial"
            },
            {
              "term": "Wishart Distribution",
              "definition": "W(Σ,ν) for positive definite matrices, conjugate prior for precision"
            },
            {
              "term": "Cholesky Decomposition",
              "definition": "Σ = LLᵀ for sampling: x = μ + Lz where z ~ N(0,I)"
            }
          ],

          "about": [
            {"name": "Multivariate Normal", "description": "Foundation of multivariate statistics and Gaussian processes"},
            {"name": "Mahalanobis Distance", "description": "Distance accounting for correlations"},
            {"name": "Dirichlet Distribution", "description": "Distribution over probability vectors"},
            {"name": "Wishart Distribution", "description": "Distribution over covariance matrices"}
          ]
        },
        {
          "id": "prob-9",
          "part": 9,
          "title": "Maximum Likelihood Estimation",
          "url": "mle.html",
          "icon": "ℒ",
          "keywords": [
            "Point Estimator",
            "Mean Square Error(MSE)",
            "Standard Error (SE)",
            "Likelihood Function",
            "Log-likelihood Function",
            "Maximum Likelihood Estimation(MLE)",
            "Binomial Distribution",
            "Sample Proportion"
          ],
          "badges": [],
          "prereqs": [
            "prob-4"
          ],
          "mapCoords": {
            "q": 0,
            "r": -5
          },
          "topicGroup": "inference",
          "tesseraMessage": "MLE finds the parameters that make your data most likely. Foundational!",

          "headline": "Maximum Likelihood Estimation: Finding Parameters from Data",
          "description": "Learn about point estimators, and maximum likelihood estimation(MLE).",
          "abstract": "Maximum Likelihood Estimation (MLE) is the foundational method for parameter estimation in statistics and machine learning. Starting from point estimators and their properties (bias, variance, MSE), we develop the likelihood function framework and demonstrate MLE on binomial and normal distributions.",
          "datePublished": "2024-11-25",
          "dateModified": "2026-01-31",

          "teaches": [
            "Point estimators and their properties",
            "Bias, variance, and mean squared error",
            "Standard error of estimators",
            "Likelihood function definition",
            "Log-likelihood for numerical stability",
            "MLE principle: maximize likelihood",
            "MLE for binomial distribution",
            "MLE for normal distribution parameters"
          ],

          "competencyRequired": [
            "Probability distributions",
            "Calculus for optimization",
            "Product of probabilities"
          ],

          "sections": [
            {"id": "point", "name": "Point Estimators"},
            {"id": "lf", "name": "Likelihood Functions"},
            {"id": "mle", "name": "Maximum Likelihood Estimation"},
            {"id": "ex1", "name": "Example 1: Binomial Distribution"},
            {"id": "ex2", "name": "Example 2: Normal Distribution"}
          ],

          "definitions": [
            {
              "term": "Point Estimator",
              "definition": "θ̂ = f(X₁,...,Xₙ), a function of sample data estimating parameter θ"
            },
            {
              "term": "Bias",
              "definition": "Bias(θ̂) = E[θ̂] - θ, systematic error in estimation"
            },
            {
              "term": "Mean Squared Error",
              "definition": "MSE(θ̂) = E[(θ̂ - θ)²] = Var(θ̂) + Bias²"
            },
            {
              "term": "Likelihood Function",
              "definition": "L(θ) = ∏ᵢ f(xᵢ|θ), joint probability viewed as function of θ"
            },
            {
              "term": "Maximum Likelihood Estimator",
              "definition": "θ̂_MLE = argmax_θ L(θ) = argmax_θ log L(θ)"
            }
          ],

          "about": [
            {"name": "Point Estimation", "description": "Estimating parameters from sample data"},
            {"name": "Likelihood Function", "description": "Data probability as function of parameters"},
            {"name": "MLE", "description": "Principle of choosing most likely parameters"},
            {"name": "Log-Likelihood", "description": "Numerically stable likelihood optimization"}
          ]
        },
        {
          "id": "prob-10",
          "part": 10,
          "title": "Statistical Inference & Hypothesis Testing",
          "url": "hypothesis_testing.html",
          "icon": "H₀ vs H₁",
          "keywords": [
            "Null Hypothesis",
            "Alternative Hypothesis",
            "Type I Error (False Negative)",
            "Type II Error (False Positive)",
            "Significance Level",
            "Test Statistic",
            "Null Hypothesis Significance Test(NHST)",
            "One Sample t-Tests",
            "Confidence intervals",
            "Critical Values",
            "z-scores",
            "Credible Intervals",
            "Bootstrap"
          ],
          "badges": [],
          "prereqs": [
            "prob-9"
          ],
          "mapCoords": {
            "q": 1,
            "r": -6
          },
          "topicGroup": "inference",
          "tesseraMessage": "Hypothesis testing helps us make decisions under uncertainty. Be careful with p-values!",

          "headline": "Hypothesis Testing: Making Decisions Under Uncertainty",
          "description": "Learn about null hypothesis significance test, confidence interval, credible interval, and bootstrap method.",
          "abstract": "Statistical inference allows us to draw conclusions about populations from samples. We cover the frequentist approach through null hypothesis significance testing (NHST), confidence intervals, and contrast it with Bayesian credible intervals. The bootstrap method provides a powerful computational alternative.",
          "datePublished": "2024-11-25",
          "dateModified": "2026-01-31",

          "teaches": [
            "Null and alternative hypotheses formulation",
            "Type I and Type II errors",
            "Significance level and p-values",
            "Test statistics and critical values",
            "One-sample t-test procedure",
            "Confidence intervals construction",
            "Frequentist vs Bayesian interpretation",
            "Bootstrap resampling method"
          ],

          "competencyRequired": [
            "Sampling distributions",
            "Normal and t-distributions",
            "Maximum likelihood estimation"
          ],

          "sections": [
            {"id": "NHST", "name": "Null Hypothesis Significance Test"},
            {"id": "test", "name": "Example: t-Tests"},
            {"id": "CI", "name": "Confidence Intervals vs Credible Intervals"},
            {"id": "BS", "name": "Bootstrap"}
          ],

          "definitions": [
            {
              "term": "Null Hypothesis",
              "definition": "H₀: the default assumption to be tested"
            },
            {
              "term": "Type I Error",
              "definition": "Rejecting H₀ when it is true (false positive), probability = α"
            },
            {
              "term": "Type II Error",
              "definition": "Failing to reject H₀ when it is false (false negative)"
            },
            {
              "term": "Confidence Interval",
              "definition": "Range where repeated sampling would capture true parameter (1-α)% of time"
            },
            {
              "term": "Bootstrap",
              "definition": "Resampling with replacement to estimate sampling distribution"
            }
          ],

          "about": [
            {"name": "Hypothesis Testing", "description": "Framework for statistical decision making"},
            {"name": "Confidence Intervals", "description": "Frequentist uncertainty quantification"},
            {"name": "Credible Intervals", "description": "Bayesian probability intervals for parameters"},
            {"name": "Bootstrap", "description": "Computational resampling for inference"}
          ]
        },
        {
          "id": "prob-11",
          "part": 11,
          "title": "Linear Regression",
          "url": "linear_regression.html",
          "icon": "LS",
          "keywords": [
            "Linear Regression",
            "Least-Squares Estimation"
          ],
          "badges": [
            "interactive"
          ],
          "prereqs": [
            "prob-9",
            "linalg-8"
          ],
          "mapCoords": {
            "q": 3,
            "r": -3
          },
          "topicGroup": "inference",
          "tesseraMessage": "Linear regression connects statistics to ML. Where prediction begins!",

          "headline": "Linear Regression: A Probabilistic Perspective",
          "description": "Learn about linear regression in probabilistic Perspective.",
          "abstract": "Linear regression from a probabilistic viewpoint reveals the connection between least-squares and maximum likelihood estimation. When errors are normally distributed, MLE produces exactly the least-squares solution. This bridges linear algebra and probability, forming the foundation for statistical learning.",
          "datePublished": "2024-11-25",
          "dateModified": "2026-01-31",

          "teaches": [
            "Linear model formulation y = Xβ + ε",
            "Design matrix and parameter vector",
            "Normal equations from linear algebra",
            "Probabilistic interpretation of regression",
            "Gaussian noise assumption",
            "Likelihood function for linear regression",
            "Equivalence of MLE and least-squares",
            "Derivation using matrix calculus"
          ],

          "competencyRequired": [
            "Maximum likelihood estimation",
            "Least-squares problems",
            "Matrix calculus basics"
          ],

          "sections": [
            {"id": "recap", "name": "Recap from Linear Algebra"},
            {"id": "lr", "name": "Linear Regression: A Probabilistic Perspective"},
            {"id": "interactive-tool", "name": "Interactive Statistical Regression Tool"}
          ],

          "definitions": [
            {
              "term": "Linear Model",
              "definition": "y = Xβ + ε where ε ~ N(0, σ²I)"
            },
            {
              "term": "Design Matrix",
              "definition": "X ∈ ℝⁿˣᵈ containing n observations of d features"
            },
            {
              "term": "Normal Equations",
              "definition": "XᵀXβ̂ = Xᵀy, necessary condition for least-squares"
            },
            {
              "term": "Least-Squares Solution",
              "definition": "β̂_LS = (XᵀX)⁻¹Xᵀy = β̂_MLE under Gaussian noise"
            },
            {
              "term": "Residual Vector",
              "definition": "ε = y - Xβ, difference between observed and predicted"
            }
          ],

          "about": [
            {"name": "Linear Regression", "description": "Predicting continuous outputs from linear combinations"},
            {"name": "Least-Squares", "description": "Minimizing sum of squared residuals"},
            {"name": "MLE Connection", "description": "Least-squares as MLE under Gaussian assumption"},
            {"name": "Normal Equations", "description": "Linear algebra characterization of optimal parameters"}
          ]
        },
        {
          "id": "prob-12",
          "part": 12,
          "title": "Entropy",
          "url": "entropy.html",
          "icon": "ℍ",
          "keywords": [
            "Information Content",
            "Entropy",
            "Joint Entropy",
            "Conditional Entropy",
            "Cross Entropy",
            "KL Divergence(Relative Entropy, Information Gain)",
            "Gibbs' Inequality",
            "Log Sum Inequality",
            "Jensen's Inequality",
            "Mutual Information (MI)"
          ],
          "badges": [],
          "prereqs": [
            "prob-4"
          ],
          "mapCoords": {
            "q": 5,
            "r": -5
          },
          "topicGroup": "information",
          "tesseraMessage": "Entropy measures uncertainty — the heart of information theory.",

          "headline": "Entropy: The Mathematical Theory of Information",
          "description": "Learn about entropy in information theory including KL divergence and mutual information.",
          "abstract": "Information theory provides mathematical tools for quantifying uncertainty and information. Starting from self-information, we build to entropy, joint and conditional entropy, cross-entropy (used in ML loss functions), KL divergence for comparing distributions, and mutual information for measuring shared information.",
          "datePublished": "2024-11-25",
          "dateModified": "2026-01-31",

          "teaches": [
            "Information content (self-information)",
            "Shannon entropy definition and properties",
            "Joint entropy for multiple variables",
            "Conditional entropy and chain rule",
            "Cross-entropy and its use in machine learning",
            "KL divergence as relative entropy",
            "Gibbs' inequality proof",
            "Mutual information between variables"
          ],

          "competencyRequired": [
            "Probability distributions",
            "Logarithms and their properties",
            "Jensen's inequality"
          ],

          "sections": [
            {"id": "entropy", "name": "Entropy"},
            {"id": "joint", "name": "Joint Entropy"},
            {"id": "conditional", "name": "Conditional Entropy"},
            {"id": "cross", "name": "Cross Entropy"},
            {"id": "kl", "name": "KL Divergence (Relative Entropy, Information Gain)"},
            {"id": "mi", "name": "Mutual Information (MI)"}
          ],

          "definitions": [
            {
              "term": "Information Content",
              "definition": "I(E) = -log p(E), measures surprise of event E"
            },
            {
              "term": "Entropy",
              "definition": "H(X) = -Σₓ p(x)log p(x), expected information content"
            },
            {
              "term": "Cross-Entropy",
              "definition": "H(p,q) = -Σₓ p(x)log q(x), entropy using wrong distribution"
            },
            {
              "term": "KL Divergence",
              "definition": "D_KL(p||q) = Σₓ p(x)log(p(x)/q(x)) ≥ 0, distance between distributions"
            },
            {
              "term": "Mutual Information",
              "definition": "I(X;Y) = H(X) - H(X|Y) = H(Y) - H(Y|X), shared information"
            }
          ],

          "about": [
            {"name": "Entropy", "description": "Fundamental measure of uncertainty"},
            {"name": "Cross-Entropy", "description": "Loss function for classification in ML"},
            {"name": "KL Divergence", "description": "Asymmetric measure of distribution difference"},
            {"name": "Mutual Information", "description": "Information shared between random variables"}
          ]
        },
        {
          "id": "prob-13",
          "part": 13,
          "title": "Convergence",
          "url": "convergence.html",
          "icon": "n→∞",
          "keywords": [
            "The Law of Large Numbers",
            "Convergence in Probability",
            "Convergence in Distribution",
            "Asymptotic(limiting) Distribution",
            "Moment Generating Function(m.g.f.)",
            "Central Limit Theorem(CLT)"
          ],
          "badges": [],
          "prereqs": [
            "prob-9"
          ],
          "mapCoords": {
            "q": -1,
            "r": -5
          },
          "topicGroup": "inference",
          "tesseraMessage": "Convergence guarantees that our estimates improve. Math meets practice!",

          "headline": "Convergence: How Random Variables Approach Their Limits",
          "description": "Learn about convergence in probability and distribution.",
          "abstract": "Statistical convergence describes how sequences of random variables approach limits. We explore the hierarchy from almost sure convergence through convergence in probability to convergence in distribution, connecting these to the Law of Large Numbers and Central Limit Theorem via moment generating functions.",
          "datePublished": "2024-12-30",
          "dateModified": "2026-01-31",

          "teaches": [
            "Modes of convergence hierarchy",
            "Almost sure convergence (strongest)",
            "Convergence in probability",
            "Convergence in distribution (weakest)",
            "Law of Large Numbers (weak and strong)",
            "Moment generating functions",
            "Continuity theorem for MGFs",
            "Central Limit Theorem proof sketch"
          ],

          "competencyRequired": [
            "Probability distributions",
            "Sequences and limits",
            "Exponential functions"
          ],

          "sections": [
            {"id": "LLA", "name": "Modes of Convergence & The Law of Large Numbers"},
            {"id": "prob", "name": "Convergence in Probability"},
            {"id": "dist", "name": "Convergence in Distribution"},
            {"id": "mgf", "name": "Moment Generating Function(mgf)"}
          ],

          "definitions": [
            {
              "term": "Almost Sure Convergence",
              "definition": "P(lim_{n→∞} Xₙ = X) = 1, strongest form"
            },
            {
              "term": "Convergence in Probability",
              "definition": "For all ε>0: lim_{n→∞} P(|Xₙ - X| > ε) = 0"
            },
            {
              "term": "Convergence in Distribution",
              "definition": "Fₙ(x) → F(x) at all continuity points of F"
            },
            {
              "term": "Law of Large Numbers",
              "definition": "X̄ₙ converges to μ as n → ∞ (weak or strong)"
            },
            {
              "term": "Moment Generating Function",
              "definition": "M_X(t) = E[e^{tX}], uniquely determines distribution"
            }
          ],

          "about": [
            {"name": "Statistical Convergence", "description": "How random sequences approach limits"},
            {"name": "Law of Large Numbers", "description": "Sample means converge to population mean"},
            {"name": "MGF", "description": "Transform that characterizes distributions"},
            {"name": "Asymptotic Theory", "description": "Behavior of estimators as n → ∞"}
          ]
        },
        {
          "id": "prob-14",
          "part": 14,
          "title": "Intro to Bayesian Statistics",
          "url": "bayesian.html",
          "icon": "p(θ|𝒟)",
          "keywords": [
            "Bayesian Inference",
            "Prior Distribution",
            "Posterior Distribution",
            "Marginal Likelihood",
            "Conjugate Prior",
            "Posterior Predictive Distribution",
            "Beta-Binomial Model",
            "Normal Distribution Model with known Variance σ²",
            "Normal Distribution Model with known Mean μ"
          ],
          "badges": [],
          "prereqs": [
            "prob-9",
            "prob-3"
          ],
          "mapCoords": {
            "q": 1,
            "r": -3
          },
          "topicGroup": "bayesian",
          "tesseraMessage": "Bayesian thinking updates beliefs with evidence. Prior → Posterior.",

          "headline": "Bayesian Statistics: Learning from Data Through Probability",
          "description": "Learn about Bayesian inference, prior and posterior distributions, conjugate priors, and predictive distributions.",
          "abstract": "Bayesian statistics provides a coherent framework for updating beliefs based on observed data. Starting from prior distributions encoding initial knowledge, we derive posterior distributions via Bayes' theorem. Conjugate priors enable tractable computations, demonstrated through the Beta-Binomial and Normal models.",
          "datePublished": "2025-01-07",
          "dateModified": "2026-01-31",

          "teaches": [
            "Bayesian inference framework",
            "Prior distribution specification",
            "Posterior distribution derivation",
            "Marginal likelihood computation",
            "Conjugate prior families",
            "Posterior predictive distribution",
            "Beta-Binomial model for proportions",
            "Normal models with known variance or mean"
          ],

          "competencyRequired": [
            "Bayes' theorem",
            "Beta and Gamma distributions",
            "Maximum likelihood estimation"
          ],

          "sections": [
            {"id": "intro", "name": "Introduction to Bayesian Inference"},
            {"id": "prior", "name": "Prior Distribution"},
            {"id": "posterior", "name": "Posterior Distribution"},
            {"id": "conjugate", "name": "Conjugate Priors"},
            {"id": "predictive", "name": "Posterior Predictive Distribution"},
            {"id": "beta-binomial", "name": "Beta-Binomial Model"},
            {"id": "normal", "name": "Normal Distribution Models"}
          ],

          "definitions": [
            {
              "term": "Prior Distribution",
              "definition": "p(θ), probability distribution encoding beliefs about θ before observing data"
            },
            {
              "term": "Posterior Distribution",
              "definition": "p(θ|𝒟) ∝ p(𝒟|θ)p(θ), updated beliefs after observing data 𝒟"
            },
            {
              "term": "Marginal Likelihood",
              "definition": "p(𝒟) = ∫p(𝒟|θ)p(θ)dθ, evidence for model comparison"
            },
            {
              "term": "Conjugate Prior",
              "definition": "Prior that yields posterior in the same family as the prior"
            },
            {
              "term": "Posterior Predictive",
              "definition": "p(x*|𝒟) = ∫p(x*|θ)p(θ|𝒟)dθ, prediction accounting for parameter uncertainty"
            }
          ],

          "about": [
            {"name": "Bayesian Inference", "description": "Updating beliefs with data via Bayes' theorem"},
            {"name": "Conjugate Priors", "description": "Computationally convenient prior families"},
            {"name": "Posterior Distribution", "description": "Complete uncertainty quantification for parameters"},
            {"name": "Predictive Distribution", "description": "Predictions integrating over parameter uncertainty"}
          ]
        },
        {
          "id": "prob-15",
          "part": 15,
          "title": "The Exponential Family",
          "url": "expfamily.html",
          "icon": "η",
          "keywords": [
            "Exponential Family",
            "Natural Parameters(Canonical Parameters)",
            "Base Measure",
            "Sufficient Statistics",
            "Partition Function",
            "Minimal Representation",
            "Natural Exponential Family(NEF)",
            "Moment Parameters",
            "Precision Matrix",
            "Information Form",
            "Moment Matching",
            "Cumulants"
          ],
          "badges": [],
          "prereqs": [
            "prob-9"
          ],
          "mapCoords": {
            "q": 1,
            "r": -5
          },
          "topicGroup": "distributions",
          "tesseraMessage": "The exponential family unifies many distributions elegantly. Beautiful structure!",

          "headline": "Exponential Family: A Unified Framework for Distributions",
          "description": "Learn about the exponential family of distributions, natural parameters, sufficient statistics, and moment matching.",
          "abstract": "The exponential family provides a unified framework encompassing most common distributions including Normal, Bernoulli, Poisson, and Gamma. The canonical form reveals deep structure: natural parameters, sufficient statistics, and the log-partition function connect to cumulants, enabling elegant Bayesian conjugacy and efficient inference.",
          "datePublished": "2025-01-30",
          "dateModified": "2026-01-31",

          "teaches": [
            "Exponential family canonical form",
            "Natural (canonical) parameters",
            "Sufficient statistics",
            "Base measure and partition function",
            "Log-partition function derivatives give cumulants",
            "Moment parameters and moment matching",
            "Natural exponential family (NEF)",
            "Information form for Gaussians"
          ],

          "competencyRequired": [
            "Common probability distributions",
            "Bayesian inference basics",
            "Calculus of variations"
          ],

          "sections": [
            {"id": "intro", "name": "Introduction to Exponential Family"},
            {"id": "canonical", "name": "Canonical Form"},
            {"id": "sufficient", "name": "Sufficient Statistics"},
            {"id": "partition", "name": "Log-Partition Function"},
            {"id": "moment", "name": "Moment Parameters"},
            {"id": "examples", "name": "Examples of Exponential Family"}
          ],

          "definitions": [
            {
              "term": "Exponential Family",
              "definition": "p(x|η) = h(x)exp(η·T(x) - A(η)) with natural parameter η"
            },
            {
              "term": "Natural Parameters",
              "definition": "η, the canonical parameters in the exponential family form"
            },
            {
              "term": "Sufficient Statistics",
              "definition": "T(x), statistics that capture all information about η from data"
            },
            {
              "term": "Log-Partition Function",
              "definition": "A(η) = log∫h(x)exp(η·T(x))dx, normalizer whose derivatives give cumulants"
            },
            {
              "term": "Moment Matching",
              "definition": "Setting distribution moments equal to sample moments for parameter estimation"
            }
          ],

          "about": [
            {"name": "Exponential Family", "description": "Unified family encompassing most common distributions"},
            {"name": "Sufficient Statistics", "description": "Data summaries containing all parameter information"},
            {"name": "Natural Parameters", "description": "Canonical parameterization with nice mathematical properties"},
            {"name": "Log-Partition Function", "description": "Cumulant generating function for the distribution"}
          ]
        },
        {
          "id": "prob-16",
          "part": 16,
          "title": "Fisher Information Matrix",
          "url": "fisher_info.html",
          "icon": "F(θ)",
          "keywords": [
            "Fisher Information Matrix(FIM)",
            "Score Function",
            "Covariance",
            "Negative Log Likelihood",
            "Log Partition Function",
            "Approximated KL Divergence",
            "Natural Gradient",
            "Jeffreys Prior",
            "Uninformative Prior",
            "Reference Prior",
            "Mutual Information"
          ],
          "badges": [],
          "prereqs": [
            "prob-15",
            "linalg-9"
          ],
          "mapCoords": {
            "q": 4,
            "r": -4
          },
          "topicGroup": "bayesian",
          "tesseraMessage": "Fisher information measures how much data tells us about parameters.",

          "headline": "Fisher Information Matrix: Quantifying Parameter Information",
          "description": "Learn about the Fisher Information Matrix, score function, natural gradient, and Jeffreys prior.",
          "abstract": "The Fisher Information Matrix quantifies how much information data carries about unknown parameters. It equals the covariance of the score function and the negative expected Hessian of log-likelihood. FIM enables natural gradient descent, defines Jeffreys prior for objective Bayesian inference, and approximates KL divergence locally.",
          "datePublished": "2025-02-11",
          "dateModified": "2026-01-31",

          "teaches": [
            "Fisher Information Matrix definition",
            "Score function and its properties",
            "FIM as covariance of score",
            "FIM as negative expected Hessian",
            "Connection to log-partition function",
            "Local KL divergence approximation",
            "Natural gradient descent",
            "Jeffreys prior construction"
          ],

          "competencyRequired": [
            "Exponential family distributions",
            "Matrix calculus",
            "KL divergence"
          ],

          "sections": [
            {"id": "intro", "name": "Introduction"},
            {"id": "score", "name": "Score Function"},
            {"id": "fim", "name": "Fisher Information Matrix"},
            {"id": "kl", "name": "KL Divergence Approximation"},
            {"id": "natural", "name": "Natural Gradient"},
            {"id": "jeffreys", "name": "Jeffreys Prior"}
          ],

          "definitions": [
            {
              "term": "Score Function",
              "definition": "s(θ) = ∇_θ log p(x|θ), gradient of log-likelihood"
            },
            {
              "term": "Fisher Information Matrix",
              "definition": "F(θ) = E[s(θ)s(θ)ᵀ] = -E[∇²log p(x|θ)]"
            },
            {
              "term": "Natural Gradient",
              "definition": "F(θ)⁻¹∇L(θ), gradient preconditioned by inverse FIM"
            },
            {
              "term": "Jeffreys Prior",
              "definition": "p(θ) ∝ √det(F(θ)), objective prior invariant to reparameterization"
            },
            {
              "term": "KL Approximation",
              "definition": "D_KL(p_θ||p_{θ+δ}) ≈ ½δᵀF(θ)δ for small δ"
            }
          ],

          "about": [
            {"name": "Fisher Information", "description": "Curvature of log-likelihood measuring parameter sensitivity"},
            {"name": "Score Function", "description": "Gradient driving maximum likelihood estimation"},
            {"name": "Natural Gradient", "description": "Optimization respecting statistical manifold geometry"},
            {"name": "Jeffreys Prior", "description": "Objective Bayesian prior from information geometry"}
          ]
        },
        {
          "id": "prob-17",
          "part": 17,
          "title": "Bayesian Decision Theory",
          "url": "decision_theory.html",
          "icon": "π",
          "keywords": [
            "Decision Theory",
            "Optimal Policy(Bayes estimator)",
            "Zero-One Loss",
            "Maximum A Posteriori (MAP) Estimate",
            "Reject Option",
            "Confusion Matrix",
            "False Positive (FP, Type I error)",
            "False Negative (FN, Type II error)",
            "Receiver Operating Characteristic (ROC) Curve",
            "Equal Error Rate (EER)",
            "Precision-Recall (PR) Curve",
            "Interpolated Precision",
            "Average Precision (AP)"
          ],
          "badges": [],
          "prereqs": [
            "prob-14"
          ],
          "mapCoords": {
            "q": 3,
            "r": -4
          },
          "topicGroup": "bayesian",
          "tesseraMessage": "Decision theory tells us how to act optimally under uncertainty.",

          "headline": "Bayesian Decision Theory: Optimal Actions Under Uncertainty",
          "description": "Learn about Bayesian decision theory, loss functions, MAP estimation, ROC curves, and precision-recall analysis.",
          "abstract": "Bayesian decision theory provides a principled framework for making optimal decisions under uncertainty. By minimizing expected loss, we derive Bayes estimators including MAP estimates. Classifier evaluation uses confusion matrices, ROC curves plotting TPR vs FPR, and precision-recall curves for imbalanced data.",
          "datePublished": "2025-03-25",
          "dateModified": "2026-01-31",

          "teaches": [
            "Decision theory framework",
            "Loss functions and expected loss",
            "Bayes optimal estimators",
            "Zero-one loss and MAP estimation",
            "Reject option for uncertain predictions",
            "Confusion matrix interpretation",
            "ROC curve construction",
            "Precision-recall analysis"
          ],

          "competencyRequired": [
            "Posterior distributions",
            "Expected value computation",
            "Classification basics"
          ],

          "sections": [
            {"id": "intro", "name": "Introduction to Decision Theory"},
            {"id": "loss", "name": "Loss Functions"},
            {"id": "bayes", "name": "Bayes Estimator"},
            {"id": "map", "name": "MAP Estimation"},
            {"id": "reject", "name": "Reject Option"},
            {"id": "confusion", "name": "Confusion Matrix"},
            {"id": "roc", "name": "ROC Curve"},
            {"id": "pr", "name": "Precision-Recall Curve"}
          ],

          "definitions": [
            {
              "term": "Bayes Estimator",
              "definition": "δ*(x) = argmin_a E[L(θ,a)|x], action minimizing expected loss"
            },
            {
              "term": "MAP Estimate",
              "definition": "θ_MAP = argmax_θ p(θ|𝒟), mode of posterior distribution"
            },
            {
              "term": "ROC Curve",
              "definition": "Plot of True Positive Rate vs False Positive Rate across thresholds"
            },
            {
              "term": "Precision",
              "definition": "TP/(TP+FP), fraction of positive predictions that are correct"
            },
            {
              "term": "Recall",
              "definition": "TP/(TP+FN), fraction of actual positives correctly identified"
            }
          ],

          "about": [
            {"name": "Decision Theory", "description": "Framework for optimal decision-making under uncertainty"},
            {"name": "MAP Estimation", "description": "Point estimate from posterior mode"},
            {"name": "ROC Analysis", "description": "Classifier performance across operating points"},
            {"name": "Precision-Recall", "description": "Evaluation metrics for imbalanced classification"}
          ]
        },
        {
          "id": "prob-18",
          "part": 18,
          "title": "Markov Chains",
          "url": "markov.html",
          "icon": "∏",
          "keywords": [
            "Probabilistic Graphical Models(PGMs)",
            "Bayesian Networks",
            "Markov Chains",
            "Language Modeling",
            "n-gram",
            "Transition Function(Kernel)",
            "Stochastic Matrix(Transition Matrix)",
            "Maximum likelihood estimation(MLE) in Markov models",
            "Sparse Data Problem",
            "Add-One Smoothing",
            "Dirichlet Prior"
          ],
          "badges": [
            "code"
          ],
          "prereqs": [
            "prob-14",
            "linalg-13"
          ],
          "mapCoords": {
            "q": 5,
            "r": -4
          },
          "topicGroup": "bayesian",
          "tesseraMessage": "Markov chains model sequences — from text to weather to DNA.",

          "headline": "Markov Chains: Sequential Modeling with Memory",
          "description": "Learn about probabilistic graphical models, Markov chains, n-gram language models, and smoothing techniques.",
          "abstract": "Markov chains model sequential data where future states depend only on the present state. As a foundation of probabilistic graphical models, they enable language modeling via n-grams. MLE estimates transition probabilities, but sparse data requires smoothing techniques like add-one or Bayesian methods with Dirichlet priors.",
          "datePublished": "2025-04-07",
          "dateModified": "2026-01-31",

          "teaches": [
            "Probabilistic graphical models overview",
            "Markov property and chain definition",
            "Transition function and stochastic matrix",
            "n-gram language models",
            "Maximum likelihood for Markov chains",
            "Sparse data problem in NLP",
            "Add-one (Laplace) smoothing",
            "Bayesian smoothing with Dirichlet priors"
          ],

          "competencyRequired": [
            "Conditional probability",
            "Maximum likelihood estimation",
            "Stochastic matrices"
          ],

          "sections": [
            {"id": "intro", "name": "Probabilistic Graphical Models"},
            {"id": "markov", "name": "Markov Chains"},
            {"id": "ngram", "name": "Language Modeling and n-grams"},
            {"id": "mle", "name": "MLE for Markov Models"},
            {"id": "smoothing", "name": "Smoothing Techniques"}
          ],

          "definitions": [
            {
              "term": "Markov Property",
              "definition": "P(Xₙ₊₁|X₁,...,Xₙ) = P(Xₙ₊₁|Xₙ), future depends only on present"
            },
            {
              "term": "Transition Matrix",
              "definition": "P with Pᵢⱼ = P(Xₙ₊₁=j|Xₙ=i), rows sum to 1"
            },
            {
              "term": "n-gram",
              "definition": "P(wₙ|wₙ₋₁,...,wₙ₋ₖ₊₁), word probability given k-1 previous words"
            },
            {
              "term": "Add-One Smoothing",
              "definition": "P(w|h) = (C(h,w)+1)/(C(h)+V), adds 1 to all counts"
            },
            {
              "term": "Dirichlet Prior",
              "definition": "Dir(α) prior on categorical parameters for Bayesian smoothing"
            }
          ],

          "about": [
            {"name": "Markov Chains", "description": "Sequential models with limited memory"},
            {"name": "n-gram Models", "description": "Foundation of statistical language modeling"},
            {"name": "Smoothing", "description": "Handling unseen events in discrete distributions"},
            {"name": "PGMs", "description": "Graphical representation of probabilistic dependencies"}
          ]
        },
        {
          "id": "prob-19",
          "part": 19,
          "title": "Monte Carlo Methods",
          "url": "monte_carlo.html",
          "icon": "p*(θ)",
          "keywords": [
            "Credible Intervals",
            "Central Credible Intervals",
            "Monte Carlo Approximation",
            "Highest Posterior Density (HPD)",
            "Markov Chain Monte Carlo (MCMC)"
          ],
          "badges": [
            "interactive"
          ],
          "prereqs": [
            "prob-14"
          ],
          "mapCoords": {
            "q": 6,
            "r": -4
          },
          "topicGroup": "bayesian",
          "tesseraMessage": "Monte Carlo turns random sampling into computational power!",

          "headline": "Monte Carlo Methods: Computation Through Random Sampling",
          "description": "Learn about Monte Carlo approximation, credible intervals, HPD regions, and MCMC methods.",
          "abstract": "Monte Carlo methods use random sampling to approximate intractable integrals and distributions. For Bayesian inference, they enable computation of posterior summaries like credible intervals and Highest Posterior Density (HPD) regions. Markov Chain Monte Carlo (MCMC) generates samples from complex posteriors.",
          "datePublished": "2025-05-11",
          "dateModified": "2026-01-31",

          "teaches": [
            "Monte Carlo integration principle",
            "Estimating posterior expectations",
            "Central credible intervals",
            "Highest Posterior Density (HPD) intervals",
            "Introduction to MCMC",
            "Convergence diagnostics",
            "Effective sample size"
          ],

          "competencyRequired": [
            "Posterior distributions",
            "Law of large numbers",
            "Random number generation"
          ],

          "sections": [
            {"id": "intro", "name": "Introduction to Monte Carlo"},
            {"id": "mc", "name": "Monte Carlo Approximation"},
            {"id": "credible", "name": "Credible Intervals"},
            {"id": "hpd", "name": "Highest Posterior Density"},
            {"id": "mcmc", "name": "Markov Chain Monte Carlo"}
          ],

          "definitions": [
            {
              "term": "Monte Carlo Approximation",
              "definition": "E[f(θ)] ≈ (1/N)Σf(θ⁽ⁱ⁾) where θ⁽ⁱ⁾ ~ p(θ)"
            },
            {
              "term": "Credible Interval",
              "definition": "Interval [a,b] where P(θ∈[a,b]|𝒟) = 1-α"
            },
            {
              "term": "HPD Region",
              "definition": "Smallest region containing (1-α) posterior probability"
            },
            {
              "term": "MCMC",
              "definition": "Markov chain with stationary distribution equal to target posterior"
            }
          ],

          "about": [
            {"name": "Monte Carlo", "description": "Random sampling for numerical integration"},
            {"name": "Credible Intervals", "description": "Bayesian probability intervals for parameters"},
            {"name": "HPD", "description": "Optimal credible regions with highest density"},
            {"name": "MCMC", "description": "Sampling from complex posterior distributions"}
          ]
        },
        {
          "id": "prob-20",
          "part": 20,
          "title": "Importance Sampling",
          "url": "importance_sampling.html",
          "icon": "φ",
          "keywords": [
            "Importance Sampling",
            "Importance Weights",
            "Direct Importance Sampling",
            "Effective Sample Size (ESS)",
            "Self-Normalized Importance Sampling (SNIS)",
            "Annealed Importance Sampling (AIS)",
            "Annealing Schedule"
          ],
          "badges": [],
          "prereqs": [
            "prob-19"
          ],
          "mapCoords": {
            "q": 6,
            "r": -5
          },
          "topicGroup": "bayesian",
          "tesseraMessage": "Importance sampling focuses computation where it matters most.",

          "headline": "Importance Sampling: Efficient Monte Carlo via Proposal Distributions",
          "description": "Learn about importance sampling, importance weights, effective sample size, and annealed importance sampling.",
          "abstract": "Importance sampling enables Monte Carlo estimation when sampling directly from the target is difficult. By sampling from a proposal distribution and reweighting, we can estimate expectations efficiently. Self-normalized importance sampling handles unknown normalizing constants, while annealed importance sampling bridges easy and hard distributions.",
          "datePublished": "2025-10-06",
          "dateModified": "2026-01-31",

          "teaches": [
            "Importance sampling principle",
            "Importance weights derivation",
            "Proposal distribution selection",
            "Effective sample size (ESS)",
            "Self-normalized importance sampling",
            "Annealed importance sampling",
            "Annealing schedule design"
          ],

          "competencyRequired": [
            "Monte Carlo basics",
            "Expectation and variance",
            "Change of variables"
          ],

          "sections": [
            {"id": "intro", "name": "Introduction"},
            {"id": "is", "name": "Direct Importance Sampling"},
            {"id": "weights", "name": "Importance Weights"},
            {"id": "ess", "name": "Effective Sample Size"},
            {"id": "snis", "name": "Self-Normalized Importance Sampling"},
            {"id": "ais", "name": "Annealed Importance Sampling"}
          ],

          "definitions": [
            {
              "term": "Importance Sampling",
              "definition": "E_p[f] = E_q[f·w] where w(x) = p(x)/q(x) are importance weights"
            },
            {
              "term": "Importance Weights",
              "definition": "w(x) = p(x)/q(x), ratio of target to proposal density"
            },
            {
              "term": "Effective Sample Size",
              "definition": "ESS = (Σwᵢ)²/Σwᵢ², measures sample efficiency"
            },
            {
              "term": "Self-Normalized IS",
              "definition": "Ê[f] = Σwᵢf(xᵢ)/Σwᵢ, handles unnormalized targets"
            },
            {
              "term": "Annealed IS",
              "definition": "Sequence of distributions p₀→p₁→...→pₖ=target with intermediate annealing"
            }
          ],

          "about": [
            {"name": "Importance Sampling", "description": "Monte Carlo with proposal distributions"},
            {"name": "ESS", "description": "Diagnostic for sampling efficiency"},
            {"name": "SNIS", "description": "Self-normalizing for unnormalized targets"},
            {"name": "AIS", "description": "Bridging distributions for difficult targets"}
          ]
        },
        {
          "id": "prob-21",
          "part": 21,
          "title": "Gaussian Processes",
          "url": "gaussian_process.html",
          "icon": "𝒢",
          "keywords": [
            "Nonparametric Models",
            "Gaussian Process (GP)",
            "Mercer Kernel",
            "radial basis function (RBF) Kernel",
            "Stationary Kernel",
            "Automatic Relevance Determination (ARD) Kernel",
            "Matérn Kernel",
            "Periodic Kernel",
            "GP Regression"
          ],
          "badges": [],
          "prereqs": [
            "prob-8",
            "linalg-9"
          ],
          "mapCoords": {
            "q": 5,
            "r": -3
          },
          "topicGroup": "gaussian-process",
          "tesseraMessage": "GPs give uncertainty estimates for free. Bayesian nonparametrics at its finest!",

          "headline": "Gaussian Processes: Distributions Over Functions",
          "description": "Learn about Gaussian processes, kernel functions, and GP regression for nonparametric Bayesian modeling.",
          "abstract": "Gaussian Processes define distributions over functions, enabling nonparametric Bayesian modeling with automatic uncertainty quantification. Kernel functions encode prior assumptions about function smoothness and periodicity. GP regression provides closed-form posterior predictions with calibrated uncertainty estimates.",
          "datePublished": "2025-10-12",
          "dateModified": "2026-01-31",

          "teaches": [
            "Gaussian process definition",
            "Function-space view of GPs",
            "Kernel functions and properties",
            "RBF (squared exponential) kernel",
            "Matérn kernel family",
            "Periodic and composite kernels",
            "GP regression derivation",
            "Posterior predictive distribution"
          ],

          "competencyRequired": [
            "Multivariate normal distribution",
            "Positive definite matrices",
            "Bayesian inference"
          ],

          "sections": [
            {"id": "intro", "name": "Introduction to Gaussian Processes"},
            {"id": "definition", "name": "GP Definition"},
            {"id": "kernels", "name": "Kernel Functions"},
            {"id": "rbf", "name": "RBF Kernel"},
            {"id": "matern", "name": "Matérn Kernel"},
            {"id": "periodic", "name": "Periodic Kernel"},
            {"id": "regression", "name": "GP Regression"}
          ],

          "definitions": [
            {
              "term": "Gaussian Process",
              "definition": "f ~ GP(m,k): any finite collection f(x₁),...,f(xₙ) is jointly Gaussian"
            },
            {
              "term": "Kernel Function",
              "definition": "k(x,x'): covariance function defining GP, must be positive semi-definite"
            },
            {
              "term": "RBF Kernel",
              "definition": "k(x,x') = σ²exp(-||x-x'||²/2ℓ²), infinitely differentiable functions"
            },
            {
              "term": "Matérn Kernel",
              "definition": "Family with smoothness parameter ν controlling differentiability"
            },
            {
              "term": "GP Posterior",
              "definition": "f*|X,y,x* ~ N(μ*, σ*²) with closed-form mean and variance"
            }
          ],

          "about": [
            {"name": "Gaussian Processes", "description": "Distributions over functions for nonparametric modeling"},
            {"name": "Kernel Functions", "description": "Covariance functions encoding prior assumptions"},
            {"name": "GP Regression", "description": "Bayesian regression with uncertainty quantification"},
            {"name": "Nonparametric Models", "description": "Flexible models without fixed parameter count"}
          ]
        }        
      ],
      "reservedSlots": [
        {
          "q": 0,
          "r": -6
        },
        {
          "q": 7,
          "r": -5
        },
        {
          "q": 6,
          "r": -3
        }
      ]
    },
    "IV": {
      "id": "discrete",
      "title": "Discrete Mathematics & Algorithms",
      "shortTitle": "Discrete Math",
      "description": "Explore the foundations of discrete mathematics and algorithms, covering graph theory, combinatorics, and the theory of computation. Learn key concepts essential for mathematical reasoning and algorithmic problem-solving.",
      "tagline": "The Mathematics of Logic and Finite Structures",
      "icon": "fa-project-diagram",
      "indexUrl": "Mathematics/Discrete/discrete_math.html",
      "baseUrl": "Mathematics/Discrete/",
      "parts": [
        {
          "id": "disc-1",
          "part": 1,
          "title": "Intro to Graph Theory",
          "url": "intro_graph.html",
          "icon": "G=(V,E)",
          "keywords": [
            "Undirected graph",
            "Directed graph",
            "Multigraph",
            "Weighted graph",
            "Complete graph",
            "Adjacent",
            "Degree of a vertex",
            "k-regular",
            "Isomorphism",
            "Bipartite graph",
            "Subgraph",
            "Induced subgraph",
            "Spanning subgraph",
            "Path",
            "Cycle",
            "Tree",
            "Acyclic graph"
          ],
          "badges": [],
          "prereqs": [],
          "mapCoords": {
            "q": 0, 
            "r": 1
          },
          "topicGroup": "graph-theory",
          "tesseraMessage": "Graphs are everywhere — social networks, molecules, the internet itself!",

          "headline": "Graph Theory: Modeling Relationships with Vertices and Edges",
          "description": "Introduction to graph theory, and isomorphism.",
          "abstract": "Graph theory provides the mathematical language for modeling relationships between objects. A graph G = (V, E) consists of vertices and edges connecting them. Key concepts include vertex degrees, graph isomorphism, subgraphs, paths, and cycles—fundamental structures underlying social networks, recommendation systems, and ML models.",
          "datePublished": "2025-02-20",
          "dateModified": "2026-01-31",

          "teaches": [
            "Graph definition: G = (V, E)",
            "Undirected, directed, weighted, and multigraphs",
            "Neighbors and adjacency",
            "Vertex degree, in-degree, and out-degree",
            "Minimum, maximum, and average degree",
            "Graph isomorphism and automorphism",
            "Graph properties and invariants",
            "Subgraphs, induced subgraphs, and spanning subgraphs",
            "Paths, cycles, and acyclic graphs (trees)"
          ],

          "competencyRequired": [
            "Basic set theory",
            "Function concepts"
          ],

          "sections": [
            {"id": "intro", "name": "Introduction"},
            {"id": "graph", "name": "Graph"},
            {"id": "neighbors", "name": "Neighbors"},
            {"id": "degree", "name": "Degrees"},
            {"id": "iso", "name": "Isomorphism"},
            {"id": "sub", "name": "Subgraphs"},
            {"id": "path", "name": "Paths & Cycles"}
          ],

          "definitions": [
            {
              "term": "Graph",
              "definition": "G = (V, E) where V is a set of vertices and E is a set of edges"
            },
            {
              "term": "Degree",
              "definition": "deg(x) = |N(x)|; number of neighbors of vertex x"
            },
            {
              "term": "Isomorphism",
              "definition": "Bijection φ: V → V' preserving edge relationships; G ≃ G'"
            },
            {
              "term": "Induced Subgraph",
              "definition": "G[V'] contains all edges of G with both endpoints in V'"
            },
            {
              "term": "Path",
              "definition": "Sequence of distinct vertices connected by edges; Pₖ has k vertices"
            },
            {
              "term": "Cycle",
              "definition": "Path with edge connecting last vertex to first; Cₖ has k vertices and k edges"
            }
          ],

          "about": [
            {"name": "Graph Theory", "description": "Mathematical study of relationships via vertices and edges"},
            {"name": "Graph Isomorphism", "description": "Structural equivalence of graphs"},
            {"name": "Subgraphs", "description": "Graphs contained within larger graphs"},
            {"name": "Paths and Cycles", "description": "Fundamental traversal structures"}
          ]
        },
        {
          "id": "disc-2",
          "part": 2,
          "title": "Intro to Combinatorics",
          "url": "intro_combinatorics.html",
          "icon": "ₙCᵣ",
          "keywords": [
            "Fundamental counting principle",
            "Rule of sum",
            "Rule of product",
            "Combination",
            "Binomial coefficient",
            "Multinomial coefficient",
            "Pascal's relation",
            "Pascal's triangle",
            "Chu's theorem",
            "Sum of powers of positive integers",
            "Pascal matrix",
            "Permutation"
          ],
          "badges": [],
          "prereqs": [],
          "mapCoords": {
            "q": 1, 
            "r": 2
          },
          "topicGroup": "combinatorics",
          "tesseraMessage": "Counting cleverly is an art. Combinatorics powers probability and CS alike.",

          "headline": "Combinatorics: The Art of Counting and Arranging",
          "description": "Introduction to combinatorics such as counting techniques.",
          "abstract": "Combinatorics studies counting, arranging, and analyzing discrete structures. The fundamental counting principle (sum and product rules), combinations C(n,r) = n!/(r!(n-r)!), and permutations form the foundation. Advanced techniques like Pascal's relation and Chu's theorem connect to linear algebra via the Pascal matrix.",
          "datePublished": "2025-02-20",
          "dateModified": "2026-01-31",

          "teaches": [
            "Rule of sum for disjoint unions",
            "Rule of product for sequential decisions",
            "Combinations: choosing r from n (order doesn't matter)",
            "Binomial coefficients C(n,r) = n!/(r!(n-r)!)",
            "Multinomial coefficients",
            "Pascal's relation: C(n+1,r) = C(n,r-1) + C(n,r)",
            "Chu's theorem: sum of C(k,r) = C(n+1,r+1)",
            "Sum of powers via combinations",
            "Pascal matrix and linear algebra connection",
            "Permutations: ordered selections"
          ],

          "competencyRequired": [
            "Basic arithmetic and factorials",
            "Algebraic manipulation"
          ],

          "sections": [
            {"id": "intro", "name": "Introduction"},
            {"id": "count", "name": "Counting"}
          ],

          "definitions": [
            {
              "term": "Combination",
              "definition": "C(n,r) = n!/(r!(n-r)!); ways to choose r items from n without order"
            },
            {
              "term": "Permutation",
              "definition": "P(n,r) = n!/(n-r)! = r!C(n,r); ordered selections"
            },
            {
              "term": "Pascal's Relation",
              "definition": "C(n+1,r) = C(n,r-1) + C(n,r) for 1 ≤ r ≤ n"
            },
            {
              "term": "Chu's Theorem",
              "definition": "∑C(k,r) from k=r to n equals C(n+1,r+1)"
            },
            {
              "term": "Multinomial Coefficient",
              "definition": "n!/(r₁!r₂!...rₖ!) for partitioning n items into k groups"
            }
          ],

          "about": [
            {"name": "Combinatorics", "description": "Mathematics of counting and arrangement"},
            {"name": "Binomial Coefficients", "description": "Foundation of combination theory"},
            {"name": "Pascal's Triangle", "description": "Visual representation of binomial coefficients"},
            {"name": "Permutations", "description": "Ordered arrangements"}
          ]
        },
        {
          "id": "disc-3",
          "part": 3,
          "title": "Intro to Theory of Computation",
          "url": "intro_automata.html",
          "icon": "M",
          "keywords": [
            "Finite automata",
            "Finite state machine",
            "Regular language",
            "State diagram",
            "Transition function",
            "Deterministic finite automata (DFAs)",
            "Nondeterministic finite automata (NFAs)",
            "Regular expression",
            "Regular operations",
            "Union",
            "Concatenation",
            "Kleene star",
            "Power set construction"
          ],
          "badges": [],
          "prereqs": [],
          "mapCoords": {
            "q": 2, 
            "r": 1
          },
          "topicGroup": "computation",
          "tesseraMessage": "Automata recognize patterns. The simplest model of computation!",

          "headline": "Theory of Computation: Finite Automata and Regular Languages",
          "description": "Learn about basic automata theory and regular expressions.",
          "abstract": "Automata theory provides the mathematical foundation for understanding computation. A finite automaton (Q, Σ, δ, q₀, F) recognizes regular languages through state transitions. DFAs and NFAs are equivalent in power, and regular expressions describe exactly the regular languages—essential for compilers and pattern matching.",
          "datePublished": "2025-02-20",
          "dateModified": "2026-01-31",

          "teaches": [
            "Finite automaton as 5-tuple (Q, Σ, δ, q₀, F)",
            "States, alphabet, transition function, start/accept states",
            "Regular languages: those recognized by finite automata",
            "State diagrams for visualizing automata",
            "Regular operations: union, concatenation, Kleene star",
            "Closure of regular languages under regular operations",
            "DFA vs NFA: deterministic vs nondeterministic",
            "NFA to DFA conversion via power set construction",
            "Regular expressions: syntax and semantics",
            "Equivalence of regular expressions and finite automata"
          ],

          "competencyRequired": [
            "Set notation",
            "Function concepts",
            "Basic proof techniques"
          ],

          "sections": [
            {"id": "intro", "name": "Introduction"},
            {"id": "fa_rl", "name": "Finite automata & Regular Languages"},
            {"id": "regular_op", "name": "Regular Operations"},
            {"id": "dfa", "name": "Deterministic & Nondeterministic Machine"},
            {"id": "r_exp", "name": "Regular Expressions"}
          ],

          "definitions": [
            {
              "term": "Finite Automaton",
              "definition": "5-tuple (Q, Σ, δ, q₀, F): states, alphabet, transition function, start state, accept states"
            },
            {
              "term": "Regular Language",
              "definition": "A language recognized by some finite automaton"
            },
            {
              "term": "DFA",
              "definition": "Deterministic FA: δ: Q × Σ → Q returns exactly one state"
            },
            {
              "term": "NFA",
              "definition": "Nondeterministic FA: δ: Q × Σε → P(Q) may return multiple states"
            },
            {
              "term": "Kleene Star",
              "definition": "R* = {ε} ∪ R ∪ RR ∪ RRR ∪ ...; zero or more repetitions"
            },
            {
              "term": "Regular Expression",
              "definition": "Built from Σ, ε, ∅ using union, concatenation, and star"
            }
          ],

          "about": [
            {"name": "Automata Theory", "description": "Study of abstract computing machines"},
            {"name": "Regular Languages", "description": "Languages recognized by finite automata"},
            {"name": "Finite State Machines", "description": "Simplest computational model"},
            {"name": "Regular Expressions", "description": "Pattern description language"}
          ]
        },
        { 
          "id": "disc-4",
          "part": 4,
          "title": "Boolean Logic",
          "url": "boolean.html",
          "icon": "0∧1",
          "keywords": [
            "Boolean logic",
            "Logical operations",
            "Boolean algebra",
            "Negation (NOT)",
            "Conjunction (AND)",
            "Disjunction (OR)",
            "Exclusive or (XOR)",
            "NAND",
            "NOR",
            "XNOR",
            "Implication",
            "Functional completeness",
            "Logic gates",
            "Boolean circuits",
            "Directed acyclic graph (DAG)"
          ],
          "badges": [],
          "prereqs": [],
          "mapCoords": {
            "q": 1, 
            "r": 1
          },
          "topicGroup": "boolean",
          "tesseraMessage": "True or false, 0 or 1 — Boolean logic is how computers think.",

          "headline": "Boolean Logic: The Foundation of Digital Computation",
          "description": "Learn about basic Boolean logic and circuits.",
          "abstract": "Boolean logic operates on TRUE/FALSE values using operations like AND (∧), OR (∨), and NOT (¬). NAND and NOR are functionally complete—any Boolean function can be built from just one of them. Boolean circuits formalize computation as DAGs of logic gates, bridging mathematics and hardware.",
          "datePublished": "2025-03-11",
          "dateModified": "2026-01-31",

          "teaches": [
            "Boolean values: TRUE (1) and FALSE (0)",
            "Fundamental operations: NOT (¬), AND (∧), OR (∨)",
            "Extended operations: XOR (⊕), XNOR (↔), implication (→)",
            "NAND (↑) and NOR (↓) as negations of AND/OR",
            "Functional completeness of NAND",
            "Constructing NOT, AND, OR from NAND alone",
            "Logic gates as hardware implementations",
            "Boolean circuits as DAGs",
            "Applications in digital electronics and complexity theory"
          ],

          "competencyRequired": [
            "Basic logic concepts",
            "Understanding of TRUE/FALSE"
          ],

          "sections": [
            {"id": "bool", "name": "Boolean Logic"},
            {"id": "negation", "name": "Negation of Conjunction & Disjunction"},
            {"id": "circuit", "name": "Circuits"}
          ],

          "definitions": [
            {
              "term": "Negation (NOT)",
              "definition": "¬P flips the truth value: ¬1 = 0, ¬0 = 1"
            },
            {
              "term": "Conjunction (AND)",
              "definition": "P ∧ Q = 1 iff both P and Q are 1"
            },
            {
              "term": "Disjunction (OR)",
              "definition": "P ∨ Q = 1 iff at least one of P or Q is 1"
            },
            {
              "term": "NAND",
              "definition": "P ↑ Q = ¬(P ∧ Q); functionally complete operation"
            },
            {
              "term": "Functional Completeness",
              "definition": "A set of operations that can express any Boolean function"
            },
            {
              "term": "Boolean Circuit",
              "definition": "DAG of AND, OR, NOT gates with inputs and output nodes"
            }
          ],

          "about": [
            {"name": "Boolean Logic", "description": "Two-valued logic system"},
            {"name": "Logic Gates", "description": "Hardware implementation of Boolean operations"},
            {"name": "Functional Completeness", "description": "Universal gate sets"},
            {"name": "Boolean Circuits", "description": "Theoretical model of digital computation"}
          ]
        },
        {
          "id": "disc-5",
          "part": 5,
          "title": "Context-Free Languages",
          "url": "context_free.html",
          "icon": "S→",
          "keywords": [
            "Context-free grammar (CFG)",
            "Context-free language (CFL)",
            "Variables",
            "Terminals",
            "Production rules",
            "Derivation",
            "Nonregular language",
            "Pumping lemma",
            "Pumping lemma for CFLs",
            "Pushdown automata (PDA)",
            "Stack",
            "Stack alphabet",
            "Deterministic pushdown automata (DPDA)",
            "Deterministic context-free languages (DCFL)",
            "Parser",
            "Syntax analyzer"
          ],
          "badges": [],
          "prereqs": ["disc-3"],
          "mapCoords": {
            "q": 2, 
            "r": 2
          },
          "topicGroup": "computation",
          "tesseraMessage": "Context-free grammars define programming languages. You use them every day!",

          "headline": "Context-Free Languages: Grammars, PDAs, and the Chomsky Hierarchy",
          "description": "Learn about context-free languages and pushdown automata.",
          "abstract": "Context-free grammars (CFGs) generate languages beyond regular languages, including recursive structures like {0ⁿ1ⁿ}. A CFG is a 4-tuple (V, Σ, R, S) with variables, terminals, rules, and start symbol. Pushdown automata (PDAs) with stack memory recognize exactly the CFLs. DPDAs power practical parsers in compilers.",
          "datePublished": "2025-03-12",
          "dateModified": "2026-01-31",

          "teaches": [
            "Context-free grammar as 4-tuple (V, Σ, R, S)",
            "Variables, terminals, production rules, start variable",
            "Derivations and string generation",
            "CFGs are more powerful than regular expressions",
            "Pumping lemma for regular languages",
            "Using pumping lemma to prove nonregularity",
            "Pushdown automata: 6-tuple with stack",
            "CFG ↔ PDA equivalence",
            "Every regular language is context-free",
            "Pumping lemma for context-free languages",
            "Non-context-free languages (e.g., {aⁿbⁿcⁿ})",
            "DPDAs and deterministic context-free languages"
          ],

          "competencyRequired": [
            "Finite automata and regular languages",
            "Set notation and proof by contradiction"
          ],

          "sections": [
            {"id": "CFG", "name": "Context-Free Grammar (CFG)"},
            {"id": "Pumping", "name": "Pumping Lemma"},
            {"id": "push", "name": "Pushdown Automata"},
            {"id": "NCF", "name": "Non-Context-Free Languages"},
            {"id": "DPDA", "name": "Deterministic Pushdown Automata (DPDAs)"}
          ],

          "definitions": [
            {
              "term": "Context-Free Grammar",
              "definition": "4-tuple (V, Σ, R, S): variables, terminals, production rules, start variable"
            },
            {
              "term": "Pumping Lemma (Regular)",
              "definition": "If L is regular, strings s ∈ L can be split xyz with xy^i z ∈ L for all i ≥ 0"
            },
            {
              "term": "Pushdown Automaton",
              "definition": "6-tuple (Q, Σ, Γ, δ, q₀, F) with stack alphabet Γ"
            },
            {
              "term": "Pumping Lemma (CFL)",
              "definition": "If L is CFL, strings split uvxyz with uv^i xy^i z ∈ L for all i ≥ 0"
            },
            {
              "term": "DPDA",
              "definition": "Deterministic PDA; recognizes DCFLs used in practical parsers"
            }
          ],

          "about": [
            {"name": "Context-Free Grammars", "description": "Generate programming language syntax"},
            {"name": "Pushdown Automata", "description": "Finite automata with stack memory"},
            {"name": "Pumping Lemmas", "description": "Tools for proving language limitations"},
            {"name": "Parsers", "description": "Practical application in compilers"}
          ]
        },
        { 
          "id": "disc-6",
          "part": 6,
          "title": "Turing Machines",
          "url": "turing_machine.html",
          "icon": "TM",
          "keywords": [
            "Turing machine",
            "Alan Turing",
            "Tape alphabet",
            "Blank symbol",
            "Transition function",
            "Configuration",
            "Turing-recognizable",
            "Turing-decidable",
            "Decidable language",
            "Undecidable language",
            "Co-Turing-recognizable",
            "Church-Turing thesis",
            "Lambda calculus",
            "Unsolvability",
            "Countable sets",
            "Uncountable sets"
          ],
          "badges": [],
          "prereqs": ["disc-5"],
          "mapCoords": {
            "q": 3, 
            "r": 1
          },
          "topicGroup": "computation",
          "tesseraMessage": "Turing machines define what's computable. Some problems have no algorithm!",

          "headline": "Turing Machines: The Universal Model of Computation",
          "description": "Learn about Turing machines, and solvability of problems.",
          "abstract": "Turing machines define what it means to compute algorithmically. A TM is a 7-tuple with infinite tape, read/write head, and states including accept/reject. The Church-Turing thesis states all algorithms can be expressed as TMs. Crucially, some languages are undecidable—provably beyond algorithmic solution.",
          "datePublished": "2025-03-14",
          "dateModified": "2026-01-31",

          "teaches": [
            "Turing machine as 7-tuple (Q, Σ, Γ, δ, q₀, qₐccept, qᵣeject)",
            "Infinite tape with read/write head",
            "Transition function: δ: Q × Γ → Q × Γ × {L, R}",
            "Configurations and computation sequences",
            "Turing-recognizable vs Turing-decidable languages",
            "Church-Turing thesis: TMs capture all algorithms",
            "Every CFL is decidable",
            "Some languages are undecidable",
            "Decidable iff both recognizable and co-recognizable",
            "Countability argument: more languages than TMs",
            "Unsolvable problems exist",
            "Approximation and heuristic algorithms for intractable problems"
          ],

          "competencyRequired": [
            "Context-free languages and PDAs",
            "Basic set theory (countable vs uncountable)"
          ],

          "sections": [
            {"id": "TM", "name": "Turing Machines (TMs)"},
            {"id": "algo", "name": "Algorithms"},
            {"id": "unsolve", "name": "Unsolvability"}
          ],

          "definitions": [
            {
              "term": "Turing Machine",
              "definition": "7-tuple (Q, Σ, Γ, δ, q₀, qₐccept, qᵣeject) with infinite tape"
            },
            {
              "term": "Turing-Recognizable",
              "definition": "Language accepted by some TM (may loop on non-members)"
            },
            {
              "term": "Turing-Decidable",
              "definition": "Language decided by TM that halts on all inputs"
            },
            {
              "term": "Church-Turing Thesis",
              "definition": "Any algorithm can be implemented as a Turing machine"
            },
            {
              "term": "Undecidable Language",
              "definition": "No TM decides it; some are recognizable but not decidable"
            }
          ],

          "about": [
            {"name": "Turing Machines", "description": "Universal model of computation"},
            {"name": "Decidability", "description": "Whether problems can be algorithmically solved"},
            {"name": "Church-Turing Thesis", "description": "Equivalence of algorithms and TMs"},
            {"name": "Unsolvability", "description": "Fundamental limits of computation"}
          ]
        },
        {
          "id": "disc-7",
          "part": 7,
          "title": "Time Complexity",
          "url": "time_complexity.html",
          "icon": "O(g(n))",
          "keywords": [
            "Time complexity",
            "Running time",
            "Asymptotic analysis",
            "Big-O notation",
            "Little-o notation",
            "Time complexity class",
            "Class P",
            "Polynomial time",
            "Exponential time",
            "Deterministic Turing machine",
            "Multitape Turing machine",
            "Polynomially equivalent",
            "Worst-case analysis"
          ],
          "badges": ["code"],
          "prereqs": ["disc-6"],
          "mapCoords": {
            "q": 3, 
            "r": 2
          },
          "topicGroup": "computation",
          "tesseraMessage": "Big-O notation tells you how algorithms scale. Essential for interviews!",

          "headline": "Time Complexity: From Big-O to Class P",
          "description": "Introduction to time complexity theory.",
          "abstract": "Time complexity measures computational resources as a function of input size. Big-O notation captures asymptotic upper bounds, ignoring constants and lower-order terms. Class P = ∪ₖ TIME(nᵏ) contains problems solvable in polynomial time—the boundary between tractable and intractable computation.",
          "datePublished": "2025-03-18",
          "dateModified": "2026-01-31",

          "teaches": [
            "Time complexity as function f(n) of input length",
            "Asymptotic analysis: focus on dominant terms",
            "Big-O notation: f(n) = O(g(n)) for upper bounds",
            "Little-o notation: f(n) = o(g(n)) for strict bounds",
            "TIME(t(n)) complexity class",
            "Polynomial vs exponential time",
            "Multitape TMs are polynomially equivalent to single-tape",
            "Class P: polynomial time decidable languages",
            "P = ∪ₖ TIME(nᵏ)",
            "Practical implications of complexity classes"
          ],

          "competencyRequired": [
            "Turing machines",
            "Basic calculus (limits)",
            "Algorithm basics"
          ],

          "sections": [
            {"id": "intro", "name": "Time Complexity"},
            {"id": "class", "name": "Time Complexity Class"},
            {"id": "P", "name": "Class P"}
          ],

          "definitions": [
            {
              "term": "Big-O Notation",
              "definition": "f(n) = O(g(n)) if ∃c, n₀: f(n) ≤ cg(n) for all n ≥ n₀"
            },
            {
              "term": "Little-o Notation",
              "definition": "f(n) = o(g(n)) if lim f(n)/g(n) = 0 as n → ∞"
            },
            {
              "term": "TIME(t(n))",
              "definition": "Languages decided by O(t(n)) time deterministic TM"
            },
            {
              "term": "Class P",
              "definition": "P = ∪ₖ TIME(nᵏ); polynomial time decidable problems"
            },
            {
              "term": "Polynomially Equivalent",
              "definition": "Models differing by at most polynomial factor in time"
            }
          ],

          "about": [
            {"name": "Asymptotic Analysis", "description": "Analyzing algorithm growth rates"},
            {"name": "Big-O Notation", "description": "Standard complexity notation"},
            {"name": "Class P", "description": "Tractable problems"},
            {"name": "Complexity Theory", "description": "Study of computational resources"}
          ]
        },
        {   
          "id": "disc-8",
          "part": 8,
          "title": "Eulerian & Hamiltonian",
          "url": "Eulerian.html",
          "icon": "Cₙ",
          "keywords": [
            "Eulerian graph",
            "Eulerian cycle (Euler tour)",
            "Eulerian path (Eulerian trail)",
            "Semi-Eulerian",
            "Seven Bridges of Königsberg",
            "Even degree",
            "Hamiltonian graph",
            "Hamiltonian cycle",
            "NP-complete",
            "Adjacency matrix",
            "Adjacency list",
            "Space complexity",
            "PATH problem"
          ],
          "badges": ["code"],
          "prereqs": ["disc-1"],
          "mapCoords": {
            "q": 0, 
            "r": 2
          },
          "topicGroup": "graph-theory",
          "tesseraMessage": "Can you traverse every edge? Every vertex? Different questions, different answers.",

          "headline": "Eulerian and Hamiltonian Cycles: Graph Traversal Classics",
          "description": "Learn about Eulerian and Hamiltonian cycles.",
          "abstract": "Eulerian cycles traverse every edge exactly once; a connected graph has one iff every vertex has even degree (Euler's 1736 theorem). Hamiltonian cycles visit every vertex exactly once—but determining existence is NP-complete, a stark contrast in computational difficulty despite superficial similarity.",
          "datePublished": "2025-03-19",
          "dateModified": "2026-01-31",

          "teaches": [
            "PATH problem: does path exist from s to t?",
            "Eulerian cycle: closed walk visiting every edge once",
            "Euler's theorem: Eulerian iff all vertices have even degree",
            "Semi-Eulerian: Eulerian path but not cycle",
            "Hamiltonian cycle: visits every vertex exactly once",
            "Hamiltonian is NP-complete (no known polynomial algorithm)",
            "Polynomial verification of proposed Hamiltonian cycles",
            "Adjacency matrix representation: O(n²) space",
            "Adjacency list representation: better for sparse graphs",
            "Space complexity considerations"
          ],

          "competencyRequired": [
            "Graph theory basics",
            "Vertex degrees"
          ],

          "sections": [
            {"id": "Eulerian", "name": "Eulerian"},
            {"id": "Hamiltonian", "name": "Hamiltonian"},
            {"id": "code", "name": "Is Hamiltonian Cycle? (Coding)"}
          ],

          "definitions": [
            {
              "term": "Eulerian Cycle",
              "definition": "Closed walk traversing every edge exactly once"
            },
            {
              "term": "Euler's Theorem",
              "definition": "Connected graph is Eulerian iff every vertex has even degree"
            },
            {
              "term": "Hamiltonian Cycle",
              "definition": "Cycle visiting every vertex exactly once"
            },
            {
              "term": "Adjacency Matrix",
              "definition": "n×n matrix where entry (i,j) = 1 if edge exists; O(n²) space"
            },
            {
              "term": "Adjacency List",
              "definition": "For each vertex, list of neighbors; efficient for sparse graphs"
            }
          ],

          "about": [
            {"name": "Eulerian Cycles", "description": "Edge traversal problem (polynomial)"},
            {"name": "Hamiltonian Cycles", "description": "Vertex traversal problem (NP-complete)"},
            {"name": "Graph Representations", "description": "Matrix vs list tradeoffs"},
            {"name": "Complexity Contrast", "description": "Similar problems, vastly different difficulty"}
          ]
        },
        { 
          "id": "disc-9",
          "part": 9,
          "title": "Class NP",
          "url": "p_vs_np.html",
          "icon": "P≠NP?",
          "keywords": [
            "Polynomial verifiability",
            "Certificate",
            "Verifier",
            "Nondeterministic polynomial time",
            "Class NP",
            "Nondeterministic Turing machine (NTM)",
            "NTIME",
            "P vs NP question",
            "NP-completeness",
            "NP-complete",
            "NP-hard",
            "Polynomial-time reduction",
            "Polynomial time computable function",
            "Traveling salesperson problem (TSP)",
            "EXPTIME",
            "Millennium Prize Problems"
          ],
          "badges": [],
          "prereqs": ["disc-7"],
          "mapCoords": {
            "q": 4, 
            "r": 1
          },
          "topicGroup": "computation",
          "tesseraMessage": "P vs NP — the million-dollar question! Is checking always easier than solving?",

          "headline": "Class NP: Polynomial Verification and the P vs NP Question",
          "description": "Learn about nondeterministic polynomial time (NP), and NP-Completeness.",
          "abstract": "NP contains problems with polynomial-time verifiers: given a certificate, membership can be verified quickly even if finding the certificate is hard. NP-complete problems are the hardest in NP—if any is in P, then P = NP. This million-dollar question remains open, with most believing P ≠ NP.",
          "datePublished": "2025-03-20",
          "dateModified": "2026-01-31",

          "teaches": [
            "Polynomial verifiability: verify solutions in polynomial time",
            "Verifier algorithm V accepts ⟨w, c⟩ for certificate c",
            "Class NP: languages with polynomial-time verifiers",
            "NP = ∪ₖ NTIME(nᵏ)",
            "Nondeterministic Turing machines",
            "P: decidable in polynomial time",
            "NP: verifiable in polynomial time",
            "P vs NP: open question, most believe P ≠ NP",
            "Known: NP ⊆ EXPTIME",
            "Polynomial-time reduction: A ≤P B",
            "NP-complete: in NP and all NP problems reduce to it",
            "NP-hard: at least as hard as NP-complete",
            "TSP (decision) is NP-complete; TSP (optimization) is NP-hard"
          ],

          "competencyRequired": [
            "Time complexity and Class P",
            "Turing machines",
            "Basic reduction concepts"
          ],

          "sections": [
            {"id": "P_verify", "name": "Polynomial Verifiability"},
            {"id": "NP", "name": "Nondeterministic Polynomial Time (NP)"},
            {"id": "P=NP", "name": "P vs NP Question"},
            {"id": "comp", "name": "NP-Completeness"}
          ],

          "definitions": [
            {
              "term": "Class NP",
              "definition": "Languages with polynomial-time verifiers; NP = ∪ₖ NTIME(nᵏ)"
            },
            {
              "term": "Certificate",
              "definition": "Evidence string c that allows polynomial-time verification"
            },
            {
              "term": "Polynomial-Time Reduction",
              "definition": "A ≤P B: polynomial-time function f where w ∈ A ↔ f(w) ∈ B"
            },
            {
              "term": "NP-Complete",
              "definition": "In NP and every NP problem reduces to it in polynomial time"
            },
            {
              "term": "NP-Hard",
              "definition": "At least as hard as NP-complete; may not be in NP itself"
            }
          ],

          "about": [
            {"name": "Class NP", "description": "Polynomial-time verifiable problems"},
            {"name": "P vs NP", "description": "Fundamental open problem in CS"},
            {"name": "NP-Completeness", "description": "Hardest problems in NP"},
            {"name": "Reductions", "description": "Comparing problem difficulty"}
          ]
        }
      ],
      "reservedSlots": [
        {
          "q": -1,
          "r": 3
        },
        {
          "q": 1,
          "r": 3
        },
        {
          "q": 4,
          "r": 2
        }
      ]
    },
    "V": {
      "id": "machine-learning",
      "title": "Machine Learning",
      "shortTitle": "ML",
      "description": "The grand synthesis of the MATH-CS COMPASS curriculum. Move from theory to implementation by exploring how high-dimensional vectors are optimized across manifolds using statistical learning theory. This section integrates algebraic structure, calculus-based optimization, and probabilistic inference to decode the underlying mathematical 'physics' of artificial intelligence.",
      "tagline": "The Mathematics of Synthesis and Intelligence",
      "icon": "fa-brain",
      "indexUrl": "Mathematics/Machine_learning/ml.html",
      "baseUrl": "Mathematics/Machine_learning/",
      "parts": [
        {
        "id": "ml-1",
        "part": 1,
        "title": "Intro to Machine Learning",
        "url": "intro_ml.html",
        "icon": "🧠?",
        "keywords": [
          "Machine learning",
          "Artificial intelligence (AI)",
          "Deep learning",
          "Supervised learning",
          "Unsupervised learning",
          "Learning process of ML",
          "Categories of machine learning"
        ],
        "badges": [],
        "prereqs": [],
        "mapCoords": {
          "q": 1,
          "r": -1
        },
        "topicGroup": "ml-foundations",
        "tesseraMessage": "Welcome to ML! This is where all the math comes together beautifully.",

        "headline": "Intro to Machine Learning: Understanding Intelligence",
        "description": "Introduction to machine learning fundamentals including supervised and unsupervised learning.",
        "abstract": "Machine learning enables computers to learn from data without explicit programming. This introduction explores the nature of intelligence, distinguishes supervised from unsupervised learning, and outlines the standard ML workflow from problem definition through deployment.",
        "datePublished": "2025-05-31",
        "dateModified": "2026-01-31",

        "teaches": [
          "The concept of artificial intelligence and machine learning",
          "Supervised vs unsupervised learning paradigms",
          "Semi-supervised and self-supervised learning",
          "Basic ML categories: regression, classification, clustering",
          "Dimensionality reduction techniques",
          "Reinforcement learning fundamentals",
          "Standard ML development process"
        ],

        "competencyRequired": [
          "Basic probability and statistics",
          "Linear algebra fundamentals"
        ],

        "sections": [
          {"id": "intro", "name": "The Law of Intelligence"},
          {"id": "sup", "name": "Supervised vs Unsupervised"},
          {"id": "basic", "name": "Basic Categories of ML"},
          {"id": "process", "name": "Standard Process of ML"}
        ],

        "definitions": [
          {
            "term": "Machine Learning",
            "definition": "Field where computers learn patterns from data without explicit programming"
          },
          {
            "term": "Supervised Learning",
            "definition": "Learning from labeled data with input-output pairs"
          },
          {
            "term": "Unsupervised Learning",
            "definition": "Learning patterns from unlabeled data without explicit targets"
          }
        ],

        "about": [
          {"name": "Machine Learning Introduction", "description": "Foundations of machine learning"},
          {"name": "Artificial Intelligence", "description": "The study of intelligent systems"},
          {"name": "Supervised Learning", "description": "Learning with labeled training data"},
          {"name": "Unsupervised Learning", "description": "Discovering hidden patterns in data"}
        ]
        },
        {
          "id": "ml-2",
          "part": 2,
          "title": "Regularized Regression",
          "url": "regression.html",
          "icon": "λ‖w‖ₚ",
          "keywords": [
            "Ridge regression",
            "Bias-variance tradeoff",
            "Generalization",
            "Regularization",
            "Cross-validation (CV)",
            "K-fold cross-validation",
            "Leave-one-out cross-validation (LOOCV)",
            "Lasso regression"
          ],
          "badges": [
            "interactive"
          ],
          "prereqs": [
            "prob-11",
            "calc-7"
          ],
          "mapCoords": {
            "q": 1,
            "r": -2
          },
          "topicGroup": "ml-foundations",
          "tesseraMessage": "Regularization prevents overfitting — the bias-variance tradeoff in action!",

          "headline": "Regularized Regression: Ridge and Lasso Methods",
          "description": "Learn about regularized regression methods including Ridge and Lasso regression.",
          "abstract": "Regularization prevents overfitting by adding penalty terms to the loss function. Ridge regression uses L2 penalty for stable solutions, while Lasso uses L1 penalty for feature selection. Cross-validation helps select optimal regularization strength.",
          "datePublished": "2025-05-31",
          "dateModified": "2026-01-31",

          "teaches": [
            "Ridge regression with L2 regularization",
            "Bias-variance tradeoff in model selection",
            "K-fold and leave-one-out cross-validation",
            "Lasso regression with L1 regularization",
            "Feature selection through sparsity",
            "Regularization parameter tuning"
          ],

          "competencyRequired": [
            "Linear regression fundamentals",
            "Matrix algebra and optimization basics"
          ],

          "sections": [
            {"id": "ridge", "name": "Ridge Regression"},
            {"id": "Demo", "name": "Demo: Ridge Regression"},
            {"id": "CV", "name": "Cross Validation"},
            {"id": "b-v", "name": "Bias-Variance Tradeoff in Ridge Regression"},
            {"id": "lasso", "name": "Lasso Regression"}
          ],

          "definitions": [
            {
              "term": "Ridge Regression",
              "definition": "Linear regression with L2 penalty: min ||y - Xw||² + λ||w||²"
            },
            {
              "term": "Lasso Regression",
              "definition": "Linear regression with L1 penalty: min ||y - Xw||² + λ||w||₁"
            },
            {
              "term": "Cross-Validation",
              "definition": "Model validation by partitioning data into training and validation sets"
            },
            {
              "term": "Bias-Variance Tradeoff",
              "definition": "Balance between model simplicity (high bias) and flexibility (high variance)"
            }
          ],

          "about": [
            {"name": "Regularized Regression", "description": "Preventing overfitting through penalties"},
            {"name": "Ridge Regression", "description": "L2 regularization for stable solutions"},
            {"name": "Lasso Regression", "description": "L1 regularization for sparse solutions"},
            {"name": "Cross Validation", "description": "Model selection technique"},
            {"name": "Bias-Variance Tradeoff", "description": "Fundamental ML concept"}
          ]
        },
        {
          "id": "ml-3",
          "part": 3,
          "title": "Intro to Classification",
          "url": "intro_classification.html",
          "icon": "𝒳↦𝒴",
          "keywords": [
            "Binary logistic regression",
            "sigmoid (logistic) function",
            "logit (pre-activation)",
            "Decision boundary",
            "Feature mapping",
            "Linearly separable",
            "Kernel trick",
            "Random fourier features",
            "RBF (Gaussian) kernel",
            "Softmax function",
            "Multinomial logistic regression"
          ],
          "badges": [
            "interactive"
          ],
          "prereqs": [
            "ml-2"
          ],
          "mapCoords": {
            "q": 2,
            "r": -3
          },
          "topicGroup": "ml-foundations",
          "tesseraMessage": "Classification draws boundaries in feature space. Kernels bend those boundaries!",

          "headline": "Classification: From Logistic Regression to Kernel Methods",
          "description": "Learn about basic classification methods including logistic regression and kernel methods.",
          "abstract": "Classification assigns inputs to discrete categories. Binary logistic regression uses the sigmoid function to model probabilities, while kernel methods enable nonlinear decision boundaries. The kernel trick allows efficient computation in high-dimensional feature spaces.",
          "datePublished": "2025-05-31",
          "dateModified": "2026-01-31",
          
          "teaches": [
            "Binary logistic regression model",
            "Sigmoid function and decision boundaries",
            "Feature mapping for nonlinear classification",
            "Kernel trick for implicit high-dimensional mapping",
            "RBF kernel and random Fourier features",
            "Softmax function for multiclass problems",
            "Multinomial logistic regression"
          ],

          "competencyRequired": [
            "Linear regression and optimization",
            "Probability fundamentals"
          ],

          "sections": [
            {"id": "intro", "name": "Binary Logistic Regression"},
            {"id": "demo", "name": "Binary Logistic Regression Demo"},
            {"id": "kernel", "name": "Kernel Methods"},
            {"id": "rbf-approx", "name": "RBF Kernel & Random Fourier Features"},
            {"id": "mlr", "name": "Multinomial logistic regression"}
          ],

          "definitions": [
            {
              "term": "Logistic Regression",
              "definition": "Classification model: p(y=1|x) = σ(wᵀx + b) using sigmoid function"
            },
            {
              "term": "Sigmoid Function",
              "definition": "σ(z) = 1/(1 + e⁻ᶻ) mapping real values to (0,1)"
            },
            {
              "term": "Kernel Trick",
              "definition": "Computing inner products in high-dimensional space without explicit mapping"
            },
            {
              "term": "Softmax Function",
              "definition": "Multiclass generalization: p(y=k|x) = exp(zₖ)/Σⱼexp(zⱼ)"
            }
          ],

          "about": [
            {"name": "Classification", "description": "Assigning inputs to discrete categories"},
            {"name": "Logistic Regression", "description": "Probabilistic binary classification"},
            {"name": "Kernel Methods", "description": "Nonlinear classification via feature mapping"},
            {"name": "Softmax", "description": "Multiclass probability distribution"}
          ]
        },
        {
          "id": "ml-4",
          "part": 4,
          "title": "Neural Networks Basics",
          "url": "neural_networks.html",
          "icon": "x↦hθ(x)",
          "keywords": [
            "Deep neural network (DNN)",
            "Multilayer perceptron (MLP)",
            "Hidden layer",
            "Activation function",
            "ReLU",
            "Vanishing gradients",
            "Backpropagation",
            "Gradient clipping",
            "Exploding gradients",
            "Graphics processing units (GPUs)"
          ],
          "badges": [
            "interactive"
          ],
          "prereqs": [
            "ml-3",
            "calc-2"
          ],
          "mapCoords": {
            "q": 1,
            "r": 0
          },
          "topicGroup": "neural-networks",
          "tesseraMessage": "Neural networks learn features automatically. Layers upon layers of abstraction!",

          "headline": "Neural Networks: Multilayer Perceptrons and Backpropagation",
          "description": "Learn about Neural Networks basics including MLP architecture and training.",
          "abstract": "Neural networks compose simple functions into complex models. Multilayer perceptrons stack linear transformations with nonlinear activations. Backpropagation efficiently computes gradients through the chain rule, enabling training via gradient descent.",
          "datePublished": "2025-05-31",
          "dateModified": "2026-01-31",

          "teaches": [
            "Multilayer perceptron architecture",
            "Hidden layers and depth",
            "Activation functions: ReLU, sigmoid, tanh",
            "Backpropagation algorithm",
            "Vanishing and exploding gradients",
            "Gradient clipping techniques",
            "GPU acceleration for deep learning"
          ],

          "competencyRequired": [
            "Logistic regression",
            "Chain rule and Jacobian matrices"
          ],

          "sections": [
            {"id": "MLP", "name": "Multilayer Perceptron (MLP)"},
            {"id": "activation", "name": "Activation Functions"},
            {"id": "learning", "name": "Learning in Neural Networks"},
            {"id": "demo", "name": "Neural Networks Demo"},
            {"id": "GPU", "name": "Development of Deep Learning"}
          ],

          "definitions": [
            {
              "term": "Multilayer Perceptron",
              "definition": "Neural network with input, hidden, and output layers: h = σ(Wx + b)"
            },
            {
              "term": "Activation Function",
              "definition": "Nonlinear function applied element-wise; enables learning complex patterns"
            },
            {
              "term": "ReLU",
              "definition": "Rectified Linear Unit: f(x) = max(0, x)"
            },
            {
              "term": "Backpropagation",
              "definition": "Efficient gradient computation via reverse-mode automatic differentiation"
            }
          ],

          "about": [
            {"name": "Neural Networks", "description": "Layered computational models"},
            {"name": "Multilayer Perceptron", "description": "Feedforward neural network architecture"},
            {"name": "Backpropagation", "description": "Training algorithm for neural networks"},
            {"name": "Activation Functions", "description": "Nonlinear transformations in networks"}
          ]
        },
        {
          "id": "ml-5",
          "part": 5,
          "title": "Automatic Differentiation",
          "url": "autodiff.html",
          "icon": "∇ℒ",
          "keywords": [
            "Automatic differentiation (AD)",
            "Computational graph"
          ],
          "badges": [
            "code"
          ],
          "prereqs": [
            "ml-4"
          ],
          "mapCoords": {
            "q": 2,
            "r": 0
          },
          "topicGroup": "neural-networks",
          "tesseraMessage": "Autodiff computes gradients automatically. The engine behind deep learning!",

          "headline": "Automatic Differentiation: The Engine Behind Deep Learning",
          "description": "Learn about automatic differentiation with analytic examples and implementation.",
          "abstract": "Automatic differentiation systematically applies the chain rule to compute exact gradients through computational graphs. Reverse-mode AD (backpropagation) efficiently computes gradients for functions with many inputs and few outputs—perfect for neural network training.",
          "datePublished": "2025-06-12",
          "dateModified": "2026-01-31",

          "teaches": [
            "Computational graphs and DAGs",
            "Forward-mode vs reverse-mode AD",
            "Reverse-mode AD implementation",
            "Analytic examples of gradient computation",
            "Applications to neural network training"
          ],

          "competencyRequired": [
            "Backpropagation basics",
            "Chain rule and Jacobians"
          ],

          "sections": [
            {"id": "AD", "name": "Automatic Differentiation"},
            {"id": "eg", "name": "Analytic Example of Reverse-Mode AD"},
            {"id": "app", "name": "Applications of AD"},
            {"id": "code", "name": "Sample Code"}
          ],

          "definitions": [
            {
              "term": "Automatic Differentiation",
              "definition": "Systematic chain rule application for exact gradient computation"
            },
            {
              "term": "Computational Graph",
              "definition": "DAG representing computation from inputs to outputs"
            },
            {
              "term": "Reverse-Mode AD",
              "definition": "Backpropagation: efficient for many inputs, few outputs"
            }
          ],

          "about": [
            {"name": "Automatic Differentiation", "description": "Exact gradient computation technique"},
            {"name": "Computational Graphs", "description": "Representing computations as graphs"},
            {"name": "Chain Rule", "description": "Foundation of differentiation through compositions"},
            {"name": "Neural Networks", "description": "Primary application of autodiff"}
          ]
        },
        {
          "id": "ml-6",
          "part": 6,
          "title": "Support Vector Machine (SVM)",
          "url": "svm.html",
          "icon": "wᵀx+w₀",
          "keywords": [
            "Support vector machine (SVM)",
            "Soft margin constraints"
          ],
          "badges": [
            "interactive"
          ],
          "prereqs": [
            "ml-3",
            "calc-9"
          ],
          "mapCoords": {
            "q": 2,
            "r": -2
          },
          "topicGroup": "ml-foundations",
          "tesseraMessage": "SVMs find the widest margin. Elegant geometry meets optimization!",

          "headline": "Support Vector Machines: Maximum Margin Classification",
          "description": "Learn about support vector machine basics including margin maximization and soft margins.",
          "abstract": "Support Vector Machines find the hyperplane maximizing margin between classes. The dual formulation enables the kernel trick for nonlinear classification. Soft margin constraints handle non-separable data by allowing controlled misclassification.",
          "datePublished": "2025-06-06",
          "dateModified": "2026-01-31",

          "teaches": [
            "Maximum margin classification",
            "Support vectors and margin geometry",
            "SVM primal and dual formulations",
            "Soft margin constraints for non-separable data",
            "Kernel SVM for nonlinear boundaries",
            "KKT conditions in SVM"
          ],

          "competencyRequired": [
            "Classification fundamentals",
            "Constrained optimization and KKT conditions"
          ],

          "sections": [
            {"id": "svm", "name": "Support Vector Machine"},
            {"id": "smc", "name": "Soft Margin Constraints"},
            {"id": "demo", "name": "SVM Demo"}
          ],

          "definitions": [
            {
              "term": "Support Vector Machine",
              "definition": "Maximum margin classifier: max margin s.t. yᵢ(wᵀxᵢ + b) ≥ 1"
            },
            {
              "term": "Support Vectors",
              "definition": "Training points lying on or violating the margin boundary"
            },
            {
              "term": "Soft Margin",
              "definition": "SVM allowing slack variables for non-separable data"
            }
          ],

          "about": [
            {"name": "Support Vector Machine", "description": "Maximum margin classifier"},
            {"name": "Kernel Methods", "description": "Nonlinear SVM via kernel trick"},
            {"name": "Margin Classification", "description": "Geometric approach to classification"},
            {"name": "KKT Conditions", "description": "Optimality conditions for SVM"}
          ]
        },
        {
          "id": "ml-7",
          "part": 7,
          "title": "PCA & Autoencoders",
          "url": "pca.html",
          "icon": "𝒦(xᵢ,xⱼ)",
          "keywords": [
            "Principal Component Analysis (PCA)",
            "Dimensionality reduction",
            "Kernel PCA",
            "Double centering trick",
            "Autoencoder",
            "Lipschitz continuity",
            "Data Reconstruction",
            "Denoising autoencoder",
            "Manifolds"
          ],
          "badges": [
            "interactive"
          ],
          "prereqs": [
            "linalg-9",
            "ml-4"
          ],
          "mapCoords": {
            "q": 3,
            "r": -2
          },
          "topicGroup": "dimensionality",
          "tesseraMessage": "PCA finds the directions of maximum variance. Autoencoders learn them!",

          "headline": "PCA & Autoencoders: Dimensionality Reduction Techniques",
          "description": "Learn about unsupervised learning through PCA and Autoencoders.",
          "abstract": "PCA finds orthogonal directions of maximum variance for linear dimensionality reduction. Kernel PCA extends this to nonlinear cases. Autoencoders learn compressed representations via neural networks, with denoising variants improving robustness.",
          "datePublished": "2025-06-17",
          "dateModified": "2026-01-31",

          "teaches": [
            "PCA as variance maximization",
            "Kernel PCA for nonlinear reduction",
            "Double centering trick",
            "Autoencoder architecture",
            "Data reconstruction and compression",
            "Denoising autoencoders",
            "Manifold learning concepts"
          ],

          "competencyRequired": [
            "SVD and eigendecomposition",
            "Neural network basics"
          ],

          "sections": [
            {"id": "intro", "name": "Recap & Introduction"},
            {"id": "kernel-pca", "name": "Kernel PCA"},
            {"id": "auto", "name": "Autoencoders"},
            {"id": "demo", "name": "Demo"},
            {"id": "noise", "name": "Denoising Autoencoders"},
            {"id": "manifolds", "name": "Manifolds"}
          ],

          "definitions": [
            {
              "term": "PCA",
              "definition": "Find directions maximizing variance: W = argmax tr(WᵀΣW)"
            },
            {
              "term": "Kernel PCA",
              "definition": "PCA in feature space using kernel matrix K"
            },
            {
              "term": "Autoencoder",
              "definition": "Neural network learning compressed representation: x → z → x̂"
            },
            {
              "term": "Manifold",
              "definition": "Low-dimensional surface embedded in high-dimensional space"
            }
          ],

          "about": [
            {"name": "Principal Component Analysis", "description": "Linear dimensionality reduction"},
            {"name": "Dimensionality Reduction", "description": "Reducing feature space dimension"},
            {"name": "Autoencoders", "description": "Neural network compression"},
            {"name": "Kernel PCA", "description": "Nonlinear PCA via kernels"}
          ]
        },
        {
        "id": "ml-8",
        "part": 8,
        "title": "Clustering",
        "url": "clustering.html",
        "icon": "fᵀLf",
        "keywords": [
          "K-means clustering",
          "Distortion",
          "One-hot encoding (Dummy encoding)",
          "K-means++",
          "Vector quantization",
          "Spectral clustering",
          "Graph Laplacian",
          "Dirichlet energy"
        ],
        "badges": [
          "interactive"
        ],
        "prereqs": [
          "ml-7",
          "linalg-14"
        ],
        "mapCoords": {
          "q": 2,
          "r": -1
        },
        "topicGroup": "dimensionality",
        "tesseraMessage": "Clustering finds structure without labels. K-means is just the beginning!",

        "headline": "Clustering: From K-means to Spectral Methods",
        "description": "Learn about clustering algorithms including K-means and spectral clustering.",
        "abstract": "Clustering groups similar data points without labels. K-means minimizes distortion through iterative assignment and centroid updates. Spectral clustering uses graph Laplacian eigenvectors to handle non-convex clusters, minimizing Dirichlet energy.",
        "datePublished": "2025-06-21",
        "dateModified": "2026-01-31",

        "teaches": [
          "K-means algorithm and convergence",
          "K-means++ initialization",
          "Vector quantization applications",
          "Graph Laplacian construction",
          "Spectral clustering algorithm",
          "Dirichlet energy minimization"
        ],

        "competencyRequired": [
          "Eigendecomposition",
          "Graph theory basics"
        ],

        "sections": [
          {"id": "intro", "name": "Introduction"},
          {"id": "K-means", "name": "K-means Clustering"},
          {"id": "vq", "name": "Vector Quantization (VQ)"},
          {"id": "sp", "name": "Spectral Clustering"},
          {"id": "demo", "name": "Demo"}
        ],

        "definitions": [
          {
            "term": "K-means",
            "definition": "Minimize distortion: Σₖ Σᵢ∈Cₖ ||xᵢ - μₖ||²"
          },
          {
            "term": "Graph Laplacian",
            "definition": "L = D - W where D is degree matrix, W is adjacency"
          },
          {
            "term": "Spectral Clustering",
            "definition": "Cluster using eigenvectors of graph Laplacian"
          },
          {
            "term": "Dirichlet Energy",
            "definition": "fᵀLf measures smoothness of f over graph"
          }
        ],

        "about": [
          {"name": "Clustering", "description": "Grouping similar data points"},
          {"name": "K-means Clustering", "description": "Centroid-based clustering"},
          {"name": "Spectral Clustering", "description": "Graph-based clustering method"},
          {"name": "Graph Laplacian", "description": "Matrix encoding graph structure"}
        ]
        },
        {
          "id": "ml-9",
          "part": 9,
          "title": "Intro to Deep Neural Networks",
          "url": "deep_nn.html",
          "icon": "🧠!",
          "keywords": [
            "Feedforward networks",
            "Convolutional Neural Networks (CNNs)",
            "Residual connection",
            "Layer normalization",
            "Attention",
            "Self-attention",
            "Multi-Head Attention (MHA)",
            "Positional encoding",
            "Transformer"
          ],
          "badges": [
            "interactive"
          ],
          "prereqs": [
            "ml-5"
          ],
          "mapCoords": {
            "q": 3,
            "r": -1
          },
          "topicGroup": "neural-networks",
          "tesseraMessage": "Transformers changed everything — attention is all you need!",

          "headline": "Deep Neural Networks: From CNNs to Transformers",
          "description": "Learn about deep neural network architectures including CNNs, attention mechanisms, and Transformers.",
          "abstract": "Modern deep learning architectures include CNNs for spatial data, ResNets with skip connections, and Transformers using attention. Self-attention enables modeling long-range dependencies, while multi-head attention captures diverse relationships.",
          "datePublished": "2025-07-01",
          "dateModified": "2026-01-31",

          "teaches": [
            "Convolutional neural networks",
            "Residual connections and ResNet",
            "Layer normalization",
            "Attention mechanisms",
            "Self-attention computation",
            "Multi-head attention",
            "Positional encoding",
            "Transformer architecture"
          ],

          "competencyRequired": [
            "Neural network basics",
            "Automatic differentiation"
          ],

          "sections": [
            {"id": "CNN", "name": "Convolutional Neural Networks (CNNs)"},
            {"id": "ResNet", "name": "ResNet: Residual Connections"},
            {"id": "normalization", "name": "Layer Normalization"},
            {"id": "attention", "name": "Attention Mechanisms"},
            {"id": "self", "name": "Self-Attention"},
            {"id": "multihead", "name": "Multi-Head Attention"},
            {"id": "position", "name": "Positional Encoding"},
            {"id": "transformer", "name": "Transformer Architecture"},
            {"id": "demo", "name": "Transformer Demo"}
          ],

          "definitions": [
            {
              "term": "CNN",
              "definition": "Neural network using convolutional layers for spatial feature extraction"
            },
            {
              "term": "Residual Connection",
              "definition": "Skip connection: y = F(x) + x enabling deeper networks"
            },
            {
              "term": "Self-Attention",
              "definition": "Attention(Q,K,V) = softmax(QKᵀ/√d)V"
            },
            {
              "term": "Transformer",
              "definition": "Architecture using self-attention without recurrence"
            }
          ],

          "about": [
            {"name": "Deep Neural Networks", "description": "Multi-layer neural architectures"},
            {"name": "Convolutional Neural Networks", "description": "CNNs for spatial data"},
            {"name": "Transformer Architecture", "description": "Attention-based architecture"},
            {"name": "Attention Mechanisms", "description": "Dynamic weighting of inputs"}
          ]
        },
        {
          "id": "ml-10",
          "part": 10,
          "title": "Intro to Reinforcement Learning",
          "url": "intro_RL.html",
          "icon": "🔁",
          "keywords": [
            "Reinforcement Learning (RL)",
            "Model-based RL",
            "Model-free RL",
            "Agent",
            "Reward",
            "Policy",
            "Markov Decision Process (MDP)",
            "Discount factor",
            "Return",
            "Value function",
            "Q-function",
            "Advantage function",
            "Bellman's equations",
            "Value Iteration",
            "Policy Iteration",
            "Temporal Difference Learning",
            "Q-Learning",
            "SARSA",
            "Exploration vs Exploitation",
            "Policy Gradient",
            "REINFORCE",
            "Actor-Critic"
          ],
          "badges": [],
          "prereqs": [
            "ml-9",
            "prob-18"
          ],
          "mapCoords": {
            "q": 4,
            "r": -2
          },
          "topicGroup": "reinfortic-learning",
          "tesseraMessage": "RL learns from interaction. Games, robotics, and beyond!",

          "headline": "Reinforcement Learning: From MDPs to Policy Gradients",
          "description": "Learn about Reinforcement Learning fundamentals including MDPs, Bellman equations, and modern algorithms.",
          "abstract": "Reinforcement learning trains agents through environment interaction to maximize cumulative reward. MDPs formalize the setting; Bellman equations enable value computation. Methods range from dynamic programming to temporal difference learning, Q-learning, and policy gradient methods.",
          "datePublished": "2025-07-12",
          "dateModified": "2026-01-31",

          "teaches": [
            "Markov Decision Process formulation",
            "Value functions and Q-functions",
            "Bellman equations",
            "Value iteration and policy iteration",
            "Monte Carlo methods",
            "Temporal difference learning",
            "Q-learning and SARSA",
            "Exploration vs exploitation",
            "Policy gradient methods",
            "Actor-critic algorithms"
          ],

          "competencyRequired": [
            "Probability and expectation",
            "Neural network basics",
            "Markov chains"
          ],

          "sections": [
            {"id": "intro", "name": "Introduction"},
            {"id": "taxonomy", "name": "Classification of RL Algorithms"},
            {"id": "exploration", "name": "Exploration vs. Exploitation"},
            {"id": "mdp", "name": "Markov Decision Process"},
            {"id": "value-functions", "name": "Value Functions & Bellman Equations"},
            {"id": "DP", "name": "Dynamic Programming Algorithms for RL"},
            {"id": "MC", "name": "Monte Carlo Control"},
            {"id": "td-learning", "name": "Temporal Difference Learning"},
            {"id": "TD_Con", "name": "TD Control Methods: Q-Learning & SARSA"},
            {"id": "policy-grad", "name": "Policy Gradient Methods"},
            {"id": "actor-critic", "name": "Actor-Critic Methods"}
          ],

          "definitions": [
            {
              "term": "MDP",
              "definition": "Tuple (S, A, T, r, γ) defining state, action, transition, reward, discount"
            },
            {
              "term": "Value Function",
              "definition": "V(s) = E[Σₜ γᵗrₜ | s₀=s] expected return from state s"
            },
            {
              "term": "Q-Function",
              "definition": "Q(s,a) = E[Σₜ γᵗrₜ | s₀=s, a₀=a] expected return from state-action"
            },
            {
              "term": "Bellman Equation",
              "definition": "V(s) = max_a [r(s,a) + γ Σ T(s'|s,a)V(s')]"
            },
            {
              "term": "Policy Gradient",
              "definition": "∇J(θ) = E[∇log π(a|s) Q(s,a)] gradient of expected return"
            }
          ],

          "about": [
            {"name": "Reinforcement Learning", "description": "Learning through environment interaction"},
            {"name": "Markov Decision Process", "description": "Mathematical framework for sequential decisions"},
            {"name": "Value Functions", "description": "Expected cumulative reward measures"},
            {"name": "Policy Optimization", "description": "Direct optimization of decision policies"},
            {"name": "Temporal Difference Learning", "description": "Bootstrapping value estimates"}
          ]
        }
      ],
      "reservedSlots": [
        {
          "q": 4,
          "r": -1
        },
        {
          "q": 5,
          "r": -1
        },
        {
          "q": 5,
          "r": -2
        }
      ]
    }
  },
  "homeNode": {
    "id": "home",
    "section": "HOME",
    "title": "MATH-CS COMPASS HOME",
    "url": "/index.html",
    "mapCoords": {
      "q": 0,
      "r": 0
    }
  }
}