{
  "meta": {
    "version": "1.0.0",
    "lastUpdated": "2026-01-15",
    "author": "Yusuke Yokota",
    "description": "Centralized curriculum data for MATH-CS COMPASS - single source of truth for section pages and compass map"
  },
  "sectionColors": {
    "I": "#1565c0",
    "II": "#2e7d32",
    "III": "#00838f",
    "IV": "#6a1b9a",
    "V": "#ef6c00"
  },
  "sections": {
    "I": {
      "id": "linear-algebra",
      "title": "Linear Algebra to Algebraic Foundations",
      "shortTitle": "Linear Algebra",
      "description": "Explore the foundations of Linear Algebra, covering key concepts such as linear equations, vector spaces, eigenvalues, orthogonality, least squares, and stochastic matrices.",
      "tagline": "The Mathematics of Structure and Space",
      "icon": "fa-vector-square",
      "indexUrl": "Mathematics/Linear_algebra/linear_algebra.html",
      "baseUrl": "Mathematics/Linear_algebra/",
      "parts": [
        {
          "id": "linalg-1",
          "part": 1,
          "title": "Linear Equations",
          "url": "linear_equations.html",
          "icon": "Ax=b",
          "keywords": [
            "System of linear equations",
            "Reduced row echelon form",
            "Linear combination",
            "Span",
            "Matrix equation",
            "Homogeneous & nonhomogeneous system",
            "Linear independence & dependence",
            "Parametric vector form"
          ],
          "badges": [],
          "prereqs": [],
          "mapCoords": {
            "q": -1,
            "r": 1
          },
          "topicGroup": "core-linalg",
          "tesseraMessage": "Where every journey begins! Ax = b is the foundation of so much in CS."
        },
        {
          "id": "linalg-2",
          "part": 2,
          "title": "Linear Transformation",
          "url": "linear_transformation.html",
          "icon": "T",
          "keywords": [
            "Linear transformation",
            "Linearity",
            "Onto",
            "One-to-one",
            "Matrix multiplication"
          ],
          "badges": [
            "interactive"
          ],
          "prereqs": [
            "linalg-1"
          ],
          "mapCoords": {
            "q": -1,
            "r": 2
          },
          "topicGroup": "core-linalg",
          "tesseraMessage": "Transformations are how we move and reshape data. Graphics, ML, everywhere!"
        },
        {
          "id": "linalg-3",
          "part": 3,
          "title": "Matrix Algebra",
          "url": "matrix_algebra.html",
          "icon": "A",
          "keywords": [
            "Diagonal matrix",
            "Identity matrix",
            "Transpose of a matrix",
            "Invertible matrix",
            "Singular matrix",
            "Elementary matrix",
            "Partitioned Matrix",
            "LU Factorization"
          ],
          "badges": [],
          "prereqs": [
            "linalg-1"
          ],
          "mapCoords": {
            "q": -2,
            "r": 1
          },
          "topicGroup": "core-linalg",
          "tesseraMessage": "Matrices are the language computers speak. Master this well!"
        },
        {
          "id": "linalg-4",
          "part": 4,
          "title": "Determinants",
          "url": "determinants.html",
          "icon": "|A|",
          "keywords": [
            "Determinant",
            "Cofactor expansion",
            "Cramer's rule",
            "Adjugate",
            "Inverse formula",
            "Invertible matrix"
          ],
          "badges": [],
          "prereqs": [
            "linalg-3"
          ],
          "mapCoords": {
            "q": -2,
            "r": 2
          },
          "topicGroup": "core-linalg",
          "tesseraMessage": "Determinants tell you if a matrix is invertible \u2014 one number, so much meaning."
        },
        {
          "id": "linalg-5",
          "part": 5,
          "title": "Vector Spaces",
          "url": "vectorspaces.html",
          "icon": "V",
          "keywords": [
            "Vector space",
            "Subspace",
            "Null space",
            "Kernel",
            "Column space",
            "Row space",
            "Basis",
            "Spanning set",
            "Coordinate systems",
            "Dimension",
            "Rank"
          ],
          "badges": [],
          "prereqs": [
            "linalg-2"
          ],
          "mapCoords": {
            "q": -2,
            "r": 3
          },
          "topicGroup": "core-linalg",
          "tesseraMessage": "Vector spaces are where linear algebra becomes truly abstract and powerful."
        },
        {
          "id": "linalg-6",
          "part": 6,
          "title": "Eigenvalues & Eigenvectors",
          "url": "eigenvectors.html",
          "icon": "\u03bb",
          "keywords": [
            "Eigenvalues",
            "Eigenvectors",
            "Eigenspace",
            "Characteristic equation",
            "Similarity",
            "Diagonalization",
            "Complex eigenvalues & eigenvectors"
          ],
          "badges": [],
          "prereqs": [
            "linalg-4",
            "linalg-5"
          ],
          "mapCoords": {
            "q": -3,
            "r": 2
          },
          "topicGroup": "core-linalg",
          "tesseraMessage": "Eigenvalues appear everywhere \u2014 from Google's PageRank to quantum mechanics!"
        },
        {
          "id": "linalg-7",
          "part": 7,
          "title": "Orthogonality",
          "url": "orthogonality.html",
          "icon": "\u22a5",
          "keywords": [
            "Inner product",
            "Euclidean norm",
            "Orthogonality",
            "Orthogonal complement",
            "Orthogonal & Orthonormal set",
            "Orthogonal projection",
            "Orthogonal matrix",
            "Gram-Schmidt algorithm",
            "QR factorization"
          ],
          "badges": [
            "interactive"
          ],
          "prereqs": [
            "linalg-5"
          ],
          "mapCoords": {
            "q": -3,
            "r": 3
          },
          "topicGroup": "core-linalg",
          "tesseraMessage": "Orthogonality keeps things clean and independent. QR is essential!"
        },
        {
          "id": "linalg-8",
          "part": 8,
          "title": "Least-Squares Problems",
          "url": "leastsquares.html",
          "icon": "min",
          "keywords": [
            "Least-squares solution",
            "Normal equation",
            "Least-squares error",
            "Linear regression",
            "Moore-Penrose pseudo-inverse"
          ],
          "badges": [
            "interactive"
          ],
          "prereqs": [
            "linalg-7"
          ],
          "mapCoords": {
            "q": -3,
            "r": 4
          },
          "topicGroup": "core-linalg",
          "tesseraMessage": "When exact solutions don't exist, least squares finds the best approximation."
        },
        {
          "id": "linalg-9",
          "part": 9,
          "title": "Symmetry",
          "url": "symmetry.html",
          "icon": "S",
          "keywords": [
            "Symmetric matrix",
            "Orthogonally diagonalizable matrix",
            "Spectrum",
            "Quadratic form",
            "Positive definite",
            "Positive semi-definite",
            "Singular value decomposition(SVD)",
            "Condition number",
            "Moore-Penrose pseudo-inverse"
          ],
          "badges": [
            "interactive"
          ],
          "prereqs": [
            "linalg-6",
            "linalg-7"
          ],
          "mapCoords": {
            "q": -4,
            "r": 3
          },
          "topicGroup": "core-linalg",
          "tesseraMessage": "SVD is one of the most useful decompositions in all of applied mathematics!"
        },
        {
          "id": "linalg-10",
          "part": 10,
          "title": "Trace and Norms",
          "url": "trace.html",
          "icon": "Tr",
          "keywords": [
            "Trace",
            "Frobenius norm",
            "Nuclear norm",
            "Induced norm",
            "Spectral norm",
            "p-norm",
            "Manhattan norm",
            "Maximum norm",
            "Normalization",
            "Regularization",
            "Metric space",
            "Normed vector space",
            "Inner product space",
            "Euclidean space"
          ],
          "badges": [],
          "prereqs": [
            "linalg-9"
          ],
          "mapCoords": {
            "q": -4,
            "r": 4
          },
          "topicGroup": "core-linalg",
          "tesseraMessage": "Norms measure size, and different norms give different perspectives."
        },
        {
          "id": "linalg-11",
          "part": 11,
          "title": "Kronecker Product & Tensor",
          "url": "kronecker.html",
          "icon": "\u2297",
          "keywords": [
            "Vectorization",
            "Kronecker product",
            "Tensor",
            "Tensor Product"
          ],
          "badges": [],
          "prereqs": [
            "linalg-10"
          ],
          "mapCoords": {
            "q": -5,
            "r": 4
          },
          "topicGroup": "core-linalg",
          "tesseraMessage": "Tensors power modern deep learning. This is where things get multidimensional!"
        },
        {
          "id": "linalg-12",
          "part": 12,
          "title": "Woodbury Matrix Identity",
          "url": "woodbury.html",
          "icon": "W",
          "keywords": [
            "Woodbury matrix identity",
            "Sherman-Morrison formula"
          ],
          "badges": [],
          "prereqs": [
            "linalg-3"
          ],
          "mapCoords": {
            "q": -3,
            "r": 1
          },
          "topicGroup": "core-linalg",
          "tesseraMessage": "A clever identity that makes updating inverses efficient. Very practical!"
        },
        {
          "id": "linalg-13",
          "part": 13,
          "title": "Stochastic Matrix",
          "url": "stochastic.html",
          "icon": "P",
          "keywords": [
            "Stochastic matrix",
            "Column-stochastic matrix",
            "Row-stochastic matrix",
            "Probability vector",
            "Markov chain",
            "Steady-state vector",
            "Spectral radius",
            "Doubly stochastic matrix"
          ],
          "badges": [],
          "prereqs": [
            "linalg-6"
          ],
          "mapCoords": {
            "q": -4,
            "r": 2
          },
          "topicGroup": "core-linalg",
          "tesseraMessage": "Markov chains are everywhere \u2014 from web search to speech recognition."
        },
        {
          "id": "linalg-14",
          "part": 14,
          "title": "Graph Laplacians and Spectral Methods",
          "url": "graph_laplacian.html",
          "icon": "L",
          "keywords": [
            "Graph Laplacian",
            "Dirichlet energy",
            "Normalized Laplacian",
            "Fiedler vector",
            "Algebraic connectivity",
            "Cheeger's inequality",
            "Graph Fourier Transform (GFT)",
            "Heat diffusion on graphs"
          ],
          "badges": [],
          "prereqs": [
            "linalg-6",
            "disc-1"
          ],
          "mapCoords": {
            "q": -5,
            "r": 3
          },
          "topicGroup": "core-linalg",
          "tesseraMessage": "Graph Laplacians connect linear algebra to network analysis. Beautiful!"
        },
        {
          "id": "linalg-15",
          "part": 15,
          "title": "Intro to Abstract Algebra",
          "url": "intro_groups.html",
          "icon": "G",
          "keywords": [
            "Abstract Algebra",
            "Groups",
            "Binary operation",
            "Closure",
            "Abelian",
            "Non-Abelian",
            "General linear group, GL(n, F)",
            "Modular arithmetic",
            "Units Modulo n, U(n)",
            "Additive vs Multiplicative groups",
            "Order of a group",
            "Order of an element",
            "Subgroups",
            "One-step subgroup test",
            "Cyclic",
            "Generator",
            "Center of a group",
            "Centralizer"
          ],
          "badges": [],
          "prereqs": [
            "linalg-3"
          ],
          "mapCoords": {
            "q": -4,
            "r": 5
          },
          "topicGroup": "abstract-algebra",
          "tesseraMessage": "Welcome to abstract algebra! Groups reveal the hidden symmetry in mathematics."
        },
        {
          "id": "linalg-16",
          "part": 16,
          "title": "More Finite Groups",
          "url": "cyclic_groups.html",
          "icon": "S\u2099",
          "keywords": [
            "Group of Integers Modulo n",
            "Fundamental Theorem of Cyclic Groups",
            "Euler Phi Function",
            "Permutation Groups",
            "Symmetric Groups",
            "Cycle Notation",
            "Products of Disjoint Cycles",
            "Product of 2-Cycles",
            "The Parity Theorem",
            "Alternating Groups"
          ],
          "badges": [],
          "prereqs": [
            "linalg-15"
          ],
          "mapCoords": {
            "q": -5,
            "r": 5
          },
          "topicGroup": "abstract-algebra",
          "tesseraMessage": "Permutations and cyclic structures \u2014 the building blocks of cryptography."
        },
        {
          "id": "linalg-17",
          "part": 17,
          "title": "Structural Group Theory",
          "url": "group_isomorphism.html",
          "icon": "\u2245",
          "keywords": [
            "Cosets",
            "Lagrange's Theorem",
            "Fermat's Little Theorem",
            "Pohlig-Hellman algorithm",
            "Normal Subgroups",
            "Factor Groups",
            "Group Homomorphisms",
            "Kernel",
            "Group Isomorphisms",
            "Group Automorphisms",
            "Inner Automorphisms",
            "Cayley's Theorem",
            "First Isomorphism Theorem"
          ],
          "badges": [],
          "prereqs": [
            "linalg-16"
          ],
          "mapCoords": {
            "q": -5,
            "r": 6
          },
          "topicGroup": "abstract-algebra",
          "tesseraMessage": "Isomorphisms show when different-looking structures are secretly the same."
        },
        {
          "id": "linalg-18",
          "part": 18,
          "title": "Classification of Finite Abelian Groups",
          "url": "group_classification.html",
          "icon": "\u2295",
          "keywords": [
            "External Direct Products",
            "Internal Direct Products",
            "Fundamental Theorem of Finite Abelian Groups"
          ],
          "badges": [],
          "prereqs": [
            "linalg-17"
          ],
          "mapCoords": {
            "q": -6,
            "r": 5
          },
          "topicGroup": "abstract-algebra",
          "tesseraMessage": "Every finite abelian group can be broken down \u2014 a beautiful classification!"
        },
        {
          "id": "linalg-19",
          "part": 19,
          "title": "The Architecture of Rings and Fields",
          "url": "intro_rings.html",
          "icon": "F",
          "keywords": [
            "Rings",
            "Unity",
            "Unit",
            "Subrings",
            "Subring Test",
            "Integral Domains",
            "Zero-Divisors",
            "Cancellation Law",
            "Fields",
            "Characteristic of a Ring"
          ],
          "badges": [],
          "prereqs": [
            "linalg-15"
          ],
          "mapCoords": {
            "q": -6,
            "r": 6
          },
          "topicGroup": "abstract-algebra",
          "tesseraMessage": "Rings and fields \u2014 the algebraic structures behind polynomials and numbers."
        },
        {
          "id": "linalg-20",
          "part": 20,
          "title": "Ideals and Factor Rings",
          "url": "ideals.html",
          "icon": "R/A",
          "keywords": [
            "Ideals",
            "Ideal Test",
            "Principal Ideals",
            "Factor Rings",
            "Prime Ideals",
            "Maximal Ideals",
            "Ring Homomorphisms",
            "Fundamental Theorem of Ring Homomorphisms"
          ],
          "badges": [],
          "prereqs": [
            "linalg-19"
          ],
          "mapCoords": {
            "q": -6,
            "r": 7
          },
          "topicGroup": "abstract-algebra",
          "tesseraMessage": "Ideals are to rings what normal subgroups are to groups. Deep connections!"
        },
        {
          "id": "linalg-21",
          "part": 21,
          "title": "Polynomial Rings",
          "url": "polynomial_rings.html",
          "icon": "R[x]",
          "keywords": [
            "Polynomial Rings",
            "Division Algorithm",
            "Irreducible Polynomials",
            "Eisenstein's Criterion",
            "Principal Ideal Domain",
            "Gauss's Lemma",
            "Unique Factorization",
            "AES Cryptography",
            "Post-Quantum Cryptography"
          ],
          "badges": [],
          "prereqs": [
            "linalg-20",
            "disc-4"
          ],
          "mapCoords": {
            "q": -7,
            "r": 6
          },
          "topicGroup": "abstract-algebra",
          "tesseraMessage": "Polynomial rings power modern cryptography \u2014 from AES to post-quantum!"
        }
      ],
      "reservedSlots": [
        {
          "q": -7,
          "r": 7
        },
        {
          "q": -8,
          "r": 6
        },
        {
          "q": -5,
          "r": 7
        }
      ]
    },
    "II": {
      "id": "calculus",
      "title": "Calculus to Optimization & Analysis",
      "shortTitle": "Calculus",
      "description": "Explore key calculus concepts essential for optimization, analysis, and machine learning. Topics include derivatives, Jacobians, gradient descent, Newton's method, constrained optimization, measure theory, and Lebesgue integration.",
      "tagline": "The Mathematics of Change and Convergence",
      "icon": "fa-chart-line",
      "indexUrl": "Mathematics/Calculus/calculus.html",
      "baseUrl": "Mathematics/Calculus/",
      "parts": [
        {
          "id": "calc-1",
          "part": 1,
          "title": "The Derivative of f:\u211d\u207f\u2192\u211d",
          "url": "linear_approximation.html",
          "icon": "\u2207",
          "keywords": [
            "Linear approximation",
            "Linearization",
            "Differentials",
            "Product rule",
            "Gradient",
            "Quadratic form",
            "L\u2082 norm"
          ],
          "badges": [],
          "prereqs": [
            "linalg-1"
          ],
          "mapCoords": {
            "q": -1,
            "r": 0
          },
          "topicGroup": "derivatives",
          "tesseraMessage": "The gradient points uphill. In ML, we usually go the opposite way!"
        },
        {
          "id": "calc-2",
          "part": 2,
          "title": "The Derivative of f:\u211d\u207f\u2192\u211d\u207f",
          "url": "jacobian.html",
          "icon": "J",
          "keywords": [
            "Jacobian matrix",
            "Chain rule",
            "Backpropagation",
            "reverse mode (forward mode) automatic differentiation"
          ],
          "badges": [],
          "prereqs": [
            "calc-1"
          ],
          "mapCoords": {
            "q": -1,
            "r": -1
          },
          "topicGroup": "derivatives",
          "tesseraMessage": "The Jacobian is the backbone of backpropagation. Chain rule magic!"
        },
        {
          "id": "calc-3",
          "part": 3,
          "title": "The Derivative of f:\u211d\u207f\u02e3\u207f\u2192\u211d\u207f\u02e3\u207f",
          "url": "matrix_cal.html",
          "icon": "M",
          "keywords": [
            "Powers of a matrix",
            "Inverse of a matrix",
            "LU decomposition"
          ],
          "badges": [],
          "prereqs": [
            "calc-2",
            "linalg-3"
          ],
          "mapCoords": {
            "q": -2,
            "r": -1
          },
          "topicGroup": "derivatives",
          "tesseraMessage": "Matrix calculus \u2014 where derivatives get truly multidimensional."
        },
        {
          "id": "calc-4",
          "part": 4,
          "title": "Intro to Numerical Computation",
          "url": "numerical_example1.html",
          "icon": "\u2248",
          "keywords": [
            "Finite-difference approximation",
            "Relative error",
            "Roundoff error"
          ],
          "badges": [
            "code"
          ],
          "prereqs": [
            "calc-1"
          ],
          "mapCoords": {
            "q": -2,
            "r": 0
          },
          "topicGroup": "numerical",
          "tesseraMessage": "Theory meets practice! Understanding numerical errors is crucial for real code."
        },
        {
          "id": "calc-5",
          "part": 5,
          "title": "The Derivative of Scalar Functions of Matrices",
          "url": "det.html",
          "icon": "det",
          "keywords": [
            "Frobenius inner product",
            "Frobenius norm",
            "Trace",
            "Determinant",
            "Cofactor",
            "Adjugate"
          ],
          "badges": [],
          "prereqs": [
            "calc-3",
            "linalg-4"
          ],
          "mapCoords": {
            "q": -2,
            "r": -2
          },
          "topicGroup": "derivatives",
          "tesseraMessage": "Derivatives of determinants and traces \u2014 essential for advanced ML proofs."
        },
        {
          "id": "calc-6",
          "part": 6,
          "title": "The Mean Value Theorem",
          "url": "mvt.html",
          "icon": "\u03bc",
          "keywords": [
            "Rolle's Theorem",
            "Lagrange's Mean Value Theorem",
            "Cauchy's Mean Value Theorem",
            "Taylor's Theorem",
            "Taylor polynomial",
            "little-o notation",
            "Higher-dimensional MVT"
          ],
          "badges": [],
          "prereqs": [
            "calc-1"
          ],
          "mapCoords": {
            "q": -3,
            "r": 0
          },
          "topicGroup": "optimization",
          "tesseraMessage": "Taylor's theorem lets us approximate any smooth function locally. Powerful!"
        },
        {
          "id": "calc-7",
          "part": 7,
          "title": "Gradient Descent (First-order Method)",
          "url": "gradient.html",
          "icon": "\u2193",
          "keywords": [
            "Optimization problems",
            "Convexity",
            "Gradient Descent (SD)",
            "Steepest Descent",
            "Stochastic Gradient Descent (SGD)",
            "Mini-batch SGD",
            "Sub-gradient",
            "Sub-differentiable"
          ],
          "badges": [
            "code"
          ],
          "prereqs": [
            "calc-6"
          ],
          "mapCoords": {
            "q": -3,
            "r": -1
          },
          "topicGroup": "optimization",
          "tesseraMessage": "Gradient descent is the workhorse of machine learning. Simple yet profound."
        },
        {
          "id": "calc-8",
          "part": 8,
          "title": "Newton's method (Second-order Method)",
          "url": "newton.html",
          "icon": "N",
          "keywords": [
            "Line search",
            "Armijo condition",
            "Curvature condition",
            "Wolfe conditions",
            "Newton's method",
            "Quasi-Newton methods",
            "BFGS",
            "Secant condition",
            "Inverse Hessian approximation",
            "Limited memory BFGS",
            "Rosenbrock function"
          ],
          "badges": [
            "code"
          ],
          "prereqs": [
            "calc-7",
            "linalg-9"
          ],
          "mapCoords": {
            "q": -4,
            "r": -1
          },
          "topicGroup": "optimization",
          "tesseraMessage": "Newton's method converges faster, but at a cost. Trade-offs everywhere!"
        },
        {
          "id": "calc-9",
          "part": 9,
          "title": "Constrained Optimization",
          "url": "constrained_opt.html",
          "icon": "\u03bb",
          "keywords": [
            "Constrained optimization problems",
            "Penalty terms",
            "Lagrange Multipliers",
            "Lagrangian",
            "Karush-Kuhn-Tucker (KKT) conditions",
            "Active set",
            "Slack variables"
          ],
          "badges": [
            "code"
          ],
          "prereqs": [
            "calc-7"
          ],
          "mapCoords": {
            "q": -4,
            "r": 0
          },
          "topicGroup": "optimization",
          "tesseraMessage": "Lagrange multipliers handle constraints elegantly. KKT conditions are key!"
        },
        {
          "id": "calc-10",
          "part": 10,
          "title": "Riemann Integration",
          "url": "riemann.html",
          "icon": "\u222b",
          "keywords": [
            "Riemann integral",
            "Riemann integrable",
            "Improper Riemann integration",
            "Dirichlet function"
          ],
          "badges": [],
          "prereqs": [
            "calc-6"
          ],
          "mapCoords": {
            "q": -3,
            "r": -2
          },
          "topicGroup": "analysis",
          "tesseraMessage": "Riemann integration \u2014 the classical approach before Lebesgue changed everything."
        },
        {
          "id": "calc-11",
          "part": 11,
          "title": "Measure Theory with Probability",
          "url": "measure.html",
          "icon": "\u03c3",
          "keywords": [
            "Sample space",
            "\u03c3-algebra",
            "Measurable set",
            "Measurable space",
            "Measure",
            "Probability measure",
            "Probability space",
            "Countable additivity",
            "Borel \u03c3-algebra",
            "Borel set",
            "Lebesgue measure"
          ],
          "badges": [],
          "prereqs": [
            "calc-10",
            "prob-1"
          ],
          "mapCoords": {
            "q": -4,
            "r": -2
          },
          "topicGroup": "analysis",
          "tesseraMessage": "Measure theory makes probability rigorous. The foundation of modern analysis."
        },
        {
          "id": "calc-12",
          "part": 12,
          "title": "Intro to Lebesgue Integration",
          "url": "lebesgue.html",
          "icon": "a.e.",
          "keywords": [
            "Lebesgue integral",
            "Characteristic function",
            "Almost everywhere(a.e.)",
            "Simple function"
          ],
          "badges": [],
          "prereqs": [
            "calc-11"
          ],
          "mapCoords": {
            "q": -5,
            "r": -1
          },
          "topicGroup": "analysis",
          "tesseraMessage": "Lebesgue integration handles functions Riemann cannot. 'Almost everywhere' matters!"
        },
        {
          "id": "calc-13",
          "part": 13,
          "title": "Duality in Optimization & Analysis",
          "url": "duality.html",
          "icon": "P\u27f7D",
          "keywords": [
            "Duality",
            "Weak duality",
            "Strong duality",
            "Duality gap",
            "Smoothness",
            "Lipschitz continuity",
            "Contraction mapping",
            "Convergence rate"
          ],
          "badges": [
            "interactive"
          ],
          "prereqs": [
            "calc-9"
          ],
          "mapCoords": {
            "q": -5,
            "r": 0
          },
          "topicGroup": "optimization",
          "tesseraMessage": "Duality connects optimization problems in surprising ways. Primal meets dual!"
        },
        {
          "id": "calc-14",
          "part": 14,
          "title": "Fourier Series",
          "url": "fourier_series.html",
          "icon": "\u223f",
          "keywords": [
            "Fourier series",
            "Fourier coefficients",
            "Orthogonality of Trigonometric Functions",
            "Complex exponential form",
            "Parseval's identity",
            "L\u00b2 convergence",
            "Gibbs phenomenon"
          ],
          "badges": [],
          "prereqs": [
            "calc-12",
            "linalg-7"
          ],
          "mapCoords": {
            "q": -5,
            "r": -2
          },
          "topicGroup": "fourier",
          "tesseraMessage": "Fourier series decompose signals into frequencies. Music, images, everything!"
        },
        {
          "id": "calc-15",
          "part": 15,
          "title": "Fourier Transform",
          "url": "fourier_transform.html",
          "icon": "\u2131",
          "keywords": [
            "Fourier transform",
            "Inverse Fourier transform",
            "Plancherel's theorem",
            "Convolution theorem",
            "Discrete Fourier Transform (DFT)",
            "Fast Fourier Transform (FFT)",
            "Fourier Neural Operators (FNO)"
          ],
          "badges": [],
          "prereqs": [
            "calc-14"
          ],
          "mapCoords": {
            "q": -6,
            "r": -1
          },
          "topicGroup": "fourier",
          "tesseraMessage": "FFT changed the world \u2014 fast signal processing makes modern tech possible."
        },
        {
          "id": "calc-16",
          "part": 16,
          "title": "Foundations of Analysis: Metric Spaces",
          "url": "metric_space.html",
          "icon": "(X, d)",
          "keywords": [
            "Metric Space",
            "Distance",
            "Isolated Points",
            "Accumulation Points",
            "Nearest Points",
            "Boundary",
            "Interior",
            "Closure",
            "Open and Closed Sets",
            "Topology",
            "Open Balls",
            "Convexity"
          ],
          "badges": [],
          "prereqs": [
            "linalg-10",
            "calc-7",
            "calc-9",
            "ml-3"
          ],
          "mapCoords": {
            "q": -6,
            "r": 0
          },
          "topicGroup": "analysis",
          "tesseraMessage": "Metric spaces abstract the notion of distance. The gateway to topology!"
        },
        {
          "id": "calc-17",
          "part": 17,
          "title": "Convergence & Boundedness",
          "url": "limit_convergence.html",
          "icon": "tailₘ(xₙ)",
          "keywords": [
            "Tails",
            "Convergence",
            "Limits",
            "Cauchy Sequences",
            "Boundedness",
            "Diameter",
            "Total Boundedness",
            "Completeness",
            "Sequences"
          ],
          "badges": [],
          "prereqs": [
            "calc-16"
          ],
          "mapCoords": {
            "q": -6,
            "r": 1
          },
          "topicGroup": "analysis",
          "tesseraMessage": "Convergence is when the 'tail' of a sequence settles into an arbitrarily small ball. It's the structural anchor for every iterative process!"
        }
      ],
      "reservedSlots": [
        {
          "q": -7,
          "r": 2
        },
        {
          "q": -6,
          "r": -2
        },
        {
          "q": -7,
          "r": -1
        },
        {
          "q": -7,
          "r": 1
        }
      ]
    },
    "III": {
      "id": "probability",
      "title": "Probability & Statistics",
      "shortTitle": "Probability",
      "description": "Explore fundamental concepts of probability and statistics essential for machine learning, including probability theory, random variables, distributions, Bayesian inference, entropy, and the Fisher Information Matrix.",
      "tagline": "The Mathematics of Uncertainty and Inference",
      "icon": "fa-dice",
      "indexUrl": "Mathematics/Probability/probability.html",
      "baseUrl": "Mathematics/Probability/",
      "parts": [
        {
          "id": "prob-1",
          "part": 1,
          "title": "Basic Probability Ideas",
          "url": "basic.html",
          "icon": "p",
          "keywords": [
            "Probability",
            "Sample Space",
            "Events",
            "Mutually Exclusive",
            "Permutation",
            "Combinations",
            "Conditional Probability",
            "Independent Events",
            "Law of Total Probability",
            "Bayes' Theorem"
          ],
          "badges": [],
          "prereqs": [],
          "mapCoords": {
            "q": 0,
            "r": -1
          },
          "topicGroup": "foundations",
          "tesseraMessage": "Probability is the language of uncertainty. Bayes' theorem is your new best friend!"
        },
        {
          "id": "prob-2",
          "part": 2,
          "title": "Random Variables",
          "url": "random_variables.html",
          "icon": "X",
          "keywords": [
            "Discrete Random Variables",
            "Continuous Random Variables",
            "Probability Mass Function (p.m.f.)",
            "Probability Density Function (p.d.f.)",
            "Cumulative Distribution Function(c.d.f.)",
            "Expected Value",
            "Variance",
            "Standard Deviation"
          ],
          "badges": [],
          "prereqs": [
            "prob-1"
          ],
          "mapCoords": {
            "q": 1,
            "r": -1
          },
          "topicGroup": "foundations",
          "tesseraMessage": "Random variables turn randomness into mathematics we can compute with."
        },
        {
          "id": "prob-3",
          "part": 3,
          "title": "Gamma & Beta Distribution",
          "url": "gamma.html",
          "icon": "\u0393",
          "keywords": [
            "Gamma Distribution",
            "Gamma Function",
            "Exponential Distribution",
            "Beta Function",
            "Beta Distribution",
            "Uniform Distribution"
          ],
          "badges": [
            "interactive"
          ],
          "prereqs": [
            "prob-2"
          ],
          "mapCoords": {
            "q": 0,
            "r": -2
          },
          "topicGroup": "distributions",
          "tesseraMessage": "Gamma and Beta distributions model waiting times and proportions beautifully."
        },
        {
          "id": "prob-4",
          "part": 4,
          "title": "Normal (Gaussian) Distribution",
          "url": "gaussian.html",
          "icon": "\ud835\udca9",
          "keywords": [
            "Gaussian Function",
            "Error Function",
            "Gaussian Integral",
            "Normal(Gaussian) Distribution",
            "Standard Normal Distribution",
            "Independent and Identically Distributed(i.i.d.)",
            "Random Sample",
            "Sample Mean",
            "Sample Variance",
            "Central Limit Theorem"
          ],
          "badges": [],
          "prereqs": [
            "prob-2"
          ],
          "mapCoords": {
            "q": 1,
            "r": -2
          },
          "topicGroup": "distributions",
          "tesseraMessage": "The Gaussian is everywhere! Central Limit Theorem explains why."
        },
        {
          "id": "prob-5",
          "part": 5,
          "title": "Student's t-Distribution",
          "url": "student.html",
          "icon": "t",
          "keywords": [
            "Student's t-Distribution",
            "Degrees of Freedom",
            "Cauchy Distribution",
            "Half Cauchy Distribution",
            "Laplace Distribution",
            "Double Sided Exponential Distribution"
          ],
          "badges": [],
          "prereqs": [
            "prob-4"
          ],
          "mapCoords": {
            "q": 2,
            "r": -1
          },
          "topicGroup": "distributions",
          "tesseraMessage": "Student's t handles small samples with grace. Heavy tails, robust inference."
        },
        {
          "id": "prob-6",
          "part": 6,
          "title": "Covariance",
          "url": "covariance.html",
          "icon": "Cov",
          "keywords": [
            "Covariance",
            "Covariance Matrix",
            "Total Variance",
            "Principal Component",
            "Principal Component Analysis(PCA)"
          ],
          "badges": [
            "code"
          ],
          "prereqs": [
            "prob-4",
            "linalg-6"
          ],
          "mapCoords": {
            "q": 0,
            "r": -3
          },
          "topicGroup": "multivariate",
          "tesseraMessage": "Covariance reveals how variables move together. PCA lives here!"
        },
        {
          "id": "prob-7",
          "part": 7,
          "title": "Correlation",
          "url": "correlation.html",
          "icon": "r",
          "keywords": [
            "Cross-Covariance Matrix",
            "Auto-Covariance Matrix",
            "Correlation Coefficient",
            "Correlation Matrix"
          ],
          "badges": [],
          "prereqs": [
            "prob-6"
          ],
          "mapCoords": {
            "q": 1,
            "r": -3
          },
          "topicGroup": "multivariate",
          "tesseraMessage": "Correlation normalizes covariance \u2014 easier to interpret, same insight."
        },
        {
          "id": "prob-8",
          "part": 8,
          "title": "Multivariate Distributions",
          "url": "mvn.html",
          "icon": "\u03a3",
          "keywords": [
            "Multivariate Normal Distribution (MVN)",
            "Mahalanobis Distance",
            "Bivariate Normal Distribution",
            "Cholesky Decomposition",
            "Dirichlet Distribution",
            "Probability Simplex",
            "Wishart Distribution",
            "Inverse Wishart Distribution"
          ],
          "badges": [],
          "prereqs": [
            "prob-6",
            "linalg-9"
          ],
          "mapCoords": {
            "q": 2,
            "r": -2
          },
          "topicGroup": "multivariate",
          "tesseraMessage": "Multivariate Gaussians are the foundation of Gaussian processes and more."
        },
        {
          "id": "prob-9",
          "part": 9,
          "title": "Maximum Likelihood Estimation",
          "url": "mle.html",
          "icon": "\u2112",
          "keywords": [
            "Point Estimator",
            "Mean Square Error(MSE)",
            "Standard Error (SE)",
            "Likelihood Function",
            "Log-likelihood Function",
            "Maximum Likelihood Estimation(MLE)",
            "Binomial Distribution",
            "Sample Proportion"
          ],
          "badges": [],
          "prereqs": [
            "prob-4"
          ],
          "mapCoords": {
            "q": 0,
            "r": -4
          },
          "topicGroup": "inference",
          "tesseraMessage": "MLE finds the parameters that make your data most likely. Foundational!"
        },
        {
          "id": "prob-10",
          "part": 10,
          "title": "Statistical Inference & Hypothesis Testing",
          "url": "hypothesis_testing.html",
          "icon": "H\u2080 vs H\u2081",
          "keywords": [
            "Null Hypothesis",
            "Alternative Hypothesis",
            "Type I Error (False Negative)",
            "Type II Error (False Positive)",
            "Significance Level",
            "Test Statistic",
            "Null Hypothesis Significance Test(NHST)",
            "One Sample t-Tests",
            "Confidence intervals",
            "Critical Values",
            "z-scores",
            "Credible Intervals",
            "Bootstrap"
          ],
          "badges": [],
          "prereqs": [
            "prob-9"
          ],
          "mapCoords": {
            "q": 1,
            "r": -4
          },
          "topicGroup": "inference",
          "tesseraMessage": "Hypothesis testing helps us make decisions under uncertainty. Be careful with p-values!"
        },
        {
          "id": "prob-11",
          "part": 11,
          "title": "Linear Regression",
          "url": "linear_regression.html",
          "icon": "LS",
          "keywords": [
            "Linear Regression",
            "Least-Squares Estimation"
          ],
          "badges": [
            "interactive"
          ],
          "prereqs": [
            "prob-9",
            "linalg-8"
          ],
          "mapCoords": {
            "q": 2,
            "r": -3
          },
          "topicGroup": "inference",
          "tesseraMessage": "Linear regression connects statistics to ML. Where prediction begins!"
        },
        {
          "id": "prob-12",
          "part": 12,
          "title": "Entropy",
          "url": "entropy.html",
          "icon": "\u210d",
          "keywords": [
            "Information Content",
            "Entropy",
            "Joint Entropy",
            "Conditional Entropy",
            "Cross Entropy",
            "KL Divergence(Relative Entropy, Information Gain)",
            "Gibbs' Inequality",
            "Log Sum Inequality",
            "Jensen's Inequality",
            "Mutual Information (MI)"
          ],
          "badges": [],
          "prereqs": [
            "prob-4"
          ],
          "mapCoords": {
            "q": 3,
            "r": -2
          },
          "topicGroup": "information",
          "tesseraMessage": "Entropy measures uncertainty \u2014 the heart of information theory."
        },
        {
          "id": "prob-13",
          "part": 13,
          "title": "Convergence",
          "url": "convergence.html",
          "icon": "n\u2192\u221e",
          "keywords": [
            "The Law of Large Numbers",
            "Convergence in Probability",
            "Convergence in Distribution",
            "Asymptotic(limiting) Distribution",
            "Moment Generating Function(m.g.f.)",
            "Central Limit Theorem(CLT)"
          ],
          "badges": [],
          "prereqs": [
            "prob-9"
          ],
          "mapCoords": {
            "q": 2,
            "r": -4
          },
          "topicGroup": "inference",
          "tesseraMessage": "Convergence guarantees that our estimates improve. Math meets practice!"
        },
        {
          "id": "prob-14",
          "part": 14,
          "title": "Intro to Bayesian Statistics",
          "url": "bayesian.html",
          "icon": "p(\u03b8|\ud835\udc9f)",
          "keywords": [
            "Bayesian Inference",
            "Prior Distribution",
            "Posterior Distribution",
            "Marginal Likelihood",
            "Conjugate Prior",
            "Posterior Predictive Distribution",
            "Beta-Binomial Model",
            "Normal Distribution Model with known Variance \u03c3\u00b2",
            "Normal Distribution Model with known Mean \u03bc"
          ],
          "badges": [],
          "prereqs": [
            "prob-9",
            "prob-3"
          ],
          "mapCoords": {
            "q": 3,
            "r": -3
          },
          "topicGroup": "bayesian",
          "tesseraMessage": "Bayesian thinking updates beliefs with evidence. Prior \u2192 Posterior."
        },
        {
          "id": "prob-15",
          "part": 15,
          "title": "The Exponential Family",
          "url": "expfamily.html",
          "icon": "\u03b7",
          "keywords": [
            "Exponential Family",
            "Natural Parameters(Canonical Parameters)",
            "Base Measure",
            "Sufficient Statistics",
            "Partition Function",
            "Minimal Representation",
            "Natural Exponential Family(NEF)",
            "Moment Parameters",
            "Precision Matrix",
            "Information Form",
            "Moment Matching",
            "Cumulants"
          ],
          "badges": [],
          "prereqs": [
            "prob-14"
          ],
          "mapCoords": {
            "q": 4,
            "r": -2
          },
          "topicGroup": "bayesian",
          "tesseraMessage": "The exponential family unifies many distributions elegantly. Beautiful structure!"
        },
        {
          "id": "prob-16",
          "part": 16,
          "title": "Fisher Information Matrix",
          "url": "fisher_info.html",
          "icon": "F(\u03b8)",
          "keywords": [
            "Fisher Information Matrix(FIM)",
            "Score Function",
            "Covariance",
            "Negative Log Likelihood",
            "Log Partition Function",
            "Approximated KL Divergence",
            "Natural Gradient",
            "Jeffreys Prior",
            "Uninformative Prior",
            "Reference Prior",
            "Mutual Information"
          ],
          "badges": [],
          "prereqs": [
            "prob-15",
            "linalg-9"
          ],
          "mapCoords": {
            "q": 4,
            "r": -3
          },
          "topicGroup": "bayesian",
          "tesseraMessage": "Fisher information measures how much data tells us about parameters."
        },
        {
          "id": "prob-17",
          "part": 17,
          "title": "Bayesian Decision Theory",
          "url": "decision_theory.html",
          "icon": "\u03c0",
          "keywords": [
            "Decision Theory",
            "Optimal Policy(Bayes estimator)",
            "Zero-One Loss",
            "Maximum A Posteriori (MAP) Estimate",
            "Reject Option",
            "Confusion Matrix",
            "False Positive (FP, Type I error)",
            "False Negative (FN, Type II error)",
            "Receiver Operating Characteristic (ROC) Curve",
            "Equal Error Rate (EER)",
            "Precision-Recall (PR) Curve",
            "Interpolated Precision",
            "Average Precision (AP)"
          ],
          "badges": [],
          "prereqs": [
            "prob-14"
          ],
          "mapCoords": {
            "q": 3,
            "r": -4
          },
          "topicGroup": "bayesian",
          "tesseraMessage": "Decision theory tells us how to act optimally under uncertainty."
        },
        {
          "id": "prob-18",
          "part": 18,
          "title": "Markov Chains",
          "url": "markov.html",
          "icon": "\u220f",
          "keywords": [
            "Probabilistic Graphical Models(PGMs)",
            "Bayesian Networks",
            "Markov Chains",
            "Language Modeling",
            "n-gram",
            "Transition Function(Kernel)",
            "Stochastic Matrix(Transition Matrix)",
            "Maximum likelihood estimation(MLE) in Markov models",
            "Sparse Data Problem",
            "Add-One Smoothing",
            "Dirichlet Prior"
          ],
          "badges": [
            "code"
          ],
          "prereqs": [
            "prob-14",
            "linalg-13"
          ],
          "mapCoords": {
            "q": 4,
            "r": -4
          },
          "topicGroup": "bayesian",
          "tesseraMessage": "Markov chains model sequences \u2014 from text to weather to DNA."
        },
        {
          "id": "prob-19",
          "part": 19,
          "title": "Monte Carlo Methods",
          "url": "monte_carlo.html",
          "icon": "p*(\u03b8)",
          "keywords": [
            "Credible Intervals",
            "Central Credible Intervals",
            "Monte Carlo Approximation",
            "Highest Posterior Density (HPD)",
            "Markov Chain Monte Carlo (MCMC)"
          ],
          "badges": [
            "interactive"
          ],
          "prereqs": [
            "prob-14"
          ],
          "mapCoords": {
            "q": 5,
            "r": -4
          },
          "topicGroup": "bayesian",
          "tesseraMessage": "Monte Carlo turns random sampling into computational power!"
        },
        {
          "id": "prob-20",
          "part": 20,
          "title": "Importance Sampling",
          "url": "importance_sampling.html",
          "icon": "\u03c6",
          "keywords": [
            "Importance Sampling",
            "Importance Weights",
            "Direct Importance Sampling",
            "Effective Sample Size (ESS)",
            "Self-Normalized Importance Sampling (SNIS)",
            "Annealed Importance Sampling (AIS)",
            "Annealing Schedule"
          ],
          "badges": [],
          "prereqs": [
            "prob-19"
          ],
          "mapCoords": {
            "q": 5,
            "r": -3
          },
          "topicGroup": "bayesian",
          "tesseraMessage": "Importance sampling focuses computation where it matters most."
        },
        {
          "id": "prob-21",
          "part": 21,
          "title": "Gaussian Processes",
          "url": "gaussian_process.html",
          "icon": "\ud835\udca6",
          "keywords": [
            "Nonparametric Models",
            "Gaussian Process (GP)",
            "Mercer Kernel",
            "radial basis function (RBF) Kernel",
            "Stationary Kernel",
            "Automatic Relevance Determination (ARD) Kernel",
            "Mat\u00e9rn Kernel",
            "Periodic Kernel",
            "GP Regression"
          ],
          "badges": [],
          "prereqs": [
            "prob-8",
            "linalg-9"
          ],
          "mapCoords": {
            "q": 5,
            "r": -2
          },
          "topicGroup": "gaussian-process",
          "tesseraMessage": "GPs give uncertainty estimates for free. Bayesian nonparametrics at its finest!"
        }
      ],
      "reservedSlots": [
        {
          "q": 6,
          "r": -2
        },
        {
          "q": 6,
          "r": -4
        },
        {
          "q": 5,
          "r": -5
        }
      ]
    },
    "IV": {
      "id": "discrete",
      "title": "Discrete Mathematics & Algorithms",
      "shortTitle": "Discrete Math",
      "description": "Explore the foundations of discrete mathematics and algorithms, covering graph theory, combinatorics, and the theory of computation. Learn key concepts essential for mathematical reasoning and algorithmic problem-solving.",
      "tagline": "The Mathematics of Logic and Finite Structures",
      "icon": "fa-project-diagram",
      "indexUrl": "Mathematics/Discrete/discrete_math.html",
      "baseUrl": "Mathematics/Discrete/",
      "parts": [
        {
          "id": "disc-1",
          "part": 1,
          "title": "Intro to Graph Theory",
          "url": "intro_graph.html",
          "icon": "G=(V,E)",
          "keywords": [
            "Undirected graph",
            "Directed graph",
            "Multigraph",
            "Weighted graph",
            "Complete graph",
            "Adjacent",
            "Degree of a vertex",
            "k-regular",
            "Isomorphism"
          ],
          "badges": [],
          "prereqs": [],
          "mapCoords": {
            "q": 0,
            "r": 1
          },
          "topicGroup": "graph-theory",
          "tesseraMessage": "Graphs are everywhere \u2014 social networks, molecules, the internet itself!"
        },
        {
          "id": "disc-2",
          "part": 2,
          "title": "Intro to Combinatorics",
          "url": "intro_combinatorics.html",
          "icon": "\u2099C\u1d63",
          "keywords": [
            "Fundamental counting principle",
            "Combination",
            "Binomial coefficient",
            "Multinomial coefficient",
            "Pascal's relation",
            "Chu's theorem",
            "Sum of powers of positive integers",
            "Permutation"
          ],
          "badges": [],
          "prereqs": [],
          "mapCoords": {
            "q": 1,
            "r": 2
          },
          "topicGroup": "combinatorics",
          "tesseraMessage": "Counting cleverly is an art. Combinatorics powers probability and CS alike."
        },
        {
          "id": "disc-3",
          "part": 3,
          "title": "Intro to Theory of Computation",
          "url": "intro_automata.html",
          "icon": "M",
          "keywords": [
            "Finite automata",
            "Regular language",
            "State diagram",
            "Deterministic finite automata (DFAs)",
            "Nondeterministic finite automata (NFAs)",
            "Regular expression"
          ],
          "badges": [],
          "prereqs": [],
          "mapCoords": {
            "q": 2,
            "r": 1
          },
          "topicGroup": "computation",
          "tesseraMessage": "Automata recognize patterns. The simplest model of computation!"
        },
        {
          "id": "disc-4",
          "part": 4,
          "title": "Boolean Logic",
          "url": "boolean.html",
          "icon": "0\u22271",
          "keywords": [
            "Logical operations",
            "Boolean algebra",
            "Circuits"
          ],
          "badges": [],
          "prereqs": [],
          "mapCoords": {
            "q": 1,
            "r": 1
          },
          "topicGroup": "boolean",
          "tesseraMessage": "True or false, 0 or 1 \u2014 Boolean logic is how computers think."
        },
        {
          "id": "disc-5",
          "part": 5,
          "title": "Context-Free Languages",
          "url": "context_free.html",
          "icon": "S\u2192",
          "keywords": [
            "Context-free grammars",
            "Nonregular language",
            "Pumping lemma",
            "Pushdown automata",
            "Deterministic pushdown automata (DPDAs)",
            "Deterministic context-free languages (DCFLs)"
          ],
          "badges": [],
          "prereqs": [
            "disc-3"
          ],
          "mapCoords": {
            "q": 2,
            "r": 2
          },
          "topicGroup": "computation",
          "tesseraMessage": "Context-free grammars define programming languages. You use them every day!"
        },
        {
          "id": "disc-6",
          "part": 6,
          "title": "Turing Machines",
          "url": "turing_machine.html",
          "icon": "TM",
          "keywords": [
            "Turing machine",
            "Turing-recognizable",
            "Decidability",
            "Computability",
            "Church-Turing thesis",
            "Unsolvability",
            "Undecidable problems"
          ],
          "badges": [],
          "prereqs": [
            "disc-5"
          ],
          "mapCoords": {
            "q": 3,
            "r": 1
          },
          "topicGroup": "computation",
          "tesseraMessage": "Turing machines define what's computable. Some problems have no algorithm!"
        },
        {
          "id": "disc-7",
          "part": 7,
          "title": "Time Complexity",
          "url": "time_complexity.html",
          "icon": "O(g(n))",
          "keywords": [
            "Time complexity(Running time)",
            "Asymptotic analysis",
            "Big-O notation",
            "Little-o notation",
            "Time complexity class",
            "Class P"
          ],
          "badges": [],
          "prereqs": [
            "disc-6"
          ],
          "mapCoords": {
            "q": 3,
            "r": 2
          },
          "topicGroup": "computation",
          "tesseraMessage": "Big-O notation tells you how algorithms scale. Essential for interviews!"
        },
        {
          "id": "disc-8",
          "part": 8,
          "title": "Eulerian & Hamiltonian",
          "url": "Eulerian.html",
          "icon": "C\u2099",
          "keywords": [
            "Eulerian",
            "Eulerian cycle (Euler tour)",
            "Eulerian path (Eulerian trail)",
            "Hamiltonian",
            "Hamiltonian cycle",
            "Adjacency matrix",
            "Adjacency list",
            "Space complexity"
          ],
          "badges": [],
          "prereqs": [
            "disc-1"
          ],
          "mapCoords": {
            "q": 0,
            "r": 2
          },
          "topicGroup": "graph-theory",
          "tesseraMessage": "Can you traverse every edge? Every vertex? Different questions, different answers."
        },
        {
          "id": "disc-9",
          "part": 9,
          "title": "Class NP",
          "url": "p_vs_np.html",
          "icon": "P\u2260NP?",
          "keywords": [
            "Polynomial verifiability",
            "Nondeterministic polynomial time",
            "NP-completeness",
            "NP-hard",
            "Polynomial-time reduction",
            "P vs NP question"
          ],
          "badges": [],
          "prereqs": [
            "disc-7"
          ],
          "mapCoords": {
            "q": 4,
            "r": 1
          },
          "topicGroup": "computation",
          "tesseraMessage": "P vs NP \u2014 the million-dollar question! Is checking always easier than solving?"
        }
      ],
      "reservedSlots": [
        {
          "q": 4,
          "r": 2
        },
        {
          "q": 5,
          "r": 1
        },
        {
          "q": 0,
          "r": 3
        }
      ]
    },
    "V": {
      "id": "machine-learning",
      "title": "Machine Learning",
      "shortTitle": "ML",
      "description": "Explore machine learning ideas and applications.",
      "tagline": "The Mathematics of Synthesis and Intelligence",
      "icon": "fa-brain",
      "indexUrl": "Mathematics/Machine_learning/ml.html",
      "baseUrl": "Mathematics/Machine_learning/",
      "parts": [
        {
          "id": "ml-1",
          "part": 1,
          "title": "Intro to Machine Learning",
          "url": "intro_ml.html",
          "icon": "\ud83e\udde0?",
          "keywords": [
            "Machine learning",
            "Artificial intelligence (AI)",
            "Deep learning",
            "Supervised learning",
            "Unsupervised learning",
            "Learning process of ML",
            "Categories of machine learning"
          ],
          "badges": [],
          "prereqs": [],
          "mapCoords": {
            "q": 1,
            "r": 0
          },
          "topicGroup": "ml-foundations",
          "tesseraMessage": "Welcome to ML! This is where all the math comes together beautifully."
        },
        {
          "id": "ml-2",
          "part": 2,
          "title": "Regularized Regression",
          "url": "regression.html",
          "icon": "\u03bb\u2016w\u2016\u209a",
          "keywords": [
            "Ridge regression",
            "Bias-variance tradeoff",
            "Generalization",
            "Regularization",
            "Cross-validation (CV)",
            "K-fold cross-validation",
            "Leave-one-out cross-validation (LOOCV)",
            "Lasso regression"
          ],
          "badges": [
            "interactive"
          ],
          "prereqs": [
            "prob-11",
            "calc-7"
          ],
          "mapCoords": {
            "q": 2,
            "r": 0
          },
          "topicGroup": "ml-foundations",
          "tesseraMessage": "Regularization prevents overfitting \u2014 the bias-variance tradeoff in action!"
        },
        {
          "id": "ml-3",
          "part": 3,
          "title": "Intro to Classification",
          "url": "intro_classification.html",
          "icon": "\ud835\udcb3\u21a6\ud835\udcb4",
          "keywords": [
            "Binary logistic regression",
            "sigmoid (logistic) function",
            "logit (pre-activation)",
            "Decision boundary",
            "Feature mapping",
            "Linearly separable",
            "Kernel trick",
            "Random fourier features",
            "RBF (Gaussian) kernel",
            "Softmax function",
            "Multinomial logistic regression"
          ],
          "badges": [
            "interactive"
          ],
          "prereqs": [
            "ml-2"
          ],
          "mapCoords": {
            "q": 3,
            "r": 0
          },
          "topicGroup": "ml-foundations",
          "tesseraMessage": "Classification draws boundaries in feature space. Kernels bend those boundaries!"
        },
        {
          "id": "ml-4",
          "part": 4,
          "title": "Neural Networks Basics",
          "url": "neural_networks.html",
          "icon": "x\u21a6h\u03b8(x)",
          "keywords": [
            "Deep neural network (DNN)",
            "Multilayer perceptron (MLP)",
            "Hidden layer",
            "Activation function",
            "ReLU",
            "Vanishing gradients",
            "Backpropagation",
            "Gradient clipping",
            "Exploding gradients",
            "Graphics processing units (GPUs)"
          ],
          "badges": [
            "interactive"
          ],
          "prereqs": [
            "ml-3",
            "calc-2"
          ],
          "mapCoords": {
            "q": 3,
            "r": -1
          },
          "topicGroup": "neural-networks",
          "tesseraMessage": "Neural networks learn features automatically. Layers upon layers of abstraction!"
        },
        {
          "id": "ml-5",
          "part": 5,
          "title": "Automatic Differentiation",
          "url": "autodiff.html",
          "icon": "\u2207\u2112",
          "keywords": [
            "Automatic differentiation (AD)",
            "Computational graph"
          ],
          "badges": [
            "code"
          ],
          "prereqs": [
            "ml-4"
          ],
          "mapCoords": {
            "q": 4,
            "r": -1
          },
          "topicGroup": "neural-networks",
          "tesseraMessage": "Autodiff computes gradients automatically. The engine behind deep learning!"
        },
        {
          "id": "ml-6",
          "part": 6,
          "title": "Support Vector Machine (SVM)",
          "url": "svm.html",
          "icon": "w\u1d40x+w\u2080",
          "keywords": [
            "Support vector machine (SVM)",
            "Soft margin constraints"
          ],
          "badges": [
            "interactive"
          ],
          "prereqs": [
            "ml-3",
            "calc-9"
          ],
          "mapCoords": {
            "q": 4,
            "r": 0
          },
          "topicGroup": "ml-foundations",
          "tesseraMessage": "SVMs find the widest margin. Elegant geometry meets optimization!"
        },
        {
          "id": "ml-7",
          "part": 7,
          "title": "PCA & Autoencoders",
          "url": "pca.html",
          "icon": "\ud835\udca6(x\u1d62,x\u2c7c)",
          "keywords": [
            "Principal Component Analysis (PCA)",
            "Dimensionality reduction",
            "Kernel PCA",
            "Double centering trick",
            "Autoencoder",
            "Lipschitz continuity",
            "Data Reconstruction",
            "Denoising autoencoder",
            "Manifolds"
          ],
          "badges": [
            "interactive"
          ],
          "prereqs": [
            "linalg-9",
            "ml-4"
          ],
          "mapCoords": {
            "q": 5,
            "r": 0
          },
          "topicGroup": "dimensionality",
          "tesseraMessage": "PCA finds the directions of maximum variance. Autoencoders learn them!"
        },
        {
          "id": "ml-8",
          "part": 8,
          "title": "Clustering",
          "url": "clustering.html",
          "icon": "f\u1d40Lf",
          "keywords": [
            "K-means clustering",
            "Distortion",
            "One-hot encoding (Dummy encoding)",
            "K-means++",
            "Vector quantization",
            "Spectral clustering",
            "Graph Laplacian",
            "Dirichlet energy"
          ],
          "badges": [
            "interactive"
          ],
          "prereqs": [
            "ml-7",
            "linalg-14"
          ],
          "mapCoords": {
            "q": 6,
            "r": 0
          },
          "topicGroup": "dimensionality",
          "tesseraMessage": "Clustering finds structure without labels. K-means is just the beginning!"
        },
        {
          "id": "ml-9",
          "part": 9,
          "title": "Intro to Deep Neural Networks",
          "url": "deep_nn.html",
          "icon": "\ud83e\udde0!",
          "keywords": [
            "Feedforward networks",
            "Convolutional Neural Networks (CNNs)",
            "Residual connection",
            "Layer normalization",
            "Attention",
            "Self-attention",
            "Multi-Head Attention (MHA)",
            "Positional encoding",
            "Transformer"
          ],
          "badges": [
            "interactive"
          ],
          "prereqs": [
            "ml-5"
          ],
          "mapCoords": {
            "q": 5,
            "r": -1
          },
          "topicGroup": "neural-networks",
          "tesseraMessage": "Transformers changed everything \u2014 attention is all you need!"
        },
        {
          "id": "ml-10",
          "part": 10,
          "title": "Intro to Reinforcement Learning",
          "url": "intro_RL.html",
          "icon": "\ud83d\udd01",
          "keywords": [
            "Reinforcement Learning (RL)",
            "Model-based RL",
            "Model-free RL",
            "Agent",
            "Reward",
            "Policy",
            "Markov Decision Process (MDP)",
            "Discount factor",
            "Return",
            "Value function",
            "Q-function",
            "Advantage function",
            "Bellman's equations",
            "Value Iteration",
            "Policy Iteration",
            "Temporal Difference Learning",
            "Q-Learning",
            "SARSA",
            "Exploration vs Exploitation",
            "Policy Gradient",
            "REINFORCE",
            "Actor-Critic"
          ],
          "badges": [],
          "prereqs": [
            "ml-9",
            "prob-18"
          ],
          "mapCoords": {
            "q": 6,
            "r": -1
          },
          "topicGroup": "reinfortic-learning",
          "tesseraMessage": "RL learns from interaction. Games, robotics, and beyond!"
        }
      ],
      "reservedSlots": [
        {
          "q": 7,
          "r": -1
        },
        {
          "q": 7,
          "r": 0
        },
        {
          "q": 6,
          "r": 1
        }
      ]
    }
  },
  "homeNode": {
    "id": "home",
    "section": "HOME",
    "title": "MATH-CS COMPASS HOME",
    "url": "/index.html",
    "mapCoords": {
      "q": 0,
      "r": 0
    }
  }
}