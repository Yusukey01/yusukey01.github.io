{
  "meta": {
    "version": "1.0.0",
    "lastUpdated": "2026-01-15",
    "author": "Yusuke Yokota",
    "description": "Centralized curriculum data for MATH-CS COMPASS - single source of truth for section pages and compass map"
  },
  "sectionColors": {
    "I": "#1565c0",
    "II": "#2e7d32",
    "III": "#00838f",
    "IV": "#6a1b9a",
    "V": "#ef6c00"
  },
  "sections": {
    "I": {
      "id": "linear-algebra",
      "title": "Linear Algebra to Algebraic Foundations",
      "shortTitle": "Linear Algebra",
      "description": "Explore the foundations of Linear Algebra, covering key concepts such as linear equations, vector spaces, eigenvalues, orthogonality, least squares, and stochastic matrices.",
      "tagline": "The Mathematics of Structure and Space",
      "icon": "fa-vector-square",
      "indexUrl": "Mathematics/Linear_algebra/linear_algebra.html",
      "baseUrl": "Mathematics/Linear_algebra/",
      "parts": [
        {
          "id": "linalg-1",
          "part": 1,
          "title": "Linear Equations",
          "url": "linear_equations.html",
          "icon": "Ax=b",
          "keywords": [
            "System of linear equations",
            "Reduced row echelon form",
            "Linear combination",
            "Span",
            "Matrix equation",
            "Homogeneous & nonhomogeneous system",
            "Linear independence & dependence",
            "Parametric vector form"
          ],
          "badges": [],
          "prereqs": [],
          "mapCoords": {
            "q": -1,
            "r": 1
          },
          "topicGroup": "core-linalg",
          "tesseraMessage": "Where every journey begins! Ax = b is the foundation of so much in CS."
        },
        {
          "id": "linalg-2",
          "part": 2,
          "title": "Linear Transformation",
          "url": "linear_transformation.html",
          "icon": "T",
          "keywords": [
            "Linear transformation",
            "Linearity",
            "Onto",
            "One-to-one",
            "Matrix multiplication"
          ],
          "badges": [
            "interactive"
          ],
          "prereqs": [
            "linalg-1"
          ],
          "mapCoords": {
            "q": -1,
            "r": 2
          },
          "topicGroup": "core-linalg",
          "tesseraMessage": "Transformations are how we move and reshape data. Graphics, ML, everywhere!"
        },
        {
          "id": "linalg-3",
          "part": 3,
          "title": "Matrix Algebra",
          "url": "matrix_algebra.html",
          "icon": "A",
          "keywords": [
            "Diagonal matrix",
            "Identity matrix",
            "Transpose of a matrix",
            "Invertible matrix",
            "Singular matrix",
            "Elementary matrix",
            "Partitioned Matrix",
            "LU Factorization"
          ],
          "badges": [],
          "prereqs": [
            "linalg-1"
          ],
          "mapCoords": {
            "q": -2,
            "r": 1
          },
          "topicGroup": "core-linalg",
          "tesseraMessage": "Matrices are the language computers speak. Master this well!"
        },
        {
          "id": "linalg-4",
          "part": 4,
          "title": "Determinants",
          "url": "determinants.html",
          "icon": "|A|",
          "keywords": [
            "Determinant",
            "Cofactor expansion",
            "Cramer's rule",
            "Adjugate",
            "Inverse formula",
            "Invertible matrix"
          ],
          "badges": [],
          "prereqs": [
            "linalg-3"
          ],
          "mapCoords": {
            "q": -2,
            "r": 2
          },
          "topicGroup": "core-linalg",
          "tesseraMessage": "Determinants tell you if a matrix is invertible \u2014 one number, so much meaning."
        },
        {
          "id": "linalg-5",
          "part": 5,
          "title": "Vector Spaces",
          "url": "vectorspaces.html",
          "icon": "V",
          "keywords": [
            "Vector space",
            "Subspace",
            "Null space",
            "Kernel",
            "Column space",
            "Row space",
            "Basis",
            "Spanning set",
            "Coordinate systems",
            "Dimension",
            "Rank"
          ],
          "badges": [],
          "prereqs": [
            "linalg-2"
          ],
          "mapCoords": {
            "q": -2,
            "r": 3
          },
          "topicGroup": "core-linalg",
          "tesseraMessage": "Vector spaces are where linear algebra becomes truly abstract and powerful."
        },
        {
          "id": "linalg-6",
          "part": 6,
          "title": "Eigenvalues & Eigenvectors",
          "url": "eigenvectors.html",
          "icon": "\u03bb",
          "keywords": [
            "Eigenvalues",
            "Eigenvectors",
            "Eigenspace",
            "Characteristic equation",
            "Similarity",
            "Diagonalization",
            "Complex eigenvalues & eigenvectors"
          ],
          "badges": [],
          "prereqs": [
            "linalg-4",
            "linalg-5"
          ],
          "mapCoords": {
            "q": -3,
            "r": 2
          },
          "topicGroup": "core-linalg",
          "tesseraMessage": "Eigenvalues appear everywhere \u2014 from Google's PageRank to quantum mechanics!"
        },
        {
          "id": "linalg-7",
          "part": 7,
          "title": "Orthogonality",
          "url": "orthogonality.html",
          "icon": "\u22a5",
          "keywords": [
            "Inner product",
            "Euclidean norm",
            "Orthogonality",
            "Orthogonal complement",
            "Orthogonal & Orthonormal set",
            "Orthogonal projection",
            "Orthogonal matrix",
            "Gram-Schmidt algorithm",
            "QR factorization"
          ],
          "badges": [
            "interactive"
          ],
          "prereqs": [
            "linalg-5"
          ],
          "mapCoords": {
            "q": -3,
            "r": 3
          },
          "topicGroup": "core-linalg",
          "tesseraMessage": "Orthogonality keeps things clean and independent. QR is essential!"
        },
        {
          "id": "linalg-8",
          "part": 8,
          "title": "Least-Squares Problems",
          "url": "leastsquares.html",
          "icon": "min",
          "keywords": [
            "Least-squares solution",
            "Normal equation",
            "Least-squares error",
            "Linear regression",
            "Moore-Penrose pseudo-inverse"
          ],
          "badges": [
            "interactive"
          ],
          "prereqs": [
            "linalg-7"
          ],
          "mapCoords": {
            "q": -3,
            "r": 4
          },
          "topicGroup": "core-linalg",
          "tesseraMessage": "When exact solutions don't exist, least squares finds the best approximation."
        },
        {
          "id": "linalg-9",
          "part": 9,
          "title": "Symmetry",
          "url": "symmetry.html",
          "icon": "S",
          "keywords": [
            "Symmetric matrix",
            "Orthogonally diagonalizable matrix",
            "Spectrum",
            "Quadratic form",
            "Positive definite",
            "Positive semi-definite",
            "Singular value decomposition(SVD)",
            "Condition number",
            "Moore-Penrose pseudo-inverse"
          ],
          "badges": [
            "interactive"
          ],
          "prereqs": [
            "linalg-6",
            "linalg-7"
          ],
          "mapCoords": {
            "q": -4,
            "r": 3
          },
          "topicGroup": "core-linalg",
          "tesseraMessage": "SVD is one of the most useful decompositions in all of applied mathematics!"
        },
        {
          "id": "linalg-10",
          "part": 10,
          "title": "Trace and Norms",
          "url": "trace.html",
          "icon": "Tr",
          "keywords": [
            "Trace",
            "Frobenius norm",
            "Nuclear norm",
            "Induced norm",
            "Spectral norm",
            "p-norm",
            "Manhattan norm",
            "Maximum norm",
            "Normalization",
            "Regularization",
            "Metric space",
            "Normed vector space",
            "Inner product space",
            "Euclidean space"
          ],
          "badges": [],
          "prereqs": [
            "linalg-9"
          ],
          "mapCoords": {
            "q": -4,
            "r": 4
          },
          "topicGroup": "core-linalg",
          "tesseraMessage": "Norms measure size, and different norms give different perspectives."
        },
        {
          "id": "linalg-11",
          "part": 11,
          "title": "Kronecker Product & Tensor",
          "url": "kronecker.html",
          "icon": "\u2297",
          "keywords": [
            "Vectorization",
            "Kronecker product",
            "Tensor",
            "Tensor Product"
          ],
          "badges": [],
          "prereqs": [
            "linalg-10"
          ],
          "mapCoords": {
            "q": -5,
            "r": 4
          },
          "topicGroup": "core-linalg",
          "tesseraMessage": "Tensors power modern deep learning. This is where things get multidimensional!"
        },
        {
          "id": "linalg-12",
          "part": 12,
          "title": "Woodbury Matrix Identity",
          "url": "woodbury.html",
          "icon": "W",
          "keywords": [
            "Woodbury matrix identity",
            "Sherman-Morrison formula"
          ],
          "badges": [],
          "prereqs": [
            "linalg-3"
          ],
          "mapCoords": {
            "q": -3,
            "r": 1
          },
          "topicGroup": "core-linalg",
          "tesseraMessage": "A clever identity that makes updating inverses efficient. Very practical!"
        },
        {
          "id": "linalg-13",
          "part": 13,
          "title": "Stochastic Matrix",
          "url": "stochastic.html",
          "icon": "P",
          "keywords": [
            "Stochastic matrix",
            "Column-stochastic matrix",
            "Row-stochastic matrix",
            "Probability vector",
            "Markov chain",
            "Steady-state vector",
            "Spectral radius",
            "Doubly stochastic matrix"
          ],
          "badges": [],
          "prereqs": [
            "linalg-6"
          ],
          "mapCoords": {
            "q": -4,
            "r": 2
          },
          "topicGroup": "core-linalg",
          "tesseraMessage": "Markov chains are everywhere \u2014 from web search to speech recognition."
        },
        {
          "id": "linalg-14",
          "part": 14,
          "title": "Graph Laplacians and Spectral Methods",
          "url": "graph_laplacian.html",
          "icon": "L",
          "keywords": [
            "Graph Laplacian",
            "Dirichlet energy",
            "Normalized Laplacian",
            "Fiedler vector",
            "Algebraic connectivity",
            "Cheeger's inequality",
            "Graph Fourier Transform (GFT)",
            "Heat diffusion on graphs"
          ],
          "badges": [],
          "prereqs": [
            "linalg-6",
            "disc-1"
          ],
          "mapCoords": {
            "q": -5,
            "r": 3
          },
          "topicGroup": "core-linalg",
          "tesseraMessage": "Graph Laplacians connect linear algebra to network analysis. Beautiful!"
        },
        {
          "id": "linalg-15",
          "part": 15,
          "title": "Intro to Abstract Algebra",
          "url": "intro_groups.html",
          "icon": "G",
          "keywords": [
            "Abstract Algebra",
            "Groups",
            "Binary operation",
            "Closure",
            "Abelian",
            "Non-Abelian",
            "General linear group, GL(n, F)",
            "Modular arithmetic",
            "Units Modulo n, U(n)",
            "Additive vs Multiplicative groups",
            "Order of a group",
            "Order of an element",
            "Subgroups",
            "One-step subgroup test",
            "Cyclic",
            "Generator",
            "Center of a group",
            "Centralizer"
          ],
          "badges": [],
          "prereqs": [
            "linalg-3"
          ],
          "mapCoords": {
            "q": -4,
            "r": 5
          },
          "topicGroup": "abstract-algebra",
          "tesseraMessage": "Welcome to abstract algebra! Groups reveal the hidden symmetry in mathematics."
        },
        {
          "id": "linalg-16",
          "part": 16,
          "title": "More Finite Groups",
          "url": "cyclic_groups.html",
          "icon": "S\u2099",
          "keywords": [
            "Group of Integers Modulo n",
            "Fundamental Theorem of Cyclic Groups",
            "Euler Phi Function",
            "Permutation Groups",
            "Symmetric Groups",
            "Cycle Notation",
            "Products of Disjoint Cycles",
            "Product of 2-Cycles",
            "The Parity Theorem",
            "Alternating Groups"
          ],
          "badges": [],
          "prereqs": [
            "linalg-15"
          ],
          "mapCoords": {
            "q": -5,
            "r": 5
          },
          "topicGroup": "abstract-algebra",
          "tesseraMessage": "Permutations and cyclic structures \u2014 the building blocks of cryptography."
        },
        {
          "id": "linalg-17",
          "part": 17,
          "title": "Structural Group Theory",
          "url": "group_isomorphism.html",
          "icon": "\u2245",
          "keywords": [
            "Cosets",
            "Lagrange's Theorem",
            "Fermat's Little Theorem",
            "Pohlig-Hellman algorithm",
            "Normal Subgroups",
            "Factor Groups",
            "Group Homomorphisms",
            "Kernel",
            "Group Isomorphisms",
            "Group Automorphisms",
            "Inner Automorphisms",
            "Cayley's Theorem",
            "First Isomorphism Theorem"
          ],
          "badges": [],
          "prereqs": [
            "linalg-16"
          ],
          "mapCoords": {
            "q": -5,
            "r": 6
          },
          "topicGroup": "abstract-algebra",
          "tesseraMessage": "Isomorphisms show when different-looking structures are secretly the same."
        },
        {
          "id": "linalg-18",
          "part": 18,
          "title": "Classification of Finite Abelian Groups",
          "url": "group_classification.html",
          "icon": "\u2295",
          "keywords": [
            "External Direct Products",
            "Internal Direct Products",
            "Fundamental Theorem of Finite Abelian Groups"
          ],
          "badges": [],
          "prereqs": [
            "linalg-17"
          ],
          "mapCoords": {
            "q": -6,
            "r": 5
          },
          "topicGroup": "abstract-algebra",
          "tesseraMessage": "Every finite abelian group can be broken down \u2014 a beautiful classification!"
        },
        {
          "id": "linalg-19",
          "part": 19,
          "title": "The Architecture of Rings and Fields",
          "url": "intro_rings.html",
          "icon": "F",
          "keywords": [
            "Rings",
            "Unity",
            "Unit",
            "Subrings",
            "Subring Test",
            "Integral Domains",
            "Zero-Divisors",
            "Cancellation Law",
            "Fields",
            "Characteristic of a Ring"
          ],
          "badges": [],
          "prereqs": [
            "linalg-15"
          ],
          "mapCoords": {
            "q": -6,
            "r": 6
          },
          "topicGroup": "abstract-algebra",
          "tesseraMessage": "Rings and fields \u2014 the algebraic structures behind polynomials and numbers."
        },
        {
          "id": "linalg-20",
          "part": 20,
          "title": "Ideals and Factor Rings",
          "url": "ideals.html",
          "icon": "R/A",
          "keywords": [
            "Ideals",
            "Ideal Test",
            "Principal Ideals",
            "Factor Rings",
            "Prime Ideals",
            "Maximal Ideals",
            "Ring Homomorphisms",
            "Fundamental Theorem of Ring Homomorphisms"
          ],
          "badges": [],
          "prereqs": [
            "linalg-19"
          ],
          "mapCoords": {
            "q": -6,
            "r": 7
          },
          "topicGroup": "abstract-algebra",
          "tesseraMessage": "Ideals are to rings what normal subgroups are to groups. Deep connections!"
        },
        {
          "id": "linalg-21",
          "part": 21,
          "title": "Polynomial Rings",
          "url": "polynomial_rings.html",
          "icon": "R[x]",
          "keywords": [
            "Polynomial Rings",
            "Division Algorithm",
            "Irreducible Polynomials",
            "Eisenstein's Criterion",
            "Principal Ideal Domain",
            "Gauss's Lemma",
            "Unique Factorization",
            "AES Cryptography",
            "Post-Quantum Cryptography"
          ],
          "badges": [],
          "prereqs": [
            "linalg-20",
            "disc-4"
          ],
          "mapCoords": {
            "q": -7,
            "r": 6
          },
          "topicGroup": "abstract-algebra",
          "tesseraMessage": "Polynomial rings power modern cryptography \u2014 from AES to post-quantum!"
        }
      ],
      "reservedSlots": [
        {
          "q": -7,
          "r": 7
        },
        {
          "q": -8,
          "r": 6
        },
        {
          "q": -5,
          "r": 7
        }
      ]
    },
    "II": {
      "id": "calculus",
      "title": "Calculus to Optimization & Analysis",
      "shortTitle": "Calculus",
      "description": "Explore key calculus concepts essential for optimization, analysis, and machine learning. Topics include derivatives, Jacobians, gradient descent, Newton's method, constrained optimization, measure theory, and Lebesgue integration.",
      "tagline": "The Mathematics of Change and Convergence",
      "icon": "fa-chart-line",
      "indexUrl": "Mathematics/Calculus/calculus.html",
      "baseUrl": "Mathematics/Calculus/",
      "parts": [
        {
        "id": "calc-1",
        "part": 1,
        "title": "The Derivative of f:ℝⁿ→ℝ",
        "url": "linear_approximation.html",
        "icon": "∇",
        "keywords": [
          "Linear approximation",
          "Linearization",
          "Differentials",
          "Product rule",
          "Gradient",
          "Quadratic form",
          "L₂ norm",
          "Euclidean norm",
          "Tangent line",
          "Linear operator"
        ],
        "badges": [],
        "prereqs": ["linalg-1"],
        "mapCoords": {"q": -1, "r": 0},
        "topicGroup": "derivatives",
        "tesseraMessage": "The gradient points uphill. In ML, we usually go the opposite way!",

        "headline": "Linear Approximations: From Scalar Calculus to Gradients",
        "description": "Learn about differentials and linearization, intro to matrix calculus.",
        "abstract": "Linear approximation extends single-variable calculus to vectors via the gradient. The differential notation df = f'(x)dx generalizes naturally to vector spaces, where f' becomes a linear operator mapping input changes to output changes—the foundation for all optimization in ML.",
        "datePublished": "2024-01-01",
        "dateModified": "2025-01-15",

        "teaches": [
          "Linear approximation of functions near a point",
          "Linearization and tangent line interpretation",
          "Differential notation: df = f'(x)dx",
          "Extending differentials to vector spaces",
          "Computing gradients of scalar-valued functions",
          "Product rule in differential notation",
          "Derivative of quadratic forms xᵀAx",
          "Derivative of L₂ norm"
        ],

        "competencyRequired": [
          "Basic calculus (limits, derivatives)",
          "Vectors and dot products",
          "Matrix-vector multiplication"
        ],

        "sections": [
          {"id": "lapp", "name": "Linear Approximations"},
          {"id": "diff", "name": "Differentials"},
          {"id": "ex1", "name": "Example 1: f(x) = xᵀx"},
          {"id": "ex2", "name": "Example 2: f(x) = xᵀAx"},
          {"id": "ex3", "name": "Example 3: f(x) = ||x||₂"}
        ],

        "definitions": [
          {
            "term": "Linear Approximation",
            "definition": "L(x) = f(x₀) + f'(x₀)(x - x₀) ≈ f(x) near x₀"
          },
          {
            "term": "Differential",
            "definition": "df = f'(x)dx where f'(x) is a linear operator mapping dx to df"
          },
          {
            "term": "Gradient",
            "definition": "∇f = column vector of partial derivatives; for f:ℝⁿ→ℝ, ∇f ∈ ℝⁿ"
          },
          {
            "term": "Quadratic Form",
            "definition": "f(x) = xᵀAx; gradient is (A + Aᵀ)x, or 2Ax when A is symmetric"
          }
        ],

        "about": [
          {"name": "Linear Approximation", "description": "Local linear model of a function"},
          {"name": "Gradient", "description": "Direction of steepest ascent"},
          {"name": "Differential Notation", "description": "Foundation for matrix calculus"},
          {"name": "Quadratic Forms", "description": "Key structure in optimization"}
        ]
        },
        { 
          "id": "calc-2",
          "part": 2,
          "title": "The Derivative of f:ℝⁿ→ℝⁿ",
          "url": "jacobian.html",
          "icon": "J",
          "keywords": [
            "Jacobian matrix",
            "Chain rule",
            "Backpropagation",
            "Reverse mode automatic differentiation",
            "Forward mode automatic differentiation",
            "Vector-Jacobian product",
            "Neural networks",
            "Loss function"
          ],
          "badges": [],
          "prereqs": ["calc-1"],
          "mapCoords": {"q": -1, "r": -1},
          "topicGroup": "derivatives",
          "tesseraMessage": "The Jacobian is the backbone of backpropagation. Chain rule magic!",

          "headline": "Jacobian Matrix: The Foundation of Backpropagation",
          "description": "Learn about Jacobian, chain rule, backpropagation.",
          "abstract": "The Jacobian matrix generalizes the derivative to vector-valued functions, with each entry Jᵢⱼ = ∂fᵢ/∂xⱼ. The chain rule becomes matrix multiplication, and backpropagation exploits this by computing vector-Jacobian products from output to input—the key insight behind training neural networks.",
          "datePublished": "2024-01-01",
          "dateModified": "2025-01-15",

          "teaches": [
            "Jacobian matrix for vector-valued functions",
            "Jacobian as linear operator mapping dx to df",
            "Chain rule as Jacobian matrix multiplication",
            "Non-commutativity of chain rule in high dimensions",
            "Backpropagation as reverse-mode autodiff",
            "Vector-Jacobian products for efficiency",
            "Forward vs reverse mode differentiation",
            "Computational efficiency for neural networks"
          ],

          "competencyRequired": [
            "Gradients of scalar functions",
            "Matrix multiplication",
            "Partial derivatives"
          ],

          "sections": [
            {"id": "jacobian", "name": "Jacobian"},
            {"id": "chain", "name": "Chain Rule"},
            {"id": "backp", "name": "Backpropagation"}
          ],

          "definitions": [
            {
              "term": "Jacobian Matrix",
              "definition": "m×n matrix J where Jᵢⱼ = ∂fᵢ/∂xⱼ for f:ℝⁿ→ℝᵐ"
            },
            {
              "term": "Chain Rule",
              "definition": "df = g'(h(x))·h'(x)·dx; Jacobians multiply in order"
            },
            {
              "term": "Backpropagation",
              "definition": "Reverse-mode autodiff computing gradients via left-to-right Jacobian products"
            },
            {
              "term": "Vector-Jacobian Product",
              "definition": "vᵀJ computed efficiently without forming full Jacobian; key to backprop"
            }
          ],

          "about": [
            {"name": "Jacobian Matrix", "description": "Derivative of vector-valued functions"},
            {"name": "Chain Rule", "description": "Composition rule via matrix multiplication"},
            {"name": "Backpropagation", "description": "Efficient gradient computation for neural networks"},
            {"name": "Automatic Differentiation", "description": "Forward vs reverse mode tradeoffs"}
          ]
        },
        {    
          "id": "calc-3",
          "part": 3,
          "title": "The Derivative of f:ℝⁿˣⁿ→ℝⁿˣⁿ",
          "url": "matrix_cal.html",
          "icon": "M",
          "keywords": [
            "Matrix calculus",
            "Powers of a matrix",
            "Inverse of a matrix",
            "LU decomposition",
            "Matrix-valued functions",
            "Product rule for matrices",
            "Non-commutative operations"
          ],
          "badges": [],
          "prereqs": ["calc-2", "linalg-3"],
          "mapCoords": {"q": -2, "r": -1},
          "topicGroup": "derivatives",
          "tesseraMessage": "Matrix calculus — where derivatives get truly multidimensional.",

          "headline": "Matrix Calculus: Derivatives of Matrix-Valued Functions",
          "description": "Learn about derivative of matrix functions.",
          "abstract": "Matrix calculus extends differentiation to matrix-valued functions of matrices. Non-commutativity requires careful ordering: d(X³) = (dX)X² + X(dX)X + X²(dX). Key results include the derivative of matrix inverse and LU decomposition—essential for advanced optimization and numerical methods.",
          "datePublished": "2024-01-01",
          "dateModified": "2025-01-15",

          "teaches": [
            "Derivative of matrix-valued functions",
            "Product rule for matrix functions",
            "Non-commutativity in matrix derivatives",
            "Derivative of matrix powers: d(X³)",
            "Derivative of matrix inverse: d(X⁻¹) = -X⁻¹(dX)X⁻¹",
            "Derivative of LU decomposition",
            "Triangular matrix properties in derivatives",
            "Symbolic vs component-wise differentiation"
          ],

          "competencyRequired": [
            "Jacobian matrices",
            "Matrix multiplication and inverses",
            "LU decomposition basics"
          ],

          "sections": [
            {"id": "square", "name": "Derivative of the square matrix functions"},
            {"id": "lu", "name": "Derivative of the LU decomposition matrix"}
          ],

          "definitions": [
            {
              "term": "Matrix Derivative",
              "definition": "df = (∂f/∂X)dX where both input and output are matrices"
            },
            {
              "term": "Derivative of X⁻¹",
              "definition": "d(X⁻¹) = -X⁻¹(dX)X⁻¹ derived from d(X⁻¹X) = 0"
            },
            {
              "term": "LU Derivative",
              "definition": "d(X) = d(LU) = (dL)U + L(dU) preserving triangular structure"
            }
          ],

          "about": [
            {"name": "Matrix Calculus", "description": "Differentiation of matrix-valued functions"},
            {"name": "Matrix Powers", "description": "Non-commutative product rule"},
            {"name": "Matrix Inverse", "description": "Key derivative for optimization"},
            {"name": "LU Decomposition", "description": "Derivatives preserving triangular structure"}
          ]
        },
        { 
          "id": "calc-4",
          "part": 4,
          "title": "Intro to Numerical Computation",
          "url": "numerical_example1.html",
          "icon": "≈",
          "keywords": [
            "Finite-difference approximation",
            "Forward difference",
            "Backward difference",
            "Relative error",
            "Roundoff error",
            "Machine epsilon",
            "Numerical stability",
            "Floating-point arithmetic"
          ],
          "badges": ["code"],
          "prereqs": ["calc-1"],
          "mapCoords": {"q": -2, "r": 0},
          "topicGroup": "numerical",
          "tesseraMessage": "Theory meets practice! Understanding numerical errors is crucial for real code.",

          "headline": "Numerical Computation: Validating Derivatives with Finite Differences",
          "description": "Introduction to numerical computation with coding.",
          "abstract": "Finite differences approximate derivatives numerically, enabling validation of analytical formulas. Choosing the right step size balances truncation error (too large) against roundoff error (too small). Relative error provides meaningful accuracy measures when comparing analytical and numerical results.",
          "datePublished": "2024-01-01",
          "dateModified": "2025-01-15",

          "teaches": [
            "Finite difference approximations",
            "Forward and backward difference methods",
            "Relative error as accuracy metric",
            "Floating-point arithmetic limitations",
            "Machine epsilon and precision limits",
            "Validating analytical derivatives numerically",
            "Choosing appropriate step sizes",
            "Random testing for robust validation"
          ],

          "competencyRequired": [
            "Differential calculus",
            "Basic Python/NumPy",
            "Matrix operations"
          ],

          "sections": [
            {"id": "ex1", "name": "Numerical Computation Example"}
          ],

          "definitions": [
            {
              "term": "Forward Difference",
              "definition": "f(x+dx) - f(x) approximates df"
            },
            {
              "term": "Backward Difference",
              "definition": "f(x) - f(x-dx) approximates df"
            },
            {
              "term": "Relative Error",
              "definition": "||True - Approximation|| / ||True||; meaningful accuracy measure"
            },
            {
              "term": "Machine Epsilon",
              "definition": "Smallest ε where 1 + ε ≠ 1 in floating-point; ~10⁻¹⁶ for double precision"
            }
          ],

          "about": [
            {"name": "Finite Differences", "description": "Numerical approximation of derivatives"},
            {"name": "Relative Error", "description": "Scale-independent accuracy metric"},
            {"name": "Numerical Stability", "description": "Balancing truncation and roundoff errors"},
            {"name": "Computational Validation", "description": "Testing analytical formulas with code"}
          ]
        },
        { 
          "id": "calc-5",
          "part": 5,
          "title": "The Derivative of Scalar Functions of Matrices",
          "url": "det.html",
          "icon": "det",
          "keywords": [
            "Frobenius inner product",
            "Frobenius norm",
            "Trace",
            "Determinant",
            "Cofactor",
            "Adjugate",
            "Characteristic polynomial",
            "Automatic differentiation"
          ],
          "badges": [],
          "prereqs": ["calc-3", "linalg-4"],
          "mapCoords": {"q": -2, "r": -2},
          "topicGroup": "derivatives",
          "tesseraMessage": "Derivatives of determinants and traces — essential for advanced ML proofs.",

          "headline": "Scalar Functions of Matrices: Frobenius Norm and Determinant Derivatives",
          "description": "Learn about derivatives of Frobenius norm and determinants.",
          "abstract": "Scalar functions of matrices arise throughout ML: the Frobenius norm measures matrix magnitude, while determinants appear in Gaussian distributions and volume calculations. The trace trick expresses derivatives elegantly: df = tr((∇f)ᵀdX). Automatic differentiation often outperforms analytical formulas numerically.",
          "datePublished": "2024-01-01",
          "dateModified": "2025-01-15",

          "teaches": [
            "Derivative of Frobenius norm: ∇||X||_F = X/||X||_F",
            "Differential notation with trace: df = tr((∇f)ᵀdX)",
            "Frobenius inner product representation",
            "Derivative of determinant via cofactors",
            "Relationship: ∇(det A) = cofactor(A) = (det A)(A⁻¹)ᵀ",
            "Derivative of characteristic polynomial",
            "Analytical vs automatic differentiation comparison",
            "Numerical stability considerations"
          ],

          "competencyRequired": [
            "Matrix calculus fundamentals",
            "Determinants and cofactors",
            "Trace properties"
          ],

          "sections": [
            {"id": "frob", "name": "Derivative of the Frobenius norm"},
            {"id": "det", "name": "Derivative of the Determinant"}
          ],

          "definitions": [
            {
              "term": "Frobenius Norm",
              "definition": "||X||_F = √(tr(XᵀX)); gradient is X/||X||_F"
            },
            {
              "term": "Trace Trick",
              "definition": "df = tr((∇f)ᵀdX) expresses scalar derivatives of matrices"
            },
            {
              "term": "Determinant Derivative",
              "definition": "d(det A) = (det A)·tr(A⁻¹dA) = tr(adj(A)dA)"
            },
            {
              "term": "Cofactor/Adjugate Relation",
              "definition": "∇(det A) = cofactor(A) = adj(A)ᵀ = (det A)(A⁻¹)ᵀ"
            }
          ],

          "about": [
            {"name": "Frobenius Norm", "description": "Matrix magnitude via trace"},
            {"name": "Trace Trick", "description": "Elegant derivative representation"},
            {"name": "Determinant Derivative", "description": "Via cofactors or adjugate"},
            {"name": "Automatic Differentiation", "description": "Practical alternative to analytical formulas"}
          ]
        },
        { 
          "id": "calc-6",
          "part": 6,
          "title": "The Mean Value Theorem",
          "url": "mvt.html",
          "icon": "μ",
          "keywords": [
            "Rolle's Theorem",
            "Lagrange's Mean Value Theorem",
            "Cauchy's Mean Value Theorem",
            "Taylor's Theorem",
            "Taylor polynomial",
            "Taylor series",
            "Lagrange remainder",
            "little-o notation",
            "Higher-dimensional MVT",
            "Linearization"
          ],
          "badges": [],
          "prereqs": ["calc-1"],
          "mapCoords": {"q": -3, "r": 0},
          "topicGroup": "optimization",
          "tesseraMessage": "Taylor's theorem lets us approximate any smooth function locally. Powerful!",

          "headline": "Mean Value Theorem: The Bridge Between Derivatives and Differences",
          "description": "Learn about Lagrange's & Cauchy's mean value theorem, and Higher-dimensional MVT.",
          "abstract": "The Mean Value Theorem connects point derivatives to average rates of change. Taylor's theorem extends this to polynomial approximations of any order. The higher-dimensional MVT provides the foundation for gradient descent in multivariable optimization.",
          "datePublished": "2024-01-01",
          "dateModified": "2025-01-15",

          "teaches": [
            "Lagrange's Mean Value Theorem and its proof",
            "Rolle's Theorem as a special case",
            "Taylor's Theorem and polynomial approximations",
            "Lagrange's form of the remainder",
            "Little-o notation in asymptotic analysis",
            "Taylor series and radius of convergence",
            "Cauchy's Mean Value Theorem",
            "Higher-dimensional MVT for gradient descent"
          ],

          "competencyRequired": [
            "Differentiation of single-variable functions",
            "Continuity and limits",
            "Basic proof techniques"
          ],

          "sections": [
            {"id": "l_mvt", "name": "Lagrange's Mean Value Theorem"},
            {"id": "taylor", "name": "Taylor's Theorem"},
            {"id": "c_mvt", "name": "Cauchy's Mean Value Theorem"},
            {"id": "h_dim", "name": "Higher-dimensional MVT"}
          ],

          "definitions": [
            {
              "term": "Lagrange's MVT",
              "definition": "If f is continuous on [a,b] and differentiable on (a,b), then ∃c ∈ (a,b) such that f'(c) = (f(b)-f(a))/(b-a)"
            },
            {
              "term": "Rolle's Theorem",
              "definition": "Special case of MVT: if f(a) = f(b), then ∃c ∈ (a,b) such that f'(c) = 0"
            },
            {
              "term": "Taylor Polynomial",
              "definition": "Tₙ(x) = Σₖ₌₀ⁿ f⁽ᵏ⁾(a)/k! · (x-a)ᵏ, the n-th order approximation of f near a"
            },
            {
              "term": "Lagrange Remainder",
              "definition": "Rₙ(x) = f⁽ⁿ⁾(c)/n! · (x-a)ⁿ where c ∈ (a,x)"
            },
            {
              "term": "Higher-dimensional MVT",
              "definition": "f(b) - f(a) = ∇f(c) · (b-a) for some c on the line segment [a,b]"
            }
          ],

          "about": [
            {"name": "Mean Value Theorem", "description": "Connects derivatives to differences over intervals"},
            {"name": "Taylor's Theorem", "description": "Polynomial approximation with quantified error"},
            {"name": "Linearization", "description": "First-order Taylor approximation"},
            {"name": "Higher-dimensional MVT", "description": "Foundation for gradient-based optimization"}
          ]
        },
        {   
        "id": "calc-7",
        "part": 7,
        "title": "Gradient Descent (First-order Method)",
        "url": "gradient.html",
        "icon": "↓",
        "keywords": [
          "Optimization problems",
          "Convexity",
          "Convex functions",
          "Gradient Descent (GD)",
          "Steepest Descent",
          "Stochastic Gradient Descent (SGD)",
          "Mini-batch SGD",
          "Sub-gradient",
          "Sub-differentiable",
          "Learning rate",
          "Local minimum",
          "Global minimum"
        ],
        "badges": ["code"],
        "prereqs": ["calc-6"],
        "mapCoords": {"q": -3, "r": -1},
        "topicGroup": "optimization",
        "tesseraMessage": "Gradient descent is the workhorse of ML — simple yet powerful optimization.",

        "headline": "Gradient Descent: The Workhorse of Machine Learning Optimization",
        "description": "Learn about convexity, gradient descent, and stochastic gradient descent.",
        "abstract": "Gradient descent iteratively moves toward minima by following the negative gradient. This first-order method, with variants like SGD and mini-batch SGD, trains virtually every neural network. Understanding convexity reveals when gradient descent finds global optima.",
        "datePublished": "2024-01-01",
        "dateModified": "2025-01-15",

        "teaches": [
          "Optimization problem formulation in machine learning",
          "Convex functions and their properties",
          "Local vs global minima and the role of convexity",
          "Gradient descent algorithm and steepest descent",
          "Learning rate selection and convergence",
          "Stochastic gradient descent for large datasets",
          "Mini-batch SGD and epoch-based training",
          "Subgradient descent for non-smooth functions"
        ],

        "competencyRequired": [
          "Partial derivatives and gradients",
          "Mean Value Theorem",
          "Matrix-vector operations"
        ],

        "sections": [
          {"id": "intro", "name": "Introduction to Optimization"},
          {"id": "convexity", "name": "Convexity"},
          {"id": "gradient", "name": "Gradient Descent"},
          {"id": "sgd", "name": "Stochastic Gradient Descent"},
          {"id": "subgradient", "name": "Sub-gradient Descent"}
        ],

        "definitions": [
          {
            "term": "Convex Function",
            "definition": "f(λx + (1-λ)y) ≤ λf(x) + (1-λ)f(y) for all x,y and λ ∈ [0,1]"
          },
          {
            "term": "Gradient Descent",
            "definition": "θ⁽ᵏ⁺¹⁾ = θ⁽ᵏ⁾ - η∇f(θ⁽ᵏ⁾), iteratively moving opposite to gradient"
          },
          {
            "term": "Stochastic Gradient Descent",
            "definition": "Update using gradient of single random sample instead of full batch"
          },
          {
            "term": "Mini-batch SGD",
            "definition": "Gradient computed on small subset B of data, balancing noise and efficiency"
          },
          {
            "term": "Subgradient",
            "definition": "g such that f(z) ≥ f(x) + gᵀ(z-x) for all z; generalizes gradient to non-smooth functions"
          }
        ],

        "about": [
          {"name": "Convexity", "description": "Property guaranteeing global optimum from any local optimum"},
          {"name": "Gradient Descent", "description": "First-order iterative optimization algorithm"},
          {"name": "SGD", "description": "Scalable variant using random sampling"},
          {"name": "Subgradient", "description": "Extension to non-differentiable convex functions"}
        ]
        },
        {   
        "id": "calc-8",
        "part": 8,
        "title": "Newton's method (Second-order Method)",
        "url": "newton.html",
        "icon": "N",
        "keywords": [
          "Line search",
          "Armijo condition",
          "Curvature condition",
          "Wolfe conditions",
          "Newton's method",
          "Quasi-Newton methods",
          "BFGS",
          "L-BFGS",
          "Secant condition",
          "Inverse Hessian approximation",
          "Rosenbrock function",
          "Second-order optimization"
        ],
        "badges": ["code"],
        "prereqs": ["calc-7", "linalg-9"],
        "mapCoords": {"q": -4, "r": -1},
        "topicGroup": "optimization",
        "tesseraMessage": "Newton's method: faster convergence using curvature. BFGS makes it practical!",

        "headline": "Newton's Method: Second-Order Optimization with Curvature",
        "description": "Learn about line search, Newton's method, and BFGS method.",
        "abstract": "Newton's method uses the Hessian matrix to achieve quadratic convergence near optima. BFGS and L-BFGS approximate the inverse Hessian efficiently, avoiding expensive matrix inversions while retaining fast convergence. Line search with Wolfe conditions ensures stable step sizes.",
        "datePublished": "2024-01-01",
        "dateModified": "2025-01-15",

        "teaches": [
          "Line search methods and step size selection",
          "Armijo condition for sufficient decrease",
          "Wolfe conditions: Armijo + curvature",
          "Newton's method using Hessian information",
          "Quasi-Newton methods avoiding explicit Hessian",
          "BFGS inverse Hessian approximation",
          "Secant condition and rank-2 updates",
          "L-BFGS for large-scale optimization",
          "Rosenbrock function as optimization benchmark"
        ],

        "competencyRequired": [
          "Gradient descent fundamentals",
          "Matrix inverses and positive definiteness",
          "Quadratic forms"
        ],

        "sections": [
          {"id": "LS", "name": "Line Search"},
          {"id": "NM", "name": "Newton's Method"},
          {"id": "BFGS", "name": "BFGS Method"}
        ],

        "definitions": [
          {
            "term": "Newton's Method",
            "definition": "θ⁽ᵏ⁺¹⁾ = θ⁽ᵏ⁾ - H⁻¹∇f(θ⁽ᵏ⁾), using Hessian for quadratic convergence"
          },
          {
            "term": "Armijo Condition",
            "definition": "f(θ + ηp) ≤ f(θ) + c₁η∇f(θ)ᵀp, ensuring sufficient decrease"
          },
          {
            "term": "Wolfe Conditions",
            "definition": "Armijo condition + curvature condition for proper step sizes"
          },
          {
            "term": "BFGS",
            "definition": "Quasi-Newton method approximating inverse Hessian via rank-2 updates"
          },
          {
            "term": "L-BFGS",
            "definition": "Limited-memory BFGS storing only m recent (s,y) pairs for large-scale problems"
          },
          {
            "term": "Secant Condition",
            "definition": "Bₖ₊₁sₖ = yₖ where sₖ = θₖ₊₁ - θₖ and yₖ = ∇f(θₖ₊₁) - ∇f(θₖ)"
          }
        ],

        "about": [
          {"name": "Newton's Method", "description": "Second-order optimization with Hessian"},
          {"name": "Line Search", "description": "Choosing step sizes satisfying Wolfe conditions"},
          {"name": "BFGS", "description": "Practical quasi-Newton without explicit Hessian"},
          {"name": "L-BFGS", "description": "Memory-efficient BFGS for high-dimensional problems"}
        ]
        },
        {
        
        "id": "calc-9",
        "part": 9,
        "title": "Constrained Optimization",
        "url": "constrained_opt.html",
        "icon": "λ",
        "keywords": [
          "Constrained optimization problems",
          "Penalty terms",
          "Lagrange Multipliers",
          "Lagrangian",
          "Karush-Kuhn-Tucker (KKT) conditions",
          "Active set",
          "Slack variables",
          "Complementary slackness",
          "Primal feasibility",
          "Dual feasibility"
        ],
        "badges": ["code"],
        "prereqs": ["calc-7"],
        "mapCoords": {"q": -4, "r": 0},
        "topicGroup": "optimization",
        "tesseraMessage": "KKT conditions are the foundation of constrained optimization. Essential theory!",

        "headline": "Constrained Optimization: KKT Conditions and Lagrange Multipliers",
        "description": "Learn about constrained optimization ideas, KKT Conditions, and Lagrange multipliers.",
        "abstract": "Constrained optimization handles equality and inequality constraints via Lagrange multipliers and KKT conditions. These necessary conditions for optimality underpin support vector machines, portfolio optimization, and many ML algorithms requiring bounded or structured solutions.",
        "datePublished": "2024-01-01",
        "dateModified": "2025-01-15",

        "teaches": [
          "Formulation of constrained optimization problems",
          "Equality constraints and Lagrange multipliers",
          "Inequality constraints and KKT conditions",
          "Stationarity, primal feasibility, dual feasibility",
          "Complementary slackness conditions",
          "Active and inactive constraints",
          "Penalty method implementation",
          "Slack variables for numerical solvers"
        ],

        "competencyRequired": [
          "Gradient descent and unconstrained optimization",
          "Partial derivatives",
          "Systems of nonlinear equations"
        ],

        "sections": [
          {"id": "intro", "name": "Constrained Optimization Problem"},
          {"id": "lm", "name": "Lagrange Multipliers"},
          {"id": "kkt", "name": "The KKT Conditions"}
        ],

        "definitions": [
          {
            "term": "Lagrangian",
            "definition": "L(x,λ,μ) = f(x) + Σλᵢhᵢ(x) + Σμⱼgⱼ(x), combining objective with constraints"
          },
          {
            "term": "KKT Conditions",
            "definition": "Necessary conditions: stationarity, primal feasibility, dual feasibility, complementary slackness"
          },
          {
            "term": "Complementary Slackness",
            "definition": "μᵢgᵢ(x) = 0 for all i; either constraint is active or multiplier is zero"
          },
          {
            "term": "Lagrange Multiplier",
            "definition": "λ measuring sensitivity of objective to constraint; shadow price"
          },
          {
            "term": "Slack Variable",
            "definition": "Transforms gᵢ(x) ≤ 0 to gᵢ(x) + sᵢ = 0 with sᵢ ≥ 0"
          }
        ],

        "about": [
          {"name": "KKT Conditions", "description": "First-order necessary conditions for constrained optimality"},
          {"name": "Lagrangian", "description": "Augmented objective incorporating constraints"},
          {"name": "Complementary Slackness", "description": "Key condition linking constraints and multipliers"},
          {"name": "Penalty Method", "description": "Approximate constrained problems via unconstrained penalties"}
        ]
        },  
        {
        "id": "calc-10",
        "part": 10,
        "title": "Riemann Integration",
        "url": "riemann.html",
        "icon": "∫",
        "keywords": [
          "Riemann integral",
          "Riemann integrable",
          "Improper Riemann integration",
          "Partition",
          "Upper sum",
          "Lower sum",
          "Dirichlet function",
          "Dense sets",
          "Gaussian integral",
          "Cauchy distribution"
        ],
        "badges": [],
        "prereqs": ["calc-6"],
        "mapCoords": {"q": -3, "r": -2},
        "topicGroup": "analysis",
        "tesseraMessage": "Riemann integration — the classical approach before Lebesgue changed everything.",

        "headline": "Riemann Integration: The Classical Approach to Integration",
        "description": "Learn about Riemann integration, and improper Riemann integration.",
        "abstract": "Riemann integration defines integrals via partitions and upper/lower sums. While sufficient for continuous functions, its limitations with highly discontinuous functions like the Dirichlet function motivate the more powerful Lebesgue integral needed in probability theory.",
        "datePublished": "2024-01-01",
        "dateModified": "2025-01-15",

        "teaches": [
          "Formal definition of Riemann integration",
          "Partitions, upper sums, and lower sums",
          "Conditions for Riemann integrability",
          "Improper integrals on unbounded intervals",
          "Handling singularities in integrals",
          "Limitations with dense discontinuities",
          "Connection to Gaussian and Cauchy integrals",
          "Motivation for Lebesgue integration"
        ],

        "competencyRequired": [
          "Limits and continuity",
          "Infimum and supremum",
          "Basic integration techniques"
        ],

        "sections": [
          {"id": "rieman", "name": "Riemann Integration"},
          {"id": "i_rieman", "name": "Improper Riemann Integration"},
          {"id": "limit", "name": "Limitation of the (Improper) Riemann integration"}
        ],

        "definitions": [
          {
            "term": "Riemann Integral",
            "definition": "∫ₐᵇf(x)dx = α when sup L(f,P) = inf U(f,P) = α over all partitions"
          },
          {
            "term": "Partition",
            "definition": "P = {x₀, x₁, ..., xₙ} with a = x₀ < x₁ < ... < xₙ = b"
          },
          {
            "term": "Upper/Lower Sum",
            "definition": "U(f,P) = Σ Mᵢ(xᵢ-xᵢ₋₁), L(f,P) = Σ mᵢ(xᵢ-xᵢ₋₁)"
          },
          {
            "term": "Improper Integral",
            "definition": "∫ₐ^∞ f(x)dx = lim_{b→∞} ∫ₐᵇ f(x)dx when the limit exists"
          },
          {
            "term": "Dense Set",
            "definition": "A ⊂ ℝ is dense if for any x < y, there exists a ∈ A with x < a < y"
          }
        ],

        "about": [
          {"name": "Riemann Integration", "description": "Classical partition-based integration"},
          {"name": "Improper Integrals", "description": "Extending integration to unbounded domains"},
          {"name": "Dirichlet Function", "description": "Example showing Riemann's limitations"},
          {"name": "Dense Discontinuities", "description": "Why Lebesgue integration is needed"}
        ]
        },
        {
        "id": "calc-11",
        "part": 11,
        "title": "Measure Theory with Probability",
        "url": "measure.html",
        "icon": "σ",
        "keywords": [
          "Sample space",
          "σ-algebra",
          "σ-field",
          "Measurable set",
          "Measurable space",
          "Measure",
          "Probability measure",
          "Probability space",
          "Countable additivity",
          "Borel σ-algebra",
          "Borel set",
          "Lebesgue measure",
          "Caratheodory Extension Theorem"
        ],
        "badges": [],
        "prereqs": ["calc-10", "prob-1"],
        "mapCoords": {"q": -4, "r": -2},
        "topicGroup": "analysis",
        "tesseraMessage": "Measure theory makes probability rigorous. σ-algebras are everywhere!",

        "headline": "Measure Theory: The Rigorous Foundation of Probability",
        "description": "Learn about measure theory in terms of probability, and Lebesgue measure.",
        "abstract": "Measure theory provides the rigorous foundation for probability via σ-algebras and probability measures. The Carathéodory extension theorem constructs the Lebesgue measure, enabling integration of functions that defeat Riemann's approach.",
        "datePublished": "2024-01-01",
        "dateModified": "2025-01-15",

        "teaches": [
          "Formal structure of probability spaces (Ω, F, P)",
          "Sample spaces and elementary outcomes",
          "σ-algebras and closure properties",
          "Probability measures and axioms",
          "Countable vs finite additivity",
          "Carathéodory's extension theorem",
          "Construction of Lebesgue measure on ℝ",
          "Borel σ-algebra and measurable sets"
        ],

        "competencyRequired": [
          "Riemann integration and its limitations",
          "Basic probability concepts",
          "Set operations and notation"
        ],

        "sections": [
          {"id": "pspace", "name": "Probability Space"},
          {"id": "sample", "name": "Sample Space"},
          {"id": "sigma", "name": "σ-algebra (σ-field)"},
          {"id": "pm", "name": "Probability Measure"},
          {"id": "add", "name": "Finite Additivity"},
          {"id": "ext", "name": "Caratheodory's Extension Theorem"},
          {"id": "leb", "name": "Lebesgue measure"}
        ],

        "definitions": [
          {
            "term": "Probability Space",
            "definition": "Triple (Ω, F, P): sample space, σ-algebra, and probability measure"
          },
          {
            "term": "σ-algebra",
            "definition": "Collection F of subsets of Ω closed under complement and countable unions, containing Ω"
          },
          {
            "term": "Probability Measure",
            "definition": "Function P: F → [0,1] with P(Ω) = 1 and countable additivity"
          },
          {
            "term": "Borel σ-algebra",
            "definition": "Smallest σ-algebra containing all intervals; denoted B"
          },
          {
            "term": "Lebesgue Measure",
            "definition": "μ on (ℝ, B) with μ([a,b]) = b - a; extends 'length' to Borel sets"
          },
          {
            "term": "Carathéodory Extension",
            "definition": "Theorem extending finitely additive pre-measure to full σ-algebra"
          }
        ],

        "about": [
          {"name": "Probability Space", "description": "Formal triple (Ω, F, P) modeling random experiments"},
          {"name": "σ-algebra", "description": "Events we can assign probabilities to"},
          {"name": "Lebesgue Measure", "description": "Generalized notion of length/volume"},
          {"name": "Carathéodory", "description": "Foundational extension theorem"}
        ]
        },
        {
        "id": "calc-12",
        "part": 12,
        "title": "Intro to Lebesgue Integration",
        "url": "lebesgue.html",
        "icon": "a.e.",
        "keywords": [
          "Lebesgue integral",
          "Abstract integration",
          "Characteristic function",
          "Indicator function",
          "Simple function",
          "Almost everywhere (a.e.)",
          "Almost surely (a.s.)",
          "Measurable function",
          "Dirichlet function",
          "Dirichlet integral",
          "Sinc function"
        ],
        "badges": [],
        "prereqs": ["calc-11"],
        "mapCoords": {"q": -5, "r": -1},
        "topicGroup": "analysis",
        "tesseraMessage": "Lebesgue integration handles functions Riemann cannot. 'Almost everywhere' matters!",

        "headline": "Lebesgue Integration: Beyond Riemann's Limitations",
        "description": "Learn about integration of measurable functions.",
        "abstract": "Lebesgue integration builds integrals from measure theory, handling highly discontinuous functions that defeat Riemann. The concept of 'almost everywhere' formalizes ignoring measure-zero exceptions—essential for probability theory where 'almost surely' governs convergence.",
        "datePublished": "2024-01-01",
        "dateModified": "2025-01-15",

        "teaches": [
          "Abstract integration in measure spaces",
          "Characteristic (indicator) functions",
          "Simple functions and their integrals",
          "Integration of nonnegative measurable functions",
          "Integration of general measurable functions via f⁺ and f⁻",
          "Concept of 'almost everywhere' (a.e.)",
          "'Almost surely' in probability contexts",
          "Comparison: Riemann vs Lebesgue integrability",
          "Conditionally convergent integrals (sinc function)"
        ],

        "competencyRequired": [
          "Measure spaces and σ-algebras",
          "Lebesgue measure",
          "Supremum and infimum"
        ],

        "sections": [
          {"id": "abs", "name": "Abstract Integration"},
          {"id": "c_f", "name": "Characteristic function"},
          {"id": "fnm", "name": "The Integral of Finite Nonnegative Measurable Functions"},
          {"id": "nm", "name": "The Integral of Nonnegative Measurable Functions"},
          {"id": "gm", "name": "The Integral of General Measurable Functions"}
        ],

        "definitions": [
          {
            "term": "Lebesgue Integral",
            "definition": "∫g dμ defined via simple function approximation on measure space (Ω, F, μ)"
          },
          {
            "term": "Characteristic Function",
            "definition": "χ_A(ω) = 1 if ω ∈ A, 0 otherwise; indicator of set A"
          },
          {
            "term": "Simple Function",
            "definition": "g = Σᵢ aᵢχ_{Aᵢ} with finitely many values; integral = Σᵢ aᵢμ(Aᵢ)"
          },
          {
            "term": "Almost Everywhere (a.e.)",
            "definition": "Property holds except on a set of measure zero"
          },
          {
            "term": "Almost Surely (a.s.)",
            "definition": "'Almost everywhere' in probability context; holds with probability 1"
          }
        ],

        "about": [
          {"name": "Lebesgue Integral", "description": "Measure-theoretic generalization of Riemann"},
          {"name": "Simple Functions", "description": "Building blocks for Lebesgue integration"},
          {"name": "Almost Everywhere", "description": "Ignoring measure-zero exceptions"},
          {"name": "Dirichlet Function", "description": "Classic example: Lebesgue integral = 0"}
        ]
        },
        {   
        "id": "calc-13",
        "part": 13,
        "title": "Duality in Optimization & Analysis",
        "url": "duality.html",
        "icon": "P⟷D",
        "keywords": [
          "Duality",
          "Primal problem",
          "Dual problem",
          "Weak duality",
          "Strong duality",
          "Duality gap",
          "Slater's condition",
          "Smoothness",
          "L-smooth",
          "Lipschitz continuity",
          "Contraction mapping",
          "Convergence rate",
          "Condition number"
        ],
        "badges": ["interactive"],
        "prereqs": ["calc-9"],
        "mapCoords": {"q": -5, "r": 0},
        "topicGroup": "optimization",
        "tesseraMessage": "Duality gives us bounds and insights. Strong duality = same optimal value!",

        "headline": "Duality: Primal-Dual Relationships and Convergence Analysis",
        "description": "Learn about duality and Lipschitz Continuity.",
        "abstract": "Duality theory provides lower bounds on optimization problems and enables efficient algorithms. Lipschitz continuity and L-smoothness govern convergence rates—the condition number determines how fast gradient descent converges for quadratic problems.",
        "datePublished": "2024-01-01",
        "dateModified": "2025-01-15",

        "teaches": [
          "Primal and dual optimization problems",
          "Weak duality: dual provides lower bound",
          "Strong duality and zero duality gap",
          "Slater's condition for strong duality",
          "Lipschitz continuity and bounded rate of change",
          "L-smooth functions and gradient Lipschitz",
          "Contraction mappings and linear convergence",
          "Condition number and convergence rate analysis"
        ],

        "competencyRequired": [
          "Constrained optimization and KKT conditions",
          "Lagrangian formulation",
          "Eigenvalues of symmetric matrices"
        ],

        "sections": [
          {"id": "duality", "name": "Duality"},
          {"id": "Lip", "name": "Lipschitz Continuity"},
          {"id": "duality-visualization", "name": "Interactive Duality Visualization"}
        ],

        "definitions": [
          {
            "term": "Weak Duality",
            "definition": "d* ≤ p* always; dual optimal value bounds primal"
          },
          {
            "term": "Strong Duality",
            "definition": "d* = p*; primal and dual have same optimal value"
          },
          {
            "term": "Duality Gap",
            "definition": "p* - d* ≥ 0; zero gap means strong duality holds"
          },
          {
            "term": "L-smooth",
            "definition": "||∇f(x) - ∇f(y)|| ≤ L||x - y||; gradient is Lipschitz with constant L"
          },
          {
            "term": "Contraction Mapping",
            "definition": "d(f(x), f(y)) ≤ k·d(x,y) with k < 1; guarantees convergence"
          },
          {
            "term": "Condition Number",
            "definition": "κ = λ_max/λ_min; determines convergence rate μ = ((κ-1)/(κ+1))²"
          }
        ],

        "about": [
          {"name": "Duality", "description": "Relationship between primal and dual problems"},
          {"name": "Strong Duality", "description": "When primal and dual optima coincide"},
          {"name": "L-smoothness", "description": "Lipschitz gradient enabling convergence bounds"},
          {"name": "Condition Number", "description": "Key factor determining optimization speed"}
        ]
        },
        { 
        "id": "calc-14",
        "part": 14,
        "title": "Fourier Series",
        "url": "fourier_series.html",
        "icon": "∿",
        "keywords": [
          "Fourier series",
          "Fourier coefficients",
          "Orthogonality of Trigonometric Functions",
          "Complex exponential form",
          "Parseval's identity",
          "L² convergence",
          "Gibbs phenomenon",
          "Periodic functions",
          "Harmonic analysis",
          "Signal decomposition"
        ],
        "badges": [],
        "prereqs": ["calc-12", "linalg-7"],
        "mapCoords": {"q": -5, "r": -2},
        "topicGroup": "fourier",
        "tesseraMessage": "Fourier series decompose signals into frequencies. Music, images, everything!",

        "headline": "Fourier Series: Decomposing Periodic Functions into Frequencies",
        "description": "Learn about Fourier series decomposition of periodic functions and their convergence properties.",
        "abstract": "Fourier series represent periodic functions as infinite sums of sines and cosines. This fundamental technique, developed by Joseph Fourier for studying heat conduction, underlies signal processing, data compression (JPEG, MP3), and modern machine learning architectures.",
        "datePublished": "2024-01-01",
        "dateModified": "2025-01-15",

        "teaches": [
          "Fourier series representation of periodic functions",
          "Computing Fourier coefficients via integration",
          "Orthogonality of trigonometric functions",
          "Complex exponential form of Fourier series",
          "Parseval's identity and energy conservation",
          "L² (mean square) convergence of Fourier series",
          "Pointwise convergence and bounded variation",
          "Gibbs phenomenon at discontinuities"
        ],

        "competencyRequired": [
          "Riemann integration",
          "Inner products and orthogonality",
          "Complex exponentials"
        ],

        "sections": [
          {"id": "intro", "name": "Introduction"},
          {"id": "ortho", "name": "Orthogonality of Trigonometric Functions"},
          {"id": "coeff", "name": "Fourier Coefficients"},
          {"id": "complex", "name": "Complex Exponential Form"},
          {"id": "parseval", "name": "Parseval's Identity"},
          {"id": "convergence", "name": "Convergence Properties"}
        ],

        "definitions": [
          {
            "term": "Fourier Series",
            "definition": "Representation of a periodic function f(x) as f(x) = a₀/2 + Σ(aₙcos(nπx/L) + bₙsin(nπx/L))"
          },
          {
            "term": "Fourier Coefficients",
            "definition": "The constants aₙ and bₙ computed by integrating f(x) against cos and sin basis functions"
          },
          {
            "term": "Parseval's Identity",
            "definition": "Energy conservation: (1/L)∫|f(x)|²dx = a₀²/2 + Σ(aₙ² + bₙ²)"
          },
          {
            "term": "Gibbs Phenomenon",
            "definition": "~9% overshoot in partial sums near jump discontinuities that persists as N→∞"
          }
        ],

        "about": [
          {"name": "Fourier Series", "description": "Infinite trigonometric series representing periodic functions"},
          {"name": "Parseval's Identity", "description": "Energy equivalence between time and frequency domains"},
          {"name": "Gibbs Phenomenon", "description": "Persistent oscillation artifact at discontinuities"},
          {"name": "L² Convergence", "description": "Mean square convergence in function space"}
        ]
        },
        {   
       "id": "calc-15",
        "part": 15,
        "title": "Fourier Transform",
        "url": "fourier_transform.html",
        "icon": "ℱ",
        "keywords": [
          "Fourier transform",
          "Inverse Fourier transform",
          "Plancherel's theorem",
          "Convolution theorem",
          "Discrete Fourier Transform (DFT)",
          "Fast Fourier Transform (FFT)",
          "Fourier Neural Operators (FNO)",
          "Frequency domain",
          "Signal processing",
          "Spectral analysis"
        ],
        "badges": ["interactive"],
        "prereqs": ["calc-14"],
        "mapCoords": {"q": -6, "r": -1},
        "topicGroup": "fourier",
        "tesseraMessage": "FFT changed the world — fast signal processing makes modern tech possible.",

        "headline": "Fourier Transform: From Time Domain to Frequency Domain",
        "description": "Learn about the Fourier transform, its properties, and applications to signal processing and machine learning.",
        "abstract": "The Fourier transform extends Fourier series to non-periodic functions, decomposing signals into continuous frequency spectra. The FFT algorithm enables efficient computation, powering everything from audio processing to Fourier Neural Operators for solving PDEs.",
        "datePublished": "2024-01-01",
        "dateModified": "2025-01-15",

        "teaches": [
          "Continuous Fourier transform and inverse transform",
          "Properties: linearity, shifting, scaling, differentiation",
          "Convolution theorem: multiplication in frequency domain",
          "Plancherel's theorem for energy preservation",
          "Discrete Fourier Transform (DFT) for sampled signals",
          "Fast Fourier Transform (FFT) algorithm and O(N log N) complexity",
          "Applications: audio processing, image filtering, neural networks",
          "Fourier Neural Operators for solving PDEs"
        ],

        "competencyRequired": [
          "Fourier series and coefficients",
          "Complex exponentials",
          "Improper integrals"
        ],

        "sections": [
          {"id": "continuous", "name": "Continuous Fourier Transform"},
          {"id": "properties", "name": "Properties of Fourier Transform"},
          {"id": "convolution", "name": "Convolution Theorem"},
          {"id": "discrete", "name": "Discrete Fourier Transform"},
          {"id": "fft", "name": "Fast Fourier Transform"},
          {"id": "ml", "name": "Applications in Machine Learning"},
          {"id": "demos", "name": "Interactive Demos"}
        ],

        "definitions": [
          {
            "term": "Fourier Transform",
            "definition": "f̂(ξ) = ∫f(x)e^(ixξ)dx, mapping a function to its frequency spectrum"
          },
          {
            "term": "Inverse Fourier Transform",
            "definition": "f(x) = (1/2π)∫f̂(ξ)e^(-ixξ)dξ, recovering the original function"
          },
          {
            "term": "Convolution Theorem",
            "definition": "ℱ{f * g} = ℱ{f} · ℱ{g}, convolution becomes multiplication in frequency domain"
          },
          {
            "term": "Fast Fourier Transform (FFT)",
            "definition": "Divide-and-conquer algorithm computing DFT in O(N log N) instead of O(N²)"
          },
          {
            "term": "Fourier Neural Operator",
            "definition": "Neural network architecture that learns operators in Fourier space for solving PDEs"
          }
        ],

        "about": [
          {"name": "Fourier Transform", "description": "Integral transform mapping functions to frequency spectra"},
          {"name": "Convolution Theorem", "description": "Fundamental connection between convolution and multiplication"},
          {"name": "FFT", "description": "Efficient algorithm enabling real-time signal processing"},
          {"name": "Fourier Neural Operators", "description": "ML architecture for learning solution operators to PDEs"}
        ]
        },
        {
        "id": "calc-16",
        "part": 16,
        "title": "Foundations of Analysis: Metric Spaces",
        "url": "metric_space.html",
        "icon": "(X, d)",
        "keywords": [
          "Metric Space",
          "Distance",
          "Isolated Points",
          "Accumulation Points",
          "Nearest Points",
          "Boundary",
          "Interior",
          "Closure",
          "Open and Closed Sets",
          "Topology",
          "Completeness",
          "Open Balls",
          "Closed Balls",
          "Convexity"
        ],
        "badges": [],
        "prereqs": ["linalg-10", "calc-7", "calc-9", "ml-3"],
        "mapCoords": {"q": -6, "r": 0},
        "topicGroup": "analysis",
        "tesseraMessage": "Metric spaces abstract the notion of distance. The gateway to topology!",

        "headline": "Metric Spaces: The Foundation for Analysis and Optimization",
        "description": "Formalize the structure of metric spaces, covering distance, boundary, open and closed sets, topology, and completeness—the foundations ensuring optimization algorithms converge to valid solutions.",
        "abstract": "Metric spaces provide the rigorous framework for understanding distance, convergence, and completeness. These concepts formalize exactly when iterative algorithms like gradient descent converge to valid solutions, extending intuition from Euclidean space to function spaces and beyond.",
        "datePublished": "2024-01-01",
        "dateModified": "2025-01-15",

        "teaches": [
          "Formal definition of metric spaces and the three metric axioms",
          "Distance from points to sets and between sets",
          "Isolated points, accumulation points, and nearest points",
          "Boundary, interior, closure, and exterior of sets",
          "Open and closed subsets of metric spaces",
          "Topology determined by a metric",
          "Complete metric spaces and their importance for optimization",
          "Open and closed balls as building blocks of topology",
          "Convex sets in normed linear spaces"
        ],

        "competencyRequired": [
          "Norms and normed vector spaces",
          "Basic set theory notation",
          "Infimum and supremum"
        ],

        "sections": [
          {"id": "intro", "name": "Introduction"},
          {"id": "metric", "name": "Metric Space"},
          {"id": "dist", "name": "Distance"},
          {"id": "bound", "name": "Boundary"},
          {"id": "opcl", "name": "Open & Closed Sets"},
          {"id": "ball", "name": "Balls"}
        ],

        "definitions": [
          {
            "term": "Metric Space",
            "definition": "A set X with distance function d: X×X → ℝ satisfying positivity, symmetry, and triangle inequality"
          },
          {
            "term": "Open Set",
            "definition": "A set equal to its interior; every point has a neighborhood contained in the set"
          },
          {
            "term": "Closed Set",
            "definition": "A set containing all its boundary points; complement of an open set"
          },
          {
            "term": "Complete Metric Space",
            "definition": "A space where X is closed in every metric superspace (equivalently, every Cauchy sequence converges)"
          },
          {
            "term": "Open Ball",
            "definition": "B[x; r) = {y ∈ X | d(x,y) < r}, all points within distance r of center x"
          },
          {
            "term": "Topology",
            "definition": "The collection of open subsets determined by the metric"
          }
        ],

        "about": [
          {"name": "Metric Spaces", "description": "Abstract spaces with well-defined notion of distance"},
          {"name": "Completeness", "description": "Property ensuring limits of Cauchy sequences exist in the space"},
          {"name": "Topology", "description": "Structure determined by which sets are open"},
          {"name": "Convexity", "description": "Sets containing all line segments between their points"}
        ]
        },
        {
          "id": "calc-17",
          "part": 17,
          "title": "Convergence & Boundedness",
          "url": "limit_convergence.html",
          "icon": "tail₍(x₎)",
          "keywords": [
            "Tails",
            "Convergence",
            "Limits",
            "Cauchy Sequences",
            "Boundedness",
            "Diameter",
            "Total Boundedness",
            "Completeness",
            "Sequences",
            "Metric Spaces",
            "Gradient Descent Convergence",
            "Iterative Algorithms"
          ],
          "badges": [],
          "prereqs": ["calc-16"],
          "mapCoords": {"q": -6, "r": 1},
          "topicGroup": "analysis",
          "tesseraMessage": "Convergence is when the 'tail' of a sequence settles into an arbitrarily small ball. It's the structural anchor for every iterative process!",

          "headline": "Convergence & Boundedness in Metric Spaces",
          "description": "Formalize convergence of sequences in metric spaces, establish the relationship between Cauchy sequences and completeness, and explore boundedness—the theoretical foundation for analyzing iterative algorithms.",
          "abstract": "This chapter shifts from pointwise ε-N definitions to structural topology, defining convergence through the behavior of sequence tails settling into arbitrarily small neighborhoods. Essential for understanding why optimization algorithms like gradient descent converge.",
          "datePublished": "2024-01-01",
          "dateModified": "2025-01-15",

          "teaches": [
            "Formal definition of sequence convergence in metric spaces",
            "Tail-based approach to convergence vs pointwise ε-N definitions",
            "Uniqueness of limits in metric spaces",
            "Cauchy sequences and the Cauchy criterion",
            "Equivalence of completeness definitions",
            "Bounded sets, bounded sequences, and diameter",
            "Total boundedness and covering numbers",
            "Applications to gradient descent, weight clipping, and regularization"
          ],

          "competencyRequired": [
            "Metric spaces and distance functions",
            "Open balls and neighborhoods",
            "Basic sequence notation"
          ],

          "sections": [
            {"id": "intro", "name": "Introduction"},
            {"id": "convergence", "name": "Convergence of Sequences"},
            {"id": "cauchy", "name": "Cauchy Sequences"},
            {"id": "bounds", "name": "Boundedness"},
            {"id": "complete-revisit", "name": "Completeness Revisited"}
          ],

          "definitions": [
            {
              "term": "Tail of a Sequence",
              "definition": "For sequence {xₙ}, the tail from m is tail_m = {xₙ | n ≥ m}"
            },
            {
              "term": "Convergent Sequence",
              "definition": "A sequence converges to limit L if for every ε > 0, some tail is contained in the ε-ball around L"
            },
            {
              "term": "Cauchy Sequence",
              "definition": "A sequence where for every ε > 0, some tail has diameter less than ε"
            },
            {
              "term": "Complete Metric Space",
              "definition": "A space where every Cauchy sequence converges to a point within the space"
            },
            {
              "term": "Bounded Set",
              "definition": "A set contained in some ball of finite radius"
            },
            {
              "term": "Totally Bounded",
              "definition": "A set coverable by finitely many balls of any given radius ε"
            }
          ],

          "about": [
            {"name": "Convergence", "description": "The property of a sequence approaching a limit"},
            {"name": "Cauchy Sequences", "description": "Sequences where terms become arbitrarily close to each other"},
            {"name": "Completeness", "description": "A metric space where every Cauchy sequence converges"},
            {"name": "Boundedness", "description": "Sets contained within a ball of finite radius"},
            {"name": "Total Boundedness", "description": "Sets coverable by finitely many balls of any given radius"}
          ]
        },
        {
        "id": "calc-18",
        "part": 18,
        "title": "Continuity",
        "url": "continuity.html",
        "icon": "ε-δ",
        "keywords": [
          "Continuity",
          "Uniform Continuity",
          "Lipschitz Continuity",
          "Lipschitz Constant",
          "Contraction",
          "Strong Contraction",
          "Isometry",
          "Homeomorphism",
          "Metric Spaces"
        ],
        "badges": [],
        "prereqs": ["calc-17"],
        "mapCoords": {"q": -7, "r": 2},
        "topicGroup": "analysis",
        "tesseraMessage": "Lipschitz constants are the guardrails of AI, ensuring our gradients don't explode and algorithms actually converge.",

        "headline": "Continuity: From ε-δ to Lipschitz Bounds",
        "description": "Formalize continuous and uniformly continuous functions between metric spaces, explore Lipschitz continuity, and understand why these properties ensure optimization algorithms behave predictably.",
        "abstract": "Continuity ensures small input changes produce small output changes. This chapter develops the hierarchy from basic continuity through uniform continuity to Lipschitz continuity—each providing stronger guarantees essential for proving convergence of gradient descent, GAN stability, and fixed-point iterations.",
        "datePublished": "2024-01-01",
        "dateModified": "2025-01-15",

        "teaches": [
          "Definition of continuity via open sets in metric spaces",
          "ε-δ characterization of continuity",
          "Sequential characterization of continuity",
          "Topological characterization via preimages of open sets",
          "Uniform continuity and global δ guarantees",
          "Lipschitz continuous functions and Lipschitz constants",
          "Contractions (L ≤ 1) and strong contractions (L < 1)",
          "Isometries as distance-preserving maps",
          "Hierarchy: Lipschitz ⟹ Uniformly Continuous ⟹ Continuous"
        ],

        "competencyRequired": [
          "Metric spaces and open sets",
          "Convergence of sequences",
          "Open balls and neighborhoods"
        ],

        "sections": [
          {"id": "intro", "name": "Introduction"},
          {"id": "continuity", "name": "Continuity"},
          {"id": "uniform", "name": "Uniform Continuity"},
          {"id": "lipschitz", "name": "Lipschitz Continuity"}
        ],

        "definitions": [
          {
            "term": "Continuous Function",
            "definition": "f: X → Y where for each open V ⊂ Y with f(z) ∈ V, there exists open U ⊂ X with z ∈ U and f(U) ⊆ V"
          },
          {
            "term": "Uniformly Continuous",
            "definition": "For every ε > 0, there exists a single δ > 0 such that d(x,z) < δ implies e(f(x),f(z)) < ε for ALL x,z"
          },
          {
            "term": "Lipschitz Continuous",
            "definition": "e(f(a),f(b)) ≤ L·d(a,b) for all a,b ∈ X, where L is the Lipschitz constant"
          },
          {
            "term": "Contraction",
            "definition": "A Lipschitz map with constant L ≤ 1; never increases distances"
          },
          {
            "term": "Strong Contraction",
            "definition": "A Lipschitz map with constant L < 1; strictly decreases distances"
          },
          {
            "term": "Isometry",
            "definition": "A map preserving distances exactly: e(φ(a),φ(b)) = d(a,b)"
          }
        ],

        "about": [
          {"name": "Continuity", "description": "Preserving nearness between domain and range"},
          {"name": "Uniform Continuity", "description": "Global stability with position-independent δ"},
          {"name": "Lipschitz Continuity", "description": "Bounded rate of change with explicit constant"},
          {"name": "Contractions", "description": "Maps fundamental for fixed-point theorems and RL convergence"}
        ]
        }
      ],
      "reservedSlots": [
        {
          "q": -8,
          "r": 3
        },
        {
          "q": -6,
          "r": -2
        },
        {
          "q": -7,
          "r": -1
        },
        {
          "q": -7,
          "r": 1
        }
      ]
    },
    "III": {
      "id": "probability",
      "title": "Probability & Statistics",
      "shortTitle": "Probability",
      "description": "Explore fundamental concepts of probability and statistics essential for machine learning, including probability theory, random variables, distributions, Bayesian inference, entropy, and the Fisher Information Matrix.",
      "tagline": "The Mathematics of Uncertainty and Inference",
      "icon": "fa-dice",
      "indexUrl": "Mathematics/Probability/probability.html",
      "baseUrl": "Mathematics/Probability/",
      "parts": [
        {
          "id": "prob-1",
          "part": 1,
          "title": "Basic Probability Ideas",
          "url": "basic.html",
          "icon": "p",
          "keywords": [
            "Probability",
            "Sample Space",
            "Events",
            "Mutually Exclusive",
            "Permutation",
            "Combinations",
            "Conditional Probability",
            "Independent Events",
            "Law of Total Probability",
            "Bayes' Theorem"
          ],
          "badges": [],
          "prereqs": [],
          "mapCoords": {
            "q": 0,
            "r": -1
          },
          "topicGroup": "foundations",
          "tesseraMessage": "Probability is the language of uncertainty. Bayes' theorem is your new best friend!"
        },
        {
          "id": "prob-2",
          "part": 2,
          "title": "Random Variables",
          "url": "random_variables.html",
          "icon": "X",
          "keywords": [
            "Discrete Random Variables",
            "Continuous Random Variables",
            "Probability Mass Function (p.m.f.)",
            "Probability Density Function (p.d.f.)",
            "Cumulative Distribution Function(c.d.f.)",
            "Expected Value",
            "Variance",
            "Standard Deviation"
          ],
          "badges": [],
          "prereqs": [
            "prob-1"
          ],
          "mapCoords": {
            "q": 1,
            "r": -1
          },
          "topicGroup": "foundations",
          "tesseraMessage": "Random variables turn randomness into mathematics we can compute with."
        },
        {
          "id": "prob-3",
          "part": 3,
          "title": "Gamma & Beta Distribution",
          "url": "gamma.html",
          "icon": "\u0393",
          "keywords": [
            "Gamma Distribution",
            "Gamma Function",
            "Exponential Distribution",
            "Beta Function",
            "Beta Distribution",
            "Uniform Distribution"
          ],
          "badges": [
            "interactive"
          ],
          "prereqs": [
            "prob-2"
          ],
          "mapCoords": {
            "q": 0,
            "r": -2
          },
          "topicGroup": "distributions",
          "tesseraMessage": "Gamma and Beta distributions model waiting times and proportions beautifully."
        },
        {
          "id": "prob-4",
          "part": 4,
          "title": "Normal (Gaussian) Distribution",
          "url": "gaussian.html",
          "icon": "\ud835\udca9",
          "keywords": [
            "Gaussian Function",
            "Error Function",
            "Gaussian Integral",
            "Normal(Gaussian) Distribution",
            "Standard Normal Distribution",
            "Independent and Identically Distributed(i.i.d.)",
            "Random Sample",
            "Sample Mean",
            "Sample Variance",
            "Central Limit Theorem"
          ],
          "badges": [],
          "prereqs": [
            "prob-2"
          ],
          "mapCoords": {
            "q": 1,
            "r": -2
          },
          "topicGroup": "distributions",
          "tesseraMessage": "The Gaussian is everywhere! Central Limit Theorem explains why."
        },
        {
          "id": "prob-5",
          "part": 5,
          "title": "Student's t-Distribution",
          "url": "student.html",
          "icon": "t",
          "keywords": [
            "Student's t-Distribution",
            "Degrees of Freedom",
            "Cauchy Distribution",
            "Half Cauchy Distribution",
            "Laplace Distribution",
            "Double Sided Exponential Distribution"
          ],
          "badges": [],
          "prereqs": [
            "prob-4"
          ],
          "mapCoords": {
            "q": 2,
            "r": -1
          },
          "topicGroup": "distributions",
          "tesseraMessage": "Student's t handles small samples with grace. Heavy tails, robust inference."
        },
        {
          "id": "prob-6",
          "part": 6,
          "title": "Covariance",
          "url": "covariance.html",
          "icon": "Cov",
          "keywords": [
            "Covariance",
            "Covariance Matrix",
            "Total Variance",
            "Principal Component",
            "Principal Component Analysis(PCA)"
          ],
          "badges": [
            "code"
          ],
          "prereqs": [
            "prob-4",
            "linalg-6"
          ],
          "mapCoords": {
            "q": 0,
            "r": -3
          },
          "topicGroup": "multivariate",
          "tesseraMessage": "Covariance reveals how variables move together. PCA lives here!"
        },
        {
          "id": "prob-7",
          "part": 7,
          "title": "Correlation",
          "url": "correlation.html",
          "icon": "r",
          "keywords": [
            "Cross-Covariance Matrix",
            "Auto-Covariance Matrix",
            "Correlation Coefficient",
            "Correlation Matrix"
          ],
          "badges": [],
          "prereqs": [
            "prob-6"
          ],
          "mapCoords": {
            "q": 1,
            "r": -3
          },
          "topicGroup": "multivariate",
          "tesseraMessage": "Correlation normalizes covariance \u2014 easier to interpret, same insight."
        },
        {
          "id": "prob-8",
          "part": 8,
          "title": "Multivariate Distributions",
          "url": "mvn.html",
          "icon": "\u03a3",
          "keywords": [
            "Multivariate Normal Distribution (MVN)",
            "Mahalanobis Distance",
            "Bivariate Normal Distribution",
            "Cholesky Decomposition",
            "Dirichlet Distribution",
            "Probability Simplex",
            "Wishart Distribution",
            "Inverse Wishart Distribution"
          ],
          "badges": [],
          "prereqs": [
            "prob-6",
            "linalg-9"
          ],
          "mapCoords": {
            "q": 2,
            "r": -2
          },
          "topicGroup": "multivariate",
          "tesseraMessage": "Multivariate Gaussians are the foundation of Gaussian processes and more."
        },
        {
          "id": "prob-9",
          "part": 9,
          "title": "Maximum Likelihood Estimation",
          "url": "mle.html",
          "icon": "\u2112",
          "keywords": [
            "Point Estimator",
            "Mean Square Error(MSE)",
            "Standard Error (SE)",
            "Likelihood Function",
            "Log-likelihood Function",
            "Maximum Likelihood Estimation(MLE)",
            "Binomial Distribution",
            "Sample Proportion"
          ],
          "badges": [],
          "prereqs": [
            "prob-4"
          ],
          "mapCoords": {
            "q": 0,
            "r": -4
          },
          "topicGroup": "inference",
          "tesseraMessage": "MLE finds the parameters that make your data most likely. Foundational!"
        },
        {
          "id": "prob-10",
          "part": 10,
          "title": "Statistical Inference & Hypothesis Testing",
          "url": "hypothesis_testing.html",
          "icon": "H\u2080 vs H\u2081",
          "keywords": [
            "Null Hypothesis",
            "Alternative Hypothesis",
            "Type I Error (False Negative)",
            "Type II Error (False Positive)",
            "Significance Level",
            "Test Statistic",
            "Null Hypothesis Significance Test(NHST)",
            "One Sample t-Tests",
            "Confidence intervals",
            "Critical Values",
            "z-scores",
            "Credible Intervals",
            "Bootstrap"
          ],
          "badges": [],
          "prereqs": [
            "prob-9"
          ],
          "mapCoords": {
            "q": 1,
            "r": -4
          },
          "topicGroup": "inference",
          "tesseraMessage": "Hypothesis testing helps us make decisions under uncertainty. Be careful with p-values!"
        },
        {
          "id": "prob-11",
          "part": 11,
          "title": "Linear Regression",
          "url": "linear_regression.html",
          "icon": "LS",
          "keywords": [
            "Linear Regression",
            "Least-Squares Estimation"
          ],
          "badges": [
            "interactive"
          ],
          "prereqs": [
            "prob-9",
            "linalg-8"
          ],
          "mapCoords": {
            "q": 2,
            "r": -3
          },
          "topicGroup": "inference",
          "tesseraMessage": "Linear regression connects statistics to ML. Where prediction begins!"
        },
        {
          "id": "prob-12",
          "part": 12,
          "title": "Entropy",
          "url": "entropy.html",
          "icon": "\u210d",
          "keywords": [
            "Information Content",
            "Entropy",
            "Joint Entropy",
            "Conditional Entropy",
            "Cross Entropy",
            "KL Divergence(Relative Entropy, Information Gain)",
            "Gibbs' Inequality",
            "Log Sum Inequality",
            "Jensen's Inequality",
            "Mutual Information (MI)"
          ],
          "badges": [],
          "prereqs": [
            "prob-4"
          ],
          "mapCoords": {
            "q": 3,
            "r": -2
          },
          "topicGroup": "information",
          "tesseraMessage": "Entropy measures uncertainty \u2014 the heart of information theory."
        },
        {
          "id": "prob-13",
          "part": 13,
          "title": "Convergence",
          "url": "convergence.html",
          "icon": "n\u2192\u221e",
          "keywords": [
            "The Law of Large Numbers",
            "Convergence in Probability",
            "Convergence in Distribution",
            "Asymptotic(limiting) Distribution",
            "Moment Generating Function(m.g.f.)",
            "Central Limit Theorem(CLT)"
          ],
          "badges": [],
          "prereqs": [
            "prob-9"
          ],
          "mapCoords": {
            "q": 2,
            "r": -4
          },
          "topicGroup": "inference",
          "tesseraMessage": "Convergence guarantees that our estimates improve. Math meets practice!"
        },
        {
          "id": "prob-14",
          "part": 14,
          "title": "Intro to Bayesian Statistics",
          "url": "bayesian.html",
          "icon": "p(\u03b8|\ud835\udc9f)",
          "keywords": [
            "Bayesian Inference",
            "Prior Distribution",
            "Posterior Distribution",
            "Marginal Likelihood",
            "Conjugate Prior",
            "Posterior Predictive Distribution",
            "Beta-Binomial Model",
            "Normal Distribution Model with known Variance \u03c3\u00b2",
            "Normal Distribution Model with known Mean \u03bc"
          ],
          "badges": [],
          "prereqs": [
            "prob-9",
            "prob-3"
          ],
          "mapCoords": {
            "q": 3,
            "r": -3
          },
          "topicGroup": "bayesian",
          "tesseraMessage": "Bayesian thinking updates beliefs with evidence. Prior \u2192 Posterior."
        },
        {
          "id": "prob-15",
          "part": 15,
          "title": "The Exponential Family",
          "url": "expfamily.html",
          "icon": "\u03b7",
          "keywords": [
            "Exponential Family",
            "Natural Parameters(Canonical Parameters)",
            "Base Measure",
            "Sufficient Statistics",
            "Partition Function",
            "Minimal Representation",
            "Natural Exponential Family(NEF)",
            "Moment Parameters",
            "Precision Matrix",
            "Information Form",
            "Moment Matching",
            "Cumulants"
          ],
          "badges": [],
          "prereqs": [
            "prob-14"
          ],
          "mapCoords": {
            "q": 4,
            "r": -2
          },
          "topicGroup": "bayesian",
          "tesseraMessage": "The exponential family unifies many distributions elegantly. Beautiful structure!"
        },
        {
          "id": "prob-16",
          "part": 16,
          "title": "Fisher Information Matrix",
          "url": "fisher_info.html",
          "icon": "F(\u03b8)",
          "keywords": [
            "Fisher Information Matrix(FIM)",
            "Score Function",
            "Covariance",
            "Negative Log Likelihood",
            "Log Partition Function",
            "Approximated KL Divergence",
            "Natural Gradient",
            "Jeffreys Prior",
            "Uninformative Prior",
            "Reference Prior",
            "Mutual Information"
          ],
          "badges": [],
          "prereqs": [
            "prob-15",
            "linalg-9"
          ],
          "mapCoords": {
            "q": 4,
            "r": -3
          },
          "topicGroup": "bayesian",
          "tesseraMessage": "Fisher information measures how much data tells us about parameters."
        },
        {
          "id": "prob-17",
          "part": 17,
          "title": "Bayesian Decision Theory",
          "url": "decision_theory.html",
          "icon": "\u03c0",
          "keywords": [
            "Decision Theory",
            "Optimal Policy(Bayes estimator)",
            "Zero-One Loss",
            "Maximum A Posteriori (MAP) Estimate",
            "Reject Option",
            "Confusion Matrix",
            "False Positive (FP, Type I error)",
            "False Negative (FN, Type II error)",
            "Receiver Operating Characteristic (ROC) Curve",
            "Equal Error Rate (EER)",
            "Precision-Recall (PR) Curve",
            "Interpolated Precision",
            "Average Precision (AP)"
          ],
          "badges": [],
          "prereqs": [
            "prob-14"
          ],
          "mapCoords": {
            "q": 3,
            "r": -4
          },
          "topicGroup": "bayesian",
          "tesseraMessage": "Decision theory tells us how to act optimally under uncertainty."
        },
        {
          "id": "prob-18",
          "part": 18,
          "title": "Markov Chains",
          "url": "markov.html",
          "icon": "\u220f",
          "keywords": [
            "Probabilistic Graphical Models(PGMs)",
            "Bayesian Networks",
            "Markov Chains",
            "Language Modeling",
            "n-gram",
            "Transition Function(Kernel)",
            "Stochastic Matrix(Transition Matrix)",
            "Maximum likelihood estimation(MLE) in Markov models",
            "Sparse Data Problem",
            "Add-One Smoothing",
            "Dirichlet Prior"
          ],
          "badges": [
            "code"
          ],
          "prereqs": [
            "prob-14",
            "linalg-13"
          ],
          "mapCoords": {
            "q": 4,
            "r": -4
          },
          "topicGroup": "bayesian",
          "tesseraMessage": "Markov chains model sequences \u2014 from text to weather to DNA."
        },
        {
          "id": "prob-19",
          "part": 19,
          "title": "Monte Carlo Methods",
          "url": "monte_carlo.html",
          "icon": "p*(\u03b8)",
          "keywords": [
            "Credible Intervals",
            "Central Credible Intervals",
            "Monte Carlo Approximation",
            "Highest Posterior Density (HPD)",
            "Markov Chain Monte Carlo (MCMC)"
          ],
          "badges": [
            "interactive"
          ],
          "prereqs": [
            "prob-14"
          ],
          "mapCoords": {
            "q": 5,
            "r": -4
          },
          "topicGroup": "bayesian",
          "tesseraMessage": "Monte Carlo turns random sampling into computational power!"
        },
        {
          "id": "prob-20",
          "part": 20,
          "title": "Importance Sampling",
          "url": "importance_sampling.html",
          "icon": "\u03c6",
          "keywords": [
            "Importance Sampling",
            "Importance Weights",
            "Direct Importance Sampling",
            "Effective Sample Size (ESS)",
            "Self-Normalized Importance Sampling (SNIS)",
            "Annealed Importance Sampling (AIS)",
            "Annealing Schedule"
          ],
          "badges": [],
          "prereqs": [
            "prob-19"
          ],
          "mapCoords": {
            "q": 5,
            "r": -3
          },
          "topicGroup": "bayesian",
          "tesseraMessage": "Importance sampling focuses computation where it matters most."
        },
        {
          "id": "prob-21",
          "part": 21,
          "title": "Gaussian Processes",
          "url": "gaussian_process.html",
          "icon": "\ud835\udca6",
          "keywords": [
            "Nonparametric Models",
            "Gaussian Process (GP)",
            "Mercer Kernel",
            "radial basis function (RBF) Kernel",
            "Stationary Kernel",
            "Automatic Relevance Determination (ARD) Kernel",
            "Mat\u00e9rn Kernel",
            "Periodic Kernel",
            "GP Regression"
          ],
          "badges": [],
          "prereqs": [
            "prob-8",
            "linalg-9"
          ],
          "mapCoords": {
            "q": 5,
            "r": -2
          },
          "topicGroup": "gaussian-process",
          "tesseraMessage": "GPs give uncertainty estimates for free. Bayesian nonparametrics at its finest!"
        }
      ],
      "reservedSlots": [
        {
          "q": 6,
          "r": -2
        },
        {
          "q": 6,
          "r": -4
        },
        {
          "q": 5,
          "r": -5
        }
      ]
    },
    "IV": {
      "id": "discrete",
      "title": "Discrete Mathematics & Algorithms",
      "shortTitle": "Discrete Math",
      "description": "Explore the foundations of discrete mathematics and algorithms, covering graph theory, combinatorics, and the theory of computation. Learn key concepts essential for mathematical reasoning and algorithmic problem-solving.",
      "tagline": "The Mathematics of Logic and Finite Structures",
      "icon": "fa-project-diagram",
      "indexUrl": "Mathematics/Discrete/discrete_math.html",
      "baseUrl": "Mathematics/Discrete/",
      "parts": [
        {
          "id": "disc-1",
          "part": 1,
          "title": "Intro to Graph Theory",
          "url": "intro_graph.html",
          "icon": "G=(V,E)",
          "keywords": [
            "Undirected graph",
            "Directed graph",
            "Multigraph",
            "Weighted graph",
            "Complete graph",
            "Adjacent",
            "Degree of a vertex",
            "k-regular",
            "Isomorphism"
          ],
          "badges": [],
          "prereqs": [],
          "mapCoords": {
            "q": 0,
            "r": 1
          },
          "topicGroup": "graph-theory",
          "tesseraMessage": "Graphs are everywhere \u2014 social networks, molecules, the internet itself!"
        },
        {
          "id": "disc-2",
          "part": 2,
          "title": "Intro to Combinatorics",
          "url": "intro_combinatorics.html",
          "icon": "\u2099C\u1d63",
          "keywords": [
            "Fundamental counting principle",
            "Combination",
            "Binomial coefficient",
            "Multinomial coefficient",
            "Pascal's relation",
            "Chu's theorem",
            "Sum of powers of positive integers",
            "Permutation"
          ],
          "badges": [],
          "prereqs": [],
          "mapCoords": {
            "q": 1,
            "r": 2
          },
          "topicGroup": "combinatorics",
          "tesseraMessage": "Counting cleverly is an art. Combinatorics powers probability and CS alike."
        },
        {
          "id": "disc-3",
          "part": 3,
          "title": "Intro to Theory of Computation",
          "url": "intro_automata.html",
          "icon": "M",
          "keywords": [
            "Finite automata",
            "Regular language",
            "State diagram",
            "Deterministic finite automata (DFAs)",
            "Nondeterministic finite automata (NFAs)",
            "Regular expression"
          ],
          "badges": [],
          "prereqs": [],
          "mapCoords": {
            "q": 2,
            "r": 1
          },
          "topicGroup": "computation",
          "tesseraMessage": "Automata recognize patterns. The simplest model of computation!"
        },
        {
          "id": "disc-4",
          "part": 4,
          "title": "Boolean Logic",
          "url": "boolean.html",
          "icon": "0\u22271",
          "keywords": [
            "Logical operations",
            "Boolean algebra",
            "Circuits"
          ],
          "badges": [],
          "prereqs": [],
          "mapCoords": {
            "q": 1,
            "r": 1
          },
          "topicGroup": "boolean",
          "tesseraMessage": "True or false, 0 or 1 \u2014 Boolean logic is how computers think."
        },
        {
          "id": "disc-5",
          "part": 5,
          "title": "Context-Free Languages",
          "url": "context_free.html",
          "icon": "S\u2192",
          "keywords": [
            "Context-free grammars",
            "Nonregular language",
            "Pumping lemma",
            "Pushdown automata",
            "Deterministic pushdown automata (DPDAs)",
            "Deterministic context-free languages (DCFLs)"
          ],
          "badges": [],
          "prereqs": [
            "disc-3"
          ],
          "mapCoords": {
            "q": 2,
            "r": 2
          },
          "topicGroup": "computation",
          "tesseraMessage": "Context-free grammars define programming languages. You use them every day!"
        },
        {
          "id": "disc-6",
          "part": 6,
          "title": "Turing Machines",
          "url": "turing_machine.html",
          "icon": "TM",
          "keywords": [
            "Turing machine",
            "Turing-recognizable",
            "Decidability",
            "Computability",
            "Church-Turing thesis",
            "Unsolvability",
            "Undecidable problems"
          ],
          "badges": [],
          "prereqs": [
            "disc-5"
          ],
          "mapCoords": {
            "q": 3,
            "r": 1
          },
          "topicGroup": "computation",
          "tesseraMessage": "Turing machines define what's computable. Some problems have no algorithm!"
        },
        {
          "id": "disc-7",
          "part": 7,
          "title": "Time Complexity",
          "url": "time_complexity.html",
          "icon": "O(g(n))",
          "keywords": [
            "Time complexity(Running time)",
            "Asymptotic analysis",
            "Big-O notation",
            "Little-o notation",
            "Time complexity class",
            "Class P"
          ],
          "badges": [],
          "prereqs": [
            "disc-6"
          ],
          "mapCoords": {
            "q": 3,
            "r": 2
          },
          "topicGroup": "computation",
          "tesseraMessage": "Big-O notation tells you how algorithms scale. Essential for interviews!"
        },
        {
          "id": "disc-8",
          "part": 8,
          "title": "Eulerian & Hamiltonian",
          "url": "Eulerian.html",
          "icon": "C\u2099",
          "keywords": [
            "Eulerian",
            "Eulerian cycle (Euler tour)",
            "Eulerian path (Eulerian trail)",
            "Hamiltonian",
            "Hamiltonian cycle",
            "Adjacency matrix",
            "Adjacency list",
            "Space complexity"
          ],
          "badges": [],
          "prereqs": [
            "disc-1"
          ],
          "mapCoords": {
            "q": 0,
            "r": 2
          },
          "topicGroup": "graph-theory",
          "tesseraMessage": "Can you traverse every edge? Every vertex? Different questions, different answers."
        },
        {
          "id": "disc-9",
          "part": 9,
          "title": "Class NP",
          "url": "p_vs_np.html",
          "icon": "P\u2260NP?",
          "keywords": [
            "Polynomial verifiability",
            "Nondeterministic polynomial time",
            "NP-completeness",
            "NP-hard",
            "Polynomial-time reduction",
            "P vs NP question"
          ],
          "badges": [],
          "prereqs": [
            "disc-7"
          ],
          "mapCoords": {
            "q": 4,
            "r": 1
          },
          "topicGroup": "computation",
          "tesseraMessage": "P vs NP \u2014 the million-dollar question! Is checking always easier than solving?"
        }
      ],
      "reservedSlots": [
        {
          "q": 4,
          "r": 2
        },
        {
          "q": 5,
          "r": 1
        },
        {
          "q": 0,
          "r": 3
        }
      ]
    },
    "V": {
      "id": "machine-learning",
      "title": "Machine Learning",
      "shortTitle": "ML",
      "description": "Explore machine learning ideas and applications.",
      "tagline": "The Mathematics of Synthesis and Intelligence",
      "icon": "fa-brain",
      "indexUrl": "Mathematics/Machine_learning/ml.html",
      "baseUrl": "Mathematics/Machine_learning/",
      "parts": [
        {
          "id": "ml-1",
          "part": 1,
          "title": "Intro to Machine Learning",
          "url": "intro_ml.html",
          "icon": "\ud83e\udde0?",
          "keywords": [
            "Machine learning",
            "Artificial intelligence (AI)",
            "Deep learning",
            "Supervised learning",
            "Unsupervised learning",
            "Learning process of ML",
            "Categories of machine learning"
          ],
          "badges": [],
          "prereqs": [],
          "mapCoords": {
            "q": 1,
            "r": 0
          },
          "topicGroup": "ml-foundations",
          "tesseraMessage": "Welcome to ML! This is where all the math comes together beautifully."
        },
        {
          "id": "ml-2",
          "part": 2,
          "title": "Regularized Regression",
          "url": "regression.html",
          "icon": "\u03bb\u2016w\u2016\u209a",
          "keywords": [
            "Ridge regression",
            "Bias-variance tradeoff",
            "Generalization",
            "Regularization",
            "Cross-validation (CV)",
            "K-fold cross-validation",
            "Leave-one-out cross-validation (LOOCV)",
            "Lasso regression"
          ],
          "badges": [
            "interactive"
          ],
          "prereqs": [
            "prob-11",
            "calc-7"
          ],
          "mapCoords": {
            "q": 2,
            "r": 0
          },
          "topicGroup": "ml-foundations",
          "tesseraMessage": "Regularization prevents overfitting \u2014 the bias-variance tradeoff in action!"
        },
        {
          "id": "ml-3",
          "part": 3,
          "title": "Intro to Classification",
          "url": "intro_classification.html",
          "icon": "\ud835\udcb3\u21a6\ud835\udcb4",
          "keywords": [
            "Binary logistic regression",
            "sigmoid (logistic) function",
            "logit (pre-activation)",
            "Decision boundary",
            "Feature mapping",
            "Linearly separable",
            "Kernel trick",
            "Random fourier features",
            "RBF (Gaussian) kernel",
            "Softmax function",
            "Multinomial logistic regression"
          ],
          "badges": [
            "interactive"
          ],
          "prereqs": [
            "ml-2"
          ],
          "mapCoords": {
            "q": 3,
            "r": 0
          },
          "topicGroup": "ml-foundations",
          "tesseraMessage": "Classification draws boundaries in feature space. Kernels bend those boundaries!"
        },
        {
          "id": "ml-4",
          "part": 4,
          "title": "Neural Networks Basics",
          "url": "neural_networks.html",
          "icon": "x\u21a6h\u03b8(x)",
          "keywords": [
            "Deep neural network (DNN)",
            "Multilayer perceptron (MLP)",
            "Hidden layer",
            "Activation function",
            "ReLU",
            "Vanishing gradients",
            "Backpropagation",
            "Gradient clipping",
            "Exploding gradients",
            "Graphics processing units (GPUs)"
          ],
          "badges": [
            "interactive"
          ],
          "prereqs": [
            "ml-3",
            "calc-2"
          ],
          "mapCoords": {
            "q": 3,
            "r": -1
          },
          "topicGroup": "neural-networks",
          "tesseraMessage": "Neural networks learn features automatically. Layers upon layers of abstraction!"
        },
        {
          "id": "ml-5",
          "part": 5,
          "title": "Automatic Differentiation",
          "url": "autodiff.html",
          "icon": "\u2207\u2112",
          "keywords": [
            "Automatic differentiation (AD)",
            "Computational graph"
          ],
          "badges": [
            "code"
          ],
          "prereqs": [
            "ml-4"
          ],
          "mapCoords": {
            "q": 4,
            "r": -1
          },
          "topicGroup": "neural-networks",
          "tesseraMessage": "Autodiff computes gradients automatically. The engine behind deep learning!"
        },
        {
          "id": "ml-6",
          "part": 6,
          "title": "Support Vector Machine (SVM)",
          "url": "svm.html",
          "icon": "w\u1d40x+w\u2080",
          "keywords": [
            "Support vector machine (SVM)",
            "Soft margin constraints"
          ],
          "badges": [
            "interactive"
          ],
          "prereqs": [
            "ml-3",
            "calc-9"
          ],
          "mapCoords": {
            "q": 4,
            "r": 0
          },
          "topicGroup": "ml-foundations",
          "tesseraMessage": "SVMs find the widest margin. Elegant geometry meets optimization!"
        },
        {
          "id": "ml-7",
          "part": 7,
          "title": "PCA & Autoencoders",
          "url": "pca.html",
          "icon": "\ud835\udca6(x\u1d62,x\u2c7c)",
          "keywords": [
            "Principal Component Analysis (PCA)",
            "Dimensionality reduction",
            "Kernel PCA",
            "Double centering trick",
            "Autoencoder",
            "Lipschitz continuity",
            "Data Reconstruction",
            "Denoising autoencoder",
            "Manifolds"
          ],
          "badges": [
            "interactive"
          ],
          "prereqs": [
            "linalg-9",
            "ml-4"
          ],
          "mapCoords": {
            "q": 5,
            "r": 0
          },
          "topicGroup": "dimensionality",
          "tesseraMessage": "PCA finds the directions of maximum variance. Autoencoders learn them!"
        },
        {
          "id": "ml-8",
          "part": 8,
          "title": "Clustering",
          "url": "clustering.html",
          "icon": "f\u1d40Lf",
          "keywords": [
            "K-means clustering",
            "Distortion",
            "One-hot encoding (Dummy encoding)",
            "K-means++",
            "Vector quantization",
            "Spectral clustering",
            "Graph Laplacian",
            "Dirichlet energy"
          ],
          "badges": [
            "interactive"
          ],
          "prereqs": [
            "ml-7",
            "linalg-14"
          ],
          "mapCoords": {
            "q": 6,
            "r": 0
          },
          "topicGroup": "dimensionality",
          "tesseraMessage": "Clustering finds structure without labels. K-means is just the beginning!"
        },
        {
          "id": "ml-9",
          "part": 9,
          "title": "Intro to Deep Neural Networks",
          "url": "deep_nn.html",
          "icon": "\ud83e\udde0!",
          "keywords": [
            "Feedforward networks",
            "Convolutional Neural Networks (CNNs)",
            "Residual connection",
            "Layer normalization",
            "Attention",
            "Self-attention",
            "Multi-Head Attention (MHA)",
            "Positional encoding",
            "Transformer"
          ],
          "badges": [
            "interactive"
          ],
          "prereqs": [
            "ml-5"
          ],
          "mapCoords": {
            "q": 5,
            "r": -1
          },
          "topicGroup": "neural-networks",
          "tesseraMessage": "Transformers changed everything \u2014 attention is all you need!"
        },
        {
          "id": "ml-10",
          "part": 10,
          "title": "Intro to Reinforcement Learning",
          "url": "intro_RL.html",
          "icon": "\ud83d\udd01",
          "keywords": [
            "Reinforcement Learning (RL)",
            "Model-based RL",
            "Model-free RL",
            "Agent",
            "Reward",
            "Policy",
            "Markov Decision Process (MDP)",
            "Discount factor",
            "Return",
            "Value function",
            "Q-function",
            "Advantage function",
            "Bellman's equations",
            "Value Iteration",
            "Policy Iteration",
            "Temporal Difference Learning",
            "Q-Learning",
            "SARSA",
            "Exploration vs Exploitation",
            "Policy Gradient",
            "REINFORCE",
            "Actor-Critic"
          ],
          "badges": [],
          "prereqs": [
            "ml-9",
            "prob-18"
          ],
          "mapCoords": {
            "q": 6,
            "r": -1
          },
          "topicGroup": "reinfortic-learning",
          "tesseraMessage": "RL learns from interaction. Games, robotics, and beyond!"
        }
      ],
      "reservedSlots": [
        {
          "q": 7,
          "r": -1
        },
        {
          "q": 7,
          "r": 0
        },
        {
          "q": 6,
          "r": 1
        }
      ]
    }
  },
  "homeNode": {
    "id": "home",
    "section": "HOME",
    "title": "MATH-CS COMPASS HOME",
    "url": "/index.html",
    "mapCoords": {
      "q": 0,
      "r": 0
    }
  }
}