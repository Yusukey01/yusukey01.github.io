{
  "meta": {
    "version": "1.0.0",
    "lastUpdated": "2026-01-15",
    "author": "Yusuke Yokota",
    "description": "Centralized curriculum data for MATH-CS COMPASS - single source of truth for section pages and compass map"
  },
  "sectionColors": {
    "I": "#1565c0",
    "II": "#2e7d32",
    "III": "#00838f",
    "IV": "#6a1b9a",
    "V": "#ef6c00"
  },
  "sections": {
    "I": {
      "id": "linear-algebra",
      "title": "Linear Algebra to Algebraic Foundations",
      "shortTitle": "Linear Algebra",
      "description": "Explore the foundations of Linear Algebra, covering key concepts such as linear equations, vector spaces, eigenvalues, orthogonality, least squares, and stochastic matrices.",
      "tagline": "The Mathematics of Structure and Space",
      "icon": "fa-vector-square",
      "indexUrl": "Mathematics/Linear_algebra/linear_algebra.html",
      "baseUrl": "Mathematics/Linear_algebra/",
      "parts": [
        {
          "id": "linalg-1",
          "part": 1,
          "title": "Linear Equations",
          "url": "linear_equations.html",
          "icon": "Ax=b",
          "keywords": [
            "System of linear equations",
            "Reduced row echelon form",
            "Linear combination",
            "Span",
            "Matrix equation",
            "Homogeneous & nonhomogeneous system",
            "Linear independence & dependence",
            "Parametric vector form"
          ],
          "badges": [],
          "prereqs": [],
          "mapCoords": {
            "q": -1,
            "r": 1
          },
          "topicGroup": "core-linalg",
          "tesseraMessage": "Where every journey begins! Ax = b is the foundation of so much in CS."
        },
        {
          "id": "linalg-2",
          "part": 2,
          "title": "Linear Transformation",
          "url": "linear_transformation.html",
          "icon": "T",
          "keywords": [
            "Linear transformation",
            "Linearity",
            "Onto",
            "One-to-one",
            "Matrix multiplication"
          ],
          "badges": [
            "interactive"
          ],
          "prereqs": [
            "linalg-1"
          ],
          "mapCoords": {
            "q": -1,
            "r": 2
          },
          "topicGroup": "core-linalg",
          "tesseraMessage": "Transformations are how we move and reshape data. Graphics, ML, everywhere!"
        },
        {
          "id": "linalg-3",
          "part": 3,
          "title": "Matrix Algebra",
          "url": "matrix_algebra.html",
          "icon": "A",
          "keywords": [
            "Diagonal matrix",
            "Identity matrix",
            "Transpose of a matrix",
            "Invertible matrix",
            "Singular matrix",
            "Elementary matrix",
            "Partitioned Matrix",
            "LU Factorization"
          ],
          "badges": [],
          "prereqs": [
            "linalg-1"
          ],
          "mapCoords": {
            "q": -2,
            "r": 1
          },
          "topicGroup": "core-linalg",
          "tesseraMessage": "Matrices are the language computers speak. Master this well!"
        },
        {
          "id": "linalg-4",
          "part": 4,
          "title": "Determinants",
          "url": "determinants.html",
          "icon": "|A|",
          "keywords": [
            "Determinant",
            "Cofactor expansion",
            "Cramer's rule",
            "Adjugate",
            "Inverse formula",
            "Invertible matrix"
          ],
          "badges": [],
          "prereqs": [
            "linalg-3"
          ],
          "mapCoords": {
            "q": -2,
            "r": 2
          },
          "topicGroup": "core-linalg",
          "tesseraMessage": "Determinants tell you if a matrix is invertible \u2014 one number, so much meaning."
        },
        {
          "id": "linalg-5",
          "part": 5,
          "title": "Vector Spaces",
          "url": "vectorspaces.html",
          "icon": "V",
          "keywords": [
            "Vector space",
            "Subspace",
            "Null space",
            "Kernel",
            "Column space",
            "Row space",
            "Basis",
            "Spanning set",
            "Coordinate systems",
            "Dimension",
            "Rank"
          ],
          "badges": [],
          "prereqs": [
            "linalg-2"
          ],
          "mapCoords": {
            "q": -2,
            "r": 3
          },
          "topicGroup": "core-linalg",
          "tesseraMessage": "Vector spaces are where linear algebra becomes truly abstract and powerful."
        },
        {
          "id": "linalg-6",
          "part": 6,
          "title": "Eigenvalues & Eigenvectors",
          "url": "eigenvectors.html",
          "icon": "\u03bb",
          "keywords": [
            "Eigenvalues",
            "Eigenvectors",
            "Eigenspace",
            "Characteristic equation",
            "Similarity",
            "Diagonalization",
            "Complex eigenvalues & eigenvectors"
          ],
          "badges": [],
          "prereqs": [
            "linalg-4",
            "linalg-5"
          ],
          "mapCoords": {
            "q": -3,
            "r": 2
          },
          "topicGroup": "core-linalg",
          "tesseraMessage": "Eigenvalues appear everywhere \u2014 from Google's PageRank to quantum mechanics!"
        },
        {
          "id": "linalg-7",
          "part": 7,
          "title": "Orthogonality",
          "url": "orthogonality.html",
          "icon": "\u22a5",
          "keywords": [
            "Inner product",
            "Euclidean norm",
            "Orthogonality",
            "Orthogonal complement",
            "Orthogonal & Orthonormal set",
            "Orthogonal projection",
            "Orthogonal matrix",
            "Gram-Schmidt algorithm",
            "QR factorization"
          ],
          "badges": [
            "interactive"
          ],
          "prereqs": [
            "linalg-5"
          ],
          "mapCoords": {
            "q": -3,
            "r": 3
          },
          "topicGroup": "core-linalg",
          "tesseraMessage": "Orthogonality keeps things clean and independent. QR is essential!"
        },
        {
          "id": "linalg-8",
          "part": 8,
          "title": "Least-Squares Problems",
          "url": "leastsquares.html",
          "icon": "min",
          "keywords": [
            "Least-squares solution",
            "Normal equation",
            "Least-squares error",
            "Linear regression",
            "Moore-Penrose pseudo-inverse"
          ],
          "badges": [
            "interactive"
          ],
          "prereqs": [
            "linalg-7"
          ],
          "mapCoords": {
            "q": -3,
            "r": 4
          },
          "topicGroup": "core-linalg",
          "tesseraMessage": "When exact solutions don't exist, least squares finds the best approximation."
        },
        {
          "id": "linalg-9",
          "part": 9,
          "title": "Symmetry",
          "url": "symmetry.html",
          "icon": "S",
          "keywords": [
            "Symmetric matrix",
            "Orthogonally diagonalizable matrix",
            "Spectrum",
            "Quadratic form",
            "Positive definite",
            "Positive semi-definite",
            "Singular value decomposition(SVD)",
            "Condition number",
            "Moore-Penrose pseudo-inverse"
          ],
          "badges": [
            "interactive"
          ],
          "prereqs": [
            "linalg-6",
            "linalg-7"
          ],
          "mapCoords": {
            "q": -4,
            "r": 3
          },
          "topicGroup": "core-linalg",
          "tesseraMessage": "SVD is one of the most useful decompositions in all of applied mathematics!"
        },
        {
          "id": "linalg-10",
          "part": 10,
          "title": "Trace and Norms",
          "url": "trace.html",
          "icon": "Tr",
          "keywords": [
            "Trace",
            "Frobenius norm",
            "Nuclear norm",
            "Induced norm",
            "Spectral norm",
            "p-norm",
            "Manhattan norm",
            "Maximum norm",
            "Normalization",
            "Regularization",
            "Metric space",
            "Normed vector space",
            "Inner product space",
            "Euclidean space"
          ],
          "badges": [],
          "prereqs": [
            "linalg-9"
          ],
          "mapCoords": {
            "q": -4,
            "r": 4
          },
          "topicGroup": "core-linalg",
          "tesseraMessage": "Norms measure size, and different norms give different perspectives."
        },
        {
          "id": "linalg-11",
          "part": 11,
          "title": "Kronecker Product & Tensor",
          "url": "kronecker.html",
          "icon": "\u2297",
          "keywords": [
            "Vectorization",
            "Kronecker product",
            "Tensor",
            "Tensor Product"
          ],
          "badges": [],
          "prereqs": [
            "linalg-10"
          ],
          "mapCoords": {
            "q": -5,
            "r": 4
          },
          "topicGroup": "core-linalg",
          "tesseraMessage": "Tensors power modern deep learning. This is where things get multidimensional!"
        },
        {
          "id": "linalg-12",
          "part": 12,
          "title": "Woodbury Matrix Identity",
          "url": "woodbury.html",
          "icon": "W",
          "keywords": [
            "Woodbury matrix identity",
            "Sherman-Morrison formula"
          ],
          "badges": [],
          "prereqs": [
            "linalg-3"
          ],
          "mapCoords": {
            "q": -3,
            "r": 1
          },
          "topicGroup": "core-linalg",
          "tesseraMessage": "A clever identity that makes updating inverses efficient. Very practical!"
        },
        {
          "id": "linalg-13",
          "part": 13,
          "title": "Stochastic Matrix",
          "url": "stochastic.html",
          "icon": "P",
          "keywords": [
            "Stochastic matrix",
            "Column-stochastic matrix",
            "Row-stochastic matrix",
            "Probability vector",
            "Markov chain",
            "Steady-state vector",
            "Spectral radius",
            "Doubly stochastic matrix"
          ],
          "badges": [],
          "prereqs": [
            "linalg-6"
          ],
          "mapCoords": {
            "q": -4,
            "r": 2
          },
          "topicGroup": "core-linalg",
          "tesseraMessage": "Markov chains are everywhere \u2014 from web search to speech recognition."
        },
        {
          "id": "linalg-14",
          "part": 14,
          "title": "Graph Laplacians and Spectral Methods",
          "url": "graph_laplacian.html",
          "icon": "L",
          "keywords": [
            "Graph Laplacian",
            "Dirichlet energy",
            "Normalized Laplacian",
            "Fiedler vector",
            "Algebraic connectivity",
            "Cheeger's inequality",
            "Graph Fourier Transform (GFT)",
            "Heat diffusion on graphs"
          ],
          "badges": [],
          "prereqs": [
            "linalg-6",
            "disc-1"
          ],
          "mapCoords": {
            "q": -5,
            "r": 3
          },
          "topicGroup": "core-linalg",
          "tesseraMessage": "Graph Laplacians connect linear algebra to network analysis. Beautiful!"
        },
        {
          "id": "linalg-15",
          "part": 15,
          "title": "Intro to Abstract Algebra",
          "url": "intro_groups.html",
          "icon": "G",
          "keywords": [
            "Abstract Algebra",
            "Groups",
            "Binary operation",
            "Closure",
            "Abelian",
            "Non-Abelian",
            "General linear group, GL(n, F)",
            "Modular arithmetic",
            "Units Modulo n, U(n)",
            "Additive vs Multiplicative groups",
            "Order of a group",
            "Order of an element",
            "Subgroups",
            "One-step subgroup test",
            "Cyclic",
            "Generator",
            "Center of a group",
            "Centralizer"
          ],
          "badges": [],
          "prereqs": [
            "linalg-3"
          ],
          "mapCoords": {
            "q": -4,
            "r": 5
          },
          "topicGroup": "abstract-algebra",
          "tesseraMessage": "Welcome to abstract algebra! Groups reveal the hidden symmetry in mathematics."
        },
        {
          "id": "linalg-16",
          "part": 16,
          "title": "More Finite Groups",
          "url": "cyclic_groups.html",
          "icon": "S\u2099",
          "keywords": [
            "Group of Integers Modulo n",
            "Fundamental Theorem of Cyclic Groups",
            "Euler Phi Function",
            "Permutation Groups",
            "Symmetric Groups",
            "Cycle Notation",
            "Products of Disjoint Cycles",
            "Product of 2-Cycles",
            "The Parity Theorem",
            "Alternating Groups"
          ],
          "badges": [],
          "prereqs": [
            "linalg-15"
          ],
          "mapCoords": {
            "q": -5,
            "r": 5
          },
          "topicGroup": "abstract-algebra",
          "tesseraMessage": "Permutations and cyclic structures \u2014 the building blocks of cryptography."
        },
        {
          "id": "linalg-17",
          "part": 17,
          "title": "Structural Group Theory",
          "url": "group_isomorphism.html",
          "icon": "\u2245",
          "keywords": [
            "Cosets",
            "Lagrange's Theorem",
            "Fermat's Little Theorem",
            "Pohlig-Hellman algorithm",
            "Normal Subgroups",
            "Factor Groups",
            "Group Homomorphisms",
            "Kernel",
            "Group Isomorphisms",
            "Group Automorphisms",
            "Inner Automorphisms",
            "Cayley's Theorem",
            "First Isomorphism Theorem"
          ],
          "badges": [],
          "prereqs": [
            "linalg-16"
          ],
          "mapCoords": {
            "q": -5,
            "r": 6
          },
          "topicGroup": "abstract-algebra",
          "tesseraMessage": "Isomorphisms show when different-looking structures are secretly the same."
        },
        {
          "id": "linalg-18",
          "part": 18,
          "title": "Classification of Finite Abelian Groups",
          "url": "group_classification.html",
          "icon": "\u2295",
          "keywords": [
            "External Direct Products",
            "Internal Direct Products",
            "Fundamental Theorem of Finite Abelian Groups"
          ],
          "badges": [],
          "prereqs": [
            "linalg-17"
          ],
          "mapCoords": {
            "q": -6,
            "r": 5
          },
          "topicGroup": "abstract-algebra",
          "tesseraMessage": "Every finite abelian group can be broken down \u2014 a beautiful classification!"
        },
        {
          "id": "linalg-19",
          "part": 19,
          "title": "The Architecture of Rings and Fields",
          "url": "intro_rings.html",
          "icon": "F",
          "keywords": [
            "Rings",
            "Unity",
            "Unit",
            "Subrings",
            "Subring Test",
            "Integral Domains",
            "Zero-Divisors",
            "Cancellation Law",
            "Fields",
            "Characteristic of a Ring"
          ],
          "badges": [],
          "prereqs": [
            "linalg-15"
          ],
          "mapCoords": {
            "q": -6,
            "r": 6
          },
          "topicGroup": "abstract-algebra",
          "tesseraMessage": "Rings and fields \u2014 the algebraic structures behind polynomials and numbers."
        },
        {
          "id": "linalg-20",
          "part": 20,
          "title": "Ideals and Factor Rings",
          "url": "ideals.html",
          "icon": "R/A",
          "keywords": [
            "Ideals",
            "Ideal Test",
            "Principal Ideals",
            "Factor Rings",
            "Prime Ideals",
            "Maximal Ideals",
            "Ring Homomorphisms",
            "Fundamental Theorem of Ring Homomorphisms"
          ],
          "badges": [],
          "prereqs": [
            "linalg-19"
          ],
          "mapCoords": {
            "q": -6,
            "r": 7
          },
          "topicGroup": "abstract-algebra",
          "tesseraMessage": "Ideals are to rings what normal subgroups are to groups. Deep connections!"
        },
        {
          "id": "linalg-21",
          "part": 21,
          "title": "Polynomial Rings",
          "url": "polynomial_rings.html",
          "icon": "R[x]",
          "keywords": [
            "Polynomial Rings",
            "Division Algorithm",
            "Irreducible Polynomials",
            "Eisenstein's Criterion",
            "Principal Ideal Domain",
            "Gauss's Lemma",
            "Unique Factorization",
            "AES Cryptography",
            "Post-Quantum Cryptography"
          ],
          "badges": [],
          "prereqs": [
            "linalg-20",
            "disc-4"
          ],
          "mapCoords": {
            "q": -7,
            "r": 6
          },
          "topicGroup": "abstract-algebra",
          "tesseraMessage": "Polynomial rings power modern cryptography \u2014 from AES to post-quantum!"
        }
      ],
      "reservedSlots": [
        {
          "q": -7,
          "r": 7
        },
        {
          "q": -8,
          "r": 6
        },
        {
          "q": -5,
          "r": 7
        }
      ]
    },
    "II": {
      "id": "calculus",
      "title": "Calculus to Optimization & Analysis",
      "shortTitle": "Calculus",
      "description": "Explore key calculus concepts essential for optimization, analysis, and machine learning. Topics include derivatives, Jacobians, gradient descent, Newton's method, constrained optimization, measure theory, and Lebesgue integration.",
      "tagline": "The Mathematics of Change and Convergence",
      "icon": "fa-chart-line",
      "indexUrl": "Mathematics/Calculus/calculus.html",
      "baseUrl": "Mathematics/Calculus/",
      "parts": [
        {
        "id": "calc-1",
        "part": 1,
        "title": "The Derivative of f:ℝⁿ→ℝ",
        "url": "linear_approximation.html",
        "icon": "∇",
        "keywords": [
          "Linear approximation",
          "Linearization",
          "Differentials",
          "Product rule",
          "Gradient",
          "Quadratic form",
          "L₂ norm",
          "Euclidean norm",
          "Tangent line",
          "Linear operator"
        ],
        "badges": [],
        "prereqs": ["linalg-1"],
        "mapCoords": {"q": -1, "r": 0},
        "topicGroup": "derivatives",
        "tesseraMessage": "The gradient points uphill. In ML, we usually go the opposite way!",

        "headline": "Linear Approximations: From Scalar Calculus to Gradients",
        "description": "Learn about differentials and linearization, intro to matrix calculus.",
        "abstract": "Linear approximation extends single-variable calculus to vectors via the gradient. The differential notation df = f'(x)dx generalizes naturally to vector spaces, where f' becomes a linear operator mapping input changes to output changes—the foundation for all optimization in ML.",
        "datePublished": "2024-01-01",
        "dateModified": "2025-01-15",

        "teaches": [
          "Linear approximation of functions near a point",
          "Linearization and tangent line interpretation",
          "Differential notation: df = f'(x)dx",
          "Extending differentials to vector spaces",
          "Computing gradients of scalar-valued functions",
          "Product rule in differential notation",
          "Derivative of quadratic forms xᵀAx",
          "Derivative of L₂ norm"
        ],

        "competencyRequired": [
          "Basic calculus (limits, derivatives)",
          "Vectors and dot products",
          "Matrix-vector multiplication"
        ],

        "sections": [
          {"id": "lapp", "name": "Linear Approximations"},
          {"id": "diff", "name": "Differentials"},
          {"id": "ex1", "name": "Example 1: f(x) = xᵀx"},
          {"id": "ex2", "name": "Example 2: f(x) = xᵀAx"},
          {"id": "ex3", "name": "Example 3: f(x) = ||x||₂"}
        ],

        "definitions": [
          {
            "term": "Linear Approximation",
            "definition": "L(x) = f(x₀) + f'(x₀)(x - x₀) ≈ f(x) near x₀"
          },
          {
            "term": "Differential",
            "definition": "df = f'(x)dx where f'(x) is a linear operator mapping dx to df"
          },
          {
            "term": "Gradient",
            "definition": "∇f = column vector of partial derivatives; for f:ℝⁿ→ℝ, ∇f ∈ ℝⁿ"
          },
          {
            "term": "Quadratic Form",
            "definition": "f(x) = xᵀAx; gradient is (A + Aᵀ)x, or 2Ax when A is symmetric"
          }
        ],

        "about": [
          {"name": "Linear Approximation", "description": "Local linear model of a function"},
          {"name": "Gradient", "description": "Direction of steepest ascent"},
          {"name": "Differential Notation", "description": "Foundation for matrix calculus"},
          {"name": "Quadratic Forms", "description": "Key structure in optimization"}
        ]
        },
        { 
          "id": "calc-2",
          "part": 2,
          "title": "The Derivative of f:ℝⁿ→ℝⁿ",
          "url": "jacobian.html",
          "icon": "J",
          "keywords": [
            "Jacobian matrix",
            "Chain rule",
            "Backpropagation",
            "Reverse mode automatic differentiation",
            "Forward mode automatic differentiation",
            "Vector-Jacobian product",
            "Neural networks",
            "Loss function"
          ],
          "badges": [],
          "prereqs": ["calc-1"],
          "mapCoords": {"q": -1, "r": -1},
          "topicGroup": "derivatives",
          "tesseraMessage": "The Jacobian is the backbone of backpropagation. Chain rule magic!",

          "headline": "Jacobian Matrix: The Foundation of Backpropagation",
          "description": "Learn about Jacobian, chain rule, backpropagation.",
          "abstract": "The Jacobian matrix generalizes the derivative to vector-valued functions, with each entry Jᵢⱼ = ∂fᵢ/∂xⱼ. The chain rule becomes matrix multiplication, and backpropagation exploits this by computing vector-Jacobian products from output to input—the key insight behind training neural networks.",
          "datePublished": "2024-01-01",
          "dateModified": "2025-01-15",

          "teaches": [
            "Jacobian matrix for vector-valued functions",
            "Jacobian as linear operator mapping dx to df",
            "Chain rule as Jacobian matrix multiplication",
            "Non-commutativity of chain rule in high dimensions",
            "Backpropagation as reverse-mode autodiff",
            "Vector-Jacobian products for efficiency",
            "Forward vs reverse mode differentiation",
            "Computational efficiency for neural networks"
          ],

          "competencyRequired": [
            "Gradients of scalar functions",
            "Matrix multiplication",
            "Partial derivatives"
          ],

          "sections": [
            {"id": "jacobian", "name": "Jacobian"},
            {"id": "chain", "name": "Chain Rule"},
            {"id": "backp", "name": "Backpropagation"}
          ],

          "definitions": [
            {
              "term": "Jacobian Matrix",
              "definition": "m×n matrix J where Jᵢⱼ = ∂fᵢ/∂xⱼ for f:ℝⁿ→ℝᵐ"
            },
            {
              "term": "Chain Rule",
              "definition": "df = g'(h(x))·h'(x)·dx; Jacobians multiply in order"
            },
            {
              "term": "Backpropagation",
              "definition": "Reverse-mode autodiff computing gradients via left-to-right Jacobian products"
            },
            {
              "term": "Vector-Jacobian Product",
              "definition": "vᵀJ computed efficiently without forming full Jacobian; key to backprop"
            }
          ],

          "about": [
            {"name": "Jacobian Matrix", "description": "Derivative of vector-valued functions"},
            {"name": "Chain Rule", "description": "Composition rule via matrix multiplication"},
            {"name": "Backpropagation", "description": "Efficient gradient computation for neural networks"},
            {"name": "Automatic Differentiation", "description": "Forward vs reverse mode tradeoffs"}
          ]
        },
        {    
          "id": "calc-3",
          "part": 3,
          "title": "The Derivative of f:ℝⁿˣⁿ→ℝⁿˣⁿ",
          "url": "matrix_cal.html",
          "icon": "M",
          "keywords": [
            "Matrix calculus",
            "Powers of a matrix",
            "Inverse of a matrix",
            "LU decomposition",
            "Matrix-valued functions",
            "Product rule for matrices",
            "Non-commutative operations"
          ],
          "badges": [],
          "prereqs": ["calc-2", "linalg-3"],
          "mapCoords": {"q": -2, "r": -1},
          "topicGroup": "derivatives",
          "tesseraMessage": "Matrix calculus — where derivatives get truly multidimensional.",

          "headline": "Matrix Calculus: Derivatives of Matrix-Valued Functions",
          "description": "Learn about derivative of matrix functions.",
          "abstract": "Matrix calculus extends differentiation to matrix-valued functions of matrices. Non-commutativity requires careful ordering: d(X³) = (dX)X² + X(dX)X + X²(dX). Key results include the derivative of matrix inverse and LU decomposition—essential for advanced optimization and numerical methods.",
          "datePublished": "2024-01-01",
          "dateModified": "2025-01-15",

          "teaches": [
            "Derivative of matrix-valued functions",
            "Product rule for matrix functions",
            "Non-commutativity in matrix derivatives",
            "Derivative of matrix powers: d(X³)",
            "Derivative of matrix inverse: d(X⁻¹) = -X⁻¹(dX)X⁻¹",
            "Derivative of LU decomposition",
            "Triangular matrix properties in derivatives",
            "Symbolic vs component-wise differentiation"
          ],

          "competencyRequired": [
            "Jacobian matrices",
            "Matrix multiplication and inverses",
            "LU decomposition basics"
          ],

          "sections": [
            {"id": "square", "name": "Derivative of the square matrix functions"},
            {"id": "lu", "name": "Derivative of the LU decomposition matrix"}
          ],

          "definitions": [
            {
              "term": "Matrix Derivative",
              "definition": "df = (∂f/∂X)dX where both input and output are matrices"
            },
            {
              "term": "Derivative of X⁻¹",
              "definition": "d(X⁻¹) = -X⁻¹(dX)X⁻¹ derived from d(X⁻¹X) = 0"
            },
            {
              "term": "LU Derivative",
              "definition": "d(X) = d(LU) = (dL)U + L(dU) preserving triangular structure"
            }
          ],

          "about": [
            {"name": "Matrix Calculus", "description": "Differentiation of matrix-valued functions"},
            {"name": "Matrix Powers", "description": "Non-commutative product rule"},
            {"name": "Matrix Inverse", "description": "Key derivative for optimization"},
            {"name": "LU Decomposition", "description": "Derivatives preserving triangular structure"}
          ]
        },
        { 
          "id": "calc-4",
          "part": 4,
          "title": "Intro to Numerical Computation",
          "url": "numerical_example1.html",
          "icon": "≈",
          "keywords": [
            "Finite-difference approximation",
            "Forward difference",
            "Backward difference",
            "Relative error",
            "Roundoff error",
            "Machine epsilon",
            "Numerical stability",
            "Floating-point arithmetic"
          ],
          "badges": ["code"],
          "prereqs": ["calc-1"],
          "mapCoords": {"q": -2, "r": 0},
          "topicGroup": "numerical",
          "tesseraMessage": "Theory meets practice! Understanding numerical errors is crucial for real code.",

          "headline": "Numerical Computation: Validating Derivatives with Finite Differences",
          "description": "Introduction to numerical computation with coding.",
          "abstract": "Finite differences approximate derivatives numerically, enabling validation of analytical formulas. Choosing the right step size balances truncation error (too large) against roundoff error (too small). Relative error provides meaningful accuracy measures when comparing analytical and numerical results.",
          "datePublished": "2024-01-01",
          "dateModified": "2025-01-15",

          "teaches": [
            "Finite difference approximations",
            "Forward and backward difference methods",
            "Relative error as accuracy metric",
            "Floating-point arithmetic limitations",
            "Machine epsilon and precision limits",
            "Validating analytical derivatives numerically",
            "Choosing appropriate step sizes",
            "Random testing for robust validation"
          ],

          "competencyRequired": [
            "Differential calculus",
            "Basic Python/NumPy",
            "Matrix operations"
          ],

          "sections": [
            {"id": "ex1", "name": "Numerical Computation Example"}
          ],

          "definitions": [
            {
              "term": "Forward Difference",
              "definition": "f(x+dx) - f(x) approximates df"
            },
            {
              "term": "Backward Difference",
              "definition": "f(x) - f(x-dx) approximates df"
            },
            {
              "term": "Relative Error",
              "definition": "||True - Approximation|| / ||True||; meaningful accuracy measure"
            },
            {
              "term": "Machine Epsilon",
              "definition": "Smallest ε where 1 + ε ≠ 1 in floating-point; ~10⁻¹⁶ for double precision"
            }
          ],

          "about": [
            {"name": "Finite Differences", "description": "Numerical approximation of derivatives"},
            {"name": "Relative Error", "description": "Scale-independent accuracy metric"},
            {"name": "Numerical Stability", "description": "Balancing truncation and roundoff errors"},
            {"name": "Computational Validation", "description": "Testing analytical formulas with code"}
          ]
        },
        { 
          "id": "calc-5",
          "part": 5,
          "title": "The Derivative of Scalar Functions of Matrices",
          "url": "det.html",
          "icon": "det",
          "keywords": [
            "Frobenius inner product",
            "Frobenius norm",
            "Trace",
            "Determinant",
            "Cofactor",
            "Adjugate",
            "Characteristic polynomial",
            "Automatic differentiation"
          ],
          "badges": [],
          "prereqs": ["calc-3", "linalg-4"],
          "mapCoords": {"q": -2, "r": -2},
          "topicGroup": "derivatives",
          "tesseraMessage": "Derivatives of determinants and traces — essential for advanced ML proofs.",

          "headline": "Scalar Functions of Matrices: Frobenius Norm and Determinant Derivatives",
          "description": "Learn about derivatives of Frobenius norm and determinants.",
          "abstract": "Scalar functions of matrices arise throughout ML: the Frobenius norm measures matrix magnitude, while determinants appear in Gaussian distributions and volume calculations. The trace trick expresses derivatives elegantly: df = tr((∇f)ᵀdX). Automatic differentiation often outperforms analytical formulas numerically.",
          "datePublished": "2024-01-01",
          "dateModified": "2025-01-15",

          "teaches": [
            "Derivative of Frobenius norm: ∇||X||_F = X/||X||_F",
            "Differential notation with trace: df = tr((∇f)ᵀdX)",
            "Frobenius inner product representation",
            "Derivative of determinant via cofactors",
            "Relationship: ∇(det A) = cofactor(A) = (det A)(A⁻¹)ᵀ",
            "Derivative of characteristic polynomial",
            "Analytical vs automatic differentiation comparison",
            "Numerical stability considerations"
          ],

          "competencyRequired": [
            "Matrix calculus fundamentals",
            "Determinants and cofactors",
            "Trace properties"
          ],

          "sections": [
            {"id": "frob", "name": "Derivative of the Frobenius norm"},
            {"id": "det", "name": "Derivative of the Determinant"}
          ],

          "definitions": [
            {
              "term": "Frobenius Norm",
              "definition": "||X||_F = √(tr(XᵀX)); gradient is X/||X||_F"
            },
            {
              "term": "Trace Trick",
              "definition": "df = tr((∇f)ᵀdX) expresses scalar derivatives of matrices"
            },
            {
              "term": "Determinant Derivative",
              "definition": "d(det A) = (det A)·tr(A⁻¹dA) = tr(adj(A)dA)"
            },
            {
              "term": "Cofactor/Adjugate Relation",
              "definition": "∇(det A) = cofactor(A) = adj(A)ᵀ = (det A)(A⁻¹)ᵀ"
            }
          ],

          "about": [
            {"name": "Frobenius Norm", "description": "Matrix magnitude via trace"},
            {"name": "Trace Trick", "description": "Elegant derivative representation"},
            {"name": "Determinant Derivative", "description": "Via cofactors or adjugate"},
            {"name": "Automatic Differentiation", "description": "Practical alternative to analytical formulas"}
          ]
        },
        { 
          "id": "calc-6",
          "part": 6,
          "title": "The Mean Value Theorem",
          "url": "mvt.html",
          "icon": "μ",
          "keywords": [
            "Rolle's Theorem",
            "Lagrange's Mean Value Theorem",
            "Cauchy's Mean Value Theorem",
            "Taylor's Theorem",
            "Taylor polynomial",
            "Taylor series",
            "Lagrange remainder",
            "little-o notation",
            "Higher-dimensional MVT",
            "Linearization"
          ],
          "badges": [],
          "prereqs": ["calc-1"],
          "mapCoords": {"q": -3, "r": 0},
          "topicGroup": "optimization",
          "tesseraMessage": "Taylor's theorem lets us approximate any smooth function locally. Powerful!",

          "headline": "Mean Value Theorem: The Bridge Between Derivatives and Differences",
          "description": "Learn about Lagrange's & Cauchy's mean value theorem, and Higher-dimensional MVT.",
          "abstract": "The Mean Value Theorem connects point derivatives to average rates of change. Taylor's theorem extends this to polynomial approximations of any order. The higher-dimensional MVT provides the foundation for gradient descent in multivariable optimization.",
          "datePublished": "2024-01-01",
          "dateModified": "2025-01-15",

          "teaches": [
            "Lagrange's Mean Value Theorem and its proof",
            "Rolle's Theorem as a special case",
            "Taylor's Theorem and polynomial approximations",
            "Lagrange's form of the remainder",
            "Little-o notation in asymptotic analysis",
            "Taylor series and radius of convergence",
            "Cauchy's Mean Value Theorem",
            "Higher-dimensional MVT for gradient descent"
          ],

          "competencyRequired": [
            "Differentiation of single-variable functions",
            "Continuity and limits",
            "Basic proof techniques"
          ],

          "sections": [
            {"id": "l_mvt", "name": "Lagrange's Mean Value Theorem"},
            {"id": "taylor", "name": "Taylor's Theorem"},
            {"id": "c_mvt", "name": "Cauchy's Mean Value Theorem"},
            {"id": "h_dim", "name": "Higher-dimensional MVT"}
          ],

          "definitions": [
            {
              "term": "Lagrange's MVT",
              "definition": "If f is continuous on [a,b] and differentiable on (a,b), then ∃c ∈ (a,b) such that f'(c) = (f(b)-f(a))/(b-a)"
            },
            {
              "term": "Rolle's Theorem",
              "definition": "Special case of MVT: if f(a) = f(b), then ∃c ∈ (a,b) such that f'(c) = 0"
            },
            {
              "term": "Taylor Polynomial",
              "definition": "Tₙ(x) = Σₖ₌₀ⁿ f⁽ᵏ⁾(a)/k! · (x-a)ᵏ, the n-th order approximation of f near a"
            },
            {
              "term": "Lagrange Remainder",
              "definition": "Rₙ(x) = f⁽ⁿ⁾(c)/n! · (x-a)ⁿ where c ∈ (a,x)"
            },
            {
              "term": "Higher-dimensional MVT",
              "definition": "f(b) - f(a) = ∇f(c) · (b-a) for some c on the line segment [a,b]"
            }
          ],

          "about": [
            {"name": "Mean Value Theorem", "description": "Connects derivatives to differences over intervals"},
            {"name": "Taylor's Theorem", "description": "Polynomial approximation with quantified error"},
            {"name": "Linearization", "description": "First-order Taylor approximation"},
            {"name": "Higher-dimensional MVT", "description": "Foundation for gradient-based optimization"}
          ]
        },
        {   
        "id": "calc-7",
        "part": 7,
        "title": "Gradient Descent (First-order Method)",
        "url": "gradient.html",
        "icon": "↓",
        "keywords": [
          "Optimization problems",
          "Convexity",
          "Convex functions",
          "Gradient Descent (GD)",
          "Steepest Descent",
          "Stochastic Gradient Descent (SGD)",
          "Mini-batch SGD",
          "Sub-gradient",
          "Sub-differentiable",
          "Learning rate",
          "Local minimum",
          "Global minimum"
        ],
        "badges": ["code"],
        "prereqs": ["calc-6"],
        "mapCoords": {"q": -3, "r": -1},
        "topicGroup": "optimization",
        "tesseraMessage": "Gradient descent is the workhorse of ML — simple yet powerful optimization.",

        "headline": "Gradient Descent: The Workhorse of Machine Learning Optimization",
        "description": "Learn about convexity, gradient descent, and stochastic gradient descent.",
        "abstract": "Gradient descent iteratively moves toward minima by following the negative gradient. This first-order method, with variants like SGD and mini-batch SGD, trains virtually every neural network. Understanding convexity reveals when gradient descent finds global optima.",
        "datePublished": "2024-01-01",
        "dateModified": "2025-01-15",

        "teaches": [
          "Optimization problem formulation in machine learning",
          "Convex functions and their properties",
          "Local vs global minima and the role of convexity",
          "Gradient descent algorithm and steepest descent",
          "Learning rate selection and convergence",
          "Stochastic gradient descent for large datasets",
          "Mini-batch SGD and epoch-based training",
          "Subgradient descent for non-smooth functions"
        ],

        "competencyRequired": [
          "Partial derivatives and gradients",
          "Mean Value Theorem",
          "Matrix-vector operations"
        ],

        "sections": [
          {"id": "intro", "name": "Introduction to Optimization"},
          {"id": "convexity", "name": "Convexity"},
          {"id": "gradient", "name": "Gradient Descent"},
          {"id": "sgd", "name": "Stochastic Gradient Descent"},
          {"id": "subgradient", "name": "Sub-gradient Descent"}
        ],

        "definitions": [
          {
            "term": "Convex Function",
            "definition": "f(λx + (1-λ)y) ≤ λf(x) + (1-λ)f(y) for all x,y and λ ∈ [0,1]"
          },
          {
            "term": "Gradient Descent",
            "definition": "θ⁽ᵏ⁺¹⁾ = θ⁽ᵏ⁾ - η∇f(θ⁽ᵏ⁾), iteratively moving opposite to gradient"
          },
          {
            "term": "Stochastic Gradient Descent",
            "definition": "Update using gradient of single random sample instead of full batch"
          },
          {
            "term": "Mini-batch SGD",
            "definition": "Gradient computed on small subset B of data, balancing noise and efficiency"
          },
          {
            "term": "Subgradient",
            "definition": "g such that f(z) ≥ f(x) + gᵀ(z-x) for all z; generalizes gradient to non-smooth functions"
          }
        ],

        "about": [
          {"name": "Convexity", "description": "Property guaranteeing global optimum from any local optimum"},
          {"name": "Gradient Descent", "description": "First-order iterative optimization algorithm"},
          {"name": "SGD", "description": "Scalable variant using random sampling"},
          {"name": "Subgradient", "description": "Extension to non-differentiable convex functions"}
        ]
        },
        {   
        "id": "calc-8",
        "part": 8,
        "title": "Newton's method (Second-order Method)",
        "url": "newton.html",
        "icon": "N",
        "keywords": [
          "Line search",
          "Armijo condition",
          "Curvature condition",
          "Wolfe conditions",
          "Newton's method",
          "Quasi-Newton methods",
          "BFGS",
          "L-BFGS",
          "Secant condition",
          "Inverse Hessian approximation",
          "Rosenbrock function",
          "Second-order optimization"
        ],
        "badges": ["code"],
        "prereqs": ["calc-7", "linalg-9"],
        "mapCoords": {"q": -4, "r": -1},
        "topicGroup": "optimization",
        "tesseraMessage": "Newton's method: faster convergence using curvature. BFGS makes it practical!",

        "headline": "Newton's Method: Second-Order Optimization with Curvature",
        "description": "Learn about line search, Newton's method, and BFGS method.",
        "abstract": "Newton's method uses the Hessian matrix to achieve quadratic convergence near optima. BFGS and L-BFGS approximate the inverse Hessian efficiently, avoiding expensive matrix inversions while retaining fast convergence. Line search with Wolfe conditions ensures stable step sizes.",
        "datePublished": "2024-01-01",
        "dateModified": "2025-01-15",

        "teaches": [
          "Line search methods and step size selection",
          "Armijo condition for sufficient decrease",
          "Wolfe conditions: Armijo + curvature",
          "Newton's method using Hessian information",
          "Quasi-Newton methods avoiding explicit Hessian",
          "BFGS inverse Hessian approximation",
          "Secant condition and rank-2 updates",
          "L-BFGS for large-scale optimization",
          "Rosenbrock function as optimization benchmark"
        ],

        "competencyRequired": [
          "Gradient descent fundamentals",
          "Matrix inverses and positive definiteness",
          "Quadratic forms"
        ],

        "sections": [
          {"id": "LS", "name": "Line Search"},
          {"id": "NM", "name": "Newton's Method"},
          {"id": "BFGS", "name": "BFGS Method"}
        ],

        "definitions": [
          {
            "term": "Newton's Method",
            "definition": "θ⁽ᵏ⁺¹⁾ = θ⁽ᵏ⁾ - H⁻¹∇f(θ⁽ᵏ⁾), using Hessian for quadratic convergence"
          },
          {
            "term": "Armijo Condition",
            "definition": "f(θ + ηp) ≤ f(θ) + c₁η∇f(θ)ᵀp, ensuring sufficient decrease"
          },
          {
            "term": "Wolfe Conditions",
            "definition": "Armijo condition + curvature condition for proper step sizes"
          },
          {
            "term": "BFGS",
            "definition": "Quasi-Newton method approximating inverse Hessian via rank-2 updates"
          },
          {
            "term": "L-BFGS",
            "definition": "Limited-memory BFGS storing only m recent (s,y) pairs for large-scale problems"
          },
          {
            "term": "Secant Condition",
            "definition": "Bₖ₊₁sₖ = yₖ where sₖ = θₖ₊₁ - θₖ and yₖ = ∇f(θₖ₊₁) - ∇f(θₖ)"
          }
        ],

        "about": [
          {"name": "Newton's Method", "description": "Second-order optimization with Hessian"},
          {"name": "Line Search", "description": "Choosing step sizes satisfying Wolfe conditions"},
          {"name": "BFGS", "description": "Practical quasi-Newton without explicit Hessian"},
          {"name": "L-BFGS", "description": "Memory-efficient BFGS for high-dimensional problems"}
        ]
        },
        {
        
        "id": "calc-9",
        "part": 9,
        "title": "Constrained Optimization",
        "url": "constrained_opt.html",
        "icon": "λ",
        "keywords": [
          "Constrained optimization problems",
          "Penalty terms",
          "Lagrange Multipliers",
          "Lagrangian",
          "Karush-Kuhn-Tucker (KKT) conditions",
          "Active set",
          "Slack variables",
          "Complementary slackness",
          "Primal feasibility",
          "Dual feasibility"
        ],
        "badges": ["code"],
        "prereqs": ["calc-7"],
        "mapCoords": {"q": -4, "r": 0},
        "topicGroup": "optimization",
        "tesseraMessage": "KKT conditions are the foundation of constrained optimization. Essential theory!",

        "headline": "Constrained Optimization: KKT Conditions and Lagrange Multipliers",
        "description": "Learn about constrained optimization ideas, KKT Conditions, and Lagrange multipliers.",
        "abstract": "Constrained optimization handles equality and inequality constraints via Lagrange multipliers and KKT conditions. These necessary conditions for optimality underpin support vector machines, portfolio optimization, and many ML algorithms requiring bounded or structured solutions.",
        "datePublished": "2024-01-01",
        "dateModified": "2025-01-15",

        "teaches": [
          "Formulation of constrained optimization problems",
          "Equality constraints and Lagrange multipliers",
          "Inequality constraints and KKT conditions",
          "Stationarity, primal feasibility, dual feasibility",
          "Complementary slackness conditions",
          "Active and inactive constraints",
          "Penalty method implementation",
          "Slack variables for numerical solvers"
        ],

        "competencyRequired": [
          "Gradient descent and unconstrained optimization",
          "Partial derivatives",
          "Systems of nonlinear equations"
        ],

        "sections": [
          {"id": "intro", "name": "Constrained Optimization Problem"},
          {"id": "lm", "name": "Lagrange Multipliers"},
          {"id": "kkt", "name": "The KKT Conditions"}
        ],

        "definitions": [
          {
            "term": "Lagrangian",
            "definition": "L(x,λ,μ) = f(x) + Σλᵢhᵢ(x) + Σμⱼgⱼ(x), combining objective with constraints"
          },
          {
            "term": "KKT Conditions",
            "definition": "Necessary conditions: stationarity, primal feasibility, dual feasibility, complementary slackness"
          },
          {
            "term": "Complementary Slackness",
            "definition": "μᵢgᵢ(x) = 0 for all i; either constraint is active or multiplier is zero"
          },
          {
            "term": "Lagrange Multiplier",
            "definition": "λ measuring sensitivity of objective to constraint; shadow price"
          },
          {
            "term": "Slack Variable",
            "definition": "Transforms gᵢ(x) ≤ 0 to gᵢ(x) + sᵢ = 0 with sᵢ ≥ 0"
          }
        ],

        "about": [
          {"name": "KKT Conditions", "description": "First-order necessary conditions for constrained optimality"},
          {"name": "Lagrangian", "description": "Augmented objective incorporating constraints"},
          {"name": "Complementary Slackness", "description": "Key condition linking constraints and multipliers"},
          {"name": "Penalty Method", "description": "Approximate constrained problems via unconstrained penalties"}
        ]
        },  
        {
        "id": "calc-10",
        "part": 10,
        "title": "Riemann Integration",
        "url": "riemann.html",
        "icon": "∫",
        "keywords": [
          "Riemann integral",
          "Riemann integrable",
          "Improper Riemann integration",
          "Partition",
          "Upper sum",
          "Lower sum",
          "Dirichlet function",
          "Dense sets",
          "Gaussian integral",
          "Cauchy distribution"
        ],
        "badges": [],
        "prereqs": ["calc-6"],
        "mapCoords": {"q": -3, "r": -2},
        "topicGroup": "analysis",
        "tesseraMessage": "Riemann integration — the classical approach before Lebesgue changed everything.",

        "headline": "Riemann Integration: The Classical Approach to Integration",
        "description": "Learn about Riemann integration, and improper Riemann integration.",
        "abstract": "Riemann integration defines integrals via partitions and upper/lower sums. While sufficient for continuous functions, its limitations with highly discontinuous functions like the Dirichlet function motivate the more powerful Lebesgue integral needed in probability theory.",
        "datePublished": "2024-01-01",
        "dateModified": "2025-01-15",

        "teaches": [
          "Formal definition of Riemann integration",
          "Partitions, upper sums, and lower sums",
          "Conditions for Riemann integrability",
          "Improper integrals on unbounded intervals",
          "Handling singularities in integrals",
          "Limitations with dense discontinuities",
          "Connection to Gaussian and Cauchy integrals",
          "Motivation for Lebesgue integration"
        ],

        "competencyRequired": [
          "Limits and continuity",
          "Infimum and supremum",
          "Basic integration techniques"
        ],

        "sections": [
          {"id": "rieman", "name": "Riemann Integration"},
          {"id": "i_rieman", "name": "Improper Riemann Integration"},
          {"id": "limit", "name": "Limitation of the (Improper) Riemann integration"}
        ],

        "definitions": [
          {
            "term": "Riemann Integral",
            "definition": "∫ₐᵇf(x)dx = α when sup L(f,P) = inf U(f,P) = α over all partitions"
          },
          {
            "term": "Partition",
            "definition": "P = {x₀, x₁, ..., xₙ} with a = x₀ < x₁ < ... < xₙ = b"
          },
          {
            "term": "Upper/Lower Sum",
            "definition": "U(f,P) = Σ Mᵢ(xᵢ-xᵢ₋₁), L(f,P) = Σ mᵢ(xᵢ-xᵢ₋₁)"
          },
          {
            "term": "Improper Integral",
            "definition": "∫ₐ^∞ f(x)dx = lim_{b→∞} ∫ₐᵇ f(x)dx when the limit exists"
          },
          {
            "term": "Dense Set",
            "definition": "A ⊂ ℝ is dense if for any x < y, there exists a ∈ A with x < a < y"
          }
        ],

        "about": [
          {"name": "Riemann Integration", "description": "Classical partition-based integration"},
          {"name": "Improper Integrals", "description": "Extending integration to unbounded domains"},
          {"name": "Dirichlet Function", "description": "Example showing Riemann's limitations"},
          {"name": "Dense Discontinuities", "description": "Why Lebesgue integration is needed"}
        ]
        },
        {
        "id": "calc-11",
        "part": 11,
        "title": "Measure Theory with Probability",
        "url": "measure.html",
        "icon": "σ",
        "keywords": [
          "Sample space",
          "σ-algebra",
          "σ-field",
          "Measurable set",
          "Measurable space",
          "Measure",
          "Probability measure",
          "Probability space",
          "Countable additivity",
          "Borel σ-algebra",
          "Borel set",
          "Lebesgue measure",
          "Caratheodory Extension Theorem"
        ],
        "badges": [],
        "prereqs": ["calc-10", "prob-1"],
        "mapCoords": {"q": -4, "r": -2},
        "topicGroup": "analysis",
        "tesseraMessage": "Measure theory makes probability rigorous. σ-algebras are everywhere!",

        "headline": "Measure Theory: The Rigorous Foundation of Probability",
        "description": "Learn about measure theory in terms of probability, and Lebesgue measure.",
        "abstract": "Measure theory provides the rigorous foundation for probability via σ-algebras and probability measures. The Carathéodory extension theorem constructs the Lebesgue measure, enabling integration of functions that defeat Riemann's approach.",
        "datePublished": "2024-01-01",
        "dateModified": "2025-01-15",

        "teaches": [
          "Formal structure of probability spaces (Ω, F, P)",
          "Sample spaces and elementary outcomes",
          "σ-algebras and closure properties",
          "Probability measures and axioms",
          "Countable vs finite additivity",
          "Carathéodory's extension theorem",
          "Construction of Lebesgue measure on ℝ",
          "Borel σ-algebra and measurable sets"
        ],

        "competencyRequired": [
          "Riemann integration and its limitations",
          "Basic probability concepts",
          "Set operations and notation"
        ],

        "sections": [
          {"id": "pspace", "name": "Probability Space"},
          {"id": "sample", "name": "Sample Space"},
          {"id": "sigma", "name": "σ-algebra (σ-field)"},
          {"id": "pm", "name": "Probability Measure"},
          {"id": "add", "name": "Finite Additivity"},
          {"id": "ext", "name": "Caratheodory's Extension Theorem"},
          {"id": "leb", "name": "Lebesgue measure"}
        ],

        "definitions": [
          {
            "term": "Probability Space",
            "definition": "Triple (Ω, F, P): sample space, σ-algebra, and probability measure"
          },
          {
            "term": "σ-algebra",
            "definition": "Collection F of subsets of Ω closed under complement and countable unions, containing Ω"
          },
          {
            "term": "Probability Measure",
            "definition": "Function P: F → [0,1] with P(Ω) = 1 and countable additivity"
          },
          {
            "term": "Borel σ-algebra",
            "definition": "Smallest σ-algebra containing all intervals; denoted B"
          },
          {
            "term": "Lebesgue Measure",
            "definition": "μ on (ℝ, B) with μ([a,b]) = b - a; extends 'length' to Borel sets"
          },
          {
            "term": "Carathéodory Extension",
            "definition": "Theorem extending finitely additive pre-measure to full σ-algebra"
          }
        ],

        "about": [
          {"name": "Probability Space", "description": "Formal triple (Ω, F, P) modeling random experiments"},
          {"name": "σ-algebra", "description": "Events we can assign probabilities to"},
          {"name": "Lebesgue Measure", "description": "Generalized notion of length/volume"},
          {"name": "Carathéodory", "description": "Foundational extension theorem"}
        ]
        },
        {
        "id": "calc-12",
        "part": 12,
        "title": "Intro to Lebesgue Integration",
        "url": "lebesgue.html",
        "icon": "a.e.",
        "keywords": [
          "Lebesgue integral",
          "Abstract integration",
          "Characteristic function",
          "Indicator function",
          "Simple function",
          "Almost everywhere (a.e.)",
          "Almost surely (a.s.)",
          "Measurable function",
          "Dirichlet function",
          "Dirichlet integral",
          "Sinc function"
        ],
        "badges": [],
        "prereqs": ["calc-11"],
        "mapCoords": {"q": -5, "r": -1},
        "topicGroup": "analysis",
        "tesseraMessage": "Lebesgue integration handles functions Riemann cannot. 'Almost everywhere' matters!",

        "headline": "Lebesgue Integration: Beyond Riemann's Limitations",
        "description": "Learn about integration of measurable functions.",
        "abstract": "Lebesgue integration builds integrals from measure theory, handling highly discontinuous functions that defeat Riemann. The concept of 'almost everywhere' formalizes ignoring measure-zero exceptions—essential for probability theory where 'almost surely' governs convergence.",
        "datePublished": "2024-01-01",
        "dateModified": "2025-01-15",

        "teaches": [
          "Abstract integration in measure spaces",
          "Characteristic (indicator) functions",
          "Simple functions and their integrals",
          "Integration of nonnegative measurable functions",
          "Integration of general measurable functions via f⁺ and f⁻",
          "Concept of 'almost everywhere' (a.e.)",
          "'Almost surely' in probability contexts",
          "Comparison: Riemann vs Lebesgue integrability",
          "Conditionally convergent integrals (sinc function)"
        ],

        "competencyRequired": [
          "Measure spaces and σ-algebras",
          "Lebesgue measure",
          "Supremum and infimum"
        ],

        "sections": [
          {"id": "abs", "name": "Abstract Integration"},
          {"id": "c_f", "name": "Characteristic function"},
          {"id": "fnm", "name": "The Integral of Finite Nonnegative Measurable Functions"},
          {"id": "nm", "name": "The Integral of Nonnegative Measurable Functions"},
          {"id": "gm", "name": "The Integral of General Measurable Functions"}
        ],

        "definitions": [
          {
            "term": "Lebesgue Integral",
            "definition": "∫g dμ defined via simple function approximation on measure space (Ω, F, μ)"
          },
          {
            "term": "Characteristic Function",
            "definition": "χ_A(ω) = 1 if ω ∈ A, 0 otherwise; indicator of set A"
          },
          {
            "term": "Simple Function",
            "definition": "g = Σᵢ aᵢχ_{Aᵢ} with finitely many values; integral = Σᵢ aᵢμ(Aᵢ)"
          },
          {
            "term": "Almost Everywhere (a.e.)",
            "definition": "Property holds except on a set of measure zero"
          },
          {
            "term": "Almost Surely (a.s.)",
            "definition": "'Almost everywhere' in probability context; holds with probability 1"
          }
        ],

        "about": [
          {"name": "Lebesgue Integral", "description": "Measure-theoretic generalization of Riemann"},
          {"name": "Simple Functions", "description": "Building blocks for Lebesgue integration"},
          {"name": "Almost Everywhere", "description": "Ignoring measure-zero exceptions"},
          {"name": "Dirichlet Function", "description": "Classic example: Lebesgue integral = 0"}
        ]
        },
        {   
        "id": "calc-13",
        "part": 13,
        "title": "Duality in Optimization & Analysis",
        "url": "duality.html",
        "icon": "P⟷D",
        "keywords": [
          "Duality",
          "Primal problem",
          "Dual problem",
          "Weak duality",
          "Strong duality",
          "Duality gap",
          "Slater's condition",
          "Smoothness",
          "L-smooth",
          "Lipschitz continuity",
          "Contraction mapping",
          "Convergence rate",
          "Condition number"
        ],
        "badges": ["interactive"],
        "prereqs": ["calc-9"],
        "mapCoords": {"q": -5, "r": 0},
        "topicGroup": "optimization",
        "tesseraMessage": "Duality gives us bounds and insights. Strong duality = same optimal value!",

        "headline": "Duality: Primal-Dual Relationships and Convergence Analysis",
        "description": "Learn about duality and Lipschitz Continuity.",
        "abstract": "Duality theory provides lower bounds on optimization problems and enables efficient algorithms. Lipschitz continuity and L-smoothness govern convergence rates—the condition number determines how fast gradient descent converges for quadratic problems.",
        "datePublished": "2024-01-01",
        "dateModified": "2025-01-15",

        "teaches": [
          "Primal and dual optimization problems",
          "Weak duality: dual provides lower bound",
          "Strong duality and zero duality gap",
          "Slater's condition for strong duality",
          "Lipschitz continuity and bounded rate of change",
          "L-smooth functions and gradient Lipschitz",
          "Contraction mappings and linear convergence",
          "Condition number and convergence rate analysis"
        ],

        "competencyRequired": [
          "Constrained optimization and KKT conditions",
          "Lagrangian formulation",
          "Eigenvalues of symmetric matrices"
        ],

        "sections": [
          {"id": "duality", "name": "Duality"},
          {"id": "Lip", "name": "Lipschitz Continuity"},
          {"id": "duality-visualization", "name": "Interactive Duality Visualization"}
        ],

        "definitions": [
          {
            "term": "Weak Duality",
            "definition": "d* ≤ p* always; dual optimal value bounds primal"
          },
          {
            "term": "Strong Duality",
            "definition": "d* = p*; primal and dual have same optimal value"
          },
          {
            "term": "Duality Gap",
            "definition": "p* - d* ≥ 0; zero gap means strong duality holds"
          },
          {
            "term": "L-smooth",
            "definition": "||∇f(x) - ∇f(y)|| ≤ L||x - y||; gradient is Lipschitz with constant L"
          },
          {
            "term": "Contraction Mapping",
            "definition": "d(f(x), f(y)) ≤ k·d(x,y) with k < 1; guarantees convergence"
          },
          {
            "term": "Condition Number",
            "definition": "κ = λ_max/λ_min; determines convergence rate μ = ((κ-1)/(κ+1))²"
          }
        ],

        "about": [
          {"name": "Duality", "description": "Relationship between primal and dual problems"},
          {"name": "Strong Duality", "description": "When primal and dual optima coincide"},
          {"name": "L-smoothness", "description": "Lipschitz gradient enabling convergence bounds"},
          {"name": "Condition Number", "description": "Key factor determining optimization speed"}
        ]
        },
        { 
        "id": "calc-14",
        "part": 14,
        "title": "Fourier Series",
        "url": "fourier_series.html",
        "icon": "∿",
        "keywords": [
          "Fourier series",
          "Fourier coefficients",
          "Orthogonality of Trigonometric Functions",
          "Complex exponential form",
          "Parseval's identity",
          "L² convergence",
          "Gibbs phenomenon",
          "Periodic functions",
          "Harmonic analysis",
          "Signal decomposition"
        ],
        "badges": [],
        "prereqs": ["calc-12", "linalg-7"],
        "mapCoords": {"q": -5, "r": -2},
        "topicGroup": "fourier",
        "tesseraMessage": "Fourier series decompose signals into frequencies. Music, images, everything!",

        "headline": "Fourier Series: Decomposing Periodic Functions into Frequencies",
        "description": "Learn about Fourier series decomposition of periodic functions and their convergence properties.",
        "abstract": "Fourier series represent periodic functions as infinite sums of sines and cosines. This fundamental technique, developed by Joseph Fourier for studying heat conduction, underlies signal processing, data compression (JPEG, MP3), and modern machine learning architectures.",
        "datePublished": "2024-01-01",
        "dateModified": "2025-01-15",

        "teaches": [
          "Fourier series representation of periodic functions",
          "Computing Fourier coefficients via integration",
          "Orthogonality of trigonometric functions",
          "Complex exponential form of Fourier series",
          "Parseval's identity and energy conservation",
          "L² (mean square) convergence of Fourier series",
          "Pointwise convergence and bounded variation",
          "Gibbs phenomenon at discontinuities"
        ],

        "competencyRequired": [
          "Riemann integration",
          "Inner products and orthogonality",
          "Complex exponentials"
        ],

        "sections": [
          {"id": "intro", "name": "Introduction"},
          {"id": "ortho", "name": "Orthogonality of Trigonometric Functions"},
          {"id": "coeff", "name": "Fourier Coefficients"},
          {"id": "complex", "name": "Complex Exponential Form"},
          {"id": "parseval", "name": "Parseval's Identity"},
          {"id": "convergence", "name": "Convergence Properties"}
        ],

        "definitions": [
          {
            "term": "Fourier Series",
            "definition": "Representation of a periodic function f(x) as f(x) = a₀/2 + Σ(aₙcos(nπx/L) + bₙsin(nπx/L))"
          },
          {
            "term": "Fourier Coefficients",
            "definition": "The constants aₙ and bₙ computed by integrating f(x) against cos and sin basis functions"
          },
          {
            "term": "Parseval's Identity",
            "definition": "Energy conservation: (1/L)∫|f(x)|²dx = a₀²/2 + Σ(aₙ² + bₙ²)"
          },
          {
            "term": "Gibbs Phenomenon",
            "definition": "~9% overshoot in partial sums near jump discontinuities that persists as N→∞"
          }
        ],

        "about": [
          {"name": "Fourier Series", "description": "Infinite trigonometric series representing periodic functions"},
          {"name": "Parseval's Identity", "description": "Energy equivalence between time and frequency domains"},
          {"name": "Gibbs Phenomenon", "description": "Persistent oscillation artifact at discontinuities"},
          {"name": "L² Convergence", "description": "Mean square convergence in function space"}
        ]
        },
        {   
       "id": "calc-15",
        "part": 15,
        "title": "Fourier Transform",
        "url": "fourier_transform.html",
        "icon": "ℱ",
        "keywords": [
          "Fourier transform",
          "Inverse Fourier transform",
          "Plancherel's theorem",
          "Convolution theorem",
          "Discrete Fourier Transform (DFT)",
          "Fast Fourier Transform (FFT)",
          "Fourier Neural Operators (FNO)",
          "Frequency domain",
          "Signal processing",
          "Spectral analysis"
        ],
        "badges": ["interactive"],
        "prereqs": ["calc-14"],
        "mapCoords": {"q": -6, "r": -1},
        "topicGroup": "fourier",
        "tesseraMessage": "FFT changed the world — fast signal processing makes modern tech possible.",

        "headline": "Fourier Transform: From Time Domain to Frequency Domain",
        "description": "Learn about the Fourier transform, its properties, and applications to signal processing and machine learning.",
        "abstract": "The Fourier transform extends Fourier series to non-periodic functions, decomposing signals into continuous frequency spectra. The FFT algorithm enables efficient computation, powering everything from audio processing to Fourier Neural Operators for solving PDEs.",
        "datePublished": "2024-01-01",
        "dateModified": "2025-01-15",

        "teaches": [
          "Continuous Fourier transform and inverse transform",
          "Properties: linearity, shifting, scaling, differentiation",
          "Convolution theorem: multiplication in frequency domain",
          "Plancherel's theorem for energy preservation",
          "Discrete Fourier Transform (DFT) for sampled signals",
          "Fast Fourier Transform (FFT) algorithm and O(N log N) complexity",
          "Applications: audio processing, image filtering, neural networks",
          "Fourier Neural Operators for solving PDEs"
        ],

        "competencyRequired": [
          "Fourier series and coefficients",
          "Complex exponentials",
          "Improper integrals"
        ],

        "sections": [
          {"id": "continuous", "name": "Continuous Fourier Transform"},
          {"id": "properties", "name": "Properties of Fourier Transform"},
          {"id": "convolution", "name": "Convolution Theorem"},
          {"id": "discrete", "name": "Discrete Fourier Transform"},
          {"id": "fft", "name": "Fast Fourier Transform"},
          {"id": "ml", "name": "Applications in Machine Learning"},
          {"id": "demos", "name": "Interactive Demos"}
        ],

        "definitions": [
          {
            "term": "Fourier Transform",
            "definition": "f̂(ξ) = ∫f(x)e^(ixξ)dx, mapping a function to its frequency spectrum"
          },
          {
            "term": "Inverse Fourier Transform",
            "definition": "f(x) = (1/2π)∫f̂(ξ)e^(-ixξ)dξ, recovering the original function"
          },
          {
            "term": "Convolution Theorem",
            "definition": "ℱ{f * g} = ℱ{f} · ℱ{g}, convolution becomes multiplication in frequency domain"
          },
          {
            "term": "Fast Fourier Transform (FFT)",
            "definition": "Divide-and-conquer algorithm computing DFT in O(N log N) instead of O(N²)"
          },
          {
            "term": "Fourier Neural Operator",
            "definition": "Neural network architecture that learns operators in Fourier space for solving PDEs"
          }
        ],

        "about": [
          {"name": "Fourier Transform", "description": "Integral transform mapping functions to frequency spectra"},
          {"name": "Convolution Theorem", "description": "Fundamental connection between convolution and multiplication"},
          {"name": "FFT", "description": "Efficient algorithm enabling real-time signal processing"},
          {"name": "Fourier Neural Operators", "description": "ML architecture for learning solution operators to PDEs"}
        ]
        },
        {
        "id": "calc-16",
        "part": 16,
        "title": "Foundations of Analysis: Metric Spaces",
        "url": "metric_space.html",
        "icon": "(X, d)",
        "keywords": [
          "Metric Space",
          "Distance",
          "Isolated Points",
          "Accumulation Points",
          "Nearest Points",
          "Boundary",
          "Interior",
          "Closure",
          "Open and Closed Sets",
          "Topology",
          "Completeness",
          "Open Balls",
          "Closed Balls",
          "Convexity"
        ],
        "badges": [],
        "prereqs": ["linalg-10", "calc-7", "calc-9", "ml-3"],
        "mapCoords": {"q": -6, "r": 0},
        "topicGroup": "analysis",
        "tesseraMessage": "Metric spaces abstract the notion of distance. The gateway to topology!",

        "headline": "Metric Spaces: The Foundation for Analysis and Optimization",
        "description": "Formalize the structure of metric spaces, covering distance, boundary, open and closed sets, topology, and completeness—the foundations ensuring optimization algorithms converge to valid solutions.",
        "abstract": "Metric spaces provide the rigorous framework for understanding distance, convergence, and completeness. These concepts formalize exactly when iterative algorithms like gradient descent converge to valid solutions, extending intuition from Euclidean space to function spaces and beyond.",
        "datePublished": "2024-01-01",
        "dateModified": "2025-01-15",

        "teaches": [
          "Formal definition of metric spaces and the three metric axioms",
          "Distance from points to sets and between sets",
          "Isolated points, accumulation points, and nearest points",
          "Boundary, interior, closure, and exterior of sets",
          "Open and closed subsets of metric spaces",
          "Topology determined by a metric",
          "Complete metric spaces and their importance for optimization",
          "Open and closed balls as building blocks of topology",
          "Convex sets in normed linear spaces"
        ],

        "competencyRequired": [
          "Norms and normed vector spaces",
          "Basic set theory notation",
          "Infimum and supremum"
        ],

        "sections": [
          {"id": "intro", "name": "Introduction"},
          {"id": "metric", "name": "Metric Space"},
          {"id": "dist", "name": "Distance"},
          {"id": "bound", "name": "Boundary"},
          {"id": "opcl", "name": "Open & Closed Sets"},
          {"id": "ball", "name": "Balls"}
        ],

        "definitions": [
          {
            "term": "Metric Space",
            "definition": "A set X with distance function d: X×X → ℝ satisfying positivity, symmetry, and triangle inequality"
          },
          {
            "term": "Open Set",
            "definition": "A set equal to its interior; every point has a neighborhood contained in the set"
          },
          {
            "term": "Closed Set",
            "definition": "A set containing all its boundary points; complement of an open set"
          },
          {
            "term": "Complete Metric Space",
            "definition": "A space where X is closed in every metric superspace (equivalently, every Cauchy sequence converges)"
          },
          {
            "term": "Open Ball",
            "definition": "B[x; r) = {y ∈ X | d(x,y) < r}, all points within distance r of center x"
          },
          {
            "term": "Topology",
            "definition": "The collection of open subsets determined by the metric"
          }
        ],

        "about": [
          {"name": "Metric Spaces", "description": "Abstract spaces with well-defined notion of distance"},
          {"name": "Completeness", "description": "Property ensuring limits of Cauchy sequences exist in the space"},
          {"name": "Topology", "description": "Structure determined by which sets are open"},
          {"name": "Convexity", "description": "Sets containing all line segments between their points"}
        ]
        },
        {
          "id": "calc-17",
          "part": 17,
          "title": "Convergence & Boundedness",
          "url": "limit_convergence.html",
          "icon": "tailₘ(xₙ)",
          "keywords": [
            "Tails",
            "Convergence",
            "Limits",
            "Cauchy Sequences",
            "Boundedness",
            "Diameter",
            "Total Boundedness",
            "Completeness",
            "Sequences",
            "Metric Spaces",
            "Gradient Descent Convergence",
            "Iterative Algorithms"
          ],
          "badges": [],
          "prereqs": ["calc-16"],
          "mapCoords": {"q": -6, "r": 1},
          "topicGroup": "analysis",
          "tesseraMessage": "Convergence is when the 'tail' of a sequence settles into an arbitrarily small ball. It's the structural anchor for every iterative process!",

          "headline": "Convergence & Boundedness in Metric Spaces",
          "description": "Formalize convergence of sequences in metric spaces, establish the relationship between Cauchy sequences and completeness, and explore boundedness—the theoretical foundation for analyzing iterative algorithms.",
          "abstract": "This chapter shifts from pointwise ε-N definitions to structural topology, defining convergence through the behavior of sequence tails settling into arbitrarily small neighborhoods. Essential for understanding why optimization algorithms like gradient descent converge.",
          "datePublished": "2024-01-01",
          "dateModified": "2025-01-15",

          "teaches": [
            "Formal definition of sequence convergence in metric spaces",
            "Tail-based approach to convergence vs pointwise ε-N definitions",
            "Uniqueness of limits in metric spaces",
            "Cauchy sequences and the Cauchy criterion",
            "Equivalence of completeness definitions",
            "Bounded sets, bounded sequences, and diameter",
            "Total boundedness and covering numbers",
            "Applications to gradient descent, weight clipping, and regularization"
          ],

          "competencyRequired": [
            "Metric spaces and distance functions",
            "Open balls and neighborhoods",
            "Basic sequence notation"
          ],

          "sections": [
            {"id": "intro", "name": "Introduction"},
            {"id": "convergence", "name": "Convergence of Sequences"},
            {"id": "cauchy", "name": "Cauchy Sequences"},
            {"id": "bounds", "name": "Boundedness"},
            {"id": "complete-revisit", "name": "Completeness Revisited"}
          ],

          "definitions": [
            {
              "term": "Tail of a Sequence",
              "definition": "For sequence {xₙ}, the tail from m is tail_m = {xₙ | n ≥ m}"
            },
            {
              "term": "Convergent Sequence",
              "definition": "A sequence converges to limit L if for every ε > 0, some tail is contained in the ε-ball around L"
            },
            {
              "term": "Cauchy Sequence",
              "definition": "A sequence where for every ε > 0, some tail has diameter less than ε"
            },
            {
              "term": "Complete Metric Space",
              "definition": "A space where every Cauchy sequence converges to a point within the space"
            },
            {
              "term": "Bounded Set",
              "definition": "A set contained in some ball of finite radius"
            },
            {
              "term": "Totally Bounded",
              "definition": "A set coverable by finitely many balls of any given radius ε"
            }
          ],

          "about": [
            {"name": "Convergence", "description": "The property of a sequence approaching a limit"},
            {"name": "Cauchy Sequences", "description": "Sequences where terms become arbitrarily close to each other"},
            {"name": "Completeness", "description": "A metric space where every Cauchy sequence converges"},
            {"name": "Boundedness", "description": "Sets contained within a ball of finite radius"},
            {"name": "Total Boundedness", "description": "Sets coverable by finitely many balls of any given radius"}
          ]
        },
        {
        "id": "calc-18",
        "part": 18,
        "title": "Continuity",
        "url": "continuity.html",
        "icon": "ε-δ",
        "keywords": [
          "Continuity",
          "Uniform Continuity",
          "Lipschitz Continuity",
          "Lipschitz Constant",
          "Contraction",
          "Strong Contraction",
          "Isometry",
          "Homeomorphism",
          "Metric Spaces"
        ],
        "badges": [],
        "prereqs": ["calc-17"],
        "mapCoords": {"q": -7, "r": 2},
        "topicGroup": "analysis",
        "tesseraMessage": "Lipschitz constants are the guardrails of AI, ensuring our gradients don't explode and algorithms actually converge.",

        "headline": "Continuity: From ε-δ to Lipschitz Bounds",
        "description": "Formalize continuous and uniformly continuous functions between metric spaces, explore Lipschitz continuity, and understand why these properties ensure optimization algorithms behave predictably.",
        "abstract": "Continuity ensures small input changes produce small output changes. This chapter develops the hierarchy from basic continuity through uniform continuity to Lipschitz continuity—each providing stronger guarantees essential for proving convergence of gradient descent, GAN stability, and fixed-point iterations.",
        "datePublished": "2024-01-01",
        "dateModified": "2025-01-15",

        "teaches": [
          "Definition of continuity via open sets in metric spaces",
          "ε-δ characterization of continuity",
          "Sequential characterization of continuity",
          "Topological characterization via preimages of open sets",
          "Uniform continuity and global δ guarantees",
          "Lipschitz continuous functions and Lipschitz constants",
          "Contractions (L ≤ 1) and strong contractions (L < 1)",
          "Isometries as distance-preserving maps",
          "Hierarchy: Lipschitz ⟹ Uniformly Continuous ⟹ Continuous"
        ],

        "competencyRequired": [
          "Metric spaces and open sets",
          "Convergence of sequences",
          "Open balls and neighborhoods"
        ],

        "sections": [
          {"id": "intro", "name": "Introduction"},
          {"id": "continuity", "name": "Continuity"},
          {"id": "uniform", "name": "Uniform Continuity"},
          {"id": "lipschitz", "name": "Lipschitz Continuity"}
        ],

        "definitions": [
          {
            "term": "Continuous Function",
            "definition": "f: X → Y where for each open V ⊂ Y with f(z) ∈ V, there exists open U ⊂ X with z ∈ U and f(U) ⊆ V"
          },
          {
            "term": "Uniformly Continuous",
            "definition": "For every ε > 0, there exists a single δ > 0 such that d(x,z) < δ implies e(f(x),f(z)) < ε for ALL x,z"
          },
          {
            "term": "Lipschitz Continuous",
            "definition": "e(f(a),f(b)) ≤ L·d(a,b) for all a,b ∈ X, where L is the Lipschitz constant"
          },
          {
            "term": "Contraction",
            "definition": "A Lipschitz map with constant L ≤ 1; never increases distances"
          },
          {
            "term": "Strong Contraction",
            "definition": "A Lipschitz map with constant L < 1; strictly decreases distances"
          },
          {
            "term": "Isometry",
            "definition": "A map preserving distances exactly: e(φ(a),φ(b)) = d(a,b)"
          }
        ],

        "about": [
          {"name": "Continuity", "description": "Preserving nearness between domain and range"},
          {"name": "Uniform Continuity", "description": "Global stability with position-independent δ"},
          {"name": "Lipschitz Continuity", "description": "Bounded rate of change with explicit constant"},
          {"name": "Contractions", "description": "Maps fundamental for fixed-point theorems and RL convergence"}
        ]
        }
      ],
      "reservedSlots": [
        {
          "q": -8,
          "r": 3
        },
        {
          "q": -6,
          "r": -2
        },
        {
          "q": -7,
          "r": -1
        },
        {
          "q": -7,
          "r": 1
        }
      ]
    },
    "III": {
      "id": "probability",
      "title": "Probability & Statistics",
      "shortTitle": "Probability",
      "description": "Explore fundamental concepts of probability and statistics essential for machine learning, including probability theory, random variables, distributions, Bayesian inference, entropy, and the Fisher Information Matrix.",
      "tagline": "The Mathematics of Uncertainty and Inference",
      "icon": "fa-dice",
      "indexUrl": "Mathematics/Probability/probability.html",
      "baseUrl": "Mathematics/Probability/",
      "parts": [
        {
          "id": "prob-1",
          "part": 1,
          "title": "Basic Probability Ideas",
          "url": "basic.html",
          "icon": "p",
          "keywords": [
            "Probability",
            "Sample Space",
            "Events",
            "Mutually Exclusive",
            "Permutation",
            "Combinations",
            "Conditional Probability",
            "Independent Events",
            "Law of Total Probability",
            "Bayes' Theorem"
          ],
          "badges": [],
          "prereqs": [],
          "mapCoords": {
            "q": 0,
            "r": -1
          },
          "topicGroup": "foundations",
          "tesseraMessage": "Probability is the language of uncertainty. Bayes' theorem is your new best friend!",

          "headline": "Basic Probability Ideas: The Foundation of Statistical Reasoning",
          "description": "Learn about basic probability ideas such as conditional probability and Bayes' theorem.",
          "abstract": "Probability theory provides the mathematical framework for quantifying uncertainty. Starting from sample spaces and events, we build up to conditional probability, independence, and the powerful Bayes' theorem. These concepts form the foundation for all statistical inference and machine learning.",
          "datePublished": "2024-01-01",
          "dateModified": "2025-01-15",

          "teaches": [
            "Sample spaces and events as subsets",
            "Probability axioms and basic properties",
            "Counting principles: permutations and combinations",
            "Conditional probability and its interpretation",
            "Independence of events",
            "Law of Total Probability for partitioned sample spaces",
            "Bayes' theorem and inverse probability",
            "Applications to classification and diagnosis"
          ],

          "competencyRequired": [
            "Basic set theory notation",
            "Combinatorics fundamentals",
            "Algebraic manipulation"
          ],

          "sections": [
            {"id": "pro", "name": "Probability"},
            {"id": "conditional", "name": "Conditional Probability"},
            {"id": "total", "name": "Law of Total Probability"},
            {"id": "bayes", "name": "Bayes' Theorem"}
          ],

          "definitions": [
            {
              "term": "Sample Space",
              "definition": "The collection S of every possible outcome of an experiment"
            },
            {
              "term": "Event",
              "definition": "A subset A ⊆ S of outcomes from the sample space"
            },
            {
              "term": "Conditional Probability",
              "definition": "P(A|B) = P(A ∩ B)/P(B), the probability of A given B has occurred"
            },
            {
              "term": "Independence",
              "definition": "Events A and B are independent if P(A ∩ B) = P(A)P(B)"
            },
            {
              "term": "Bayes' Theorem",
              "definition": "P(Bⱼ|A) = P(A|Bⱼ)P(Bⱼ) / Σᵢ P(A|Bᵢ)P(Bᵢ)"
            }
          ],

          "about": [
            {"name": "Probability Theory", "description": "Mathematical framework for quantifying uncertainty"},
            {"name": "Conditional Probability", "description": "Updating probabilities given new information"},
            {"name": "Bayes' Theorem", "description": "Foundation of Bayesian inference and classification"},
            {"name": "Combinatorics", "description": "Counting techniques for computing probabilities"}
          ]
        },
        {
          "id": "prob-2",
          "part": 2,
          "title": "Random Variables",
          "url": "random_variables.html",
          "icon": "X",
          "keywords": [
            "Discrete Random Variables",
            "Continuous Random Variables",
            "Probability Mass Function (p.m.f.)",
            "Probability Density Function (p.d.f.)",
            "Cumulative Distribution Function(c.d.f.)",
            "Expected Value",
            "Variance",
            "Standard Deviation"
          ],
          "badges": [],
          "prereqs": [
            "prob-1"
          ],
          "mapCoords": {
            "q": 1,
            "r": -1
          },
          "topicGroup": "foundations",
          "tesseraMessage": "Random variables turn randomness into mathematics we can compute with.",

          "headline": "Random Variables: Bridging Probability and Numerical Analysis",
          "description": "Learn about random variables, expected values, variance.",
          "abstract": "Random variables map outcomes to numerical values, enabling mathematical analysis of uncertainty. We distinguish discrete variables (with probability mass functions) from continuous variables (with probability density functions), and introduce the fundamental concepts of expected value and variance that characterize distributions.",
          "datePublished": "2024-01-01",
          "dateModified": "2025-01-15",

          "teaches": [
            "Random variables as functions from sample spaces to real numbers",
            "Discrete random variables and probability mass functions",
            "Continuous random variables and probability density functions",
            "Cumulative distribution functions and their properties",
            "Expected value as weighted average",
            "Variance and standard deviation",
            "Linear transformations of random variables",
            "Fundamental Theorem of Calculus connection to CDFs"
          ],

          "competencyRequired": [
            "Basic probability concepts",
            "Integration and summation",
            "Function concepts"
          ],

          "sections": [
            {"id": "rv", "name": "Random Variables"},
            {"id": "exp", "name": "Expected Value"},
            {"id": "var", "name": "Variance"}
          ],

          "definitions": [
            {
              "term": "Random Variable",
              "definition": "A function mapping each outcome in sample space to a numerical value"
            },
            {
              "term": "Probability Mass Function",
              "definition": "f(x) = P(X = x) for discrete random variables"
            },
            {
              "term": "Probability Density Function",
              "definition": "f(x) where P(a ≤ X ≤ b) = ∫ₐᵇ f(x)dx for continuous variables"
            },
            {
              "term": "Expected Value",
              "definition": "E[X] = μ = Σₓ x·f(x) (discrete) or ∫ x·f(x)dx (continuous)"
            },
            {
              "term": "Variance",
              "definition": "Var(X) = E[(X - μ)²] = E[X²] - μ²"
            }
          ],

          "about": [
            {"name": "Random Variables", "description": "Numerical representation of random outcomes"},
            {"name": "Distribution Functions", "description": "PMF, PDF, and CDF characterizing random variables"},
            {"name": "Expected Value", "description": "Center of mass of a probability distribution"},
            {"name": "Variance", "description": "Measure of spread around the mean"}
          ]
        },
        {
          "id": "prob-3",
          "part": 3,
          "title": "Gamma & Beta Distribution",
          "url": "gamma.html",
          "icon": "Γ",
          "keywords": [
            "Gamma Distribution",
            "Gamma Function",
            "Exponential Distribution",
            "Beta Function",
            "Beta Distribution",
            "Uniform Distribution"
          ],
          "badges": [
            "interactive"
          ],
          "prereqs": [
            "prob-2"
          ],
          "mapCoords": {
            "q": 0,
            "r": -2
          },
          "topicGroup": "distributions",
          "tesseraMessage": "Gamma and Beta distributions model waiting times and proportions beautifully.",

          "headline": "Gamma & Beta Distributions: Special Functions Meet Probability",
          "description": "Learn about Gamma and Beta distribution.",
          "abstract": "The Gamma and Beta distributions are fundamental continuous distributions built on special functions. The Gamma distribution generalizes the exponential distribution and models waiting times, while the Beta distribution models probabilities and proportions on [0,1]. Both play crucial roles as conjugate priors in Bayesian inference.",
          "datePublished": "2024-01-01",
          "dateModified": "2025-01-15",

          "teaches": [
            "Gamma function as generalized factorial",
            "Gamma distribution parameterization and properties",
            "Exponential distribution as special case",
            "Beta function and its relationship to Gamma",
            "Beta distribution for modeling proportions",
            "Uniform distribution as Beta(1,1)",
            "Shape and rate parameters interpretation",
            "Integration by parts for Gamma function"
          ],

          "competencyRequired": [
            "Random variables and PDFs",
            "Integration techniques",
            "Special integrals"
          ],

          "sections": [
            {"id": "gamma_f", "name": "Gamma Function"},
            {"id": "gamma_d", "name": "Gamma Distribution"},
            {"id": "beta_f", "name": "Beta Function"},
            {"id": "beta_d", "name": "Beta Distribution"},
            {"id": "demo", "name": "Interactive Visualization"}
          ],

          "definitions": [
            {
              "term": "Gamma Function",
              "definition": "Γ(z) = ∫₀^∞ t^(z-1)e^(-t)dt, with Γ(n+1) = n! for positive integers"
            },
            {
              "term": "Gamma Distribution",
              "definition": "f(x|α,β) ∝ x^(α-1)e^(-βx) for x > 0 with shape α and rate β"
            },
            {
              "term": "Beta Function",
              "definition": "B(a,b) = Γ(a)Γ(b)/Γ(a+b) = ∫₀¹ t^(a-1)(1-t)^(b-1)dt"
            },
            {
              "term": "Beta Distribution",
              "definition": "f(x|a,b) = x^(a-1)(1-x)^(b-1)/B(a,b) for x ∈ [0,1]"
            },
            {
              "term": "Exponential Distribution",
              "definition": "Gamma(1, λ), models time between events in Poisson process"
            }
          ],

          "about": [
            {"name": "Gamma Function", "description": "Extension of factorial to real and complex numbers"},
            {"name": "Gamma Distribution", "description": "Models waiting times and sums of exponentials"},
            {"name": "Beta Distribution", "description": "Natural distribution for probabilities on [0,1]"},
            {"name": "Special Functions", "description": "Mathematical functions with important properties"}
          ]
        },
        {
          "id": "prob-4",
          "part": 4,
          "title": "Normal (Gaussian) Distribution",
          "url": "gaussian.html",
          "icon": "𝒩",
          "keywords": [
            "Gaussian Function",
            "Error Function",
            "Gaussian Integral",
            "Normal(Gaussian) Distribution",
            "Standard Normal Distribution",
            "Independent and Identically Distributed(i.i.d.)",
            "Random Sample",
            "Sample Mean",
            "Sample Variance",
            "Central Limit Theorem"
          ],
          "badges": [],
          "prereqs": [
            "prob-2"
          ],
          "mapCoords": {
            "q": 1,
            "r": -2
          },
          "topicGroup": "distributions",
          "tesseraMessage": "The Gaussian is everywhere! Central Limit Theorem explains why.",

          "headline": "Normal Distribution: The Bell Curve at the Heart of Statistics",
          "description": "Learn about normal distribution and central limit theorem.",
          "abstract": "The Normal (Gaussian) distribution is the most important probability distribution in statistics and machine learning. Starting from the Gaussian function and its integral, we derive the standard normal distribution and explore why it appears everywhere through the Central Limit Theorem.",
          "datePublished": "2024-01-01",
          "dateModified": "2025-01-15",

          "teaches": [
            "Gaussian function and its parameterization",
            "Gaussian integral evaluation technique",
            "Error function for Gaussian CDF",
            "Normal distribution properties",
            "Standardization and z-scores",
            "Sum of independent normal variables",
            "Sample mean and sample variance",
            "Central Limit Theorem statement and implications"
          ],

          "competencyRequired": [
            "Random variables and distributions",
            "Integration techniques",
            "Exponential functions"
          ],

          "sections": [
            {"id": "gaussian_f", "name": "Gaussian Function"},
            {"id": "normal", "name": "Normal(Gaussian) Distribution"},
            {"id": "clt", "name": "Central Limit Theorem"}
          ],

          "definitions": [
            {
              "term": "Gaussian Function",
              "definition": "f(x) = ae^(-(x-b)²/2c²) with parameters controlling height, center, and width"
            },
            {
              "term": "Normal Distribution",
              "definition": "X ~ N(μ,σ²) with PDF f(x) = (1/σ√(2π))exp(-(x-μ)²/2σ²)"
            },
            {
              "term": "Standard Normal",
              "definition": "Z ~ N(0,1), the normal distribution with mean 0 and variance 1"
            },
            {
              "term": "Error Function",
              "definition": "erf(z) = (2/√π)∫₀^z e^(-t²)dt, used to express Gaussian CDF"
            },
            {
              "term": "Central Limit Theorem",
              "definition": "Sample mean of n iid variables approaches N(μ, σ²/n) as n → ∞"
            }
          ],

          "about": [
            {"name": "Normal Distribution", "description": "The ubiquitous bell curve in statistics"},
            {"name": "Gaussian Integral", "description": "∫₋∞^∞ e^(-x²)dx = √π, fundamental improper integral"},
            {"name": "Central Limit Theorem", "description": "Explains universality of normal distribution"},
            {"name": "Standardization", "description": "Converting any normal to standard normal"}
          ]
        },
        {
          "id": "prob-5",
          "part": 5,
          "title": "Student's t-Distribution",
          "url": "student.html",
          "icon": "t",
          "keywords": [
            "Student's t-Distribution",
            "Degrees of Freedom",
            "Cauchy Distribution",
            "Half Cauchy Distribution",
            "Laplace Distribution",
            "Double Sided Exponential Distribution"
          ],
          "badges": [],
          "prereqs": [
            "prob-4"
          ],
          "mapCoords": {
            "q": 2,
            "r": -1
          },
          "topicGroup": "distributions",
          "tesseraMessage": "Student's t handles small samples with grace. Heavy tails, robust inference.",

          "headline": "Student's t-Distribution: Robust Statistics with Heavy Tails",
          "description": "Learn about Student's t-distribution, Cauchy distribution, and Laplace distribution.",
          "abstract": "The Student's t-distribution provides a robust alternative to the normal distribution with heavier tails that better handle outliers. We explore its properties, special cases like the Cauchy distribution, and related distributions like the Laplace that are essential for robust statistical modeling.",
          "datePublished": "2024-01-01",
          "dateModified": "2025-01-15",

          "teaches": [
            "Student's t-distribution and degrees of freedom",
            "Heavy tails and robustness to outliers",
            "Cauchy distribution as t with ν=1",
            "Non-existence of mean for Cauchy",
            "Half Cauchy distribution for positive parameters",
            "Laplace (double-sided exponential) distribution",
            "Applications in robust regression",
            "Convergence to normal as ν increases"
          ],

          "competencyRequired": [
            "Normal distribution",
            "PDFs and moments",
            "Improper integrals"
          ],

          "sections": [
            {"id": "student", "name": "Student's t-Distribution"},
            {"id": "cauchy", "name": "Cauchy Distribution"},
            {"id": "laplace", "name": "Laplace Distribution"}
          ],

          "definitions": [
            {
              "term": "Student's t-Distribution",
              "definition": "f(y|μ,σ²,ν) ∝ [1 + (1/ν)((y-μ)/σ)²]^(-(ν+1)/2) with ν degrees of freedom"
            },
            {
              "term": "Degrees of Freedom",
              "definition": "Parameter ν controlling tail heaviness; larger ν approaches normal"
            },
            {
              "term": "Cauchy Distribution",
              "definition": "t-distribution with ν=1; f(x|μ,γ) = 1/(γπ[1+((x-μ)/γ)²])"
            },
            {
              "term": "Laplace Distribution",
              "definition": "f(y|μ,b) = (1/2b)exp(-|y-μ|/b), double-sided exponential"
            },
            {
              "term": "Heavy Tails",
              "definition": "More probability mass in tails than normal distribution"
            }
          ],

          "about": [
            {"name": "Student's t-Distribution", "description": "Robust alternative to normal for small samples"},
            {"name": "Cauchy Distribution", "description": "Extreme heavy-tailed distribution without finite mean"},
            {"name": "Laplace Distribution", "description": "Double exponential used in robust regression"},
            {"name": "Robustness", "description": "Resistance to outlier influence"}
          ]
        },
        {
          "id": "prob-6",
          "part": 6,
          "title": "Covariance",
          "url": "covariance.html",
          "icon": "Cov",
          "keywords": [
            "Covariance",
            "Covariance Matrix",
            "Total Variance",
            "Principal Component",
            "Principal Component Analysis(PCA)"
          ],
          "badges": [
            "code"
          ],
          "prereqs": [
            "prob-4",
            "linalg-6"
          ],
          "mapCoords": {
            "q": 0,
            "r": -3
          },
          "topicGroup": "multivariate",
          "tesseraMessage": "Covariance reveals how variables move together. PCA lives here!",

          "headline": "Covariance Matrix: Understanding Multivariate Relationships",
          "description": "Learn about covariance matrix and principal component analysis(PCA).",
          "abstract": "Moving to multivariate statistics, the covariance matrix captures how multiple variables vary together. We explore its properties, connection to total variance, and its central role in Principal Component Analysis (PCA), the foundational dimensionality reduction technique.",
          "datePublished": "2024-01-01",
          "dateModified": "2025-01-15",

          "teaches": [
            "Covariance between two random variables",
            "Covariance matrix for random vectors",
            "Population vs sample covariance matrix",
            "Properties of positive semi-definite matrices",
            "Total variance and trace",
            "Principal Component Analysis basics",
            "SVD connection to PCA",
            "Eigendecomposition of covariance matrix"
          ],

          "competencyRequired": [
            "Random variables and variance",
            "Matrix operations",
            "Eigenvalues and eigenvectors"
          ],

          "sections": [
            {"id": "covariance", "name": "Covariance Matrix"},
            {"id": "pca", "name": "Principal Component Analysis (PCA)"},
            {"id": "svd", "name": "PCA with Singular Value Decomposition(SVD)"}
          ],

          "definitions": [
            {
              "term": "Covariance",
              "definition": "Cov[X,Y] = E[(X - E[X])(Y - E[Y])], measures joint variation"
            },
            {
              "term": "Covariance Matrix",
              "definition": "Σ = E[(x - E[x])(x - E[x])ᵀ], symmetric positive semi-definite"
            },
            {
              "term": "Sample Covariance Matrix",
              "definition": "S = (1/(n-1))(X - X̄)ᵀ(X - X̄), unbiased estimator of Σ"
            },
            {
              "term": "Principal Components",
              "definition": "Eigenvectors of covariance matrix, directions of maximum variance"
            },
            {
              "term": "Total Variance",
              "definition": "tr(Σ) = Σᵢ Var(Xᵢ), sum of all diagonal elements"
            }
          ],

          "about": [
            {"name": "Covariance Matrix", "description": "Captures pairwise relationships between variables"},
            {"name": "PCA", "description": "Dimensionality reduction via variance maximization"},
            {"name": "SVD", "description": "Computational approach to PCA"},
            {"name": "Multivariate Statistics", "description": "Analysis of vector-valued random variables"}
          ]
        },
        {
          "id": "prob-7",
          "part": 7,
          "title": "Correlation",
          "url": "correlation.html",
          "icon": "r",
          "keywords": [
            "Cross-Covariance Matrix",
            "Auto-Covariance Matrix",
            "Correlation Coefficient",
            "Correlation Matrix"
          ],
          "badges": [],
          "prereqs": [
            "prob-6"
          ],
          "mapCoords": {
            "q": 1,
            "r": -3
          },
          "topicGroup": "multivariate",
          "tesseraMessage": "Correlation normalizes covariance — easier to interpret, same insight.",

          "headline": "Correlation: Standardized Measures of Linear Relationship",
          "description": "Learn about correlation.",
          "abstract": "Correlation standardizes covariance to produce scale-free measures of linear relationship between -1 and 1. We cover cross-covariance for different datasets, the correlation coefficient, and the correlation matrix that enables fair comparison across features with different scales.",
          "datePublished": "2024-01-01",
          "dateModified": "2025-01-15",

          "teaches": [
            "Cross-covariance between different datasets",
            "Auto-covariance as same-dataset covariance",
            "Population correlation coefficient",
            "Sample correlation coefficient",
            "Boundedness proof using Cauchy-Schwarz",
            "Correlation matrix construction",
            "Standardization from covariance to correlation",
            "Applications in feature scaling for PCA"
          ],

          "competencyRequired": [
            "Covariance and covariance matrix",
            "Standard deviation",
            "Cauchy-Schwarz inequality"
          ],

          "sections": [
            {"id": "cross", "name": "Cross-Covariance"},
            {"id": "corr", "name": "Correlation"}
          ],

          "definitions": [
            {
              "term": "Cross-Covariance Matrix",
              "definition": "K_AB = (1/(m-1))(A-Ā)ᵀ(B-B̄) for datasets A and B"
            },
            {
              "term": "Auto-Covariance Matrix",
              "definition": "K_AA, the cross-covariance of dataset with itself (= covariance matrix)"
            },
            {
              "term": "Correlation Coefficient",
              "definition": "ρ_XY = Cov[X,Y]/(σ_X σ_Y), standardized covariance in [-1,1]"
            },
            {
              "term": "Correlation Matrix",
              "definition": "R = (diag(K))^(-1/2) K (diag(K))^(-1/2), standardized covariance matrix"
            },
            {
              "term": "Sample Correlation",
              "definition": "r_xy = Σ(xᵢ-x̄)(yᵢ-ȳ) / [(n-1)s_x s_y]"
            }
          ],

          "about": [
            {"name": "Correlation Coefficient", "description": "Scale-free measure of linear relationship"},
            {"name": "Cross-Covariance", "description": "Relationships between different variable sets"},
            {"name": "Correlation Matrix", "description": "Normalized covariance for fair feature comparison"},
            {"name": "Standardization", "description": "Removing scale effects from covariance"}
          ]
        },
        {
          "id": "prob-8",
          "part": 8,
          "title": "Multivariate Distributions",
          "url": "mvn.html",
          "icon": "Σ",
          "keywords": [
            "Multivariate Normal Distribution (MVN)",
            "Mahalanobis Distance",
            "Bivariate Normal Distribution",
            "Cholesky Decomposition",
            "Dirichlet Distribution",
            "Probability Simplex",
            "Wishart Distribution",
            "Inverse Wishart Distribution"
          ],
          "badges": [],
          "prereqs": [
            "prob-6",
            "linalg-9"
          ],
          "mapCoords": {
            "q": 2,
            "r": -2
          },
          "topicGroup": "multivariate",
          "tesseraMessage": "Multivariate Gaussians are the foundation of Gaussian processes and more.",

          "headline": "Multivariate Distributions: Joint Probability in Higher Dimensions",
          "description": "Learn about multivariate normal distribution, Dirichlet distribution, and Wishart distribution.",
          "abstract": "Multivariate distributions extend probability theory to random vectors. The multivariate normal is the cornerstone of multivariate statistics, while the Dirichlet distribution models probability vectors and the Wishart distribution models covariance matrices—both essential as conjugate priors in Bayesian inference.",
          "datePublished": "2024-01-01",
          "dateModified": "2025-01-15",

          "teaches": [
            "Multivariate normal distribution PDF",
            "Mahalanobis distance and elliptical contours",
            "Bivariate normal special case",
            "Sampling via Cholesky decomposition",
            "Dirichlet distribution on probability simplex",
            "Dirichlet as conjugate prior for multinomial",
            "Wishart distribution for covariance matrices",
            "Inverse Wishart as conjugate prior"
          ],

          "competencyRequired": [
            "Covariance matrices",
            "Matrix decompositions",
            "Normal distribution"
          ],

          "sections": [
            {"id": "mn", "name": "Multivariate Normal Distribution"},
            {"id": "cholesky", "name": "Cholesky Decomposition"},
            {"id": "dirichlet", "name": "Dirichlet Distribution"},
            {"id": "wishart", "name": "Wishart Distribution"}
          ],

          "definitions": [
            {
              "term": "Multivariate Normal",
              "definition": "X ~ N(μ,Σ) with PDF ∝ exp(-½(x-μ)ᵀΣ⁻¹(x-μ))"
            },
            {
              "term": "Mahalanobis Distance",
              "definition": "d_M(x) = √((x-μ)ᵀΣ⁻¹(x-μ)), scale-invariant distance"
            },
            {
              "term": "Dirichlet Distribution",
              "definition": "Dir(α) on simplex {x: xᵢ≥0, Σxᵢ=1}, conjugate prior for multinomial"
            },
            {
              "term": "Wishart Distribution",
              "definition": "W(Σ,ν) for positive definite matrices, conjugate prior for precision"
            },
            {
              "term": "Cholesky Decomposition",
              "definition": "Σ = LLᵀ for sampling: x = μ + Lz where z ~ N(0,I)"
            }
          ],

          "about": [
            {"name": "Multivariate Normal", "description": "Foundation of multivariate statistics and Gaussian processes"},
            {"name": "Mahalanobis Distance", "description": "Distance accounting for correlations"},
            {"name": "Dirichlet Distribution", "description": "Distribution over probability vectors"},
            {"name": "Wishart Distribution", "description": "Distribution over covariance matrices"}
          ]
        },
        {
          "id": "prob-9",
          "part": 9,
          "title": "Maximum Likelihood Estimation",
          "url": "mle.html",
          "icon": "ℒ",
          "keywords": [
            "Point Estimator",
            "Mean Square Error(MSE)",
            "Standard Error (SE)",
            "Likelihood Function",
            "Log-likelihood Function",
            "Maximum Likelihood Estimation(MLE)",
            "Binomial Distribution",
            "Sample Proportion"
          ],
          "badges": [],
          "prereqs": [
            "prob-4"
          ],
          "mapCoords": {
            "q": 0,
            "r": -4
          },
          "topicGroup": "inference",
          "tesseraMessage": "MLE finds the parameters that make your data most likely. Foundational!",

          "headline": "Maximum Likelihood Estimation: Finding Parameters from Data",
          "description": "Learn about point estimators, and maximum likelihood estimation(MLE).",
          "abstract": "Maximum Likelihood Estimation (MLE) is the foundational method for parameter estimation in statistics and machine learning. Starting from point estimators and their properties (bias, variance, MSE), we develop the likelihood function framework and demonstrate MLE on binomial and normal distributions.",
          "datePublished": "2024-01-01",
          "dateModified": "2025-01-15",

          "teaches": [
            "Point estimators and their properties",
            "Bias, variance, and mean squared error",
            "Standard error of estimators",
            "Likelihood function definition",
            "Log-likelihood for numerical stability",
            "MLE principle: maximize likelihood",
            "MLE for binomial distribution",
            "MLE for normal distribution parameters"
          ],

          "competencyRequired": [
            "Probability distributions",
            "Calculus for optimization",
            "Product of probabilities"
          ],

          "sections": [
            {"id": "point", "name": "Point Estimators"},
            {"id": "lf", "name": "Likelihood Functions"},
            {"id": "mle", "name": "Maximum Likelihood Estimation"},
            {"id": "ex1", "name": "Example 1: Binomial Distribution"},
            {"id": "ex2", "name": "Example 2: Normal Distribution"}
          ],

          "definitions": [
            {
              "term": "Point Estimator",
              "definition": "θ̂ = f(X₁,...,Xₙ), a function of sample data estimating parameter θ"
            },
            {
              "term": "Bias",
              "definition": "Bias(θ̂) = E[θ̂] - θ, systematic error in estimation"
            },
            {
              "term": "Mean Squared Error",
              "definition": "MSE(θ̂) = E[(θ̂ - θ)²] = Var(θ̂) + Bias²"
            },
            {
              "term": "Likelihood Function",
              "definition": "L(θ) = ∏ᵢ f(xᵢ|θ), joint probability viewed as function of θ"
            },
            {
              "term": "Maximum Likelihood Estimator",
              "definition": "θ̂_MLE = argmax_θ L(θ) = argmax_θ log L(θ)"
            }
          ],

          "about": [
            {"name": "Point Estimation", "description": "Estimating parameters from sample data"},
            {"name": "Likelihood Function", "description": "Data probability as function of parameters"},
            {"name": "MLE", "description": "Principle of choosing most likely parameters"},
            {"name": "Log-Likelihood", "description": "Numerically stable likelihood optimization"}
          ]
        },
        {
          "id": "prob-10",
          "part": 10,
          "title": "Statistical Inference & Hypothesis Testing",
          "url": "hypothesis_testing.html",
          "icon": "H₀ vs H₁",
          "keywords": [
            "Null Hypothesis",
            "Alternative Hypothesis",
            "Type I Error (False Negative)",
            "Type II Error (False Positive)",
            "Significance Level",
            "Test Statistic",
            "Null Hypothesis Significance Test(NHST)",
            "One Sample t-Tests",
            "Confidence intervals",
            "Critical Values",
            "z-scores",
            "Credible Intervals",
            "Bootstrap"
          ],
          "badges": [],
          "prereqs": [
            "prob-9"
          ],
          "mapCoords": {
            "q": 1,
            "r": -4
          },
          "topicGroup": "inference",
          "tesseraMessage": "Hypothesis testing helps us make decisions under uncertainty. Be careful with p-values!",

          "headline": "Hypothesis Testing: Making Decisions Under Uncertainty",
          "description": "Learn about null hypothesis significance test, confidence interval, credible interval, and bootstrap method.",
          "abstract": "Statistical inference allows us to draw conclusions about populations from samples. We cover the frequentist approach through null hypothesis significance testing (NHST), confidence intervals, and contrast it with Bayesian credible intervals. The bootstrap method provides a powerful computational alternative.",
          "datePublished": "2024-01-01",
          "dateModified": "2025-01-15",

          "teaches": [
            "Null and alternative hypotheses formulation",
            "Type I and Type II errors",
            "Significance level and p-values",
            "Test statistics and critical values",
            "One-sample t-test procedure",
            "Confidence intervals construction",
            "Frequentist vs Bayesian interpretation",
            "Bootstrap resampling method"
          ],

          "competencyRequired": [
            "Sampling distributions",
            "Normal and t-distributions",
            "Maximum likelihood estimation"
          ],

          "sections": [
            {"id": "NHST", "name": "Null Hypothesis Significance Test"},
            {"id": "test", "name": "Example: t-Tests"},
            {"id": "CI", "name": "Confidence Intervals vs Credible Intervals"},
            {"id": "BS", "name": "Bootstrap"}
          ],

          "definitions": [
            {
              "term": "Null Hypothesis",
              "definition": "H₀: the default assumption to be tested"
            },
            {
              "term": "Type I Error",
              "definition": "Rejecting H₀ when it is true (false positive), probability = α"
            },
            {
              "term": "Type II Error",
              "definition": "Failing to reject H₀ when it is false (false negative)"
            },
            {
              "term": "Confidence Interval",
              "definition": "Range where repeated sampling would capture true parameter (1-α)% of time"
            },
            {
              "term": "Bootstrap",
              "definition": "Resampling with replacement to estimate sampling distribution"
            }
          ],

          "about": [
            {"name": "Hypothesis Testing", "description": "Framework for statistical decision making"},
            {"name": "Confidence Intervals", "description": "Frequentist uncertainty quantification"},
            {"name": "Credible Intervals", "description": "Bayesian probability intervals for parameters"},
            {"name": "Bootstrap", "description": "Computational resampling for inference"}
          ]
        },
        {
          "id": "prob-11",
          "part": 11,
          "title": "Linear Regression",
          "url": "linear_regression.html",
          "icon": "LS",
          "keywords": [
            "Linear Regression",
            "Least-Squares Estimation"
          ],
          "badges": [
            "interactive"
          ],
          "prereqs": [
            "prob-9",
            "linalg-8"
          ],
          "mapCoords": {
            "q": 2,
            "r": -3
          },
          "topicGroup": "inference",
          "tesseraMessage": "Linear regression connects statistics to ML. Where prediction begins!",

          "headline": "Linear Regression: A Probabilistic Perspective",
          "description": "Learn about linear regression in probabilistic Perspective.",
          "abstract": "Linear regression from a probabilistic viewpoint reveals the connection between least-squares and maximum likelihood estimation. When errors are normally distributed, MLE produces exactly the least-squares solution. This bridges linear algebra and probability, forming the foundation for statistical learning.",
          "datePublished": "2024-01-01",
          "dateModified": "2025-01-15",

          "teaches": [
            "Linear model formulation y = Xβ + ε",
            "Design matrix and parameter vector",
            "Normal equations from linear algebra",
            "Probabilistic interpretation of regression",
            "Gaussian noise assumption",
            "Likelihood function for linear regression",
            "Equivalence of MLE and least-squares",
            "Derivation using matrix calculus"
          ],

          "competencyRequired": [
            "Maximum likelihood estimation",
            "Least-squares problems",
            "Matrix calculus basics"
          ],

          "sections": [
            {"id": "recap", "name": "Recap from Linear Algebra"},
            {"id": "lr", "name": "Linear Regression: A Probabilistic Perspective"},
            {"id": "interactive-tool", "name": "Interactive Statistical Regression Tool"}
          ],

          "definitions": [
            {
              "term": "Linear Model",
              "definition": "y = Xβ + ε where ε ~ N(0, σ²I)"
            },
            {
              "term": "Design Matrix",
              "definition": "X ∈ ℝⁿˣᵈ containing n observations of d features"
            },
            {
              "term": "Normal Equations",
              "definition": "XᵀXβ̂ = Xᵀy, necessary condition for least-squares"
            },
            {
              "term": "Least-Squares Solution",
              "definition": "β̂_LS = (XᵀX)⁻¹Xᵀy = β̂_MLE under Gaussian noise"
            },
            {
              "term": "Residual Vector",
              "definition": "ε = y - Xβ, difference between observed and predicted"
            }
          ],

          "about": [
            {"name": "Linear Regression", "description": "Predicting continuous outputs from linear combinations"},
            {"name": "Least-Squares", "description": "Minimizing sum of squared residuals"},
            {"name": "MLE Connection", "description": "Least-squares as MLE under Gaussian assumption"},
            {"name": "Normal Equations", "description": "Linear algebra characterization of optimal parameters"}
          ]
        },
        {
          "id": "prob-12",
          "part": 12,
          "title": "Entropy",
          "url": "entropy.html",
          "icon": "ℍ",
          "keywords": [
            "Information Content",
            "Entropy",
            "Joint Entropy",
            "Conditional Entropy",
            "Cross Entropy",
            "KL Divergence(Relative Entropy, Information Gain)",
            "Gibbs' Inequality",
            "Log Sum Inequality",
            "Jensen's Inequality",
            "Mutual Information (MI)"
          ],
          "badges": [],
          "prereqs": [
            "prob-4"
          ],
          "mapCoords": {
            "q": 3,
            "r": -2
          },
          "topicGroup": "information",
          "tesseraMessage": "Entropy measures uncertainty — the heart of information theory.",

          "headline": "Entropy: The Mathematical Theory of Information",
          "description": "Learn about entropy in information theory including KL divergence and mutual information.",
          "abstract": "Information theory provides mathematical tools for quantifying uncertainty and information. Starting from self-information, we build to entropy, joint and conditional entropy, cross-entropy (used in ML loss functions), KL divergence for comparing distributions, and mutual information for measuring shared information.",
          "datePublished": "2024-01-01",
          "dateModified": "2025-01-15",

          "teaches": [
            "Information content (self-information)",
            "Shannon entropy definition and properties",
            "Joint entropy for multiple variables",
            "Conditional entropy and chain rule",
            "Cross-entropy and its use in machine learning",
            "KL divergence as relative entropy",
            "Gibbs' inequality proof",
            "Mutual information between variables"
          ],

          "competencyRequired": [
            "Probability distributions",
            "Logarithms and their properties",
            "Jensen's inequality"
          ],

          "sections": [
            {"id": "entropy", "name": "Entropy"},
            {"id": "joint", "name": "Joint Entropy"},
            {"id": "conditional", "name": "Conditional Entropy"},
            {"id": "cross", "name": "Cross Entropy"},
            {"id": "kl", "name": "KL Divergence (Relative Entropy, Information Gain)"},
            {"id": "mi", "name": "Mutual Information (MI)"}
          ],

          "definitions": [
            {
              "term": "Information Content",
              "definition": "I(E) = -log p(E), measures surprise of event E"
            },
            {
              "term": "Entropy",
              "definition": "H(X) = -Σₓ p(x)log p(x), expected information content"
            },
            {
              "term": "Cross-Entropy",
              "definition": "H(p,q) = -Σₓ p(x)log q(x), entropy using wrong distribution"
            },
            {
              "term": "KL Divergence",
              "definition": "D_KL(p||q) = Σₓ p(x)log(p(x)/q(x)) ≥ 0, distance between distributions"
            },
            {
              "term": "Mutual Information",
              "definition": "I(X;Y) = H(X) - H(X|Y) = H(Y) - H(Y|X), shared information"
            }
          ],

          "about": [
            {"name": "Entropy", "description": "Fundamental measure of uncertainty"},
            {"name": "Cross-Entropy", "description": "Loss function for classification in ML"},
            {"name": "KL Divergence", "description": "Asymmetric measure of distribution difference"},
            {"name": "Mutual Information", "description": "Information shared between random variables"}
          ]
        },
        {
          "id": "prob-13",
          "part": 13,
          "title": "Convergence",
          "url": "convergence.html",
          "icon": "n→∞",
          "keywords": [
            "The Law of Large Numbers",
            "Convergence in Probability",
            "Convergence in Distribution",
            "Asymptotic(limiting) Distribution",
            "Moment Generating Function(m.g.f.)",
            "Central Limit Theorem(CLT)"
          ],
          "badges": [],
          "prereqs": [
            "prob-9"
          ],
          "mapCoords": {
            "q": 2,
            "r": -4
          },
          "topicGroup": "inference",
          "tesseraMessage": "Convergence guarantees that our estimates improve. Math meets practice!",

          "headline": "Convergence: How Random Variables Approach Their Limits",
          "description": "Learn about convergence in probability and distribution.",
          "abstract": "Statistical convergence describes how sequences of random variables approach limits. We explore the hierarchy from almost sure convergence through convergence in probability to convergence in distribution, connecting these to the Law of Large Numbers and Central Limit Theorem via moment generating functions.",
          "datePublished": "2024-01-01",
          "dateModified": "2025-01-15",

          "teaches": [
            "Modes of convergence hierarchy",
            "Almost sure convergence (strongest)",
            "Convergence in probability",
            "Convergence in distribution (weakest)",
            "Law of Large Numbers (weak and strong)",
            "Moment generating functions",
            "Continuity theorem for MGFs",
            "Central Limit Theorem proof sketch"
          ],

          "competencyRequired": [
            "Probability distributions",
            "Sequences and limits",
            "Exponential functions"
          ],

          "sections": [
            {"id": "LLA", "name": "Modes of Convergence & The Law of Large Numbers"},
            {"id": "prob", "name": "Convergence in Probability"},
            {"id": "dist", "name": "Convergence in Distribution"},
            {"id": "mgf", "name": "Moment Generating Function(mgf)"}
          ],

          "definitions": [
            {
              "term": "Almost Sure Convergence",
              "definition": "P(lim_{n→∞} Xₙ = X) = 1, strongest form"
            },
            {
              "term": "Convergence in Probability",
              "definition": "For all ε>0: lim_{n→∞} P(|Xₙ - X| > ε) = 0"
            },
            {
              "term": "Convergence in Distribution",
              "definition": "Fₙ(x) → F(x) at all continuity points of F"
            },
            {
              "term": "Law of Large Numbers",
              "definition": "X̄ₙ converges to μ as n → ∞ (weak or strong)"
            },
            {
              "term": "Moment Generating Function",
              "definition": "M_X(t) = E[e^{tX}], uniquely determines distribution"
            }
          ],

          "about": [
            {"name": "Statistical Convergence", "description": "How random sequences approach limits"},
            {"name": "Law of Large Numbers", "description": "Sample means converge to population mean"},
            {"name": "MGF", "description": "Transform that characterizes distributions"},
            {"name": "Asymptotic Theory", "description": "Behavior of estimators as n → ∞"}
          ]
        },
        {
          "id": "prob-14",
          "part": 14,
          "title": "Intro to Bayesian Statistics",
          "url": "bayesian.html",
          "icon": "p(\u03b8|\ud835\udc9f)",
          "keywords": [
            "Bayesian Inference",
            "Prior Distribution",
            "Posterior Distribution",
            "Marginal Likelihood",
            "Conjugate Prior",
            "Posterior Predictive Distribution",
            "Beta-Binomial Model",
            "Normal Distribution Model with known Variance \u03c3\u00b2",
            "Normal Distribution Model with known Mean \u03bc"
          ],
          "badges": [],
          "prereqs": [
            "prob-9",
            "prob-3"
          ],
          "mapCoords": {
            "q": 3,
            "r": -3
          },
          "topicGroup": "bayesian",
          "tesseraMessage": "Bayesian thinking updates beliefs with evidence. Prior \u2192 Posterior."
        },
        {
          "id": "prob-15",
          "part": 15,
          "title": "The Exponential Family",
          "url": "expfamily.html",
          "icon": "\u03b7",
          "keywords": [
            "Exponential Family",
            "Natural Parameters(Canonical Parameters)",
            "Base Measure",
            "Sufficient Statistics",
            "Partition Function",
            "Minimal Representation",
            "Natural Exponential Family(NEF)",
            "Moment Parameters",
            "Precision Matrix",
            "Information Form",
            "Moment Matching",
            "Cumulants"
          ],
          "badges": [],
          "prereqs": [
            "prob-14"
          ],
          "mapCoords": {
            "q": 4,
            "r": -2
          },
          "topicGroup": "bayesian",
          "tesseraMessage": "The exponential family unifies many distributions elegantly. Beautiful structure!"
        },
        {
          "id": "prob-16",
          "part": 16,
          "title": "Fisher Information Matrix",
          "url": "fisher_info.html",
          "icon": "F(\u03b8)",
          "keywords": [
            "Fisher Information Matrix(FIM)",
            "Score Function",
            "Covariance",
            "Negative Log Likelihood",
            "Log Partition Function",
            "Approximated KL Divergence",
            "Natural Gradient",
            "Jeffreys Prior",
            "Uninformative Prior",
            "Reference Prior",
            "Mutual Information"
          ],
          "badges": [],
          "prereqs": [
            "prob-15",
            "linalg-9"
          ],
          "mapCoords": {
            "q": 4,
            "r": -3
          },
          "topicGroup": "bayesian",
          "tesseraMessage": "Fisher information measures how much data tells us about parameters."
        },
        {
          "id": "prob-17",
          "part": 17,
          "title": "Bayesian Decision Theory",
          "url": "decision_theory.html",
          "icon": "\u03c0",
          "keywords": [
            "Decision Theory",
            "Optimal Policy(Bayes estimator)",
            "Zero-One Loss",
            "Maximum A Posteriori (MAP) Estimate",
            "Reject Option",
            "Confusion Matrix",
            "False Positive (FP, Type I error)",
            "False Negative (FN, Type II error)",
            "Receiver Operating Characteristic (ROC) Curve",
            "Equal Error Rate (EER)",
            "Precision-Recall (PR) Curve",
            "Interpolated Precision",
            "Average Precision (AP)"
          ],
          "badges": [],
          "prereqs": [
            "prob-14"
          ],
          "mapCoords": {
            "q": 3,
            "r": -4
          },
          "topicGroup": "bayesian",
          "tesseraMessage": "Decision theory tells us how to act optimally under uncertainty."
        },
        {
          "id": "prob-18",
          "part": 18,
          "title": "Markov Chains",
          "url": "markov.html",
          "icon": "\u220f",
          "keywords": [
            "Probabilistic Graphical Models(PGMs)",
            "Bayesian Networks",
            "Markov Chains",
            "Language Modeling",
            "n-gram",
            "Transition Function(Kernel)",
            "Stochastic Matrix(Transition Matrix)",
            "Maximum likelihood estimation(MLE) in Markov models",
            "Sparse Data Problem",
            "Add-One Smoothing",
            "Dirichlet Prior"
          ],
          "badges": [
            "code"
          ],
          "prereqs": [
            "prob-14",
            "linalg-13"
          ],
          "mapCoords": {
            "q": 4,
            "r": -4
          },
          "topicGroup": "bayesian",
          "tesseraMessage": "Markov chains model sequences \u2014 from text to weather to DNA."
        },
        {
          "id": "prob-19",
          "part": 19,
          "title": "Monte Carlo Methods",
          "url": "monte_carlo.html",
          "icon": "p*(\u03b8)",
          "keywords": [
            "Credible Intervals",
            "Central Credible Intervals",
            "Monte Carlo Approximation",
            "Highest Posterior Density (HPD)",
            "Markov Chain Monte Carlo (MCMC)"
          ],
          "badges": [
            "interactive"
          ],
          "prereqs": [
            "prob-14"
          ],
          "mapCoords": {
            "q": 5,
            "r": -4
          },
          "topicGroup": "bayesian",
          "tesseraMessage": "Monte Carlo turns random sampling into computational power!"
        },
        {
          "id": "prob-20",
          "part": 20,
          "title": "Importance Sampling",
          "url": "importance_sampling.html",
          "icon": "\u03c6",
          "keywords": [
            "Importance Sampling",
            "Importance Weights",
            "Direct Importance Sampling",
            "Effective Sample Size (ESS)",
            "Self-Normalized Importance Sampling (SNIS)",
            "Annealed Importance Sampling (AIS)",
            "Annealing Schedule"
          ],
          "badges": [],
          "prereqs": [
            "prob-19"
          ],
          "mapCoords": {
            "q": 5,
            "r": -3
          },
          "topicGroup": "bayesian",
          "tesseraMessage": "Importance sampling focuses computation where it matters most."
        },
        {
          "id": "prob-21",
          "part": 21,
          "title": "Gaussian Processes",
          "url": "gaussian_process.html",
          "icon": "\ud835\udca6",
          "keywords": [
            "Nonparametric Models",
            "Gaussian Process (GP)",
            "Mercer Kernel",
            "radial basis function (RBF) Kernel",
            "Stationary Kernel",
            "Automatic Relevance Determination (ARD) Kernel",
            "Mat\u00e9rn Kernel",
            "Periodic Kernel",
            "GP Regression"
          ],
          "badges": [],
          "prereqs": [
            "prob-8",
            "linalg-9"
          ],
          "mapCoords": {
            "q": 5,
            "r": -2
          },
          "topicGroup": "gaussian-process",
          "tesseraMessage": "GPs give uncertainty estimates for free. Bayesian nonparametrics at its finest!"
        }
      ],
      "reservedSlots": [
        {
          "q": 6,
          "r": -2
        },
        {
          "q": 6,
          "r": -4
        },
        {
          "q": 5,
          "r": -5
        }
      ]
    },
    "IV": {
      "id": "discrete",
      "title": "Discrete Mathematics & Algorithms",
      "shortTitle": "Discrete Math",
      "description": "Explore the foundations of discrete mathematics and algorithms, covering graph theory, combinatorics, and the theory of computation. Learn key concepts essential for mathematical reasoning and algorithmic problem-solving.",
      "tagline": "The Mathematics of Logic and Finite Structures",
      "icon": "fa-project-diagram",
      "indexUrl": "Mathematics/Discrete/discrete_math.html",
      "baseUrl": "Mathematics/Discrete/",
      "parts": [
        {
          "id": "disc-1",
          "part": 1,
          "title": "Intro to Graph Theory",
          "url": "intro_graph.html",
          "icon": "G=(V,E)",
          "keywords": [
            "Undirected graph",
            "Directed graph",
            "Multigraph",
            "Weighted graph",
            "Complete graph",
            "Adjacent",
            "Degree of a vertex",
            "k-regular",
            "Isomorphism",
            "Bipartite graph",
            "Subgraph",
            "Induced subgraph",
            "Spanning subgraph",
            "Path",
            "Cycle",
            "Tree",
            "Acyclic graph"
          ],
          "badges": [],
          "prereqs": [],
          "mapCoords": {"q": 0, "r": 1},
          "topicGroup": "graph-theory",
          "tesseraMessage": "Graphs are everywhere — social networks, molecules, the internet itself!",

          "headline": "Graph Theory: Modeling Relationships with Vertices and Edges",
          "description": "Introduction to graph theory, and isomorphism.",
          "abstract": "Graph theory provides the mathematical language for modeling relationships between objects. A graph G = (V, E) consists of vertices and edges connecting them. Key concepts include vertex degrees, graph isomorphism, subgraphs, paths, and cycles—fundamental structures underlying social networks, recommendation systems, and ML models.",
          "datePublished": "2024-01-01",
          "dateModified": "2025-01-15",

          "teaches": [
            "Graph definition: G = (V, E)",
            "Undirected, directed, weighted, and multigraphs",
            "Neighbors and adjacency",
            "Vertex degree, in-degree, and out-degree",
            "Minimum, maximum, and average degree",
            "Graph isomorphism and automorphism",
            "Graph properties and invariants",
            "Subgraphs, induced subgraphs, and spanning subgraphs",
            "Paths, cycles, and acyclic graphs (trees)"
          ],

          "competencyRequired": [
            "Basic set theory",
            "Function concepts"
          ],

          "sections": [
            {"id": "intro", "name": "Introduction"},
            {"id": "graph", "name": "Graph"},
            {"id": "neighbors", "name": "Neighbors"},
            {"id": "degree", "name": "Degrees"},
            {"id": "iso", "name": "Isomorphism"},
            {"id": "sub", "name": "Subgraphs"},
            {"id": "path", "name": "Paths & Cycles"}
          ],

          "definitions": [
            {
              "term": "Graph",
              "definition": "G = (V, E) where V is a set of vertices and E is a set of edges"
            },
            {
              "term": "Degree",
              "definition": "deg(x) = |N(x)|; number of neighbors of vertex x"
            },
            {
              "term": "Isomorphism",
              "definition": "Bijection φ: V → V' preserving edge relationships; G ≃ G'"
            },
            {
              "term": "Induced Subgraph",
              "definition": "G[V'] contains all edges of G with both endpoints in V'"
            },
            {
              "term": "Path",
              "definition": "Sequence of distinct vertices connected by edges; Pₖ has k vertices"
            },
            {
              "term": "Cycle",
              "definition": "Path with edge connecting last vertex to first; Cₖ has k vertices and k edges"
            }
          ],

          "about": [
            {"name": "Graph Theory", "description": "Mathematical study of relationships via vertices and edges"},
            {"name": "Graph Isomorphism", "description": "Structural equivalence of graphs"},
            {"name": "Subgraphs", "description": "Graphs contained within larger graphs"},
            {"name": "Paths and Cycles", "description": "Fundamental traversal structures"}
          ]
        },
        {
          "id": "disc-2",
          "part": 2,
          "title": "Intro to Combinatorics",
          "url": "intro_combinatorics.html",
          "icon": "\\(_n C_r\\)",
          "keywords": [
            "Fundamental counting principle",
            "Rule of sum",
            "Rule of product",
            "Combination",
            "Binomial coefficient",
            "Multinomial coefficient",
            "Pascal's relation",
            "Pascal's triangle",
            "Chu's theorem",
            "Sum of powers of positive integers",
            "Pascal matrix",
            "Permutation"
          ],
          "badges": [],
          "prereqs": [],
          "mapCoords": {"q": 1, "r": 2},
          "topicGroup": "combinatorics",
          "tesseraMessage": "Counting cleverly is an art. Combinatorics powers probability and CS alike.",

          "headline": "Combinatorics: The Art of Counting and Arranging",
          "description": "Introduction to combinatorics such as counting techniques.",
          "abstract": "Combinatorics studies counting, arranging, and analyzing discrete structures. The fundamental counting principle (sum and product rules), combinations C(n,r) = n!/(r!(n-r)!), and permutations form the foundation. Advanced techniques like Pascal's relation and Chu's theorem connect to linear algebra via the Pascal matrix.",
          "datePublished": "2024-01-01",
          "dateModified": "2025-01-15",

          "teaches": [
            "Rule of sum for disjoint unions",
            "Rule of product for sequential decisions",
            "Combinations: choosing r from n (order doesn't matter)",
            "Binomial coefficients C(n,r) = n!/(r!(n-r)!)",
            "Multinomial coefficients",
            "Pascal's relation: C(n+1,r) = C(n,r-1) + C(n,r)",
            "Chu's theorem: sum of C(k,r) = C(n+1,r+1)",
            "Sum of powers via combinations",
            "Pascal matrix and linear algebra connection",
            "Permutations: ordered selections"
          ],

          "competencyRequired": [
            "Basic arithmetic and factorials",
            "Algebraic manipulation"
          ],

          "sections": [
            {"id": "intro", "name": "Introduction"},
            {"id": "count", "name": "Counting"}
          ],

          "definitions": [
            {
              "term": "Combination",
              "definition": "C(n,r) = n!/(r!(n-r)!); ways to choose r items from n without order"
            },
            {
              "term": "Permutation",
              "definition": "P(n,r) = n!/(n-r)! = r!C(n,r); ordered selections"
            },
            {
              "term": "Pascal's Relation",
              "definition": "C(n+1,r) = C(n,r-1) + C(n,r) for 1 ≤ r ≤ n"
            },
            {
              "term": "Chu's Theorem",
              "definition": "∑C(k,r) from k=r to n equals C(n+1,r+1)"
            },
            {
              "term": "Multinomial Coefficient",
              "definition": "n!/(r₁!r₂!...rₖ!) for partitioning n items into k groups"
            }
          ],

          "about": [
            {"name": "Combinatorics", "description": "Mathematics of counting and arrangement"},
            {"name": "Binomial Coefficients", "description": "Foundation of combination theory"},
            {"name": "Pascal's Triangle", "description": "Visual representation of binomial coefficients"},
            {"name": "Permutations", "description": "Ordered arrangements"}
          ]
        },
        {
          "id": "disc-3",
          "part": 3,
          "title": "Intro to Theory of Computation",
          "url": "intro_automata.html",
          "icon": "M",
          "keywords": [
            "Finite automata",
            "Finite state machine",
            "Regular language",
            "State diagram",
            "Transition function",
            "Deterministic finite automata (DFAs)",
            "Nondeterministic finite automata (NFAs)",
            "Regular expression",
            "Regular operations",
            "Union",
            "Concatenation",
            "Kleene star",
            "Power set construction"
          ],
          "badges": [],
          "prereqs": [],
          "mapCoords": {"q": 2, "r": 1},
          "topicGroup": "computation",
          "tesseraMessage": "Automata recognize patterns. The simplest model of computation!",

          "headline": "Theory of Computation: Finite Automata and Regular Languages",
          "description": "Learn about basic automata theory and regular expressions.",
          "abstract": "Automata theory provides the mathematical foundation for understanding computation. A finite automaton (Q, Σ, δ, q₀, F) recognizes regular languages through state transitions. DFAs and NFAs are equivalent in power, and regular expressions describe exactly the regular languages—essential for compilers and pattern matching.",
          "datePublished": "2024-01-01",
          "dateModified": "2025-01-15",

          "teaches": [
            "Finite automaton as 5-tuple (Q, Σ, δ, q₀, F)",
            "States, alphabet, transition function, start/accept states",
            "Regular languages: those recognized by finite automata",
            "State diagrams for visualizing automata",
            "Regular operations: union, concatenation, Kleene star",
            "Closure of regular languages under regular operations",
            "DFA vs NFA: deterministic vs nondeterministic",
            "NFA to DFA conversion via power set construction",
            "Regular expressions: syntax and semantics",
            "Equivalence of regular expressions and finite automata"
          ],

          "competencyRequired": [
            "Set notation",
            "Function concepts",
            "Basic proof techniques"
          ],

          "sections": [
            {"id": "intro", "name": "Introduction"},
            {"id": "fa_rl", "name": "Finite automata & Regular Languages"},
            {"id": "regular_op", "name": "Regular Operations"},
            {"id": "dfa", "name": "Deterministic & Nondeterministic Machine"},
            {"id": "r_exp", "name": "Regular Expressions"}
          ],

          "definitions": [
            {
              "term": "Finite Automaton",
              "definition": "5-tuple (Q, Σ, δ, q₀, F): states, alphabet, transition function, start state, accept states"
            },
            {
              "term": "Regular Language",
              "definition": "A language recognized by some finite automaton"
            },
            {
              "term": "DFA",
              "definition": "Deterministic FA: δ: Q × Σ → Q returns exactly one state"
            },
            {
              "term": "NFA",
              "definition": "Nondeterministic FA: δ: Q × Σε → P(Q) may return multiple states"
            },
            {
              "term": "Kleene Star",
              "definition": "R* = {ε} ∪ R ∪ RR ∪ RRR ∪ ...; zero or more repetitions"
            },
            {
              "term": "Regular Expression",
              "definition": "Built from Σ, ε, ∅ using union, concatenation, and star"
            }
          ],

          "about": [
            {"name": "Automata Theory", "description": "Study of abstract computing machines"},
            {"name": "Regular Languages", "description": "Languages recognized by finite automata"},
            {"name": "Finite State Machines", "description": "Simplest computational model"},
            {"name": "Regular Expressions", "description": "Pattern description language"}
          ]
        },
        { 
          "id": "disc-4",
          "part": 4,
          "title": "Boolean Logic",
          "url": "boolean.html",
          "icon": "0∧1",
          "keywords": [
            "Boolean logic",
            "Logical operations",
            "Boolean algebra",
            "Negation (NOT)",
            "Conjunction (AND)",
            "Disjunction (OR)",
            "Exclusive or (XOR)",
            "NAND",
            "NOR",
            "XNOR",
            "Implication",
            "Functional completeness",
            "Logic gates",
            "Boolean circuits",
            "Directed acyclic graph (DAG)"
          ],
          "badges": [],
          "prereqs": [],
          "mapCoords": {"q": 1, "r": 1},
          "topicGroup": "boolean",
          "tesseraMessage": "True or false, 0 or 1 — Boolean logic is how computers think.",

          "headline": "Boolean Logic: The Foundation of Digital Computation",
          "description": "Learn about basic Boolean logic and circuits.",
          "abstract": "Boolean logic operates on TRUE/FALSE values using operations like AND (∧), OR (∨), and NOT (¬). NAND and NOR are functionally complete—any Boolean function can be built from just one of them. Boolean circuits formalize computation as DAGs of logic gates, bridging mathematics and hardware.",
          "datePublished": "2024-01-01",
          "dateModified": "2025-01-15",

          "teaches": [
            "Boolean values: TRUE (1) and FALSE (0)",
            "Fundamental operations: NOT (¬), AND (∧), OR (∨)",
            "Extended operations: XOR (⊕), XNOR (↔), implication (→)",
            "NAND (↑) and NOR (↓) as negations of AND/OR",
            "Functional completeness of NAND",
            "Constructing NOT, AND, OR from NAND alone",
            "Logic gates as hardware implementations",
            "Boolean circuits as DAGs",
            "Applications in digital electronics and complexity theory"
          ],

          "competencyRequired": [
            "Basic logic concepts",
            "Understanding of TRUE/FALSE"
          ],

          "sections": [
            {"id": "bool", "name": "Boolean Logic"},
            {"id": "negation", "name": "Negation of Conjunction & Disjunction"},
            {"id": "circuit", "name": "Circuits"}
          ],

          "definitions": [
            {
              "term": "Negation (NOT)",
              "definition": "¬P flips the truth value: ¬1 = 0, ¬0 = 1"
            },
            {
              "term": "Conjunction (AND)",
              "definition": "P ∧ Q = 1 iff both P and Q are 1"
            },
            {
              "term": "Disjunction (OR)",
              "definition": "P ∨ Q = 1 iff at least one of P or Q is 1"
            },
            {
              "term": "NAND",
              "definition": "P ↑ Q = ¬(P ∧ Q); functionally complete operation"
            },
            {
              "term": "Functional Completeness",
              "definition": "A set of operations that can express any Boolean function"
            },
            {
              "term": "Boolean Circuit",
              "definition": "DAG of AND, OR, NOT gates with inputs and output nodes"
            }
          ],

          "about": [
            {"name": "Boolean Logic", "description": "Two-valued logic system"},
            {"name": "Logic Gates", "description": "Hardware implementation of Boolean operations"},
            {"name": "Functional Completeness", "description": "Universal gate sets"},
            {"name": "Boolean Circuits", "description": "Theoretical model of digital computation"}
          ]
        },
        {
          "id": "disc-5",
          "part": 5,
          "title": "Context-Free Languages",
          "url": "context_free.html",
          "icon": "S→",
          "keywords": [
            "Context-free grammar (CFG)",
            "Context-free language (CFL)",
            "Variables",
            "Terminals",
            "Production rules",
            "Derivation",
            "Nonregular language",
            "Pumping lemma",
            "Pumping lemma for CFLs",
            "Pushdown automata (PDA)",
            "Stack",
            "Stack alphabet",
            "Deterministic pushdown automata (DPDA)",
            "Deterministic context-free languages (DCFL)",
            "Parser",
            "Syntax analyzer"
          ],
          "badges": [],
          "prereqs": ["disc-3"],
          "mapCoords": {"q": 2, "r": 2},
          "topicGroup": "computation",
          "tesseraMessage": "Context-free grammars define programming languages. You use them every day!",

          "headline": "Context-Free Languages: Grammars, PDAs, and the Chomsky Hierarchy",
          "description": "Learn about context-free languages and pushdown automata.",
          "abstract": "Context-free grammars (CFGs) generate languages beyond regular languages, including recursive structures like {0ⁿ1ⁿ}. A CFG is a 4-tuple (V, Σ, R, S) with variables, terminals, rules, and start symbol. Pushdown automata (PDAs) with stack memory recognize exactly the CFLs. DPDAs power practical parsers in compilers.",
          "datePublished": "2024-01-01",
          "dateModified": "2025-01-15",

          "teaches": [
            "Context-free grammar as 4-tuple (V, Σ, R, S)",
            "Variables, terminals, production rules, start variable",
            "Derivations and string generation",
            "CFGs are more powerful than regular expressions",
            "Pumping lemma for regular languages",
            "Using pumping lemma to prove nonregularity",
            "Pushdown automata: 6-tuple with stack",
            "CFG ↔ PDA equivalence",
            "Every regular language is context-free",
            "Pumping lemma for context-free languages",
            "Non-context-free languages (e.g., {aⁿbⁿcⁿ})",
            "DPDAs and deterministic context-free languages"
          ],

          "competencyRequired": [
            "Finite automata and regular languages",
            "Set notation and proof by contradiction"
          ],

          "sections": [
            {"id": "CFG", "name": "Context-Free Grammar (CFG)"},
            {"id": "Pumping", "name": "Pumping Lemma"},
            {"id": "push", "name": "Pushdown Automata"},
            {"id": "NCF", "name": "Non-Context-Free Languages"},
            {"id": "DPDA", "name": "Deterministic Pushdown Automata (DPDAs)"}
          ],

          "definitions": [
            {
              "term": "Context-Free Grammar",
              "definition": "4-tuple (V, Σ, R, S): variables, terminals, production rules, start variable"
            },
            {
              "term": "Pumping Lemma (Regular)",
              "definition": "If L is regular, strings s ∈ L can be split xyz with xy^i z ∈ L for all i ≥ 0"
            },
            {
              "term": "Pushdown Automaton",
              "definition": "6-tuple (Q, Σ, Γ, δ, q₀, F) with stack alphabet Γ"
            },
            {
              "term": "Pumping Lemma (CFL)",
              "definition": "If L is CFL, strings split uvxyz with uv^i xy^i z ∈ L for all i ≥ 0"
            },
            {
              "term": "DPDA",
              "definition": "Deterministic PDA; recognizes DCFLs used in practical parsers"
            }
          ],

          "about": [
            {"name": "Context-Free Grammars", "description": "Generate programming language syntax"},
            {"name": "Pushdown Automata", "description": "Finite automata with stack memory"},
            {"name": "Pumping Lemmas", "description": "Tools for proving language limitations"},
            {"name": "Parsers", "description": "Practical application in compilers"}
          ]
        },
        { 
          "id": "disc-6",
          "part": 6,
          "title": "Turing Machines",
          "url": "turing_machine.html",
          "icon": "TM",
          "keywords": [
            "Turing machine",
            "Alan Turing",
            "Tape alphabet",
            "Blank symbol",
            "Transition function",
            "Configuration",
            "Turing-recognizable",
            "Turing-decidable",
            "Decidable language",
            "Undecidable language",
            "Co-Turing-recognizable",
            "Church-Turing thesis",
            "Lambda calculus",
            "Unsolvability",
            "Countable sets",
            "Uncountable sets"
          ],
          "badges": [],
          "prereqs": ["disc-5"],
          "mapCoords": {"q": 3, "r": 1},
          "topicGroup": "computation",
          "tesseraMessage": "Turing machines define what's computable. Some problems have no algorithm!",

          "headline": "Turing Machines: The Universal Model of Computation",
          "description": "Learn about Turing machines, and solvability of problems.",
          "abstract": "Turing machines define what it means to compute algorithmically. A TM is a 7-tuple with infinite tape, read/write head, and states including accept/reject. The Church-Turing thesis states all algorithms can be expressed as TMs. Crucially, some languages are undecidable—provably beyond algorithmic solution.",
          "datePublished": "2024-01-01",
          "dateModified": "2025-01-15",

          "teaches": [
            "Turing machine as 7-tuple (Q, Σ, Γ, δ, q₀, qₐccept, qᵣeject)",
            "Infinite tape with read/write head",
            "Transition function: δ: Q × Γ → Q × Γ × {L, R}",
            "Configurations and computation sequences",
            "Turing-recognizable vs Turing-decidable languages",
            "Church-Turing thesis: TMs capture all algorithms",
            "Every CFL is decidable",
            "Some languages are undecidable",
            "Decidable iff both recognizable and co-recognizable",
            "Countability argument: more languages than TMs",
            "Unsolvable problems exist",
            "Approximation and heuristic algorithms for intractable problems"
          ],

          "competencyRequired": [
            "Context-free languages and PDAs",
            "Basic set theory (countable vs uncountable)"
          ],

          "sections": [
            {"id": "TM", "name": "Turing Machines (TMs)"},
            {"id": "algo", "name": "Algorithms"},
            {"id": "unsolve", "name": "Unsolvability"}
          ],

          "definitions": [
            {
              "term": "Turing Machine",
              "definition": "7-tuple (Q, Σ, Γ, δ, q₀, qₐccept, qᵣeject) with infinite tape"
            },
            {
              "term": "Turing-Recognizable",
              "definition": "Language accepted by some TM (may loop on non-members)"
            },
            {
              "term": "Turing-Decidable",
              "definition": "Language decided by TM that halts on all inputs"
            },
            {
              "term": "Church-Turing Thesis",
              "definition": "Any algorithm can be implemented as a Turing machine"
            },
            {
              "term": "Undecidable Language",
              "definition": "No TM decides it; some are recognizable but not decidable"
            }
          ],

          "about": [
            {"name": "Turing Machines", "description": "Universal model of computation"},
            {"name": "Decidability", "description": "Whether problems can be algorithmically solved"},
            {"name": "Church-Turing Thesis", "description": "Equivalence of algorithms and TMs"},
            {"name": "Unsolvability", "description": "Fundamental limits of computation"}
          ]
        },
        {
          "id": "disc-7",
          "part": 7,
          "title": "Time Complexity",
          "url": "time_complexity.html",
          "icon": "O(g(n))",
          "keywords": [
            "Time complexity",
            "Running time",
            "Asymptotic analysis",
            "Big-O notation",
            "Little-o notation",
            "Time complexity class",
            "Class P",
            "Polynomial time",
            "Exponential time",
            "Deterministic Turing machine",
            "Multitape Turing machine",
            "Polynomially equivalent",
            "Worst-case analysis"
          ],
          "badges": ["code"],
          "prereqs": ["disc-6"],
          "mapCoords": {"q": 3, "r": 2},
          "topicGroup": "computation",
          "tesseraMessage": "Big-O notation tells you how algorithms scale. Essential for interviews!",

          "headline": "Time Complexity: From Big-O to Class P",
          "description": "Introduction to time complexity theory.",
          "abstract": "Time complexity measures computational resources as a function of input size. Big-O notation captures asymptotic upper bounds, ignoring constants and lower-order terms. Class P = ∪ₖ TIME(nᵏ) contains problems solvable in polynomial time—the boundary between tractable and intractable computation.",
          "datePublished": "2024-01-01",
          "dateModified": "2025-01-15",

          "teaches": [
            "Time complexity as function f(n) of input length",
            "Asymptotic analysis: focus on dominant terms",
            "Big-O notation: f(n) = O(g(n)) for upper bounds",
            "Little-o notation: f(n) = o(g(n)) for strict bounds",
            "TIME(t(n)) complexity class",
            "Polynomial vs exponential time",
            "Multitape TMs are polynomially equivalent to single-tape",
            "Class P: polynomial time decidable languages",
            "P = ∪ₖ TIME(nᵏ)",
            "Practical implications of complexity classes"
          ],

          "competencyRequired": [
            "Turing machines",
            "Basic calculus (limits)",
            "Algorithm basics"
          ],

          "sections": [
            {"id": "intro", "name": "Time Complexity"},
            {"id": "class", "name": "Time Complexity Class"},
            {"id": "P", "name": "Class P"}
          ],

          "definitions": [
            {
              "term": "Big-O Notation",
              "definition": "f(n) = O(g(n)) if ∃c, n₀: f(n) ≤ cg(n) for all n ≥ n₀"
            },
            {
              "term": "Little-o Notation",
              "definition": "f(n) = o(g(n)) if lim f(n)/g(n) = 0 as n → ∞"
            },
            {
              "term": "TIME(t(n))",
              "definition": "Languages decided by O(t(n)) time deterministic TM"
            },
            {
              "term": "Class P",
              "definition": "P = ∪ₖ TIME(nᵏ); polynomial time decidable problems"
            },
            {
              "term": "Polynomially Equivalent",
              "definition": "Models differing by at most polynomial factor in time"
            }
          ],

          "about": [
            {"name": "Asymptotic Analysis", "description": "Analyzing algorithm growth rates"},
            {"name": "Big-O Notation", "description": "Standard complexity notation"},
            {"name": "Class P", "description": "Tractable problems"},
            {"name": "Complexity Theory", "description": "Study of computational resources"}
          ]
        },
        {   
          "id": "disc-8",
          "part": 8,
          "title": "Eulerian & Hamiltonian",
          "url": "Eulerian.html",
          "icon": "Cₙ",
          "keywords": [
            "Eulerian graph",
            "Eulerian cycle (Euler tour)",
            "Eulerian path (Eulerian trail)",
            "Semi-Eulerian",
            "Seven Bridges of Königsberg",
            "Even degree",
            "Hamiltonian graph",
            "Hamiltonian cycle",
            "NP-complete",
            "Adjacency matrix",
            "Adjacency list",
            "Space complexity",
            "PATH problem"
          ],
          "badges": ["code"],
          "prereqs": ["disc-1"],
          "mapCoords": {"q": 0, "r": 2},
          "topicGroup": "graph-theory",
          "tesseraMessage": "Can you traverse every edge? Every vertex? Different questions, different answers.",

          "headline": "Eulerian and Hamiltonian Cycles: Graph Traversal Classics",
          "description": "Learn about Eulerian and Hamiltonian cycles.",
          "abstract": "Eulerian cycles traverse every edge exactly once; a connected graph has one iff every vertex has even degree (Euler's 1736 theorem). Hamiltonian cycles visit every vertex exactly once—but determining existence is NP-complete, a stark contrast in computational difficulty despite superficial similarity.",
          "datePublished": "2024-01-01",
          "dateModified": "2025-01-15",

          "teaches": [
            "PATH problem: does path exist from s to t?",
            "Eulerian cycle: closed walk visiting every edge once",
            "Euler's theorem: Eulerian iff all vertices have even degree",
            "Semi-Eulerian: Eulerian path but not cycle",
            "Hamiltonian cycle: visits every vertex exactly once",
            "Hamiltonian is NP-complete (no known polynomial algorithm)",
            "Polynomial verification of proposed Hamiltonian cycles",
            "Adjacency matrix representation: O(n²) space",
            "Adjacency list representation: better for sparse graphs",
            "Space complexity considerations"
          ],

          "competencyRequired": [
            "Graph theory basics",
            "Vertex degrees"
          ],

          "sections": [
            {"id": "Eulerian", "name": "Eulerian"},
            {"id": "Hamiltonian", "name": "Hamiltonian"},
            {"id": "code", "name": "Is Hamiltonian Cycle? (Coding)"}
          ],

          "definitions": [
            {
              "term": "Eulerian Cycle",
              "definition": "Closed walk traversing every edge exactly once"
            },
            {
              "term": "Euler's Theorem",
              "definition": "Connected graph is Eulerian iff every vertex has even degree"
            },
            {
              "term": "Hamiltonian Cycle",
              "definition": "Cycle visiting every vertex exactly once"
            },
            {
              "term": "Adjacency Matrix",
              "definition": "n×n matrix where entry (i,j) = 1 if edge exists; O(n²) space"
            },
            {
              "term": "Adjacency List",
              "definition": "For each vertex, list of neighbors; efficient for sparse graphs"
            }
          ],

          "about": [
            {"name": "Eulerian Cycles", "description": "Edge traversal problem (polynomial)"},
            {"name": "Hamiltonian Cycles", "description": "Vertex traversal problem (NP-complete)"},
            {"name": "Graph Representations", "description": "Matrix vs list tradeoffs"},
            {"name": "Complexity Contrast", "description": "Similar problems, vastly different difficulty"}
          ]
        },
        { 
          "id": "disc-9",
          "part": 9,
          "title": "Class NP",
          "url": "p_vs_np.html",
          "icon": "P≠NP?",
          "keywords": [
            "Polynomial verifiability",
            "Certificate",
            "Verifier",
            "Nondeterministic polynomial time",
            "Class NP",
            "Nondeterministic Turing machine (NTM)",
            "NTIME",
            "P vs NP question",
            "NP-completeness",
            "NP-complete",
            "NP-hard",
            "Polynomial-time reduction",
            "Polynomial time computable function",
            "Traveling salesperson problem (TSP)",
            "EXPTIME",
            "Millennium Prize Problems"
          ],
          "badges": [],
          "prereqs": ["disc-7"],
          "mapCoords": {"q": 4, "r": 1},
          "topicGroup": "computation",
          "tesseraMessage": "P vs NP — the million-dollar question! Is checking always easier than solving?",

          "headline": "Class NP: Polynomial Verification and the P vs NP Question",
          "description": "Learn about nondeterministic polynomial time (NP), and NP-Completeness.",
          "abstract": "NP contains problems with polynomial-time verifiers: given a certificate, membership can be verified quickly even if finding the certificate is hard. NP-complete problems are the hardest in NP—if any is in P, then P = NP. This million-dollar question remains open, with most believing P ≠ NP.",
          "datePublished": "2024-01-01",
          "dateModified": "2025-01-15",

          "teaches": [
            "Polynomial verifiability: verify solutions in polynomial time",
            "Verifier algorithm V accepts ⟨w, c⟩ for certificate c",
            "Class NP: languages with polynomial-time verifiers",
            "NP = ∪ₖ NTIME(nᵏ)",
            "Nondeterministic Turing machines",
            "P: decidable in polynomial time",
            "NP: verifiable in polynomial time",
            "P vs NP: open question, most believe P ≠ NP",
            "Known: NP ⊆ EXPTIME",
            "Polynomial-time reduction: A ≤P B",
            "NP-complete: in NP and all NP problems reduce to it",
            "NP-hard: at least as hard as NP-complete",
            "TSP (decision) is NP-complete; TSP (optimization) is NP-hard"
          ],

          "competencyRequired": [
            "Time complexity and Class P",
            "Turing machines",
            "Basic reduction concepts"
          ],

          "sections": [
            {"id": "P_verify", "name": "Polynomial Verifiability"},
            {"id": "NP", "name": "Nondeterministic Polynomial Time (NP)"},
            {"id": "P=NP", "name": "P vs NP Question"},
            {"id": "comp", "name": "NP-Completeness"}
          ],

          "definitions": [
            {
              "term": "Class NP",
              "definition": "Languages with polynomial-time verifiers; NP = ∪ₖ NTIME(nᵏ)"
            },
            {
              "term": "Certificate",
              "definition": "Evidence string c that allows polynomial-time verification"
            },
            {
              "term": "Polynomial-Time Reduction",
              "definition": "A ≤P B: polynomial-time function f where w ∈ A ↔ f(w) ∈ B"
            },
            {
              "term": "NP-Complete",
              "definition": "In NP and every NP problem reduces to it in polynomial time"
            },
            {
              "term": "NP-Hard",
              "definition": "At least as hard as NP-complete; may not be in NP itself"
            }
          ],

          "about": [
            {"name": "Class NP", "description": "Polynomial-time verifiable problems"},
            {"name": "P vs NP", "description": "Fundamental open problem in CS"},
            {"name": "NP-Completeness", "description": "Hardest problems in NP"},
            {"name": "Reductions", "description": "Comparing problem difficulty"}
          ]
        }
      ],
      "reservedSlots": [
        {
          "q": 4,
          "r": 2
        },
        {
          "q": 5,
          "r": 1
        },
        {
          "q": 0,
          "r": 3
        }
      ]
    },
    "V": {
      "id": "machine-learning",
      "title": "Machine Learning",
      "shortTitle": "ML",
      "description": "Explore machine learning ideas and applications.",
      "tagline": "The Mathematics of Synthesis and Intelligence",
      "icon": "fa-brain",
      "indexUrl": "Mathematics/Machine_learning/ml.html",
      "baseUrl": "Mathematics/Machine_learning/",
      "parts": [
        {
        "id": "ml-1",
        "part": 1,
        "title": "Intro to Machine Learning",
        "url": "intro_ml.html",
        "icon": "🧠?",
        "keywords": [
          "Machine learning",
          "Artificial intelligence (AI)",
          "Deep learning",
          "Supervised learning",
          "Unsupervised learning",
          "Learning process of ML",
          "Categories of machine learning"
        ],
        "badges": [],
        "prereqs": [],
        "mapCoords": {
          "q": 1,
          "r": 0
        },
        "topicGroup": "ml-foundations",
        "tesseraMessage": "Welcome to ML! This is where all the math comes together beautifully.",

        "headline": "Intro to Machine Learning: Understanding Intelligence",
        "description": "Introduction to machine learning fundamentals including supervised and unsupervised learning.",
        "abstract": "Machine learning enables computers to learn from data without explicit programming. This introduction explores the nature of intelligence, distinguishes supervised from unsupervised learning, and outlines the standard ML workflow from problem definition through deployment.",
        "datePublished": "2024-01-01",
        "dateModified": "2025-01-15",

        "teaches": [
          "The concept of artificial intelligence and machine learning",
          "Supervised vs unsupervised learning paradigms",
          "Semi-supervised and self-supervised learning",
          "Basic ML categories: regression, classification, clustering",
          "Dimensionality reduction techniques",
          "Reinforcement learning fundamentals",
          "Standard ML development process"
        ],

        "competencyRequired": [
          "Basic probability and statistics",
          "Linear algebra fundamentals"
        ],

        "sections": [
          {"id": "intro", "name": "The Law of Intelligence"},
          {"id": "sup", "name": "Supervised vs Unsupervised"},
          {"id": "basic", "name": "Basic Categories of ML"},
          {"id": "process", "name": "Standard Process of ML"}
        ],

        "definitions": [
          {
            "term": "Machine Learning",
            "definition": "Field where computers learn patterns from data without explicit programming"
          },
          {
            "term": "Supervised Learning",
            "definition": "Learning from labeled data with input-output pairs"
          },
          {
            "term": "Unsupervised Learning",
            "definition": "Learning patterns from unlabeled data without explicit targets"
          }
        ],

        "about": [
          {"name": "Machine Learning Introduction", "description": "Foundations of machine learning"},
          {"name": "Artificial Intelligence", "description": "The study of intelligent systems"},
          {"name": "Supervised Learning", "description": "Learning with labeled training data"},
          {"name": "Unsupervised Learning", "description": "Discovering hidden patterns in data"}
        ]
      },
      {
        "id": "ml-2",
        "part": 2,
        "title": "Regularized Regression",
        "url": "regression.html",
        "icon": "λ‖w‖ₚ",
        "keywords": [
          "Ridge regression",
          "Bias-variance tradeoff",
          "Generalization",
          "Regularization",
          "Cross-validation (CV)",
          "K-fold cross-validation",
          "Leave-one-out cross-validation (LOOCV)",
          "Lasso regression"
        ],
        "badges": [
          "interactive"
        ],
        "prereqs": [
          "prob-11",
          "calc-7"
        ],
        "mapCoords": {
          "q": 2,
          "r": 0
        },
        "topicGroup": "ml-foundations",
        "tesseraMessage": "Regularization prevents overfitting — the bias-variance tradeoff in action!",

        "headline": "Regularized Regression: Ridge and Lasso Methods",
        "description": "Learn about regularized regression methods including Ridge and Lasso regression.",
        "abstract": "Regularization prevents overfitting by adding penalty terms to the loss function. Ridge regression uses L2 penalty for stable solutions, while Lasso uses L1 penalty for feature selection. Cross-validation helps select optimal regularization strength.",
        "datePublished": "2024-01-01",
        "dateModified": "2025-01-15",

        "teaches": [
          "Ridge regression with L2 regularization",
          "Bias-variance tradeoff in model selection",
          "K-fold and leave-one-out cross-validation",
          "Lasso regression with L1 regularization",
          "Feature selection through sparsity",
          "Regularization parameter tuning"
        ],

        "competencyRequired": [
          "Linear regression fundamentals",
          "Matrix algebra and optimization basics"
        ],

        "sections": [
          {"id": "ridge", "name": "Ridge Regression"},
          {"id": "Demo", "name": "Demo: Ridge Regression"},
          {"id": "CV", "name": "Cross Validation"},
          {"id": "b-v", "name": "Bias-Variance Tradeoff in Ridge Regression"},
          {"id": "lasso", "name": "Lasso Regression"}
        ],

        "definitions": [
          {
            "term": "Ridge Regression",
            "definition": "Linear regression with L2 penalty: min ||y - Xw||² + λ||w||²"
          },
          {
            "term": "Lasso Regression",
            "definition": "Linear regression with L1 penalty: min ||y - Xw||² + λ||w||₁"
          },
          {
            "term": "Cross-Validation",
            "definition": "Model validation by partitioning data into training and validation sets"
          },
          {
            "term": "Bias-Variance Tradeoff",
            "definition": "Balance between model simplicity (high bias) and flexibility (high variance)"
          }
        ],

        "about": [
          {"name": "Regularized Regression", "description": "Preventing overfitting through penalties"},
          {"name": "Ridge Regression", "description": "L2 regularization for stable solutions"},
          {"name": "Lasso Regression", "description": "L1 regularization for sparse solutions"},
          {"name": "Cross Validation", "description": "Model selection technique"},
          {"name": "Bias-Variance Tradeoff", "description": "Fundamental ML concept"}
        ]
      },
      {
        "id": "ml-3",
        "part": 3,
        "title": "Intro to Classification",
        "url": "intro_classification.html",
        "icon": "𝒳↦𝒴",
        "keywords": [
          "Binary logistic regression",
          "sigmoid (logistic) function",
          "logit (pre-activation)",
          "Decision boundary",
          "Feature mapping",
          "Linearly separable",
          "Kernel trick",
          "Random fourier features",
          "RBF (Gaussian) kernel",
          "Softmax function",
          "Multinomial logistic regression"
        ],
        "badges": [
          "interactive"
        ],
        "prereqs": [
          "ml-2"
        ],
        "mapCoords": {
          "q": 3,
          "r": 0
        },
        "topicGroup": "ml-foundations",
        "tesseraMessage": "Classification draws boundaries in feature space. Kernels bend those boundaries!",

        "headline": "Classification: From Logistic Regression to Kernel Methods",
        "description": "Learn about basic classification methods including logistic regression and kernel methods.",
        "abstract": "Classification assigns inputs to discrete categories. Binary logistic regression uses the sigmoid function to model probabilities, while kernel methods enable nonlinear decision boundaries. The kernel trick allows efficient computation in high-dimensional feature spaces.",
        "datePublished": "2024-01-01",
        "dateModified": "2025-01-15",

        "teaches": [
          "Binary logistic regression model",
          "Sigmoid function and decision boundaries",
          "Feature mapping for nonlinear classification",
          "Kernel trick for implicit high-dimensional mapping",
          "RBF kernel and random Fourier features",
          "Softmax function for multiclass problems",
          "Multinomial logistic regression"
        ],

        "competencyRequired": [
          "Linear regression and optimization",
          "Probability fundamentals"
        ],

        "sections": [
          {"id": "intro", "name": "Binary Logistic Regression"},
          {"id": "demo", "name": "Binary Logistic Regression Demo"},
          {"id": "kernel", "name": "Kernel Methods"},
          {"id": "rbf-approx", "name": "RBF Kernel & Random Fourier Features"},
          {"id": "mlr", "name": "Multinomial logistic regression"}
        ],

        "definitions": [
          {
            "term": "Logistic Regression",
            "definition": "Classification model: p(y=1|x) = σ(wᵀx + b) using sigmoid function"
          },
          {
            "term": "Sigmoid Function",
            "definition": "σ(z) = 1/(1 + e⁻ᶻ) mapping real values to (0,1)"
          },
          {
            "term": "Kernel Trick",
            "definition": "Computing inner products in high-dimensional space without explicit mapping"
          },
          {
            "term": "Softmax Function",
            "definition": "Multiclass generalization: p(y=k|x) = exp(zₖ)/Σⱼexp(zⱼ)"
          }
        ],

        "about": [
          {"name": "Classification", "description": "Assigning inputs to discrete categories"},
          {"name": "Logistic Regression", "description": "Probabilistic binary classification"},
          {"name": "Kernel Methods", "description": "Nonlinear classification via feature mapping"},
          {"name": "Softmax", "description": "Multiclass probability distribution"}
        ]
      },
      {
        "id": "ml-4",
        "part": 4,
        "title": "Neural Networks Basics",
        "url": "neural_networks.html",
        "icon": "x↦hθ(x)",
        "keywords": [
          "Deep neural network (DNN)",
          "Multilayer perceptron (MLP)",
          "Hidden layer",
          "Activation function",
          "ReLU",
          "Vanishing gradients",
          "Backpropagation",
          "Gradient clipping",
          "Exploding gradients",
          "Graphics processing units (GPUs)"
        ],
        "badges": [
          "interactive"
        ],
        "prereqs": [
          "ml-3",
          "calc-2"
        ],
        "mapCoords": {
          "q": 3,
          "r": -1
        },
        "topicGroup": "neural-networks",
        "tesseraMessage": "Neural networks learn features automatically. Layers upon layers of abstraction!",

        "headline": "Neural Networks: Multilayer Perceptrons and Backpropagation",
        "description": "Learn about Neural Networks basics including MLP architecture and training.",
        "abstract": "Neural networks compose simple functions into complex models. Multilayer perceptrons stack linear transformations with nonlinear activations. Backpropagation efficiently computes gradients through the chain rule, enabling training via gradient descent.",
        "datePublished": "2024-01-01",
        "dateModified": "2025-01-15",

        "teaches": [
          "Multilayer perceptron architecture",
          "Hidden layers and depth",
          "Activation functions: ReLU, sigmoid, tanh",
          "Backpropagation algorithm",
          "Vanishing and exploding gradients",
          "Gradient clipping techniques",
          "GPU acceleration for deep learning"
        ],

        "competencyRequired": [
          "Logistic regression",
          "Chain rule and Jacobian matrices"
        ],

        "sections": [
          {"id": "MLP", "name": "Multilayer Perceptron (MLP)"},
          {"id": "activation", "name": "Activation Functions"},
          {"id": "learning", "name": "Learning in Neural Networks"},
          {"id": "demo", "name": "Neural Networks Demo"},
          {"id": "GPU", "name": "Development of Deep Learning"}
        ],

        "definitions": [
          {
            "term": "Multilayer Perceptron",
            "definition": "Neural network with input, hidden, and output layers: h = σ(Wx + b)"
          },
          {
            "term": "Activation Function",
            "definition": "Nonlinear function applied element-wise; enables learning complex patterns"
          },
          {
            "term": "ReLU",
            "definition": "Rectified Linear Unit: f(x) = max(0, x)"
          },
          {
            "term": "Backpropagation",
            "definition": "Efficient gradient computation via reverse-mode automatic differentiation"
          }
        ],

        "about": [
          {"name": "Neural Networks", "description": "Layered computational models"},
          {"name": "Multilayer Perceptron", "description": "Feedforward neural network architecture"},
          {"name": "Backpropagation", "description": "Training algorithm for neural networks"},
          {"name": "Activation Functions", "description": "Nonlinear transformations in networks"}
        ]
      },
      {
        "id": "ml-5",
        "part": 5,
        "title": "Automatic Differentiation",
        "url": "autodiff.html",
        "icon": "∇ℒ",
        "keywords": [
          "Automatic differentiation (AD)",
          "Computational graph"
        ],
        "badges": [
          "code"
        ],
        "prereqs": [
          "ml-4"
        ],
        "mapCoords": {
          "q": 4,
          "r": -1
        },
        "topicGroup": "neural-networks",
        "tesseraMessage": "Autodiff computes gradients automatically. The engine behind deep learning!",

        "headline": "Automatic Differentiation: The Engine Behind Deep Learning",
        "description": "Learn about automatic differentiation with analytic examples and implementation.",
        "abstract": "Automatic differentiation systematically applies the chain rule to compute exact gradients through computational graphs. Reverse-mode AD (backpropagation) efficiently computes gradients for functions with many inputs and few outputs—perfect for neural network training.",
        "datePublished": "2024-01-01",
        "dateModified": "2025-01-15",

        "teaches": [
          "Computational graphs and DAGs",
          "Forward-mode vs reverse-mode AD",
          "Reverse-mode AD implementation",
          "Analytic examples of gradient computation",
          "Applications to neural network training"
        ],

        "competencyRequired": [
          "Backpropagation basics",
          "Chain rule and Jacobians"
        ],

        "sections": [
          {"id": "AD", "name": "Automatic Differentiation"},
          {"id": "eg", "name": "Analytic Example of Reverse-Mode AD"},
          {"id": "app", "name": "Applications of AD"},
          {"id": "code", "name": "Sample Code"}
        ],

        "definitions": [
          {
            "term": "Automatic Differentiation",
            "definition": "Systematic chain rule application for exact gradient computation"
          },
          {
            "term": "Computational Graph",
            "definition": "DAG representing computation from inputs to outputs"
          },
          {
            "term": "Reverse-Mode AD",
            "definition": "Backpropagation: efficient for many inputs, few outputs"
          }
        ],

        "about": [
          {"name": "Automatic Differentiation", "description": "Exact gradient computation technique"},
          {"name": "Computational Graphs", "description": "Representing computations as graphs"},
          {"name": "Chain Rule", "description": "Foundation of differentiation through compositions"},
          {"name": "Neural Networks", "description": "Primary application of autodiff"}
        ]
      },
      {
        "id": "ml-6",
        "part": 6,
        "title": "Support Vector Machine (SVM)",
        "url": "svm.html",
        "icon": "wᵀx+w₀",
        "keywords": [
          "Support vector machine (SVM)",
          "Soft margin constraints"
        ],
        "badges": [
          "interactive"
        ],
        "prereqs": [
          "ml-3",
          "calc-9"
        ],
        "mapCoords": {
          "q": 4,
          "r": 0
        },
        "topicGroup": "ml-foundations",
        "tesseraMessage": "SVMs find the widest margin. Elegant geometry meets optimization!",

        "headline": "Support Vector Machines: Maximum Margin Classification",
        "description": "Learn about support vector machine basics including margin maximization and soft margins.",
        "abstract": "Support Vector Machines find the hyperplane maximizing margin between classes. The dual formulation enables the kernel trick for nonlinear classification. Soft margin constraints handle non-separable data by allowing controlled misclassification.",
        "datePublished": "2024-01-01",
        "dateModified": "2025-01-15",

        "teaches": [
          "Maximum margin classification",
          "Support vectors and margin geometry",
          "SVM primal and dual formulations",
          "Soft margin constraints for non-separable data",
          "Kernel SVM for nonlinear boundaries",
          "KKT conditions in SVM"
        ],

        "competencyRequired": [
          "Classification fundamentals",
          "Constrained optimization and KKT conditions"
        ],

        "sections": [
          {"id": "svm", "name": "Support Vector Machine"},
          {"id": "smc", "name": "Soft Margin Constraints"},
          {"id": "demo", "name": "SVM Demo"}
        ],

        "definitions": [
          {
            "term": "Support Vector Machine",
            "definition": "Maximum margin classifier: max margin s.t. yᵢ(wᵀxᵢ + b) ≥ 1"
          },
          {
            "term": "Support Vectors",
            "definition": "Training points lying on or violating the margin boundary"
          },
          {
            "term": "Soft Margin",
            "definition": "SVM allowing slack variables for non-separable data"
          }
        ],

        "about": [
          {"name": "Support Vector Machine", "description": "Maximum margin classifier"},
          {"name": "Kernel Methods", "description": "Nonlinear SVM via kernel trick"},
          {"name": "Margin Classification", "description": "Geometric approach to classification"},
          {"name": "KKT Conditions", "description": "Optimality conditions for SVM"}
        ]
      },
      {
        "id": "ml-7",
        "part": 7,
        "title": "PCA & Autoencoders",
        "url": "pca.html",
        "icon": "𝒦(xᵢ,xⱼ)",
        "keywords": [
          "Principal Component Analysis (PCA)",
          "Dimensionality reduction",
          "Kernel PCA",
          "Double centering trick",
          "Autoencoder",
          "Lipschitz continuity",
          "Data Reconstruction",
          "Denoising autoencoder",
          "Manifolds"
        ],
        "badges": [
          "interactive"
        ],
        "prereqs": [
          "linalg-9",
          "ml-4"
        ],
        "mapCoords": {
          "q": 5,
          "r": 0
        },
        "topicGroup": "dimensionality",
        "tesseraMessage": "PCA finds the directions of maximum variance. Autoencoders learn them!",

        "headline": "PCA & Autoencoders: Dimensionality Reduction Techniques",
        "description": "Learn about unsupervised learning through PCA and Autoencoders.",
        "abstract": "PCA finds orthogonal directions of maximum variance for linear dimensionality reduction. Kernel PCA extends this to nonlinear cases. Autoencoders learn compressed representations via neural networks, with denoising variants improving robustness.",
        "datePublished": "2024-01-01",
        "dateModified": "2025-01-15",

        "teaches": [
          "PCA as variance maximization",
          "Kernel PCA for nonlinear reduction",
          "Double centering trick",
          "Autoencoder architecture",
          "Data reconstruction and compression",
          "Denoising autoencoders",
          "Manifold learning concepts"
        ],

        "competencyRequired": [
          "SVD and eigendecomposition",
          "Neural network basics"
        ],

        "sections": [
          {"id": "intro", "name": "Recap & Introduction"},
          {"id": "kernel-pca", "name": "Kernel PCA"},
          {"id": "auto", "name": "Autoencoders"},
          {"id": "demo", "name": "Demo"},
          {"id": "noise", "name": "Denoising Autoencoders"},
          {"id": "manifolds", "name": "Manifolds"}
        ],

        "definitions": [
          {
            "term": "PCA",
            "definition": "Find directions maximizing variance: W = argmax tr(WᵀΣW)"
          },
          {
            "term": "Kernel PCA",
            "definition": "PCA in feature space using kernel matrix K"
          },
          {
            "term": "Autoencoder",
            "definition": "Neural network learning compressed representation: x → z → x̂"
          },
          {
            "term": "Manifold",
            "definition": "Low-dimensional surface embedded in high-dimensional space"
          }
        ],

        "about": [
          {"name": "Principal Component Analysis", "description": "Linear dimensionality reduction"},
          {"name": "Dimensionality Reduction", "description": "Reducing feature space dimension"},
          {"name": "Autoencoders", "description": "Neural network compression"},
          {"name": "Kernel PCA", "description": "Nonlinear PCA via kernels"}
        ]
      },
      {
        "id": "ml-8",
        "part": 8,
        "title": "Clustering",
        "url": "clustering.html",
        "icon": "fᵀLf",
        "keywords": [
          "K-means clustering",
          "Distortion",
          "One-hot encoding (Dummy encoding)",
          "K-means++",
          "Vector quantization",
          "Spectral clustering",
          "Graph Laplacian",
          "Dirichlet energy"
        ],
        "badges": [
          "interactive"
        ],
        "prereqs": [
          "ml-7",
          "linalg-14"
        ],
        "mapCoords": {
          "q": 6,
          "r": 0
        },
        "topicGroup": "dimensionality",
        "tesseraMessage": "Clustering finds structure without labels. K-means is just the beginning!",

        "headline": "Clustering: From K-means to Spectral Methods",
        "description": "Learn about clustering algorithms including K-means and spectral clustering.",
        "abstract": "Clustering groups similar data points without labels. K-means minimizes distortion through iterative assignment and centroid updates. Spectral clustering uses graph Laplacian eigenvectors to handle non-convex clusters, minimizing Dirichlet energy.",
        "datePublished": "2024-01-01",
        "dateModified": "2025-01-15",

        "teaches": [
          "K-means algorithm and convergence",
          "K-means++ initialization",
          "Vector quantization applications",
          "Graph Laplacian construction",
          "Spectral clustering algorithm",
          "Dirichlet energy minimization"
        ],

        "competencyRequired": [
          "Eigendecomposition",
          "Graph theory basics"
        ],

        "sections": [
          {"id": "intro", "name": "Introduction"},
          {"id": "K-means", "name": "K-means Clustering"},
          {"id": "vq", "name": "Vector Quantization (VQ)"},
          {"id": "sp", "name": "Spectral Clustering"},
          {"id": "demo", "name": "Demo"}
        ],

        "definitions": [
          {
            "term": "K-means",
            "definition": "Minimize distortion: Σₖ Σᵢ∈Cₖ ||xᵢ - μₖ||²"
          },
          {
            "term": "Graph Laplacian",
            "definition": "L = D - W where D is degree matrix, W is adjacency"
          },
          {
            "term": "Spectral Clustering",
            "definition": "Cluster using eigenvectors of graph Laplacian"
          },
          {
            "term": "Dirichlet Energy",
            "definition": "fᵀLf measures smoothness of f over graph"
          }
        ],

        "about": [
          {"name": "Clustering", "description": "Grouping similar data points"},
          {"name": "K-means Clustering", "description": "Centroid-based clustering"},
          {"name": "Spectral Clustering", "description": "Graph-based clustering method"},
          {"name": "Graph Laplacian", "description": "Matrix encoding graph structure"}
        ]
      },
      {
        "id": "ml-9",
        "part": 9,
        "title": "Intro to Deep Neural Networks",
        "url": "deep_nn.html",
        "icon": "🧠!",
        "keywords": [
          "Feedforward networks",
          "Convolutional Neural Networks (CNNs)",
          "Residual connection",
          "Layer normalization",
          "Attention",
          "Self-attention",
          "Multi-Head Attention (MHA)",
          "Positional encoding",
          "Transformer"
        ],
        "badges": [
          "interactive"
        ],
        "prereqs": [
          "ml-5"
        ],
        "mapCoords": {
          "q": 5,
          "r": -1
        },
        "topicGroup": "neural-networks",
        "tesseraMessage": "Transformers changed everything — attention is all you need!",

        "headline": "Deep Neural Networks: From CNNs to Transformers",
        "description": "Learn about deep neural network architectures including CNNs, attention mechanisms, and Transformers.",
        "abstract": "Modern deep learning architectures include CNNs for spatial data, ResNets with skip connections, and Transformers using attention. Self-attention enables modeling long-range dependencies, while multi-head attention captures diverse relationships.",
        "datePublished": "2024-01-01",
        "dateModified": "2025-01-15",

        "teaches": [
          "Convolutional neural networks",
          "Residual connections and ResNet",
          "Layer normalization",
          "Attention mechanisms",
          "Self-attention computation",
          "Multi-head attention",
          "Positional encoding",
          "Transformer architecture"
        ],

        "competencyRequired": [
          "Neural network basics",
          "Automatic differentiation"
        ],

        "sections": [
          {"id": "CNN", "name": "Convolutional Neural Networks (CNNs)"},
          {"id": "ResNet", "name": "ResNet: Residual Connections"},
          {"id": "normalization", "name": "Layer Normalization"},
          {"id": "attention", "name": "Attention Mechanisms"},
          {"id": "self", "name": "Self-Attention"},
          {"id": "multihead", "name": "Multi-Head Attention"},
          {"id": "position", "name": "Positional Encoding"},
          {"id": "transformer", "name": "Transformer Architecture"},
          {"id": "demo", "name": "Transformer Demo"}
        ],

        "definitions": [
          {
            "term": "CNN",
            "definition": "Neural network using convolutional layers for spatial feature extraction"
          },
          {
            "term": "Residual Connection",
            "definition": "Skip connection: y = F(x) + x enabling deeper networks"
          },
          {
            "term": "Self-Attention",
            "definition": "Attention(Q,K,V) = softmax(QKᵀ/√d)V"
          },
          {
            "term": "Transformer",
            "definition": "Architecture using self-attention without recurrence"
          }
        ],

        "about": [
          {"name": "Deep Neural Networks", "description": "Multi-layer neural architectures"},
          {"name": "Convolutional Neural Networks", "description": "CNNs for spatial data"},
          {"name": "Transformer Architecture", "description": "Attention-based architecture"},
          {"name": "Attention Mechanisms", "description": "Dynamic weighting of inputs"}
        ]
      },
      {
        "id": "ml-10",
        "part": 10,
        "title": "Intro to Reinforcement Learning",
        "url": "intro_RL.html",
        "icon": "🔁",
        "keywords": [
          "Reinforcement Learning (RL)",
          "Model-based RL",
          "Model-free RL",
          "Agent",
          "Reward",
          "Policy",
          "Markov Decision Process (MDP)",
          "Discount factor",
          "Return",
          "Value function",
          "Q-function",
          "Advantage function",
          "Bellman's equations",
          "Value Iteration",
          "Policy Iteration",
          "Temporal Difference Learning",
          "Q-Learning",
          "SARSA",
          "Exploration vs Exploitation",
          "Policy Gradient",
          "REINFORCE",
          "Actor-Critic"
        ],
        "badges": [],
        "prereqs": [
          "ml-9",
          "prob-18"
        ],
        "mapCoords": {
          "q": 6,
          "r": -1
        },
        "topicGroup": "reinfortic-learning",
        "tesseraMessage": "RL learns from interaction. Games, robotics, and beyond!",

        "headline": "Reinforcement Learning: From MDPs to Policy Gradients",
        "description": "Learn about Reinforcement Learning fundamentals including MDPs, Bellman equations, and modern algorithms.",
        "abstract": "Reinforcement learning trains agents through environment interaction to maximize cumulative reward. MDPs formalize the setting; Bellman equations enable value computation. Methods range from dynamic programming to temporal difference learning, Q-learning, and policy gradient methods.",
        "datePublished": "2024-01-01",
        "dateModified": "2025-01-15",

        "teaches": [
          "Markov Decision Process formulation",
          "Value functions and Q-functions",
          "Bellman equations",
          "Value iteration and policy iteration",
          "Monte Carlo methods",
          "Temporal difference learning",
          "Q-learning and SARSA",
          "Exploration vs exploitation",
          "Policy gradient methods",
          "Actor-critic algorithms"
        ],

        "competencyRequired": [
          "Probability and expectation",
          "Neural network basics",
          "Markov chains"
        ],

        "sections": [
          {"id": "intro", "name": "Introduction"},
          {"id": "taxonomy", "name": "Classification of RL Algorithms"},
          {"id": "exploration", "name": "Exploration vs. Exploitation"},
          {"id": "mdp", "name": "Markov Decision Process"},
          {"id": "value-functions", "name": "Value Functions & Bellman Equations"},
          {"id": "DP", "name": "Dynamic Programming Algorithms for RL"},
          {"id": "MC", "name": "Monte Carlo Control"},
          {"id": "td-learning", "name": "Temporal Difference Learning"},
          {"id": "TD_Con", "name": "TD Control Methods: Q-Learning & SARSA"},
          {"id": "policy-grad", "name": "Policy Gradient Methods"},
          {"id": "actor-critic", "name": "Actor-Critic Methods"}
        ],

        "definitions": [
          {
            "term": "MDP",
            "definition": "Tuple (S, A, T, r, γ) defining state, action, transition, reward, discount"
          },
          {
            "term": "Value Function",
            "definition": "V(s) = E[Σₜ γᵗrₜ | s₀=s] expected return from state s"
          },
          {
            "term": "Q-Function",
            "definition": "Q(s,a) = E[Σₜ γᵗrₜ | s₀=s, a₀=a] expected return from state-action"
          },
          {
            "term": "Bellman Equation",
            "definition": "V(s) = max_a [r(s,a) + γ Σ T(s'|s,a)V(s')]"
          },
          {
            "term": "Policy Gradient",
            "definition": "∇J(θ) = E[∇log π(a|s) Q(s,a)] gradient of expected return"
          }
        ],

        "about": [
          {"name": "Reinforcement Learning", "description": "Learning through environment interaction"},
          {"name": "Markov Decision Process", "description": "Mathematical framework for sequential decisions"},
          {"name": "Value Functions", "description": "Expected cumulative reward measures"},
          {"name": "Policy Optimization", "description": "Direct optimization of decision policies"},
          {"name": "Temporal Difference Learning", "description": "Bootstrapping value estimates"}
        ]
      }
      ],
      "reservedSlots": [
        {
          "q": 7,
          "r": -1
        },
        {
          "q": 7,
          "r": 0
        },
        {
          "q": 6,
          "r": 1
        }
      ]
    }
  },
  "homeNode": {
    "id": "home",
    "section": "HOME",
    "title": "MATH-CS COMPASS HOME",
    "url": "/index.html",
    "mapCoords": {
      "q": 0,
      "r": 0
    }
  }
}